## Loading Libraries
## Reproducibility
## Distributions After Rank Gauss and PCA
"### Distributions of ""data_all"""
 \n ## 2- Load packages\n   \n## 2-1 Import
 \n##  2-2 Setup
 \n## 2-3 Version\n
"- From the table of descriptive statistics, we observe that mean and median values of **bmi** are very close to each other.\n- Hence, we will fill the missing values with the **mean values**."
# Exploratory Data Analysis
### Target Variable Visualization (stroke) : 
"- Clearly, the dataset is unbalanced in the favour of **no stroke**.\n- **19 : 1** ratio is observed for **No Stroke : Stroke!**\n- Thus, due to such heavy bias towards cases of **No Stroke**, predictions cannot be trusted!"
### Discrete Features :\n\n#### Distribution of Discrete Features :
"- Data distribution for **age** has dominant values around : **10**, **60** & **80**.\n- **avg_glucose_level** has 2 peaks of uneven heights present at values around : **100** & **200**.\n- **bmi** has a near about **normal distribution** but it has values in low numbers towards the right side! "
"We can see that there are leading spaces (spaces at the start of the string name) in the dataframe. So, I will remove these leading spaces."
I have removed the leading spaces from the column names. Let's again view the column names to confirm the same.
# Load packages
## Parameters
The classes are equaly distributed in the train set (10% each). Let's check the same for the test set.    \nLet's also plot the class distribution.\n\n
### Test set images class distribution
Let's now plot the images.   \nThe labels are shown above each image.
### Test set images\n\nLet's plot now a selection of the test set images.  \nLabels are as well added (they are known).  
# Get our environment set up\n________\n\nThe first thing we'll need to do is load in the libraries and datasets we'll be using. \n\n> **Important!** Make sure you run this cell yourself or the rest of your code won't work!
"Now that we're set up, let's learn about scaling & normalization. (If you like, you can take this opportunity to take a look at some of the data.)"
"# Scaling vs. Normalization: What's the difference?\n____\n\nOne of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that, in scaling, you're changing the *range* of your data while in normalization you're changing the *shape of the distribution* of your data. Let's talk a little more in-depth about each of these options. \n\n___\n\n## **Scaling**\n\nThis means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1.  You want to scale data when you're using methods based on measures of how far apart data points, like [support vector machines, or SVM](https://en.wikipedia.org/wiki/Support_vector_machine) or [k-nearest neighbors, or KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). With these algorithms, a change of ""1"" in any numeric feature is given the same importance. \n\nFor example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n\nBy scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in just a second, this is just to help illustrate my point.)\n"
"Notice that the *shape* of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n\n___\n## Normalization\n\nScaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n\n> **[Normal distribution:](https://en.wikipedia.org/wiki/Normal_distribution)** Also known as the ""bell curve"", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n\nIn general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with ""Gaussian"" in the name probably assumes normality.)\n\nThe method were  using to normalize here is called the [Box-Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Let's take a quick peek at what normalizing some data looks like:"
"Notice that the *shape* of our data has changed. Before normalizing it was almost L-shaped. But after normalizing it looks more like the outline of a bell (hence ""bell curve""). \n\n___\n## Your turn!\n\nFor the following example, decide whether scaling or normalization makes more sense. \n\n* You want to build a linear regression model to predict someone's grades given how much time they spend on various activities during a normal school week.  You notice that your measurements for how much time students spend studying aren't normally distributed: some students spend almost no time studying and others study for four or more hours every day. Should you scale or normalize this variable?\n* You're still working on your grades study, but you want to include information on how students perform on several fitness tests as well. You have information on how many jumping jacks and push-ups each student can complete in a minute. However, you notice that students perform far more jumping jacks than push-ups: the average for the former is 40, and for the latter only 10. Should you scale or normalize these variables?"
"# Practice scaling\n___\n\nTo practice scaling and normalization, we're going to be using a dataset of Kickstarter campaigns. (Kickstarter is a website where people can ask people to invest in various projects and concept products.)\n\nLet's start by scaling the goals of each campaign, which is how much money they were asking for."
You can see that scaling changed the scales of the plots dramatically (but not the shape of the data: it looks like most campaigns have small goals but a few have very large ones)
"# Practice normalization\n___\n\nOk, now let's try practicing normalization. We're going to normalize the amount of money pledged to each campaign."
It's not perfect (it looks like a lot pledges got very few pledges) but it is much closer to normal!
"# 4. Choosing a better plot type\nI was not satisfied with a bar plot for this visualization. I wanted that could display the same information and allow comparing different countries. Adding multiple bar charts for multiple countries would not look good at all. Then I tryed a line chart, since adding multiple lines works better than multiple columns.\n"
"Much better... now lets try to add countries, instead of the aggregated of all of them. "
Looking better than the bars... But still not good. My next idea was to transform the x axis into a radial.
This looks interesting! I think I can go forward with this chart type.
"# 5. Changing the data to be plotted from absolute to relative\nSome countries have much more absolute rows than others. This creates a distortion in the chart, so I will change the metric and plot the relative percentage for each job title in each country instead. This should allow us to compare them! "
Indeed! Now all countries are at the same scale. Comparing them proportionaly is much better than comparing absolute values.
"# 6. Closing the lines\nThis is a little bit trickier... To ""close"" the radar chart on Plotly, you need to add the first data point again to the end.[[](http://)](http://)"
"# 7. Creating a hierarchy in the axis titles\nIf you look at the axis they don't currently have an hierarchy or any specific order. I wan't to create one by keeping close togheter job titles that are similar, from the least technichal to the most technical. For me it makes sense to organize it in the following order:\n\n> Business Analyst **"
"### Libraries\n\nMake a compact section where you put all the libraries. I will leave them unhidden in this notebook, but usually it's best to hide all the cells with a lot of code within an Analysis Competition, so they don't take away from the *story* that you want to tell."
"### 📌 Color Scheme\n\nMantain a color scheme throughout the whole notebook. Some of the apps I am using are:\n* [HTML color codes](https://htmlcolorcodes.com/)\n* [Coolors](https://coolors.co/)\n* [ColorHunt](https://colorhunt.co/)\n* You can use [Image Color Picker](https://imagecolorpicker.com/en) to pick colors from an image\n* Google around, it sometimes helps for inspiration.\n\nFor this notebook I have chosen a rainbow color scheme, as gems have multiple colors, so a rainbow color scheme seemed fitted. You can also go ahead and use tones of purple and blues (like Sapphires), greend (Emeralds) or warmer tones like orange and red (like Amber or Ruby).\n\n\n\n### 📌 Add COLOR to highligh\n\nWhen writing out your reseach, use color to emphasise something important. You can use different tones to emphasise what you mean (for example, red might be a disclaimer, or something bad, while green might represent something positive).\n\n\nHere is what you need to copy-paste in your markdown:\n\n```\nsearch for the treasure\n```\n\n* background - the highlight color\n* font weight - either normal, bold, italic etc.\n* color - the color of the font itself\n\n### 📌 CSS Alerts\n\nMake use of alerts in order to conclude an important point or to raise attention from the reader onto certain aspects of the analysis.\n\nIn this notebook I am using a customized alert type that I have created a year ago in [my Treasure Hunt](https://www.kaggle.com/code/andradaolteanu/treasure-hunt-what-gives-to-be-really-good). These alerts have different colors and a shadow cast onto them. \n\n> You can go within the `alerts.css` file, download it, change to `.txt` format and customize the colors and shade however you want. When you're done, change it back to .css and .... you have **your own customized alerts to play with**!"
### I. Simple Showcase\n\nBelow we create a barplot to *show all kagglers that were mentioned more than once* within Martin's Hidden Gems series.\n\n**💡 Steps for easy plotting**:\n* we define the figure size and ax\n* we create the barplot using `seaborn`\n* we add a simple title for context
"### II. Adding a Twist 🍸\n\n**💡 Additionat steps for an elevated plot**:\n* add a custom palette\n* show the title a bit of attention and love by changing the `size` and `weight`\n* show values on each bar instead of using the x axis (using `show_values_on_bars()` custom function)\n* add a *vertical line* to sepparate the most mentioned kaggler than the rest\n* add an *image* of the most favorite kaggler profile and an *arrow* to give more detail and context\n* use `sns.despine()` to remove the borders of the plot, so it appears more clean and easy to read"
### 📌 🐝 W&B Plot Logging\n\nBelow is a simple example of how to log the `barplot` to you own W&B Dashboard. \n\n
"### 📌 Other Plotting options\n\nSeaborn and Matplotlib are a great combo and they can both do a great job in creating a beautiful analysis. If you feel more adventurous, you can also try:\n* **Plotly**: Check out the work of [Schubert de Abreu](https://www.kaggle.com/spitfire2nd) and his use of plotly - https://www.kaggle.com/code/spitfire2nd/enthusiast-to-data-professional-what-changes\n* **D3.js**: If you are not familiar with Javascript, this could be a slow learning curve, but once you get the hang of it you can create almost anything! Check out [my notebook that uses D3 only](https://www.kaggle.com/code/andradaolteanu/how-are-the-ladies-and-the-gents-doing/notebook). I have also made a [discussion post](https://www.kaggle.com/discussions/general/293879) with some points to consider before starting learning D3.\n\n### Additional Questions to be asked\n* Does Martin influence how famous you are?\n* Does Martin make you a Grandmaster?\n* Do the notebooks have more versions than the average notebook on Kaggle?\n\n# 3. Let's help Martin with his reviews!\n\nLet's create a Text Generator using **PyTorch** that helps Martin create reviews based on his old ones!\n\n### 📌 Create a Shaped Wordcloud\n\n* Get the text from `reviews` and create an single string out of it\n* Create the `WordCloud` object\n* Add the size of the figure and a title"
### 📌 Wordcloud with a Twist 🍸\n\n* Make a custom shape of the cloud: In order for the wordcloud to take the shape of the image you should input a `.jpg` image with **white background** (NOT black and NOT transparent - because the function will interpret the transparent background as black).\n* More custom fonts like I used below can be found here: https://www.dafont.com/\n* Create a customized palette using `similar_color_func()`\n* Give the title some love\n* Increase the size of the plot and remove axis
"\n  💡 Note: The following code is heavily inspired by this notebook: Beginners Guide to Text Generation(Pytorch)\n\n\n## I. Text Cleaning\n\n### 📌 How to make Schemas\n\nI use [Google Slides](https://www.google.com/slides/about/) for all my schemas, no matter if they are just an explanation of the data, a view of the machine learning pipeline or a deep dive into a new concept.\n\nI LOVE schemas: they are such a easy way to express something that might be hard to understand by using just words. I am also **lazy sometimes** and I don't want to read, so by looking at pictures I get the overall information ^^. Is this a good habbit? Of course not, but I do it anyways 😅\n\n> A schema that shows the cleaning process of the text (before applying the model).\n\n\n\n### 📌 Comment your code\n\n💡 You know what you're doing, **but maybe others don't**! Try your best to *comment your code and explain everything that you're doing*, *printing* your results occasonaly, so the readers can stay on track."
"### There is 77% of missing data in the cabin column, it's usually way too much for this column to be exploitable, but as we have a small amount of data, we will still try to use it in feature engineering. \n### For the age, we will either interpolate missing values or we will fill it with the mean for the corresponding category (in term of class, age, sex) of passenger. \n### There is only two missing values for the embarked column, let's try to replace it. Below is the distribution of Embarked according to Fare and sex, and the two observations with missing ""Embarked"" value. Let's look at there two observations and choose the best matching embarked value according to their fare value and sex:"
"### Both passengers are female who paid 80 dollars as fare for their tickets. Moreover, they have the same ticket and cabin, so they probably had to board at the same place! According to the distribution above, the more probable embarked value for them is Cherbourg (C). We'll replace these two missing values later during features engineering part. \n\n### Finally, let's plot some histograms to visualise the distributions of our variables:"
"### With this first exploration, we can see that :\n\n* Only aproximately 35% of passengers survived ...\n* More than the half of passengers are in the lowest class (pclass = 3)\n* Most of the fare tickets are below 50\n* Majority of passengers are alone (sibsp and parch)\n\n*Note : this EDA is not complete at all since it is not the purpose of this kernel to make a deep exploration of data. However, you can look at my [**EDA kernel**](https://www.kaggle.com/nhlr21/titanic-colorful-eda) for this competition if this interests you.*"
> Let's take a look at the Average Weekly Sales per Year and find out if there are another holiday peak sales that were not considered by 'IsHoliday' field.
"> As we can see, there is one important Holiday not included in 'IsHoliday'. It's the Easter Day. It is always in a Sunday, but can fall on different weeks. \n    >* In 2010 is in Week 13\n    >* In 2011, Week 16\n    >* Week 14 in 2012\n    >* and, finally, Week 13 in 2013 for Test set\n\n> So, we can change to 'True' these Weeks in each Year."
### Pclass - Survived
### Embarked - Survived
### SibSp - Survived
### Parch - Survived
### Age - Survived
### Fare - Survived
\n## 2-Correlation Between Features
"# Characteristics/ Demographics\nLet's look at the demographics or characteristics first, like Country, Gender , age , highest education etc.\nAlthough you can't work your way through most of these, but it is crucial in understanding our data."
">*  Most of the Data Scientists in the Survey come from USA and India, like the last two years.They are \nfollowed by Brazil and Russia. \n* We have just 5 countries from the African Sub-continent, with very low participation. The other countries because of very low responses must be grouped in Others.\n* The same goes for South America,except for Brazil, there is very low participation in the survey.\n"
"> * **Male Data Scientits are more than 5 times their female counterparts**. This leads to *bias* in machine learning. ML algorithms are representative of the data they eat and the people who build them. An interesting incident can be cited here, back in 2014 Amazon trained a ML system to hire people and it was unreasonably throwing away women's resumes. A further study in the problem found out that the Consequently, the AI concluded that men were preferable. It downgraded resumes containing the words ""women's"",and filtered out candidates who'd attended two women's only colleges.[[1]](https://www.businessinsider.in/tech/amazon-built-an-ai-to-hire-people-but-reportedly-had-to-shut-it-down-because-it-was-discriminating-against-women/articleshow/66148420.cms)\n* **Most of the population is in their late twenties**. We find fewer and fewer people in Data Science as we move away from this age group. This is probably because Data Science is a relatively new field, the older generation does not know about it , let alone be skilled at it. However, I believe this trend will shift with time.\n* Having work environments more inclusive of women and the elderly will help us to create better and more diverse models.\n* It seems like having a Master's is quite common for a Data Scientist. So ,getting a Master's seems like a safe bet. But is it really necessary to invest that amount of time and money? I think this question is better answered in [Shivan Bansal's notebook](https://www.kaggle.com/shivamb/spending-for-ms-in-data-science-worth-it)."
# Common Practices and Preferences of Data Scientists\nLet's start with preferences first. Getting to know which MOOC platforms or Blogs most of the Data Scientists prefer can help you steer your learning process in the right direction.\n
">* Coursera seems like the obvious choice of Data Scientists,maybe becuase of the quality of content and assignments over there. I personally like it too. There is also an increasing trend in University Courses and it's the second most  prefered thing.\n* When it comes to  favorite Media sources for DS related topics , Medium has a clear edge followed by Kaggle.Probably because a lot of Data Scientists themselves publish there articles on Medium, it seems like a credible source. Adding to this there is a huge variety of Data Science stuff that can be found on Medium from collecting data to deploying models.\n* Kaggle is the second most popular information resource, because of it's content and probably because it's fully free. They won't ask you to make a subscription after you have reached your monthly limit.\n\nI'm not plotting the most common programming language among Data Scientits because python always comes as the clear winner. So, just go with it. There are a whole lot of Machine Learning and Deep Learning libraries in python, plus it is also easy to use.\n\nLet's move on to the next question now.\n\n### What activity makes up most of their time at work?\n\nAs aspiring Data Scientists people spend most of their time building models but this is evidently not the case in actual Data Science scenarios. Data Scientists spend most of their times analyzing and cleaning data which is confirmed by this survey."
">* Analyzing data to find insights that drive decisions and finding places to apply ML models seems to be the most time consuming process. \n* These days every other company wants to use Data Science to generate more revenue and help their business grow. But deciding where actually Data Science can do wonders, is the job of a Data Scientist. To quote someone,pardon me because I don't remember the source *""A good Data Scientist knows where to apply Machine Learning and a great Data Scientist knows where not to apply Machine Learning.""*\n* It's a huge responsibility to put a Machine Learning model into production and believe me the real game actually starts once you have trained and deployed the model. When it goes live unanticipated scenarios are encountered and changes have to be made. Therefore, Data Scientists spend most of their time in improving existing models.\n* Data Scientists are least interested in improving the state of the art, that's the job of a Research Scietist.\n* **Note**:- Spending more time on finding insights and looking out for areas where Machine Learning can be applied can prepare you better for the job.\n\n**Since analayzing data seems like the most widely performed activity, let's find out how Data Scientists do it.**"
"> * Okay, so most Data Scientists have 3-5 years of experience writing code to ananlyze data.\n* Local development environments are widely used (Jupyter and RStudio),almost  more than 4 times of the other tools.\n* No Surprises that Tableau and other BI softwares are not popular among Data Scientists, because they are primarily used by Business Analysts .\n* Regarding the Visualization Libraries, Matplolib comes as a winner and most of the other libraries(Seaborn and Plotly) are wrappers built on it's powerful internals.\n\n#### Mostly used hosted notebook products against years of experience."
"> * Kaggle Notebooks and Google Colab are the most widely used notebook products because of the good computation power provided by them with GPUs(all for free with a limit). Another biggest advantage over traditional environments is there is no need to download and setup libraries and I think most people would agree it's time consuming and awfully boring stuff.\n* Still a lot of people prefer to work in the traditional way on their local systems and there are more experienced people in this category.\n* Kaggle notebooks and Google Colab are mostly used by Data Scientists for competitions and independent projects and hence we have more of the younger generation using these.\n* People do not use these for company work and rather go for local systems and if the ML infrastructure of the company is huge they go for paid products like AWS and GCP, and hence we can see a lot of the older generation using these.\n\n#### ML algo used on a regular basis vs years of experience"
"Deductions:-\n> * Looking at the ML algorithms used on a Daily basis, I can say most of the Data Scientists work on machine learning as is evident by the number of people using Linear/Logistic Regression, Decision Trees  and Boosting methods.\n* I think most Data Scientits would agree that a lot of problems can be solved by Linear algorithms, if not by them then by using trees. Jumping on neural networks without exhausting ML algorithms for classification/regression problems is always avoided by Data scientits. More you move towards a complex model, the more assumptions it make. It might give you better performance but debugging becomes trickier. One should never underestimate the power of simple ML algorithms.\n* A smaller subset of Data Scientists deal with Deep Learning problems and most of them work on Computer Vision(image data) problems, seen through the Convolutional Neural Network bars.\n* Only a few work on NLP problems(text data) and Recurrent Neural Networks are more popular amongst the younger generation while Transformers could be equally or more popular with the older population( I can't be sure because we have unequal number of samples for each age group).\n* I know there could be an overlap between Data Scientists using ML algorithms of all the three types(ML,NLP,CV), but that would rarely happen given the question asks on a *""regular basis""*.\n\nLet's also look at other ML algorithms not provided as options in the survey."
"SVM and clustering algorithms seem like popular ones. In deep learning I can see neural networks and reinforcement learning but ML algorithms dominate here too.\n\nIf you are interested in Deep Learning and would want to work in the field, let's look at some common practices in those fields exclusively.I'll start with Computer Vision first.\n\nWhile filtering people for CV or NLP, I won't consider Dense Networks for algorithms as it is rather ambigous, whether a person using them works on NLP or CV or ML(tabular data). For CV, I consider CNN and GANs and for NLP, I take RNNs and Transformers."
"Deductions:-\n> * Image Classification methods seem like the most common group of methods used by CV people along with general purpose pretrained models like VGG16, Inception, Resnets etc that could be used in a variety of tasks.\n* General purpose Image/ video tools are used in preprocessing basically therefore are the second most widely used methods.\n* Data Scientists in the Autonomous Vehicle industry extensively use Image segmentation and Object detection.\n* Traditional word embedding techniques like GLoVe and Word2Vec remain the most popular one in the Data Science Industry(Even I work with them).\n* Encoder-decoder models are used in making chatbots and machine translation.\n* Transformer models are slowly catching wind, but I think they are more complex to use.\n* Note:- If you want to enter the CV/NLP field, having worked on an Image classification or text classification model with good understanding of it would give you an edge. It worked for me, it could work for you too.\n\n\n\n# Common practices followed by Companies\nI believe there are certain trends that companies follow in hiring Data Scientists, or deciding which cloud based platform to use or whether to automate some processes or not according to thier need and resource availability.\n\nLet's look at some of these.\n\n#### Role of years of experience in determining the size of the company (and hence the worth) that will hire you.\n\n"
">* There is an obvious trend here, which seems quite practical too. \n* People with less experience in ML work in a small sized company or startup, as the work experience increases more people shift towards big MNCs.\n* So , as a fresher if you land a job in a big company, consider yourself lucky.\n\nNow,let's take a look at the **Company size vs. the number of people responsible for DS workloads** to analyze what kind of Companies do Data Scientists work in."
">* The first bar where there are 0 people responsible for DS workload in a company seems a bit strange to me , as a person calling himself a Data Scientist should atleast count himself.\n* In the last bar, the small companies(0-49,50-249 range) employing 20+ Data Scientists must be DS Consulting firms or companies fully based on AI like H20.ai or OpenAI.\n\n> There are two kinds of majorities here. \n* Small companies(0-49 employees) having 1-2 DS people to improve decision making or work on specific tasks like recommender systems, Sentiment Analysis, Sales Prediction etc.\n* MNCs with more than 10k employees having 20+ DS leveraging AI in full capacity like Facebook, Google etc.\n\nIt would also be useful to know which ML product companies use and you can add some experience in them to get you some edge over other contenders."
Let's Explore the target feature i.e SalesPrice.
**Red line in histogram indicates the mean of the SalePrice and the Green line indicates the median**
"From the above plots, we can observe that the Sales Price is not normally distributed. From the boxplot we can observe that the dataset have a number of outliers.\n\nFor the probability plot, the red line represents those points which would have been plotted for y-axis points **if those were normally distributed**. But the blue points represents the actual scenario. We can see that there is a lot of deviation on the both the ends i.e on the top right and bottom left."
"Skewness refers to the amount of asymmetry in the given feature or in other words amount of distortions from the normal distribution. \n\nHere we can observe that the value of skewness is quite high which means that there is a large amount of asymmetry. \n\nThe peak of the histogram represents the mode i.e the price for which maximum number of houses were sold.\n\nWhat kind of skewness is present in the given case?\n\nAs the mean of the feature is greater than the median which is greater than the mode and the line is flat towards the right in the histogram, the given feature is **Positively Skewed**. Most of the houses were sold less than the average price.\n"
There is an amazing library called missingno which helps us to visualize the number of Null values present in each feature.
Just by going through the plot we can see that there are a lot of NULL values
"## How to detrend a time series?\n\nDetrending a time series is to remove the trend component from a time series. But how to extract the trend? There are multiple approaches.\n\n- Subtract the line of best fit from the time series. The line of best fit may be obtained from a linear regression model with the time steps as the predictor. For more complex trends, you may want to use quadratic terms (x^2) in the model.\n- Subtract the trend component obtained from time series decomposition we saw earlier.\n- Subtract the mean\n- Apply a filter like Baxter-King filter(statsmodels.tsa.filters.bkfilter) or the Hodrick-Prescott Filter (statsmodels.tsa.filters.hpfilter) to remove the moving average trend lines or the cyclical components."
 🗃 Patterns Recognition - Seasonality
"## How to test for seasonality of a time series?\n\nThe common way is to plot the series and check for repeatable patterns in fixed time intervals. So, the types of seasonality is determined by the clock or the calendar:\n- Hour of day\n- Day of month\n- Weekly\n- Monthly\n- Yearly"
 🚲 Patterns Recognition - Cyclic behaviour
# 2. Importing Libraries 📚\n👉 **Importing libraries** that will be used in this notebook.
"# 3. Reading Data Set 👓\n👉 After importing libraries, we will also **import the dataset** that will be used."
## 5.1 Drug Type Distribution 💊
## 5.2 Gender Distribution 👫
## 5.3 Blood Pressure Distribution 🩸
## 5.4 Cholesterol Distribution 🥛
"## 1. Introduction\n\nThis is my first kernel at Kaggle. I choosed the Titanic competition which is a good way to introduce feature engineering and ensemble modeling. Firstly, I will display some feature analyses then ill focus on the feature engineering. Last part concerns modeling and predicting the survival on the Titanic using an voting procedure. \n\nThis script follows three main parts:\n\n* **Feature analysis**\n* **Feature engineering**\n* **Modeling**"
## 2. Load and check data\n### 2.1 Load data
## 3. Feature analysis\n### 3.1 Numerical values
"Only Fare feature seems to have a significative correlation with the survival probability.\n\nIt doesn't mean that the other features are not usefull. Subpopulations in these features can be correlated with the survival. To determine this, we need to explore in detail these features"
#### SibSP
"It seems that passengers having a lot of siblings/spouses have less chance to survive\n\nSingle passengers (0 SibSP) or with two other persons (SibSP 1 or 2) have more chance to survive\n\nThis observation is quite interesting, we can consider a new feature describing these categories (See feature engineering)"
#### Parch
"Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6 ).\n\nBe carefull there is an important standard deviation in the survival of passengers with 3 parents/children "
#### Age
"Age distribution seems to be a tailed distribution, maybe a gaussian distribution.\n\nWe notice that age distributions are not the same in the survived and not survived subpopulations. Indeed, there is a peak corresponding to young passengers, that have survived. We also see that passengers between 60-80 have less survived. \n\nSo, even if ""Age"" is not correlated with ""Survived"", we can see that there is age categories of passengers that of have more or less chance to survive.\n\nIt seems that very young passengers have more chance to survive."
"When we superimpose the two densities , we cleary see a peak correponsing (between 0 and 5) to babies and very young childrens."
Let's take look at how number of atoms is connected with target variable:
No clear dependence is observed so we need some more features to be exctracted.\nThe next obvious step is to count numbers of the most common atoms.\nRDkit supports subpattern search represented by ***GetSubstructMatches()*** method. It takes a MOL of a substructure pattern as an argument. So you can futher extract occurance of each pattern you'd like.
"## Introduction ##\n\nThis is my first work of machine learning. the notebook is written in python and has inspired from [""Exploring Survival on Titanic"" by Megan Risdal, a Kernel in R on Kaggle][1].\n\n\n  [1]: https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic"
# Feature Engineering #
# Classifier Comparison #
# Prediction #\nnow we can use SVC classifier to predict our data.
\n\n1.1 Libraries and Utilities
\n\n1.2 Data Preprocessing 
Data storytelling is a popular method to convey most of the information in terms of simple plots rather with complex analysis. This techique's main objective to perform explinatory data analysis rather than exploratory data analysis. All the plots made are as simple as possible with no interactivity and complecations. For whole visualization a simple color palette shown in palplot was used. Few key points are highlighted in the plot it self.
\n  2.0 Distribution of Targets \n
"From distribution it is clear that every 5 people out of 100 people are having strokes from our sampling data. Moreover,this is a highly unbalanced data distribution, and null accuracy score of this distribution it self is 95%, whcih imploys any dump model should randomly predictions of stroke could reach accuracy of 95%. So, while modeling and training data, either over sampling or under sampling has to be done to obtain best results."
\n 2.1 Univariate analysis of continuous variables\n
"Age is an important feature. Age feature distribution is not a normal distriubtion, which needs to be tranformed later. From catergorical features it can be seen that old age people are mostly having strokes, compared to younger ones."
Glucose level distribution is skewed towards left and most ofhte strkes can be seen people with regular glucose levels.
"BMI is highly skewed and high bmi , high possibility of having strokes."
"# Shopee Price Match Guarantee: Before we start\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/24286/logos/header.png?t=2021-01-07-16-57-37)\n\nDo you scan online retailers in search of the best deals? You're joined by the many savvy shoppers who don't like paying extra for the same product depending on where they shop. Retail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help.\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.\n\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n\nIn this competition, you’ll apply your machine learning skills to build a model that predicts which items are the same products.\n\nThe applications go far beyond Shopee or other retailers. Your contributions to product matching could support more accurate product categorization and uncover marketplace spam. Customers will benefit from more accurate listings of the same or similar products as they shop. Perhaps most importantly, this will aid you and your fellow shoppers in your hunt for the very best deals."
Work directory
"An MVP Aproach\n=========\n\n 1. Introduction\n---------------------------------\nAs a (admittedly lazy) Data Scientist I love accessing information in the easiest way possible. Google is my best friend, for example. With the exception of a few things, there is almost nothing you can't find with a simple search.\n\nThis kernel is written and developed using IPython Notebook and XGBoost, with the aim of being ready-to-submit. By that I mean any user can run each code block, submit the output, and achieve a comfortable top-half score. Breaking into the top 10% of the submissions would require quite a bit more effort and a more sophisticated approach. In the final section I include some comments and suggestions of some alterations/variations one might try to improve the score. Those suggestions, if carried out correctly, do result in a strong score on the leaderboard. \n\nThis then serves as an introductory tutorial to approaching this particular problem (rather than giving the best solution outright) leaving enough room for creativity and innovation in the solution space to allow any user to better the end result.\nIn short, the whole project is about LEARNING, sharing some ideas, and ultimately improving the degree of Data Scientists out there. \nEnjoy! \n\n1.1 The Problem \nNew York is riddled with one-ways, small side streets, and an almost incalculable amount of pedestrians at any given point in time. Not to mention the amount of cars/motorcycles/bicycles clogging up the roads. Combine this with a mad rush to get from point A to point B, and you'll find yourself late for whatever you need to be on time for.\n\nThe solution to getting from A to B when living in a city like New York (without losing your mind) is easy: take a taxi/Uber/Lyft/etc. You don't need to stress about the traffic or pedestrians and you have a moment to do something else, like catch up on emails. Although this sounds simple enough, it doesn't mean you'll get to your destination in time. So you need to have your driver take the shortest trip possible. By shortest, we're talking time. If a route A is X kilometers *longer*, but gets you there Y minutes *faster* than route B would, rather take that one.\n\nTo know which route is the best one to take, we need to be able to predict how long the trip will last when taking a specific route. Therefore, *the goal of this playground competition is to predict the the duration of each trip in the test data set, given start and end coordinates.*\n\n1.2 The Libraries & Functions\n\nUsing Python 3.6.1, import the following libraries. Note the use of `%matplotlib inline`, allowing the display of graphs inline in iPython Notebook.\n\nDocumentation\n[Scikit-Learn](http://scikit-learn.org/stable/documentation.html ""Scikit-Learn"")\n[Pandas](http://pandas.pydata.org/pandas-docs/stable/ ""Pandas"")\n[Numpy](https://docs.scipy.org/doc/ ""Numpy"")\n[XGBoost](http://xgboost.readthedocs.io/en/latest/python/python_intro.html ""XGBoost"")\n[Seaborn](https://seaborn.pydata.org/index.html ""Seaborn"")\n\nI used Scikit-Learn (or sklearn) for a few of the machine learning operations that was carried out. Pandas is used for data manipulation. Numpy is the fundamental package for scientific computation in Python. XGBoost is the classification algorithm used to make the final predictions. Seaborn is a nice tool for data visualisation built on top of matplotlib. The import code is as follows:"
1.3 Loading the Data\nLoad the data using the Pandas `read_csv` function:
"1. 3. Data Visualisation and Analysis\n----------------------------------\n These next steps involve looking at the data visually. Often you'll discover looking at something significant as a graph rather than a table (for example) will give you far greater insight into its nature and what you might need to do to work with it. Of course, the opposite could also be considered true, so don't neglect the first section we went through.\n\n3.1 Initial Analysis\n\n Let's plot a simple histogram of the trip duration, throwing the data into 100 bins. Change this around to get a feel for what binning does to your data. Simply put, binning involves taking your data's max and min points, subtracting it to get the length, dividing that length by the number of bins to get the interval length, and grouping the data points into those intervals. Here's what that looks like: "
"This is a good opportunity to play with some data transformations to see if notable patterns emerge in the data when applying certain transforms, for example a log transform. In this case, applying a log transformation to the trip duration makes sense, since we are doing this to accommodate the leaderboard's scoring metric. That would look like this:"
"One might also be interested to view the number of trips over time, since this could reveal not only apparent seasonality in the data and certain trends, but could point out any siginficant outliers (if not already cleaned out of the data set of course) and indicate missing values (again, only if it wasn't already checked and cleaned in the data set).\n\nFor this we'll simply plot a timeseries line graph of both the test and training data to not only look into identifying possible trends/seasonality but to see if both data sets follow the same pattern shape. Reasonably we'd expect the two datasets to follow a very similar shape since the test data would be/is a randomly selected sample from the original dataset containing all data points. By picking the test points randomly, each data point has the same likelihood of being picked as a test point, thus ensuring uniformity in the test data sample."
"Clearly the test and train datasets follow a very simila shape, as expected. A couple of points stand out at first glance. Around late-Jan/early-Feb there is a massive drop in the number of trips taken. A slightly less drastic drop is apparent about four months later. The first drop could be related to the season: it's winter in New York so you'd expect less trips being taken (who wants to ride around when it's near freezing outside?). However, this seems unlikely because the dip looks to be isolated around a single day or couple of days. In my opinion there's a greater chance that there were strikes (if you've got the South African mindset I do, but this is New York, so probably not that) or that there was an issue with the data system recording the trips. Whatever the reason, it's worth noting these 'outliers'.\n\n Let's see how significantly (or not significantly at all) the two vendors differ in their respective mean trip durations:"
"So it doesn't look like there's much of a difference between travel times for the two vendors. One would assume that knowing which routes are the fastest to take from A to B is no secret, and that it is more a trick of the trade, rather than IP. But something doesn't look right, so there's another feature we can use to see if after all there is a significant difference in mean travel time: the `store_and_fwd_flag`."
"So it would seem that the `store_and_fwd_flag` discriminates well between travel times. Clearly there is a slight skew in the data where some of the vendor employees didn't record their travel times accurately.\n\nAs mentioned earlier when digging into the variables, I thought about the impact that the number of passengers per trip might have on travel time. The thought process was that more passengers might equal more stops, hence prolonging the travel time from start to finish (unless the different drop-offs were recorded as separate trips). So in order to find out if there is infact a significant influence in travel time, let's group the mean travel times by the `passenger_count`:"
"So no significant difference evident that could be explained by the number of passengers in the vehicle for any given trip. It is interesting to note that there is are on average +-4min trips associated with no passengers. This is probably a mistake made in recording the data unless the vendor employee is into charging him/herself for trips whilst on the job.\n\nAgain, we need to check whether the test and train data matches with respect to the number of trips containing X-number of passengers:"
"- We can note the twin peak is associated with the lower `Reynolds` number, which is much higher `spl` than the lower `Reynolds` peak. \n- Lower `Reynolds` number cases are also associated with higher `spl` noise, in general.  \n- It's not quite straightforward to differentiate the two possible sources, low & high `Reynolds` numbers are also quite subjective. \n- What we can note that the higher `Reynolds` have two distrinctive peaks, which tend to tranform into a levelled out `plateau` at higher `freq` as the `Reynolds` number is reduced. \n- If there is a clear location a model might stumble on it would be the twin peak at `u_inf=71.3` which is quite probably associated with the low `Reynolds` associated `LBL-VS` noise.\n\n\n    3.5 | HIGHER FLOW SEPARATION CASES\n\n\n- Whilst `aoa=0` can still have flow separation, most notably at very high `u_inf`, the flow separation is very minor \n- Larger flow separation is associated with higher aoa values.\n- Let's group all the higher aoa case we have & see if we can note anything associated to `spl` as `aoa` is increased."
"- An increase in `aoa` tends to move the spectrum diagonally towards lower `freq` and `spl` simultaneously\n- The higher the `aoa` becomes, the more notable the low `freq` peaks become, quite often generating a higher local `spl` maximum at a very narrow `freq` range.\n- Medium sized `l_chord` & `aoa` tend to be associated with more freqent localised `spl` variation in the mid frequency range."
# Load Libraries
# Load Data
"## 5.1. Checking some patients\nPyMC3 comes with a very powerful visualization tool called [ArviZ](https://arviz-devs.github.io/arviz/index.html). However, I didn't figure out how to use yet... Let's use Matplotlib and Seaborn."
"Here I plotted 100 out of the 4000 personalized models each patient has! In green we can see the fitted regression line, in yellow the standard deviation. Let's ensemble all that!"
#### Active Cases = Number of Confirmed Cases - Number of Recovered Cases - Number of Death Cases\n#### Increase in number of Active Cases is probably an indication of Recovered case or Death case number is dropping in comparison to number of Confirmed Cases drastically. Will look for the conclusive evidence for the same in the notebook ahead.
#### Closed Cases = Number of Recovered Cases + Number of Death Cases \n#### Increase in number of Closed classes imply either more patients are getting recovered from the disease or more pepole are dying because of COVID-19
"#### Growth rate of Confirmed, Recovered and Death Cases "
#### Moratality and Recovery Rate analysis around the World
"#### Mortality rate = (Number of Death Cases / Number of Confirmed Cases) x 100\n#### Recovery Rate= (Number of Recoverd Cases / Number of Confirmed Cases) x 100\n#### Mortality rate is showing a considerable for a pretty long time, which is positive sign\n#### Recovery Rate has started to pick up again which is a good sign, another supportive reason to why number of Closed Cases are increasing"
#### - Insights\n- It is quite a big dataframe (the train.csv file is more than 2GB large). It has more than 13 millions rows and a few features.\n- The test dataframe has an extra columns called `session_level` which will be used to create the submission file.\n\nLet's see if all features are relevant by analysing them.\n## 💡 Missing values
"### - Insights\n- There are a lot of missing values in some columns.\n- We will drop the `fullscreen`, `hq` and `music` features at least, and maybe the ones with more than 80% of missing values. To make this decision about keeping or not the columns with high missing ratio, we will have to evaluate the relevance of them.\n- The train and test dataframes have similar ratios of missing values which are good news.\n\n## 💡 Analysis of the features\nLet's analyse all the features in order to understand what they mean.\n### 'Session id' and 'index'\n> The ID of the session the event took place in\n\n> The index of the event for the session"
"#### - Insights\n- **Train set:**\n - A session typically encompasses anywhere between several hundred to a few thousand individual events.\n - Both the median and the mean are slightly over 1000.\n - The distribution appears to be normal with a slight positive skew.\n - Above the 90th percentile, the values for the number of events are very large. I set a x limit for visualization but the number of events go very far. Should we consider them as outliers?\n- **Test set:**\n - Only 3 sessions are represented in the test set having a fair amount of events.\n - The session with 1501 events is in the tail of the gaussian distribution but still in a reasonable area.\n - Do not forget that the available test set only represents the half of the final one.\n- **Events vs Level:**\n - Currently, we have only analyzed the number of events that occur during the sessions.\n - Keep in mind that a session is divided into question levels, and we aim to predict their success. However, the number of events can vary for each question level. It's possible that the number of actions (clicks) during a question may be related to a correct or incorrect answer. It would be interesting to inspect that later.\n \n### 'Elapsed time' and 'index'\n> How much time has passed (in milliseconds) between the start of the session and when the event was recorded.\n\nThe elapsed time should be correlated with the event index. As more time is spent, it is expected that a higher number of events will occur. This is why I have decided to calculate the mean for each event index and plot it."
"#### - Insights\n- As expected, the average elapsed time increases with the increase in event index, but this trend only holds until around index 2800. After that, the behavior becomes erratic with a peak and then drops and remains constant until much later.\n- This phenomenon can be explained by the fact that as the number of events increases, there are fewer and fewer examples, and the average starts to represent unique cases which are likely outliers. These outliers may include individuals who did ""spam clicks"" to achieve a high number of events in a shorter amount of time or, in the case of the peak, individuals who were inactive for an extended period.\n\nLet's now check the elapsed times for individual cases and see if this trend remains the same."
"#### - Insights\n- Almost all of the examples shown exhibit gaps, indicating brief pauses in activity.\n- Some examples, however, exhibit huge gaps due to extended periods of inactivity.\n- The maximum elapsed time in the test set appears to be more reasonable than the outliers observed in the training set. It may be beneficial to get rid of sessions in which the individual was inactive for an extended period of time.\n\n### 'Event name' and 'name'\n> The name of the event type.\n\n> The event name (e.g. identifies whether a notebook_click is is opening or closing the notebook)\n\nThese 2 columns are quite similar since they describe the action of the user. Let's display there distributions independently and in a pivot table. The data are from the train set."
"#### - Insights\n- The events mainly consist of clicks and occasionally hovers. I am uncertain about the distinction between the various types of clicks.\n- Certain ""names"" only occur with specific ""event_names"". Not all combinations are possible.\n- I have also checked the distribution in the test set, and it is comparable to the distribution in the training set.\n\n### 'Index' and 'level'\n> What level of the game the event occurred in (0 to 22)"
"#### - Insights\n- A greater number of events take place during specific periods. For instance, there is a heightened level of event activity observed at level 18.\n- This pattern can also be observed in various individual cases as examples. However, each one remains unique.\n\n### 'Page'\n>  The page number of the event (only for notebook-related events)"
#### - Insights\n- A lot of values are still missing in this columns (almost 98%) because a page is only indicated when the event is notebook-related.\n- I don't know if this information is relevant or not. Is it worth it to keep it and how should we fill in the missing values? With zeros?\n- The test set has a slightly different distribution from the train set for the page number.\n\n### 'Hover duration'\n> How long (in milliseconds) the hover happened for (only for hover events)
"#### - Insights\n- Similar to the page feature, the hover duration is indicated only for a few rows during an hover event. Typically, the duration lasts anywhere from a few milliseconds to a few seconds, with rare instances lasting more than 4 seconds.\n- In the training set, we can observe that the mean is significantly higher than the median, which is due to the presence of outliers where the user remained on hover for an extended period. This type of examples should be avoided.\n\n### 'Text'\n> The text the player sees during this event"
"## 1) Import Necessary Libraries\nFirst off, we need to import several Python libraries such as numpy, pandas, matplotlib and seaborn."
"## 2) Read in and Explore the Data \nIt's time to read in our training and testing data using `pd.read_csv`, and take a first look at the training data using the `describe()` function."
### Sex Feature
"As predicted, females have a much higher chance of survival than males. The Sex feature is essential in our predictions."
### Pclass Feature
"As predicted, people with higher socioeconomic class had a higher rate of survival. (62.9% vs. 47.3% vs. 24.2%)"
### SibSp Feature
"In general, it's clear that people with more siblings or spouses aboard were less likely to survive. However, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)"
### Parch Feature
"People with less than four parents or children aboard are more likely to survive than those with four or more. Again, people traveling alone are less likely to survive than those with 1-3 parents or children."
### Age Feature
Babies are more likely to survive than any other age group. 
"### Cabin Feature\nI think the idea here is that people with recorded cabin numbers are of higher socioeconomic class, and thus more likely to survive. Thanks for the tips, [@salvus82](https://www.kaggle.com/salvus82) and [Daniel Ellis](https://www.kaggle.com/dellis83)!"
"People with a recorded Cabin number are, in fact, more likely to survive. (66.6% vs 29.9%)"
"How we propose a forecasting problem using other sciences such as classical statistics, probability theory and mathematical economics?  \nIn statistics, we are limited to methods such as descriptive statistics, histograms, means, variance which one cant give us appropriate results and inference.   \n_fact: Usual the most data science courses end education after statistics._"
"[back to top](#table-of-contents)\n\n# **3. 💹 Introduction to econometrics** \nLike machine learning, the difficulty of econometric modeling lies in the fact that most processes cannot be accurately estimated. We often have to choose one or the other. So let's stick to one golden rule:\n> «Models should be as simple as possible, but not simpler» F. Engels  \n\nI think these are the words of Einstein...\n\n**It is possible to single out the main classes of econometric models:**\n1. Regression Models\n2. Systems of simultaneous equations or regression models\n3. Single Equation Time Series Models\n4. Time series models with multiple equations. Vector autoregression.\n\n**Basic data types in econometric models:**\n1. Time Series\n2. Spatial or cross-sectional data\n3. Panel data"
"General view of this regression model:\n$y = w_{0} + w_{1} \cdot x_{1} + \varepsilon$\n* $y - \text{response variable}$\n* $w_{0} - \text{constant}$\n* $w_{1} - \text{slope and regression coefficient of } x_1 $\n* $x_1 - \text{independent variable, regressor}$\n* $\varepsilon - \text{residual}$\n\nI gonna show you how work this coefficients\n\nLeets se whats going on while we changing $w_0$:\n$y = w_{0} + w_{1} \cdot x_{1}$  \nif $w_{1} = 0$, $y = w_{0} + 0 \cdot x_{1} = w_{0}$  \nif $y \rightarrow n$ then $ w_{0} \rightarrow n $  \nAs we can see $w_{0}$ is responsible for level of response variable"
Leets see whats going on while we changing $w_1$:\n\n$y = w_0 + w_1 \cdot x_{1}$  \nif $w_0 = 0$ then $y= w_1 \cdot x_{1} \Rightarrow \frac{y}{x_1} = w_1 $\n\nAnd we can see that  $a_1$ is slope of regressor and response variable
"[back to top](#table-of-contents)\n\n​\n# **3.2. 🧨 Ordinary Least Squares**\nThe most popular method optimization linear regression - OLS. First of all it should be said that optimization regression split of two methods:  __Precise Analytical Method__ and __Approximate Numerical Method__. And in this topic we will talk more about first type. But now let's figure it out what is OLS and why you need to know this. As we know that regression finction look like $ f_{w}(x_i) = \langle x, w \rangle + w_{0}$ and you can check out that im told about w_0 is constant, but why? \n\nOften we can ignore $w_0$, because we can achieve the same result if make new feature $x_i$ equal 1. Then new created feature will be have weight equal $w_0$.\n\n$$ \begin{pmatrix}x_{i1},\dotsc,x_{iD}\end{pmatrix} \cdot \begin{pmatrix} w_{1}\\\vdots\\x_{D}\\\end{pmatrix} + w_{0} = \begin{pmatrix} 1, x_{i1},\dotsc,x_{iD}\end{pmatrix} \cdot \begin{pmatrix} w_{0}\\ w_{1}\\\vdots\\x_{D}\\\end{pmatrix}$$\n\nWe want that our regression model as better as possible approximate our dependent."
### Import the Libraries
### Load the Data
### Exploratory Data analysis\n
### EDA using Pandas Profiling\n\npandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\n\nGithub Repo : https://github.com/pandas-profiling/pandas-profiling\n\n\n![](https://drive.google.com/uc?id=1QEEcCjfj5cnA_9vRfj2nZLRK0QlQGuBV)\n
"American Express Default Prediction\n\n# 1 | Competition Objective\nWhether out at a restaurant or buying tickets to a concert, modern life counts on the convenience of a credit card to make daily purchases. It saves us from carrying large amounts of cash and also can advance a full purchase that can be paid over time. How do card issuers know we'll pay back what we charge? That's a complex problem with many existing solutions—and even more potential improvements, to be explored in this competition.\n\nCredit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics. Current models exist to help manage risk. But it's possible to create better models that can outperform those currently in use.\n\nAmerican Express is a globally integrated payments company. The largest payment card issuer in the world, they provide customers with access to products, insights, and experiences that enrich lives and build business success.\n\nThe objective of [this competition](https://www.kaggle.com/competitions/amex-default-prediction) is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. In this competition, you'll apply your machine learning skills to predict credit default. Specifically, you will leverage an industrial scale data set to build a machine learning model that challenges the current model in production. Training, validation, and testing datasets include time-series behavioral data and anonymized customer profile information. You're free to explore any technique to create the most powerful model, from creating features to using the data in a more organic way within a model.\n\nIf successful, you'll help create a better customer experience for cardholders by making it easier to be approved for a credit card. Top solutions could challenge the credit default prediction model used by the world's largest payment card issuer—earning you cash prizes, the opportunity to interview with American Express, and potentially a rewarding new career.\n\n# 2 | Data Overview\nThe target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n\nThe dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:  \n**`D_*`:** Delinquency variables  \n**`S_*`:** Spend variables  \n**`P_*`:** Payment variables  \n**`B_*`:** Balance variables  \n**`R_*`:** Risk variables  \nWith the following features being categorical: `B_30`, `B_38`, `D_63`, `D_64`, `D_66`, `D_68`, `D_114`, `D_116`, `D_117`, `D_120`, `D_126`. \n\nThere are a total of 190 variables in the dataset with approximately 450,000 customers in the training set and 925,000 in the test set. Due to the dataset size, I will use the compressed version of the train and test sets provided by @munumbutt's [AMEX-Feather-Dataset](https://www.kaggle.com/datasets/munumbutt/amexfeather) and take the last statement for each customer."
"In the training set, the last statement of all customers was in March 2018, while in the test set the date of customers' last statements range from April through October 2019. "
"About 25% of customers in the training data have defaulted. This proportion is consistent across each day in the training set, with a weekly seasonal trend in the day of the month when customers receive their statements."
# 3.1 EDA of Delinquency Variables
"There are several highly correlated Delinquency variables, with a few pairs perfectly positively correlated at 1.0. There are also a number of missing correlations, particularly in `Delinquency 87`, due to null values in the data. Below are the relationships between some of the most correlated Delinquency variables. "
# 3.2 EDA of Spend Variables
"#### Imports\n\nWe'll use a familiar stack of data science libraries: `Pandas`, `numpy`, `matplotlib`, `seaborn`, and eventually `sklearn` for modeling. "
### Read in Data and Look at Summary Information
"#### Integer Columns\n\nLet's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot."
"The columns with only 2 unique values represent Booleans (0 or 1). In a lot of cases, this boolean information is already on a household level. For example, the `refrig` column says whether or not the household has a refrigerator. When it comes time to make features from the Boolean columns that are on the household level, we will _not need to aggregate_ these. However, the Boolean columns that are on the individual level will need to be aggregated. \n\n#### Float Columns\n\nAnother column type is floats which represent continuous variables. We can make a quick distribution plot to show the distribution of all float columns. We'll use an [`OrderedDict`](https://pymotw.com/2/collections/ordereddict.html) to map the poverty levels to colors because this keeps the keys and values in the same order as we specify (unlike a regular Python dictionary).\n\nThe following graphs shows the distributions of the `float` columns colored by the value of the `Target`. With these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level."
"Later on we'll calculate correlations between the variables and the `Target` to gauge the relationships between the features, but these plots can already give us a sense of which variables may be most ""relevant"" to a model. For example, the `meaneduc`, representing the average education of the adults in the household appears to be related to the poverty level: __a higher average adult education leads to higher values of the target which are less severe levels of poverty__. The theme of the importance of education is one we will come back to again and again in this notebook! "
"## Exploring Label Distribution\n\nNext, we can get an idea of how imbalanced the problem is by looking at the distribution of labels. There are four possible integer levels, indicating four different levels of poverty. To look at the correct labels, we'll subset only to the columns where `parentesco1 == 1` because this is the head of household, the correct label for each household.\n\nThe bar plot below shows the distribution of training labels (since there are no testing labels)."
"We are dealing with an imbalanced class problem (which makes it intriguing why the contest organizers choose the _macro_ F1 score as the metric instead of _weighted_ F1!). There are many more households that classify as _non vulnerable_ than in any other category. The _extreme_ poverty class is the smallest (I guess this should make us optimistic!).\n\nOne problem with imbalanced classification problems is that the machine learning model can have a difficult time predicting the minority classes because it sees far less examples. Think about this in human terms: if we are classifiying poverty and we see far more cases of no poverty than extreme poverty, it will make it more difficult for us to identify the high poverty households because of less exposure. One potential method to address class imbalanceds is through oversampling  (which is covered in more advanced notebooks)."
Lets look at the class field and check how many fraudulent transactions we have in this data 
Fraudulent transactions provided here contributes mere **0.17%** which indicates\n**we have a highly imbalanced data to work on.** \nIf I can summarize what Andrew Ng has mentioned in his lecture on Anomaly detection is \nSupervised Classification technique is not the perfect candidate for highly imbalanced data. In this case it is \n 0.172% (near to 0)\n\nIf We think from the persepctive of building the model to find out the anomalous data which is not seen very frequently \nWe should go for Anomaly detection technique using Gaussian Distribution.  \n
"Below is the most crucial function used to detect how well we are doing with our subset (Cross validation subset) .\nI have decided values for Epsilon for detecting the fradulent transactions from the Subsets.  \n**(Tip :- Ideally you should provide range of epsilon values, due to time constraint on running this kernel i have provided few values here for demonstration purpose)**\n\n **For now remember Epsilon value is the threshold value below which we will mark transaction as Anomalous.**\n           ----\n\nRewriting above sentense again \nP(x) for X if less than the epsilon value then mark that transaction as anomalous transaction. \n\nWe need to maintain healthy balance between the Recall and Precision . We may get Recall value above 0.80 and close to 0.90 here but at the expense of reducing our precision which is not advisable.\n"
**Lets visualize which features are not much of help in detecting the anamoly **
"If we compare the individual values, we actually see that they are fairly close together when we consider the entire search grid! \n\n## Distribution of Scores\n\nLet's plot the distribution of scores for both models in a kernel density estimate plot."
"Bayesian optimization did not produce the highest individual score, but it did tend to spend more time evaluating ""better"" values of hyperparameters. __Random search got lucky and found the best values but Bayesian optimization tended to ""concentrate"" on better-scoring values__. That's pretty much what we expect: random search does a good job of exploring the search space which means it will probably happen upon a high-scoring set of values (if the space is not extremely high-dimensional) while Bayesian optimization will tend to focus on a set of values that yield higher scores. __If all you wanted was the conclusion, then you're probably good to go. If you really enjoy making plots and doing exploratory data analysis and want to gain a better understanding of how these methods work, then read on!__ In the next few sections, we will thoroughly explore these results.\n\nOur plan for going through the results is as follows:\n\n* Distribution of scores\n    * Overall distribution\n    * Score versus the iteration (did scores improve as search progressed)\n* Distribution of hyperparameters\n    * Overall distribution including the hyperparameter grid for a reference\n    * Hyperparameters versus iteration to look at _evolution_ of values\n* Hyperparameter values versus the score\n    * Do scores improve with certain values of hyperparameters (correlations)\n    * 3D plots looking at effects of 2 hyperparameters at a time on the score\n* Additional Plots\n    * Time to run each evaluation for Bayesian optimization\n    * Correlation heatmaps of hyperparameters with score\n    \nThere will be all sorts of plots: heatmaps, 3D scatterplots, density plots, bar charts (hey even bar charts can be helpful!)\n\nAfter going through the results, we will do a little meta-machine learning, and implement the best model on the full set of features."
"## Score versus Iteration\n\nNow, to see if either method improves over the course of the search, we need to plot the score as a function of the iteration. "
"Again keeping in mind that Bayesian optimization has not yet finished, we can see a clear upward trend for this method and no trend whatsoever for random search. \n\n### Linear Regression of Scores versus Iteration\n\nTo show that Bayesian optimization improves over time, we can regress the score by the iteration. Then, we can use this to extrapolate into the future, __a wildly inappropriate technique in this case, but fun nonetheless!__\n\nHere we use `np.polyfit` with a degree of 1 for the linear regression (you can compare the results with `LinearRegression`  from `sklearn.linear_model`."
~20% of entries for passenger age are missing. Let's see what the 'Age' variable looks like in general.
"Since ""Age"" is (right) skewed, using the mean might give us biased results by filling in ages that are older than desired. To deal with this, we'll use the median to impute the missing values. "
"## Step 1: Load in your packages\nTo help with the readability of your notebooks, it is helpful to have all of your packages loaded in at the top of your notebook. As you go through and find that you need more packages to accomplish what you want, go back to this first cell block, write in the code to import each new package, and re-run the cell. Then get back to coding."
"## Step 2: Where is the data?\nThis code block is an example of a for loop that walks through the directory for this competition and prints out all the file names. This is a lot of files though, so it's a good opportunity to show of %%capture. This hides the output of a cell block if you put it at the beginning of a cell!\n\nHere's what the first 5 lines would look like without %%capture:\n1. /kaggle/input/petfinder-pawpularity-score/sample_submission.csv\n2. /kaggle/input/petfinder-pawpularity-score/train.csv\n3. /kaggle/input/petfinder-pawpularity-score/test.csv\n4. /kaggle/input/petfinder-pawpularity-score/test/c978013571258ed6d4637f6e8cc9d6a3.jpg\n5. /kaggle/input/petfinder-pawpularity-score/test/4e429cead1848a298432a0acad014c9d.jpg"
"Note that the dataframe still contains the Id's of the photos. You won't be using this when you build your models.\n\nIt's also useful to first see the distribution of your target variable when you first start analyzing a dataset. In our case, that is the Pawpularity score, which is in the range of 1-100. One way to do this is with a simple histogram. "
"Looking at this histogram, note the skew in the distribution of the pawpularity scores. Interesting there is a small curve close to zero Pawpularity as well. Also, note the close to 300 pawpularity scores at 100! Start thinking about why this might be. \n* Is there something unique about these animals? \n* Is there a particular breed, color, age of animal that is most desirable by the people visiting the site?\n* Is there something about the way the photos were taken that is leading to more clicks and thus a higher Pawpularity score?\n* Does this have do to with the Pawpularity score itself? \n* Are these outliers that need to be removed from the training data to improve the models you will build?\n\nKeep these types of questions in mind as you go about solving this problem."
"### Now lets visualize the distribution of Pawpularity scores across each feature variable\n\nTo do this, we can use some simple box plots and histograms. Basically, we'll plot the Pawpularity scores on the y axes and the 0s and 1s of each feature variable on the x axes for the boxplots. For the histograms, we plot the pawpularity on the x axes and count the 0s and 1s at each Pawpularity score. This could help us visualize if 0s or 1s in each feature variable have an impact on the Pawpularity scores."
"Notice how the boxplots are almost identical? Going to be difficult for any algorithm to use this data to predict Pawpularity using this feature. With respect to the histogram, notice that there are many more 1 values than 0 values for this feature, but again distribution is very similar."
"### Quick Analysis\nYou can't always tell just from looking at chart if you're going to be able to build a highly predictive model, but intuition is telling me that models are not going to be great based on these charts. The reason is that the distribution of pawpularity scores is very similar for each variable and class - in other words changing the features doesn't seem to influence the pawpularity scores that much. This might mean that a competition winning solution to this problem will require using the images and not the .csv metadata provided."
Nice! We have read in and displayed our first image! Looks like a cute dog to me. It would be better if we could see the Pawpularity score along with each image though. Let's try that next. We'll put the Pawpularity score for each image as the title. We can get this because the name of the image files is also stored in the Id column in the .csv metadata. We have this in our train_df DataFrame from earlier.
"Cool we've used a for loop to visualize the first 3 images and their dimensions. Notice that image 0's dimensions are: (960, 720, 3), while image 2's dimensions are: (514, 315, 3). This tells us that we need to reshape or resize the images when we end up building our models. The 3 at the end tells that this is an RGB image where each color channel has pixel values between 0-255. Try out type(image_array) to show that it is a numpy array.\n\nTime to build on our loop to display the Pawpularity score of each associated image.\n\nTo do this, we need to get the image filenames without the directory and .jpg at the end. This will let us search the Id column in the train_df dataframe for corresponding Pawpularity scores. When you have a file path like: '../input/petfinder-pawpularity-score/train/7954ebb5c90d9618e34959df0ad5f062.jpg'and you want to get just that stem, you can use from pathlib import Path. "
"# Preparation \n\nOk, we need to load the packages..."
... and I would like to select an example to try out the preprocessing methods that are usually used. Let's load the data together with the path of the npy files:
# Understanding the data \n\n## How does the data look like?
"### Insights\n\nOk, the three signals originating from different detectors all look a bit different. How was this data generated? We are given...\n\n* detector noise from three real detectors (LIGO Hanford, LIGO Livingston, and Virgo). As far as I understand this noise is not simulated. \n* A simulated gravitational wave signal hidden in this noisy data in the case of hot targets. You can't see it with your eyes! "
"As a first step, I load all the modules that will be used in this notebook:"
"Then, I load the data. Once done, I also give some basic informations on the content of the dataframe: the type of the various variables, the number of null values and their percentage with respect to the total number of entries:"
and show the result on a chloropleth map:
"We see that the dataset is largely dominated by orders made from the UK.\n\n___\n### 2.2 Customers and products\n\nThe dataframe contains $\sim$400,000 entries. What are the number of users and products in these entries ?"
## Loading packages
## Loading data
"### 2.1) Importing Libraries \n\nThis is our very first step, we shall import the necessary libraries that we want into python. We can add libraries along the way. Always import the basic libraries first."
"### 2.2) Import Datasets \n\nOur second step is to import our datasets, in this case, we import train and test datasets. There are various ways to import your datasets into python. Here I give an overview of what train and test datasets mean.\n\n\n**Training and Testing Sets**\n\n   a) **Training data:** Training data is the information used to train an algorithm. The training data includes both input data and the corresponding expected output. We can train different machine learning algorithms on the training data, in the hopes of finding one best algorithm for a particular problem that predicts well. Bear in mind that even if our algorithm performs extremely well on the training data, it may be a result of overfitting, and may not replicate the same accuracy when the algorithm is applied to some unseen (test) data. \n\n   b) **Testing data:** On the other hand, includes only input data, not the corresponding expected output. The testing data is used to assess how well your algorithm was trained, and to estimate model properties.\n   \n\n     \nSo in our training data, we see that our second column says whether the passenger survived or not. The survived column is our output.  And in our test data, the output is not there, it is for us to TEST if our model developed based on the training set is accurate. However the preceding sentence seems to have a problem, IF, in the real world, we do not have the 'answers' for the test data, how then, would we be able to tell how well the training algorithm is performing on the test set. To resolve that issue, we usually perform train test split/cross validation method on the given training set first. See section 5.03 for more details."
"The code below is useful to understand if you want to plot a barplot using Seaborn package. You can mimic my code for future scenarios.\n\n\n1. We specify the plot size in the first line and name our axis survive_bar.\n\n2. I wish to use the package seaborn to plot my bar plot. So it is a good habit to look up the seaborn package online to see what parameters I should input.\n\n3. we want to input x = train[""Survived""].value_counts().index, meaning that in the x axis, we have the index of the survived - which is 0 and 1. Next, y = train[""Survived""].value_counts() means that in the y axis input the frequency counts for 0 and 1 respectively. Lastly, ax = survive_bar just means we want to input all our graph on this axis called survive_bar where we defined a size for survive_bar earlier. Note that you can define your own axis to your likings.\n\n4. survive_bar.set_xticklabels(['Not Survived', 'Survived']) means we manually change the x axis's variable name from 0 and 1 to Not survived and Survived. survive_bar.set_ylabel('Frequency Count') just means we label the y axis as Frequency Count.\n\n5. survive_bar.set_title('Count of Survival', fontsize = 16) simply means setting the title for our graph.\n\n6. The last chunk of long codes just gives the count labels inside the graph. Sometimes it is visually pleasing to label the counts on the graph."
The survive_bar plot gives you the information that 62% of the passengers died.
#### **Embarked Count**
Some interesting observations is that most of the passengers boarded from Southampton. 
"#### CODING classes gather here! \nI always thought that knowing how to make your graphs neater is a good skill to have. Let us try,\n\n#### Category variables count\n\nThe below plot is very useful:\n\n\n1. We first create a plot grid, with subplots on it, we define the figure size as usual, and now we pass some more conditions in it, we want to make plot multiple graphs on a grid of say 2 by 3 grid. In this way, if your code consists of repetitive graphing of a bar chart in the same fashion, we can just run a loop on this grid! \n\n2. For example, we want to plot a very simple frequency count for titanic's categorical variables, namely, survived, pclass, sex, sibsp, parch and embarked. So we can run the below code.\n\n3. Note for (u,v,z) in [[""Survived"",0,0],[""Pclass"",0,1],[""Sex"",0,2], [""SibSp"",1,0], [""Parch"",1,1],[""Embarked"",1,2]] just means for each element (u,v,z) in the list, we have u,v,z corresponding to each cross product in the list. Note, to say it explicitly, u is ""survived"", ""pclass"", etc.. v =0,1,2,..., z = 0,1,2,...\n\n    Now I am telling you that you need to pass 3 elements (u,v,z) in the for loop, but when you are creating the code the first time, you may not realise at first, so the computational thinking is you write out the codes like how you would manually:\n    \n    -sns.barplot(x = train[""Survived""].value_counts().index,y = train[""Survived""].value_counts(), ax  = myplot[0,0])\n    -sns.barplot(x = train[""Pclass""].value_counts().index,y = train[""Pclass""].value_counts(), ax  = myplot[0,1])\n    -sns.barplot(x = train[""Sex""].value_counts().index,y = train[""Sex""].value_counts(), ax  = myplot[0,2])\n    etc...\n    \n    So I suspect you need to loop through the categorical variables, survived, pclass, sex etc. And of course since you are plotting your graph on a 2x3 grid, we let our first graph appears in the place [0,0], second graph appears in the place [0,1], etc... So we want to loop through 3 things! So I should tell myself I need 3 elements to loop through in the for loop. And to do that it is now easy to see why I said: for (u,v,z) in [[""Survived"",0,0],[""Pclass"",0,1],...], However the [[..],[..],...] is hardcoded, but hey it is a good start to slowly cultivate your programming logic. \n    \n   I added on a bit of matplotlib codes, which should be relatively googlable, just want to add the same set of features onto each of my graph on myplot[v,z]. Lastly, adjust is to make some spacing between each graphs, I am very particular about visuals, although I am not very good at it yet."
"**Age Distribution**\n\nAge is a variable that we can create bins for, but that will be another topic for another time. For now, let us look at the distribution and see what the distribution is. But be reminded, the distribution we are showing here is the training data set, whereby there are 177 missing ages supposedly. We **have not account for the missing values**. We call this plot **preimputation**. We will definitely come back to this plot later to compare when we have imputed the missing values of age."
Let's import some standard libraries
We will demonstrate and compare different algorithms on diabetes dataset from sklearn.datasets. Let's load it.
We have managed to improve the results. But spent a lot of time on it. Let's look how our parameters have been changing from iteration to iteration:
"We can see that for example max_depth is the least important parameter it does not influence score significantly. But we are searching over 8 different values of max_depth, and with any fixed value search over other parameters. It is obvious waste of time and resources.\n\nLet's try a RandomizedSearch approach now."
Let's try to use RandomizedSearchCV from sklearn.model_selection.\n\nWe will start with very broad parameters space and make only 50 random steps:
"As we can see, the results are already better than GridSearchCV. We have spent less time and made more complete search. Let's look at our visualization:"
"As we can see every step is completely random. It helps not to spent time on useless parameters, but it still does not use the information gathered on the first steps to improve outcomes of the latter ones."
"So, without doing anything we have a **CV score** of **0.553**."
"# 4. PyTorch Dataset\n\nWe'll create a Dataset class called `ShopeeDataset` that will:\n1. Receive the metadata\n2. Read in the `image` and `title`\n3. Perform image augmentation and tokenization\n4. Return the necessary information to feed into the model afterwards\n\n\n### The Bert Tokenizer ([data from Abhishek Thakur](https://www.kaggle.com/abhishek/bert-base-uncased/code?datasetId=431504&sortBy=voteCount)):\n* Pretrained tokenizer that splits sentences into tokens (source from `transformers` library - [click here for more info](https://huggingface.co/transformers/preprocessing.html))\n* The output is as follows:\n    * `input_ids`: indices corresponding to each token in the sentence\n    * `attention_mask`: indicates to the model which tokens should be attended to, and which should not ([documentation on attention_mask here](https://huggingface.co/transformers/glossary.html#attention-mask))\n"
"Let's plot the data. Informally, the classification problem in this case is to build some ""good"" boundary separating the two classes (the red dots from the yellow). Machine learning for this case boils down to choosing a good separating border. A straight line will be too simple while some complex curve snaking by each red dot will be too complex and will lead us to making mistakes on new samples. Intuitively, some smooth boundary, or at least a straight line or a hyperplane, would work well on new data."
Let's try to separate these two classes by training an `Sklearn` decision tree. We will use `max_depth` parameter that limits the depth of the tree. Let's visualize the resulting separating boundary.
## Look at a few train/test input/output pairs \n\nThese are some of the pairs present in the training data. I use functions from Walter's excellent starter kernel to plot these pairs.
## Number frequency 
"From the above graph, we can clearly see that the number distribution has a string positive skew. Most numbers in the matrices are clearly 0. This is reflected by the dominance of black color in most matrices."
"## Loading packages \n\nFirst of all, let's load some packages..."
## Loading data \n\nNow we will use the digits of the digit-recognizer competition. Let's check: 
"Before we start with building targeted and non-targeted attacks, let's have a look at the first digits of the test set:"
Some digits are not easily recognized by human eyes... 
"# Preface\nPeople who have read other kernels of mine, would probably be surprised by this one. Indeed, this is **my very first Python kernel**. Am I going to abandon R? Certainly not! I just want to broaden my options and knowledge, and strongly believe that I will continue to use both in the future. My initial thoughts on this are:\n\n* Use R for EDAs. Although visualizations with Python are not bad, ggplot just looks better and I find it quite easy to compose visualizations with ggplot. Also, Rmarkdown offers neat additional features that Python does not have (such as Tabs, and automatic generation of the Table of Content).\n* Use R for statistics\n* Use R for creating interactive web apps (with Shiny)\n* Use Python for machine learning and deep learning\n* Use Python for big datasets. I know that R can handle big datasets too with data.table, but I am a big fan of tidyverse. The readability of code decreases when using data.table, and I chose not to learn it. \n* Chose R or Python depending on the libraries available. CRAN (R) has many more (data science) libraries than Python. I know for instance that there are domain specific packages for Finance. If R or Python offers a specific package that really suits the task at hand, I will likely choose the language with the best libraries on offer.\n\nInsideairbnb.com is a website on which web scraped datasets of ""snapshots"" of cities are published. I have downloaded the files of Amsterdam of the situation on December 6th, 2018. I thought it is a fun dataset to take on. Besides basic data wrangling and plots, I have also added **interactive Folium maps, interactive plotly graphs, and text mining of the review comments.**\n\n\n\n\n\n# Table of contents\n\n* [1. Loading libraries and data](#1.-Loading-libraries-and-data)\n  * [1.1 Loading libraries](#1.1-Loading-libraries)\n  * [1.2 The listings and listing details files](#1.2-The-listings-and-listing-details-files)\n* [2. Data exploration](#2.-Data-exploration)\n  * [2.1 Neighbourhoods](#2.1-Neighbourhoods)\n  * [2.2 Room types and property types](#2.2-Room-types-and-property-types)\n  * [2.3 Accommodates (number of people)](#2.3-Accommodates-(number-of-people))\n* [3. Advice to the municipality of Amsterdam](#3.-Advice-to-the-municipality-of-Amsterdam)\n  * [3.1 Finding possibly illegal hotels](#3.1-Finding-possibly-illegal-hotels)\n  * [3.2 Unwanted effects of professional hosts?](#3.2-Unwanted-effects-of-professional-hosts?)\n* [4. Advice to the tourists](#4.-Advice-to-the-tourists)\n  * [4.1 Average daily price per neighbourhood](#4.1-Average-daily-price-per-neighbourhood)\n  * [4.2 Neighbourhood safety](#4.2-Neighbourhood-safety)\n  * [4.3 Review scores location, and location scores versus price](#4.3-Review-scores-location,-and-location-scores-versus-price)\n  * [4.4 How to use review scores](#4.4-How-to-use-review-scores)\n  * [4.5 Finding a good host](#4.5-Finding-a-good-host)\n  * [4.6 Availability over time](#4.6-Availability-over-time)\n  * [4.7 Average price by date](#4.7-Average-price-by-date)\n* [5. Text mining the Review comments](#5.-Text-mining-the-Review-comments)\n\n# 1. Loading libraries and data\n## 1.1 Loading libraries"
"## 1.2 The listings and listing details files\nThe dataset that I created contains a total of 7 files. The listings file is an overview file that insideairbnb labels as ""good for visualizations"". The unique identifier in the dataset is the ""listing"" id. This is basically the id of the advertisement. Overall, there were 20,030 Airbnb-listings in Amsterdam on December 6th, 2018."
"We see that neighbourhood_group is useless in Amsterdam, as it contains zero non-null objects. Below, I getting rid of this variable, and am showing the head of the dataframe that I am going to work with."
"# 2. Data exploration\n## 2.1 Neighbourhoods\nNeighbourhood ""De Baarsjes"" holds most listings, and altogether eight neigbourhoods have over one thousand listings.\n\n"
"Below, you can see that most listings are in the city centre. This map is interactive, and you can zoom-in on the clusters to eventually find the individual locations of the listings.\n\nNote: In a later version I made a map on the average daily price per neighoudhood (section 4.1). I think this map would also be better at this point (as it uses the neighbourhood shapefile), but as my main goal of this kernel is to learn as much as possible I left the map below unchanged as FastMarkerCluster seems useful for the future."
"## 2.2 Room types and property types\n### 2.2.1 Room types\nThe room type is very important in Amsterdam, because Amsterdam has a rule that Entire homes/apartments can only be rented out via Airbnb for a maximum of 60 days a year. Below, we can see that this restriction applies to most of the listings."
"### 2.2.2 Property types\nIn the dataset, we find a lot of different property types."
"However, many of those property types have very few listings in Amsterdam. In the figure below, I am only displaying property types with at least 100 listings. As we can see, the vast majority of the properties in Amsterdam are apartments."
"## 2.3 Accommodates (number of people)\nAs expected, most listings are for 2 people. In addition, Airbnb uses a maximum of 16 guests per listing."
"However, Amsterdam has an additional restriction. Due to fire hazard considerations and also taking possible noisy group into account, owners are only allowed to rent their property to groups with a maximum of 4 people. This actually means that the listings that indicate that the maximum number of people is above 4 are breaking this rule!"
"## Basic Exploration for five securities\n\nLooking at Close values of some of the stocks. It is observed that sometimes all the stocks fall together but quantum of downside varies. But it is evident that with time, all stocks behave differently."
"## Advance to Decline Ratio\n\nAdvance and Decline ratio is defined as the number of periods a stock has advanced over number of periods it has declined. This is being examined to identify if there is a trend here. For example, if there is an indication that in a particular month, stock tend to generally decline or advance.\n\n**Observation** : There does not appear to be an evidence if Stock advancement is tied to a month."
"## Exploring and finding features\n\n- The next step is to explore features which could be helpful in predictive modelling. Here, we are looking at two variables, Volume and RSI. While the Volume is provided, RSI is a metric which has been found useful while studying movement of stock prices and is calculated.\n- Here, we see that stock could move either way (up or down) when volumes are high, this indicates heavy buying and heavy selling. This indicates that with high volumes, movement of stock is imminent, but the direction is not known.\n- On the other hand, an increasing RSI represents bullishness in a stock, while a decreasing RSI appears to push the prices down."
"## Closer look at RSI\n\nSince RSI appears to correlate better to Stock Prices, here is another way of analyzing this. The RSI values are categorized in Low, Medium and High category. Looking at the RSI categories, it appears that when RSI is in medium range, the price movement is most favourable. RSI in higher range is sometimes an indication of impending correction. \n\nThis looks promising and a strategy can be developed to remain invested in a stock when RSI values are in medium range. Or to be precise - **Do not buy a stock when RSI is in Low category.**\n\n**Note**: This phenomenon is quite pronounced for Security code 1377."
## RSI distribution\n\nA look at how RSI is distributed can also be helpful to understand this variable. This is heartening to see that RSI distribution is similar to the securities which are examined. This appears to be a variable which can be useful in model creation.
"## Exploring more of RSI\n\nSince RSI appears to be a good indicator, trying to see if this can help in predicting price increase. Here I am trying to see which RSI period helps in predicting the Target variable. This can be a feature for building models. This is done using calculating correlation between `Target` variable and RSI for various RSI periods between 5 to 20.\n\nThe results show that RSI period 5 has got best correlation followed by 19. This will be used as a feature for Machine Learning models.\n\n**Note: Below code is commented which generates this statistic. This is done to save processing time. Interestd ones can uncomment the code.**"
"## Return Analysis\n\nReturn Analysis gives the information that how much an unit investment is worth. This is an useful feature which tells the investor that how much her/his investment is worth today.\n\nThis helps in comparing the returns on a Normalized scale, since stock prices of various stocks vary, it is difficult to compare them. Calculating a return index gives a more holistic comparison of various securities."
"## Analyzing SMA (Simple Moving Averages) and EMA (Exponential Moving Averages)\n\nSMA and EMA are some well know pointers when it comes to Price Tracking and making decisions based on them. These methods help in identifying trends related to stock prices. While as the name suggests, SMA are jsut the average of a period where as EMA attach weights to the calculation and sensitive to recent price movements."
## Bollinger Bands\n\n- Bollinger Bands are another useful feature when designing strategies.\n- These are indicator of volatitility.\n- These can be used to draw a support curve and a resistance curve.\n- Share values remain within the curve most of the time (likely 95%).\n- This is done based on Moving Average (SMA) for 20 sessions.\n- We see that this behaviour is confirmed by the stocks chosen below. Often the hitting the support and resistance indicates that trend reversal is coming.
## Comparing daily returns of few securities (Volatility)\n\nIt is interesting to see how daily returns of various stocks fare with each other. Distributions of some stocks daily returns are plotted below. \n\n-  Max and Min range of daily returns are around -30 to +30%.\n-  One stock (1435) is more volatile than the others.\n- This is also seen that stocks who rise high also fall higher.
"## Examining VWAP\n\nVWAP is an important indicator used in trading setups. This is calculated by adding up the value traded for every transaction (price multiplied by the number of shares traded) and then dividing by the total shares traded.\n\nWe are looking to examine the impact of VWAP on Stock Price movement. Since this is being done to evaluate the various indicators as features, here I am looking to see the impact of previous day's VWAP on current day's target. The results are encouraging. "
"This concludes basic analysis of Volume ,RSI and Moving Averages on stock prices. Will be examining more features in due course of time. Stay tuned!"
- **No null values** present in the data!
"- Average customer **Age** in the dataset is in the late 30s i.e **38.85**. \n- Average **Annual Income (k\\$)** of the customers is **60.56** i.e just short of the 2018 median income of USA citizen, 63k$.\n- **Spending Score (1-100)** average of the mall customer is in the center with **50.20**."
### Distribution of Categorical and Numerical Features :
"- **Gender**, the only categorical feature, data displays a **normally distribution**.\n- Distribution of **Age** and **Annual Income (k\\$)** is **positively or righly skewed**.\n- **Spending Score (1-100)** data distribution is similar to the **Head and Shoulder** pattern observed in stock charts. \n- It displays a stock's price rising to a peak and then declines back to the base of the prior up-move. Something similar can be observed with the **2 shoulders** forming around the values **20 & 80** with **head** being centered in between **40 - 60**.\n- We will drop the **CustomerID** feature as it is just a number that is tagged to a customer."
### Categorical Features :
"- For the above dataset, **female** customers just edge out the **male** customers. "
### Numerical Features vs Categorical Features :
"- **Age** range of **female** customers is from **30 to just below 50** whereas **male** customers **Age** ranges from **just below 30 to 50**. \n- For both **Genders**, a bulge at the age of **30 - 35** can be observed. Median **Age** of **male** is slightly more than those of **female**.\n- For **Annual Income (k\\$)**, **female** customer's income starts from **40k** whereas **male** customer's is above this value. \n- Median **Annual Income (k\\$)** for both **Genders** is near about **60k**. **Annual Income (k\\$)** of **male** customers tapers very sharply at the apex with some outliers as compared to **female** customers.\n- Median **Spending Score (1-100)** of both **Genders** is same despite having different starting points of **just below 40** & **just above 20** for **female** and **male** customers respectively.\n- Both the **Genders** display a strong bulge at the median value especially **female** customers. However, **male** customers display a small but significant bulge in the range of **0 - 20** as well."
### Numerical Features vs Numerical Features w.r.t Categorical Feature :
"- **Annual Income (k\\$)** datapoints are present throughout the all the **Age** values, **Gender** does not provide any significant information. \n- For **Spending Score (1-100)**, it can be clearly observed that **Age** ranges **20 - 30** display very high spending habits. \n- **Age** group **30 - 40** highlights both the extremes of spending habits in the customer. **40 - 70 Age** group customer displays the other end of spending habits with low values.    \n- From the **Annual Income (k\\$) vs Spending Score (1-100)**, distinct 5 groups can be observed. For **Annual Income (k\\$)** values between **0 - 40**, data highlights 2 groups of customers with **Spending Score (1-100)** between **0 - 40** and **60 - 100**.\n- After displaying this 1 extreme, data highlights the middle group of customers that have an **Annual Income (k\\$)** between **40 - 70k** and **Spending Score (1-100)** between **40 - 60**.\n- **Annual Income (k\\$)** values between **70 - 140k** define the other extreme that is divided into 2 groups based on **Spending Score (1-100)** values of **0 - 40** and **60 - 100**."
\nLibraries And Utilities
\nData Loading
\nUnivariate analysis of continuous variables
\nUnivariate analysis of categorical variables
\nPlot a general overview of data
\nTransform numerical variables into categorical variables
\nRelationship between two continuous variables
\nRelationship between two categorical variables and one continuous variable
\nHow to display correlation between variables
\nNon usual plots
\n\n3D plots
\nGeographical plots
\nReferences
[Back to table of content](#table)
"Next to create our wordclouds, I will import the python module ""wordcloud"". "
But generating a normal wordcloud is rather boring so I would like to introduce to you a technique of importing pictures (something relevant) and using the outline of that picture as a mask for our wordclouds. Therefore the pictures that I have chosen are the ones I feel most representative for their authors:\n\n1. ) The Raven for Edgar Allen Poe 2.) Octopus Cthulu-thingy for HP Lovecraft and 3.) Frankenstein for Mary Shelly\n\nThe way I am loading in the pictures on Kaggle is a sort of a feature hack although readers familiar to my work know this trick. I first derive the Base64 encoding of whatever images I want to use and then use that particular encoding and re-convert the picture back on the notebook. The cell below contains the Base64 encoding of the three images I will use but I have hidden them so that I do not pollute this notebook with just long streteches of text - unhide them if you want to see the encoding.
# Importing Libraries 
# Loding Dataset
# Takeaways\n1. Plotting good visual for Eda\n2. Dyanic visual plots\n3. Plotly.express library
# Box Plots And Voilin plots
### Boxplot
### Violin Plot
### Histogram
## Multivaiate  plot of Age vs Categorical columns
- **child_mort : Economically backward nations have a high infant mortality rate!**
"- **Haiti** has the highest children's deaths. **African countries** have significant positions in this statistic.\n- At the other extreme of **child_mort**, countries from **Asia and Europe** has some solid presence. "
"- **exports : It is a very important factor for building the nation's economy. Higher the exports of a nation, stronger the nation's economy and more is the wealth generated by the nation!**"
"- **exports** of a nation are usually goods and services created domestically but sold to other nations. Goods and services exported depends on factors like the geographical location, natural resources, population size & their preference towards specific skills, etc.\n- Despite **Singapore's** population size not being in the top 100, they have the highest number of **exports**. **Luxembourg** & **Malta** have probably followed the same route.\n- **Afghanistan** & **Nepal** are present in the lower end of **exports**. Geographical locations of these nations have a heavy influence. Countries with lower exports also have small geographical area."
- **health : Citizens of the developed nations have higher income and thus they don't have a problem on spending more on healthy lifestyle!**
- **US** stands at the top when it comes to spending on health with **17%** of the individual GDP contribution.\n- **6%** seems to be the mean values of the citizens spending on their **health**.\n- **Asian** countries dominate the lower end of **health** with less than **3%**. They are the most ignorant citizens when it comes to health.
## Helper Functions\n
"# Non-Parametric\n\nCounterpart to Parametric models, Parametric models does not make any assumptions about the data generating process’ distribution. For example, in statistical test, Non-Parametric models utilize rank and medians, instead of the mean and variance! On the other hand, they function in a infinite space of parameters, making their name counter-intuitive, but also highlighting their practical approach to representation; enabling them to increase their flexibility indefinitely."
"### Dear all,\nThe objective of this notebook is to shed some light on the question of *How good is my score in the Titanic competition?*\nWe all know the feeling of clicking on the '*Jump to your position on the leaderboard*', and we get a score of 0.7xxx (well, as least I do) then the next thing we do is scroll up and see all these 1.00000 results and we feel that \nwe are doing something wrong. \n\nThe first thing to say is that these *1.00000* submissions are cheating! Basically it works because the *hidden* data against which your submission is tested is actually [*public*](https://en.wikipedia.org/wiki/Passengers_of_the_RMS_Titanic#Passenger_list) data, which can be found on Wikipedia, etc. (it is the *Titanic* after all). In other words, if you hand-craft the submission file you will score a 1, with no machine learning required; hardly any need for python or R code, no need for trying new methods and learning new techniques, etc. etc.\n\nWhat is the point in doing that? **I really don't know...**\n\nOk, so, how good is my score? First we shall load in a snapshot of the leaderboard. This data can be downloaded directly from [the Titanic leaderboard page ](https://www.kaggle.com/c/3136/publicleaderboarddata.zip) where it says Raw Data.\n"
"Firstly, we shall make a frequency plot of the *whole* leaderboard to get an overall feeling for the data:"
"We can see that the majority of the scores lie between `0.6` and `0.85`, so we shall zoom in on that area:"
"We can clearly see a distribution of scores, and in particular, three interesting peaks.\n### The 0.76555 peak:\nThis is by far the highest peak and corresponds to correctly classifying 320 results. This peak is due to people submitting the default `gender_submission.csv` file provided by the competition. The submission of this file alone represents more than 20% of the results that are seen on the leaderboard. This is not entirely surprising, given that the excellent [Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) by Alexis Cook suggests doing this as an exercise."
"## General information\n\nIn this kernel I work with IEEE Fraud Detection competition.\n\nEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.\n\nWe have a binary classification problem with a heavy imbalance which is an inherent property of such problems.\nAt first I'll explore the data and try to find valuable insights, maybe I'll do some feature engineering and then it wil be time to build models.\n\n![](https://cis.ieee.org/images/files/slideshow/abstract01.jpg)\n\n*Work in progress*"
Importing libraries
## Functions used in this kernel\nThey are in the hidden cell below.
"## Data loading and overview\n\nData is separated into two datasets: information about the identity of the customer and transaction information. Not all transactions belong to identities, which are available. Maybe it would be possible to use additional transactions to generate new features."
"## Data Exploration\nLet's start with identity information.\nid_01 - id_11 are continuous variables, id_12 - id_38 are categorical and the last two columns are obviously also categorical."
`id_01` has an interesting distribution: it has 77 unique non-positive values with skeweness to 0.
22% of values in `id_11` are equal to 100and 76% are missing. Quite strange.
"Some of features seem to be normalized. So if someone wants to normalize all variables, it would be necessary to separate such variables which seem to be already normalized."
Here we can see some information about client's device. It is important to be careful here - some of info could be for old devices and may be absent from test data.\n\nNow let's have a look at transaction data.
"A very important idea: it seems that train and test transaction dates don't overlap, so it would be prudent to use time-based split for validation.\nThis was already noted in abother kernel: https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda"
"# Table of content\n\n1. Introduction - Loading libraries and dataset\n2. Exploratory analysis, engineering and cleaning features - Bi-variate analysis\n3. Correlation analysis - Tri-variate analysis\n4. Predictive modelling, cross-validation, hyperparameters and ensembling\n5. Submitting results\n6. Credits\n\n### Check other Kaggle notebooks from [Yvon Dalat](https://www.kaggle.com/ydalat):\n* [Titanic, a step-by-step intro to Machine Learning](https://www.kaggle.com/ydalat/titanic-a-step-by-step-intro-to-machine-learning): **a practice run ar EDA and ML-classification**\n* [HappyDB, a step-by-step application of Natural Language Processing](https://www.kaggle.com/ydalat/happydb-what-100-000-happy-moments-are-telling-us): **find out what 100,000 happy moments are telling us**\n* [Work-Life Balance survey, an Exploratory Data Analysis of lifestyle best practices](https://www.kaggle.com/ydalat/work-life-balance-best-practices-eda): **key insights into the factors affecting our work-life balance**\n*  [Work-Life Balance survey, a Machine-Learning analysis of best practices to rebalance our lives](https://www.kaggle.com/ydalat/work-life-balance-predictors-and-clustering): **discover the strongest predictors of work-life balance**\n\n**Interested in more facts and data to balance your life, check the [360 Living guide](https://amzn.to/2MFO6Iy) ![360 Living: Practical guidance for a balanced life](https://images-na.ssl-images-amazon.com/images/I/61EhntLIyBL.jpg)**\n\n**Note:** Ever feel burnt out? Missing a deeper meaning? Sometimes life gets off-balance, but with the right steps, we can find the personal path to authentic happiness and balance.\n[Check out how Machine Learning and statistical analysis](https://www.amazon.com/dp/B07BNRRP7J?ref_=cm_sw_r_kb_dp_TZzTAbQND85EE&tag=kpembed-20&linkCode=kpe) sift through 10,000 responses to help us define our unique path to better living.\n\n# 1. Introduction - Loading libraries and dataset\nI created this Python notebook as the learning notes of my first Machine Learning project.\nSo many new terms, new functions, new approaches, but the subject really interested me; so I dived into it, studied one line of code at a time, and captured the references and explanations in this notebook.\n\nThe algorithm itself is a fork from **Anisotropic's Introduction to Ensembling/Stacking in Python**, a great notebook in itself.\nHis notebook was itself based on **Faron's ""Stacking Starter""**, as well as **Sina's Best Working Classfier**. \nI also used multiple functions from **Yassine Ghouzam**.\nI added many variations and additional features to improve the code (as much as I could) as well as additional visualization.\n\nSome key take away from my personal experiments and what-if analysis over the last couple of weeks:\n\n* **The engineering of the right features is absolutely key**. The goal there is to create the right categories between survived and not survived. They do not have to be the same size or equally distributed. What helped best is to group together passengers with the same survival rates.\n\n* ** I tried many, many different algorightms. Many overfit the training data** (up to 90%) but do not generate more accurate predictions with the test data. What worked better is to use the cross-validation on selected algotirhms. It is OK to select algorithms with various results as there is strenght in diversity. \n\n* **Lastly, the right ensembling was best achieved** with a votingclassifier with soft voting parameter\n\nOne last word: please use this kernel as a first project to practice your ML/Python skills. I shameless ley sotle and learnt from many Kagglers through my learning process, please do the same with the code in this kernel.\n\nI also welcome your comments, questions and feedback.\n\nYvon\n\n## 1.1. Importing Library"
## 1.2. Loading dataset
## 1.4. A very first look into the data
"This is only a quick of the relationships between features before we start a more detailed analysis.\n\n\n# 2. Exploratory Data Analysis (EDA), Cleaning and Engineering features\n\nWe will start with a standard approach of any kernel: correct, complete, engineer the right features for analysis.\n\n## 2.1. Correcting and completing features\n### Detecting and correcting outliers\nReviewing the data, there does not appear to be any aberrant or non-acceptable data inputs.\n\nThere are potential outliers that we will identify (steps from Yassine Ghouzam):\n* It creates firset a function called detect_outliers, implementing the Tukey method\n* For each column of the dataframe, this function calculates the 25th percentile (Q1) and 75th percentile (Q3) values.\n* The  interquartile range (IQR) is a measure of statistical dispersion, being equal to the difference between the 75th and 25th percentiles, or between upper and lower quartiles.\n* Any data points outside 1.5 time the IQR (1.5 time IQR below Q1, or 1.5 time IQR above Q3), is considered an outlier.\n* The outlier_list_col column captures the indices of these outliers. All outlier data get then pulled into the outlier_indices dataframe.\n* Finally, the detect_outliers function will select only the outliers happening multiple times. This is the datadframe that will be returned."
## 2.3 Feature Engineering - Bi-variate statistical analysis\n\nOne of the first tasks in Data Analytics is to **convert the variables into numerical/ordinal values**.\nThere are multiple types of data\n\n**a) Qualitative data: discrete**\n* Nominal: no natural order between categories. In this case: Name\n* Categorical: Sex\n\n**b) Numeric or quantitative data**\n* Discrete: could be ordinal like Pclass or not like Survived.\n* Continuous. e.g.: age\nMany feature engineering steps were taken from Anisotropic's excellent kernel.\n\n### Pclass
Embarked does not seem to have a clear impact on the survival rate. We will analyse it further in the next sections and drop it if we cannot demonstrate a proven relationship to Survived. \n\n### Name_length
"The first graph shows the amount of people by Name_length.\n\nThe second one, their average survival rates.\n\nThe proposed categories are: less than 23 (mostly men), 24 to 28, 29 to 40, 41 and more (mostly women).\nThe categories are sized to group passengers with similar Survival rates."
### Age
The best categories for age are:\n* 0:  Less than 14\n* 1:  14 to 30\n* 2:  30 to 40\n* 3:  40 to 50\n* 4:  50 to 60\n* 5:  60 and more
### Family: SibSp and Parch\n\nThis section creates a new feature called FamilySize consisting of SibSp and Parch.
"IsAlone does not result in a significant difference of survival rate. In addition, the slight difference between men and women go in different direction, i.e. IsAlone alone is not a good predictor of survival. O will drop this feature.\n\n### Fare"
"# Predicting Molecular Properties\nCan you measure the magnetic interactions between a pair of atoms?\n\nIn this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant).\n\n![](http://www.chem.ucalgary.ca/courses/350/Carey5th/Ch13/coupling04.gif)\n\n**NOTE : Some (but not all) of the text in this kernel was taken from the competition details. I do this to show exactly what the description and rules are for the competition alongside some exploritory code. Be sure to read the competition details yourself directly from the website here: https://www.kaggle.com/c/champs-scalar-coupling/overview/description. **"
"# The Data\n\nIn this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files.\n\nFor this competition, you will not be predicting all the atom pairs in each molecule rather, you will only need to predict the pairs that are explicitly listed in the train and test files. For example, some molecules contain Fluorine (F), but you will not be predicting the scalar coupling constant for any pair that includes F.\n\nThe training and test splits are by molecule, so that no molecule in the training data is found in the test data."
"\n    It can be seen that dataset has successfully imported. In the dataset, there are 31 columns with 119040 observations. The details of each variables also can be seen above. After examining columns and column data types, the next step will inspect null values in each column.\n\n"
"\n    Since most of the columns have many null values, and this notebook only uses a few columns, the unused columns will be dropped.\n\n"
"\n    Based on the results above, it can be concluded that:\n    \n        \n            From the results of the average CV score, it can be said that the model's performance is good. In addition, from CV standard deviation value, it can be concluded that the model has very minimum variance between each CV scores.\n            From evaluation metrics score, it can be seen that RMSE and MAE value is close to 0. In addition, the R squared value also reveals that 77% of the variability observed in the target variable is explained by the regression model. Based on those results, it can be concluded that the model has good performance.\n            Based on the intercept and coefficients result, the linear regression formula will be:\n            $$ .: y = 10.6635 + 0.9205x :. $$\n            with:\n                \n                    $ x $ = independent variable (MinTemp)\n                    $ y $ = dependent variable (MaxTemp)\n                \n        \n    \n\n"
"\n    The following plot will compare the actual and predicted temperatures. Due to large amount of data, the following plot will use the first 15 data as samples to compare actual and predicted temperatures.\n\n"
# Set up environment
"# Detect TPU\nWhat we're doing with our code here is making sure that we'll be sending our data across a TPU. What you're looking for is a printout of `Number of replicas: 8`, corresponding to the 8 cores of a TPU. If your printout instead says `Number of replicas: 1` you likely do not have TPUs enabled in your notebook.   \n\nTo enable TPUs navigate to the panel on the right and click on `Accelerator`. Choose TPU from the dropdown.  \n\nIf you'd like more TPU troubleshooting and optimization guidelines check out our **[Learn With Me: Troubleshooting and Optimizing TPUs video](https://youtu.be/BSeWHzjMHMU)**.  "
# Check sales for holidays
# The average holiday sales are equivalent to Saturday and Sunday sales.
## What we want?\n1. Gathering Data\n2. Analysis the target and understand what is the important features\n3. Looking for missing values\n4. Feature Engineering\n5. Converting categorical to numerical\n6. Modeling
## 1. Gathering Data
"### Are we need a specialist or a broker to know what are the most important features that affect home prices?\nOf course not, we can know the important features by sea. So let's go and explore the data.\n"
"Ok, now as you see the correlation between features.. The colours show to us the strong and weak correlation.\nBut what we really need? we need the highest correlation between features and SalesPrice, so let's do it."
"#### What we note?\n* It's important to know what you do and how benefit from it. We can see 'OverQual' in the top of highest correlation it's 0.79!\n* 'GarageCars' & 'GarageArea' like each other (correlation between them is 0.88) \n* 'TotalBsmtSF' & '1stFlrSF' also like each other (correlation betwwen them is 0.82), so we can keep either one of them or add the1stFlrSF to the Toltal.\n* 'TotRmsAbvGrd' & 'GrLivArea' also has a strong correlation (0.83), I decided to keep 'GrLivArea' because it's correlation with 'SalePrice' is higher.\n"
#### ok let's focus on the features have highest correlation.
"Now, We explored the data and know the important features."
"# Quora Question-pair classification\n\nThis competition is about modelling whether a pair of questions on Quora is asking the same question. For this problem we have about **400.000** training examples. Each row consists of two sentences and a binary label that indicates to us whether the two questions were the same or not.\n\nInspired by this nice [kernel](https://www.kaggle.com/arthurtok/d/mcdonalds/nutrition-facts/super-sized-we-macdonald-s-nutritional-metrics) from [Anisotropic](https://www.kaggle.com/arthurtok) I've added a few interactive 2D and 3D scatter plots.\nTo get an insight into how the duplicates evolve over the number of words in the questions, I've added a plotly animation that encodes number of words and word share similarity in a scatter plot.\n\n**We will be looking in detail at:**\n\n* question pair TF-IDF encodings\n* basic feature engineering and their embeddings in lower dimensional spaces\n* parallel coordinates visualization\n* model selection and evaluation + sample submission.\n\nIf you like this kernel, please upvote it :D, thanks!\n\n\n----------\n\n\nAdded a final section for cross-validated model selection and evaluation. We will look at standard binary classification metrics, like ROC and PR curves and their AUCs. The best (linear) model that we found then generates a submission."
So we have six columns in total one of which is the label.
We have a fairly balanced dataset here.
# Feature construction\n\nWe will now construct a basic set of features that we will later use to embed our samples with.\n\nThe first we will be looking at is rather standard TF-IDF encoding for each of the questions. In order to limit the computational complexity and storage requirements we will only encode the top terms across all documents with TF-IDF and also look at a subsample of the data.
"The subsample still has a very similar label distribution, ok to continue like that, without taking a deeper look how to achieve better sampling than just taking the first rows of the dataset.\n\nCreate a dataframe where the top 50% of rows have only question 1 and the bottom 50% have only question 2, same ordering per halve as in the original dataframe."
## Feature EDA\n\nLet us now construct a few features\n\n* character length of questions 1 and 2\n* number of words in question 1 and 2\n* normalized word share count.\n\nWe can then have a look at how well each of these separate the two classes.
"The distributions for normalized word share have some overlap on the far right hand side, meaning there are quite a lot of questions with high word similarity but are both duplicates and non-duplicates."
Scatter plot of question pair character lengths where color indicates duplicates and the size the word share coefficient we've calculated earlier.
"# Animation over average number of words\n\nFor that we will calculate the average number of words in both questions for each row.\n\nIn the end we want to have a scatter plot, just like the one above, but giving us one more dimension, in that case the average number of words in both questions. That will allow us to see the dependence on that variable. We also expect that as the number of words is increased, the character lengths of Q1 and Q2 will increase."
"We have >21,000 images! Hopefully, we can develop a highly-predictive, robust, and generalizable model with this dataset. \n\nLet's check the distribution of the different classes:"
"In this case, we have 5 labels (4 diseases and healthy):\n0. Cassava Bacterial Blight (CBB)\n1. Cassava Brown Streak Disease (CBSD)\n2. Cassava Green Mottle (CGM)\n3. Cassava Mosaic Disease (CMD)\n4. Healthy\n\nIn this case label 3, [Cassava Mosaic Disease (CMD)](https://en.wikipedia.org/wiki/Cassava_mosaic_virus) is the most common label. This imbalance may have to be addressed with a weighted loss function or oversampling. I might try this in a future iteration of this kernel or in a new kernel.\n\nLet's check an example image to see what it looks like:"
## Import Libraries
"To start exploring your data, you’ll need to start by actually loading in your data. You’ll probably know this already, but thanks to the Pandas library, this becomes an easy task: you import the package as pd, following the convention, and you use the read_csv() function, to which you pass the URL in which the data can be found and a header argument. This last argument is one that you can use to make sure that your data is read in correctly: the first row of your data won’t be interpreted as the column names of your DataFrame.\n\nAlternatively, there are also other arguments that you can specify to ensure that your data is read in correctly: you can specify the delimiter to use with the sep or delimiter arguments, the column names to use with names or the column to use as the row labels for the resulting DataFrame with index_col."
"#### Dendrogram\n\nThe dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:"
"The dendrogram uses a hierarchical clustering algorithm (courtesy of scipy) to bin variables against one another by their nullity correlation (measured in terms of binary distance). At each step of the tree the variables are split up based on which combination minimizes the distance of the remaining clusters. The more monotone the set of variables, the closer their total distance is to zero, and the closer their average distance (the y-axis) is to zero.\n\nTo interpret this graph, read it from a top-down perspective. Cluster leaves which linked together at a distance of zero fully predict one another's presence—one variable might always be empty when another is filled, or they might always both be filled or both empty, and so on. In this specific example the dendrogram glues together the variables which are required and therefore present in every record.\n\nCluster leaves which split close to zero, but not at it, predict one another very well, but still imperfectly. If your own interpretation of the dataset is that these columns actually are or ought to be match each other in nullity , then the height of the cluster leaf tells you, in absolute terms, how often the records are ""mismatched"" or incorrectly filed—that is, how many values you would have to fill in or drop, if you are so inclined.\n\nAs with matrix, only up to 50 labeled columns will comfortably display in this configuration. However the dendrogram more elegantly handles extremely large datasets by simply flipping to a horizontal configuration."
" Observations: \nThe following columns are the one's that show the greatest correlation with our diagnosis column. There are two things that can be done. \n* We can either use only the columns which have greatest correlation, or we can continue to use all the columns.\n* I will be using all these columns to predict our result\n* You can eliminate a few and see if the accuracy improves!"
" Observations: \nLooks wonderful, isn't it! \n* There are only a handful of columns that show negative correlation with the 'diagnosis column'\n* Around half of our columns are more than 50% positively correlated to diagnosis column.\n\nWe have to select which of the attributes we want to use in building our model!\n"
"# Introduction\n\nThis competition is hosted by the third largest insurance company in Brazil: [Porto Seguro](https://en.wikipedia.org/wiki/Porto_Seguro_S.A.) with the task of predicting the *probability that a driver will initiate an insurance claim in the next year.*\n\nThis notebook will aim to provide some interactive charts and analysis of the competition data by way of the Python visualisation library Plot.ly and hopefully bring some insights and beautiful plots that others can take and replicate. Plot.ly is one of the main products offered by the software company - [Plotly](https://plot.ly/) which specializes in providing online graphical and statistical visualisations (charts and dashboards) as well as providing an API to a whole rich suite of programming languages and tools such as Python, R, Matlab, Node.js etc.\n\nListed below for easy convenience are links to the various Plotly plots in this notebook:\n\n* Simple horizontal bar plot - Used to inspect the Target variable distribution\n* Correlation Heatmap plot  - Inspect the correlation between the different features\n* Scatter plot - Compare the feature importances generated by Random Forest and Gradient-Boosted model\n* Vertical bar plot - List in Descending order, the importance of the various features\n* 3D Scatter plot \n\nThe themes in this notebook can be briefly summarized follows:\n\n   [**1. Data Quality Checks**](#quality) - Visualising and evaluating all missing/Null values (values that are -1)\n\n**2. Feature inspection and filtering** - Correlation and feature Mutual information plots against the target variable. Inspection of the Binary, categorical and other variables.\n\n**3. Feature importance ranking via learning models** \n/n Building a Random Forest and Gradient Boosted model to help us rank features based off the learning process.\n\nLet's Go"
Let us load in the training data provided using Pandas:
"**Target variable inspection**\n\nAnother standard check normally conducted on the data is with regards to our target variable, where in this case, the column is conveniently titled ""target"". The target value also comes by the moniker of class/label/correct answer and is used in supervised learning models along with the corresponding data that is given (in our case all our train data except the id column) to learn the function that best maps the data to our target in the hope that this learned function can generalize and predict well with new unseen data."
"Hmmn, the target variable is rather imbalanced so it might be something to keep in mind. An imbalanced target will prove quite"
"## Correlation plots\n\nAs a starter, let us generate some linear correlation plots just to have a quick look at how a feature is linearly correlated to the next and perhaps start gaining some insights from here. At this juncture, I will use the seaborn statistical visualisation package to plot a heatmap of the correlation values. Conveniently, Pandas dataframes come with the corr() method inbuilt, which calculates the Pearson correlation. Also as convenient is Seaborn's way of invoking a correlation plot. Just literally the word ""heatmap""\n\n**Correlation of float features**"
"From the correlation plot, we can see that the majority of the features display zero or no correlation to one another. This is quite an interesting observation that will warrant our further investigation later down. For now, the paired features that display a positive linear correlation are listed as follows:\n\n**(ps_reg_01, ps_reg_03)**\n\n**(ps_reg_02, ps_reg_03)**\n\n**(ps_car_12, ps_car_13)**\n\n**(ps_car_13, ps_car_15)**"
"**Correlation of integer features**\n\nFor the columns of interger datatype, I shall now switch to using the Plotly library to show how one can also generate a heatmap of correlation values interactively. Much like our earlier Plotly plot, we generate a heatmap object by simply invoking the ""go.Heatmap"". Here we have to provide values to three different axes, where x and y axes take in the column names while the correlation value is provided by the z-axis. The colorscale attribute takes in keywords that correspond to different color palettes that you will see in the heatmap where in this example, I have used the Greys colorscale (others include Portland and Viridis - try it for yourself). "
"Similarly, we can observe that there are a huge number of columns that are not linearly correlated with each other at all, evident from the fact that we observe quite a lot of 0 value cells in our correlation plot. This is quite a useful observation to us, especially if we are trying to perform dimensionality reduction transformations such as Principal Component Analysis (PCA), this would require a certain degree of correlation  . We can note some features of interest are as follows:\n\n***Negatively correlated features*** : ps_ind_06_bin, ps_ind_07_bin,  ps_ind_08_bin,  ps_ind_09_bin\n\nOne interesting aspect to note is that in our earlier analysis on nullity, ps_car_03_cat and ps_car_05_cat were found to contain many missing or null values. Therefore it should come as no surprise that both these features show quite a strong positive linear correlation to each other on this basis, albeit one that may not really reflect the underlying truth for the data."
"# **W & B Artifacts**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)"
# **Basic Statistics of Features**
# **Target Variable Distribution**
# **Target Class Balance**
# **Distribution of features**
\n# **Logging to W & B environment**
## Date of Recordings
## Birds Seen
## Pitch
## Sampling Rate\n\nSampling rate (audio) or sampling frequency defines the number of samples per second.
## Volume
"## Channels\nChannel is the passage way a signal or data is transported.One Channel is usually referred to as mono, while more Channels could either indicate stereo, surround sound and the like."
## Recordist\nLet us find out the number of people who provided the recordings
We import the standard Keras library
"Loading the train and test files, as usual"
\n### 2. Data Loading and Description
\n#### 2.1  Loading the data files 
\n#### 3.3 Dealing with Missing values
\n#### 3.4 Transforming Sex
"### 1.5 Entities Extraction  \nThe extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of parts of speech (POS) tags. The nouns and the proper nouns would be our entities.\n\nHowever, when an entity spans across multiple words, then POS tags alone are not sufficient. We need to parse the dependency tree of the sentence.\n\nTo build a knowledge graph, the most important things are the nodes and the edges between them.\n\nThese nodes are going to be the entities that are present in the Wikipedia sentences. Edges are the relationships connecting these entities to one another. We will extract these elements in an unsupervised manner, i.e., we will use the grammar of the sentences.\n\nThe main idea is to go through a sentence and extract the subject and the object as and when they are encountered. However, there are a few challenges ⁠— an entity can span across multiple words, eg., “red wine”, and the dependency parsers tag only the individual words as subjects or objects.\n\nSo, I have created a function below to extract the subject and the object (entities) from a sentence while also overcoming the challenges mentioned above. I have partitioned the code into multiple chunks for your convenience:"
"**Chunk 1**\n\nDefined a few empty variables in this chunk. prv_tok_dep and prv_tok_text will hold the dependency tag of the previous word in the sentence and that previous word itself, respectively. prefix and modifier will hold the text that is associated with the subject or the object.\n\n**Chunk 2**\n\nNext, we will loop through the tokens in the sentence. We will first check if the token is a punctuation mark or not. If yes, then we will ignore it and move on to the next token. If the token is a part of a compound word (dependency tag = “compound”), we will keep it in the prefix variable. A compound word is a combination of multiple words linked to form a word with a new meaning (example – “Football Stadium”, “animal lover”).\n\nAs and when we come across a subject or an object in the sentence, we will add this prefix to it. We will do the same thing with the modifier words, such as “nice shirt”, “big house”, etc.\n\n**Chunk 3**\n\nHere, if the token is the subject, then it will be captured as the first entity in the ent1 variable. Variables such as prefix, modifier, prv_tok_dep, and prv_tok_text will be reset.\n\n**Chunk 4**\n\nHere, if the token is the object, then it will be captured as the second entity in the ent2 variable. Variables such as prefix, modifier, prv_tok_dep, and prv_tok_text will again be reset.\n\n**Chunk 5**\n\nOnce we have captured the subject and the object in the sentence, we will update the previous token and its dependency tag.\n\nLet’s test this function on a sentence:"
"Well, this is not exactly what we were hoping for (still looks quite a sight though!).\n\nIt turns out that we have created a graph with all the relations that we had. It becomes really hard to visualize a graph with these many relations or predicates.\n\nSo, it’s advisable to use only a few important relations to visualize a graph. I will take one relation at a time. Let’s start with the relation “composed by”:"
"That’s a much cleaner graph. Here the arrows point towards the composers. For instance, A.R. Rahman, who is a renowned music composer, has entities like “soundtrack score”, “film score”, and “music” connected to him in the graph above.\n\nLet’s check out a few more relations.\n\nNow I would like to visualize the graph for the “written by” relation:"
"This knowledge graph is giving us some extraordinary information. Guys like Javed Akhtar, Krishna Chaitanya, and Jaideep Sahni are all famous lyricists and this graph beautifully captures this relationship.\n\nLet’s see the knowledge graph of another important predicate, i.e., the “released in”:"
## Preparation 
## Discrete and Continuous Variables 
"Here we visualize a PMF of a binomial distribution. You can see that the possible values are all integers. For example, no values are between 50 and 51. \n\nThe PMF of a binomial distribution in function form:\n\n![](http://reliabilityace.com/formulas/binomial-pmf.png)\n\nSee the ""[Distributions](#3)"" sections for more information on binomial distributions."
### PDF (Probability Density Functions)
"The PDF is the same as a PMF, but continuous. It can be said that the distribution has an infinite number of possible values. Here we visualize a simple normal distribution with a mean of 0 and standard deviation of 1.\n\nPDF of a normal distribution in formula form:\n\n![](https://www.mhnederlof.nl/images/normalpdf.jpg)"
### CDF (Cumulative Distribution Function)
"The CDF maps the probability that a random variable X will take a value of less than or equal to a value x (P(X ≤  x)). CDF's can be discrete or continuous. In this section we visualize the continuous case. You can see in the plot that the CDF accumulates all probabilities and is therefore bounded between 0 ≤ x ≤ 1.\n\nThe CDF of a normal distribution as a formula:\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/187f33664b79492eedf4406c66d67f9fe5f524ea)\n\n*Note: erf means ""[error function](https://en.wikipedia.org/wiki/Error_function)"".*"
## Distributions 
"A Binomial Distribution has a countable number of outcomes and is therefore discrete.\n\nBinomial distributions must meet the following three criteria:\n\n1. The number of observations or trials is fixed. In other words, you can only figure out the probability of something happening if you do it a certain number of times.\n2. Each observation or trial is independent. In other words, none of your trials have an effect on the probability of the next trial.\n3. The probability of success is exactly the same from one trial to another.\n\nAn intuitive explanation of a binomial distribution is flipping a coin 10 times. If we have a fair coin our chance of getting heads (p) is 0.50. Now we throw the coin 10 times and count how many times it comes up heads. In most situations we will get heads 5 times, but there is also a change that we get heads 9 times. The PMF of a binomial distribution will give these probabilities if we say N = 10 and p = 0.5. We say that the x for heads is 1 and 0 for tails.\n\nPMF:\n\n![](http://reliabilityace.com/formulas/binomial-pmf.png)\n\nCDF:\n\n![](http://reliabilityace.com/formulas/binomial-cpf.png)\n\n\nA **Bernoulli Distribution** is a special case of a Binomial Distribution.\n\nAll values in a Bernoulli Distribution are either 0 or 1. \n\nFor example, if we take an unfair coin which falls on heads 60 % of the time, we can describe the Bernoulli distribution as follows:\n\np (change of heads) = 0.6\n\n1 - p (change of tails) = 0.4\n\nheads = 1\n\ntails = 0\n\nFormally, we can describe a Bernoulli distribution with the following PMF (Probability Mass Function):\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a9207475ab305d280d2958f5c259f996415548e9)\n"
### Poisson Distribution
## Setup
"Kaggle is a hot spot for what is trending in data science and machine learning.\n\nDue to its competitiveness, the top players are constantly looking for new tools, technologies, and frameworks that give them an edge over others. If a new package or an algorithm delivers actionable value, there is a good chance it receives immediate adoption and becomes popular.\n\nThis post is about 7 of such trendies packages that are direct replacements for many tools and technologies that are either outdated or need urgent upgrades."
A simple GroupBy operation:
# 3. Lazypredict
"## References\n\n해당 커널은 아래의 멋진 분석 커널을 참고하였습니다.  \n멋진 분석을 제공해준 모든 분들께 감사드립니다. 항상 많은 도움이 되고 있습니다.\n\n* [colab, 전태균님의 타이타닉 분석](https://colab.research.google.com/drive/1cqv5yD9uLHHrVFL-TGM9NPSD1ZyF4AC1#scrollTo=jwBNmHcw3w91)\n* [블로그, 이유한님의 타이타닉 분석 튜토리얼](http://kaggle-kr.tistory.com/17?category=821486)\n* [공개 커널, Keras를 이용한 간단한 모델 구현!](https://www.kaggle.com/everystep/keras)"
## 1. 데이터 셋 확인\n---\n일단 필요한 패키지(라이브러리)를 import 합니다.\n
"train set과 test set에 비슷하게 `Age` 피쳐에 약 20%, `Cabin` 피쳐에 약 80%의  \n결측치가 있음을 확인할 수 있습니다.\n\n### **1.2 Target Label, `Survived` 확인**\n\ntarget label 이 어떤 distribution 을 가지고 있는 지 확인해봐야 합니다.  \n지금 같은 binary classification 문제의 경우에서,  \n1과 0의 분포가 어떠냐에 따라 모델의 평가 방법이 달라 질 수 있습니다."
"target label 의 분포가 제법 균일(balanced)합니다.  \n불균일한 경우, 예를 들어서 100중 1이 99, 0이 1개인 경우에는  \n만약 모델이 모든것을 1이라 해도 정확도가 99%가 나오게 됩니다.  \n0을 찾는 문제라면 이 모델은 원하는 결과를 줄 수 없게 됩니다.  \n지금 문제에서는 그렇지 않으니 계속 진행하겠습니다."
Let's check our information for gaps.
"There are gaps in our data. Since the gaps are insignificant, we will fill them in using information from open sources."
"# Time Series Forecasting Youtube Tutorial\n## Using Machine Learning to Forecast Energy Consumption\n\nThis notebook is accompanied by a Youtube tutorial.\n\n[WATCH THE VIDEO HERE](https://youtu.be/vV12dGe_Fho)\n\n[You can find it on my channel here!](https://www.youtube.com/channel/UCxladMszXan-jfgzyeIMyvw)\n\n![](https://res.cloudinary.com/monday-blogs/w_768,h_384,c_fit/fl_lossy,f_auto,q_auto/wp-blog/2021/05/sales-forecasting-software.jpg)"
## Types of Time Series Data\n\n![](https://miro.medium.com/max/1400/1*V_RKPeIxCB9CS_2SsLyKXw.jpeg)\n\nreference: https://engineering.99x.io/time-series-forecasting-in-machine-learning-3972f7a7a467
# Feature Importance
# Forecast on Test
"# Visualizing the data\n\n## 1. Map of the location and population of housing districts, along with a heatmap to show where homes are the most expensive.\n\nThis plot is almost directly from Aurélien Géron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow' (Chapter 2).  It gives a good representation of the original dataset and provides a reference prior to our discussion of the engineered features below."
## 2. Below we have a graph showing the size and location of the cities in the dataset.
"## 3. Engineered Features - Length of vectors between districts and nearest town (Vincenty's formulae)\n\nThe map below provides a visualization of three of the features added to the data. The black lines represent the vectors connecting the districts to the nearest town. Based on these lines, the distance to the nearest town (length of black line), the population of the nearest town (point the black line arrives at) and a categorical for the name of the nearest town were assigned to the district.\n\nThese features help to capture the general trend of houses getting cheaper the further away from downtown one gets (a fairly universal pattern... with plenty of exceptions). They can also aid the model by providing a rough representation of more abstract aspects associated with position in town, such as the commute time for people working in city centers. The nearest town can also capture important effects such as 'who do you pay property taxes to?' and 'what is the infastructure like for this municipality?' which are the types of things that could have impacts of the values of homes."
"## 4. Other Engineered Features - Distance to the big city\n\nThis set of features shows which big city (San Francisco, San Jose, San Diego or Los Angeles... those are all really religious spanish names!) a given district is closest to. The cutoff for 'big city' was 500,000 people (this is 1990 data remember). The black lines here represent the same information as the previous plot, and each district was given a value for the distance to the nearest big city (Vincenty) and a categorical for the name of the nearest big city.  I decided to add these features in because the proximity to a large city is somthing I intuitively thought would impact a house price. Access to things like major aiports and major entertainment events/venues could all have a small but important effect of the price of a home.\n\nLooking at the map below, we can see that this clustering also has an interesting side effect of effectively splitting California into quarters. So having the categorical San Francisco as the closest big city is roughly equivalent to being from the northern 1/4 of the state, while the San Diego category is roughly equivalent to the bottom 1/4 of California (I said roughy... don't get out the rulers!). So this feature may be adding some extra structure to the data in ways I had not initially intended."
"The large block of code below does some cleaning on the existing features, performs a train test split, one-hot encodes the categorical variables, scales the numerical variables and then recombines the data to produce the final, cleaned version of the train and test dataframes"
This kernel mainly focuses on what parameters are important for a student to get into a graduate school.\n\nBy the end of this kernel it will be clear of what are the scores required for different tests to have better admission chances and get into a good graduate school.
# 2.Understanding Data\n\n        Back to the table of contents
# 3.Data Analysis\n\n        Back to the table of contents
"Here we can see that the chance of admit is highly correlated with CGPA, GRE and TOEFEL scores are also correlated."
Inferences from the above pairplot:\n* GRE score TOEFL score and CGPA all are linearly related to each other \n* Research Students tend to Score higher by all means
From the above 2 graphs its clear that people tend to score above 310 in GRE and above 100 in TOEFL
Ratings of university increase with the increase in the CGPA
Loading libraries...
Loading datasets...
Outlier detection
Distribution of cat features....
### Load packages
### Load data
**73 groups**\n**Each group_id is a unique recording session and has only one surface type **
### Target feature - surface and group_id distribution\nLet's show now the distribution of target feature - surface and group_id.\nby @gpreda.
We need to classify on which surface our robot is standing.\n\nMulti-class Multi-output\n\n9 classes (suface)
"**So, we have 3810 train series, and 3816 test series.\nLet's engineer some features!**\n\n## Example: Series 1\n\nLet's have a look at the values of features in a single time-series, for example series 1  ```series_id=0```\n\nClick to see all measurements of the **first series** "
# **Import of Libraries**
![Kaggle TPS '21-11 1.png](attachment:45f45d9e-997a-4ea6-9307-95d4cb912f2a.png)
"> **""id"" and ""target"" columns are int64.** We do not really need ""id"" column. We do not really need ""target"" as int64.\nMoreover, we can downscale float64 for the sake of faster computation. We will work on it later on. \n\n> **Now let's plot our target values:**"
"> **Good news, our classes are well balanced!**\n\n> **Now we are going to plot some features (we go with 12) against its target values.**\n>\n> Additionally to the above, let's get 30000 sample for a faster run time."
"\n# 3. Exploratory Data Analysis 📊 \n\n> **The problem statement of this competition says that the features are anonymized.** In reality, there are not many things we can observe and make decisions on. Let's take a closer look at the distribution of the features:"
"> Special thanks to **EDUARDO GUTIERREZ** and his notebook for .describe method representation and styling. [**Eduardo's notebook**](https://www.kaggle.com/eduardogutierrez/tps-nov-21-exploratory-data-analysis). \n>\n> **What is the distribution of the features?**\n>* Features **""f2""**, **""f35""** and **""f44""** are of a higher magnitude comparing to other features;\n>* The standard deviation is in the range of .05 to 1.78 if we exclude above mentioned features;\n>* Some features are almost identical in terms of their scale and magnitude."
> **Let's take a closer look at the distribution of the features:**
"> **We have plotted Probability Density Function estimation for each feature. What does it tell us?**\n>* The features are distributed differently;\n>* More than half of the features are candle-like distributed (looks like Pearson Type 6 or even Poison distribution);\n>* The rest of the features are bell-shaped-like (e.g., Gaussian distribution);\n>* It might be a good idea to transform some of the features."
> **Let's us take a look at features correlation matrix**:
"> If we wish to label the strength of the features association, for absolute values of correlation, **0-0.19** is regarded as very weak (our example is even weaker: **0-0.10**). "
## A look at some of the images in the training set
"Images as Arrays \nAn image is nothing but a standard Numpy array containing pixels of data points. You can think of pixels to be tiny blocks of information arranged in the form of a 2 D grid, and the depth of a pixel refers to the colour information present in it"
"**Coloured images** are represented as a combination of Red, Blue, and Green, and all the other colours can be achieved by mixing these primary colours in the correct proportions."
"A **grayscale image** consists of 8 bits per pixel. This means it can have 256 different shades where 0 pixels will represent black colour while 255 denotes white. For example, the image below shows a grayscale image represented in the form of an array. A grayscale image has only 1 channel where the channel represents dimension."
 1.1 Flipping with skimage  
 1.2 Rotation with skimage  
[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n**4.4. Finding Optimum Number of Principle Component**
"In the figure above, it can be seen that 90 and more PCA components represent the same data. Now let's make the classification process using 90 PCA components."
"\n# HEART FAILURE PREDICTION \n \n\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\nHeart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help. \n\n\n\n    \n TABLE OF CONTENTS \n\n### [**1. IMPORTING LIBRARIES**](#title-one)\n    \n### [**2. LOADING DATA**](#title-two)\n\n### [**3. DATA ANALYSIS**](#title-three)\n\n### [**4. DATA PREPROCESSING**](#title-four)\n\n### [**MODEL BUILDING**](#title-MB) \n    \n### [**5. SVM**](#title-five) \n    \n### [**6. ANN**](#title-six)\n\n### [**7. END**](#title-seven)\n    \n\nIMPORTING LIBRARIES "
\nLOADING DATA 
### **Note:** \n* There are 299 non-null values in all the attributes thus no missing values.\n* Datatype is also either 'float64' or 'int64' which works well while feeded to an algorithm.
### **Note:** \n* Target labels are 203 versus 96 thus there is an imbalance in the data. 
"### **Note:** \n* Features ""creatinine_phosphokinase"" & ""serum creatinine"" are significantly skewed. \n* All the other features almost shows the normal distribution, since mean is equal to their respective medians. "
"### **Note:**\n* ""time"" is the most important feature as it would've been very crucial to get diagnosed early with cardivascular issue so as to get timely treatment thus, reducing the chances of any fatality. (Evident from the inverse relationship) \n\n* ""serum_creatinine"" is the next important feature as serum's (essential component of blood) abundancy in blood makes it easier for heart to function. \n\n* ""ejection_fraction"" has also significant influence on target variable which is expected since it is basically the efficiency of the heart. \n\n* Can be seen from the inverse relation pattern that heart's functioning declines with ageing. "
"### **Note:** \n* Few Outliers can be seen in almost all the features \n* Considering the size of the dataset and relevancy of it, we won't be dropping such outliers in data preprocessing which wouldn't bring any statistical fluke. "
"### **Note:**\n* With less follow-up days, patients often died only when they aged more. \n* More the follow-up days  more the probability is, of any fatality."
"** This is how you can do your own random sampling**\n\nSince 'skiprows' can take in a list of rows you want to skip, you can make a list of random rows you want to input.   I.e. you can sample your data anyway you like!\n\nRecall how many rows the train set in TalkingData has:"
Let's say you want to pull a random sample of 1 million lines out of the total dataset.  That means that you want a list of `lines - 1 - 1000000` random numbers ranging from 1 to 184903891. \n\nNote: generating such long list also takes a lot of space and  some time.  Be patient and make sure to use del and gc.collect() when done!
"In my previous notebook (https://www.kaggle.com/yuliagm/talkingdata-eda-plus-time-patterns) we found that the data is organized by click time.  Therefore if our random sampling went according to plan, the resulting set should roughly span the full time period and mimick the click pattern. \n\nWe see from above that first and last click span the 4 day period.\n\nLet's try a chart to see if the pattern looks consistent:"
Looks all-right!  \n\nNow you can analyze your own subsample and run models on it.
"#  Be humble, sit down and check out the visualizations\n\n#### Last Update - 15 Sept 2017 \n- Added pairplot for feature interaction using **Holoviews** in second round of feature engineering\n- **Multiple Interactive histograms using Plotly** to check different distance features between pickup-drop\n- Correlation matrix heatmap using **Seaborn** to check the correlated variables\n- Histogram of test and validation data using **Seaborn**\n"
#### Importing packages for analysis
"### Reading and checking the head of training data and data from OSRM fastest route dataset\nTraining data is given as primary dataset for this competition and is available from the beginning of the competition. After almost 2 weeks from the start of this competition, oscarleo uploaded this dataset in Kaggle datasets. This dataset is generated from Open Source Routing Machine. This is similar to Gmaps but this is open source, so a user can make any number of queries using this engine ( Unlike Gmaps, where per day limit of free requests is 2000. Though I have compared the results of OSRM with that if Gmaps, Gmaps results considerably different than this one, almost all the results for time duration are different **with the same fraction**. So, while training any regressor, it will help very much in predicting trip duration. I am importing both of these datasets in the start of our analysis. \n"
"### Lets visualize the trip duration given using log-scale distplot in sns\nWe are asked to predict trip_duration of the test set, so we first check what kind of trips durations are present in the dataset. First I plotted it on a plain scale and not on a log scale, and some of the records have very long trip durations ~100 hours. Such long trips are making all another trip invisible in the histogram on plain scale => We go ahead with the log scale. Another reason of using the log scale for visualizing trip-duration on the log scale is that this competition uses rmsle matrix so it would make sense to visualize the target variable in log scale only."
"**Findings** - It is clear with the above histogram and kernel density plot that the trip-durations are like Gaussian and few trips have very large duration, like ~350000 seconds which is 100 hours (which is weird, as long as it isn't an intercity taxi ride from NYC to SF or Alaska), while most of the trips are e^4 = 1 minute to e^8 ~ 60 minutes. and probably are taken inside Manhattan or in new york only. Let's check the lat-long distributions are then used them to have a heat map kind of view of given lat-longs."
"**Findings** - From the plot above it is clear that pick and drop latitude are centered around 40 to 41, and longitude are situated around -74 ton-73. We are not getting any histogram kind of plots when we are plotting lat-long as the distplot function of sns is getting affected by outliers, trips which are very far from each other like lat 32 to lat 44, are taking very long time, and have affected this plot such that it is coming off as a spike. Let's remove those large duration trip by using a cap on lat-long and visualize the distributions of latitude and longitude given to us."
"**Findings** - We put the following caps on lat-long -\n- latitude should be between 40.6 to 40.9\n- Longitude should be between -74.05 to -73.70 \n\nWe get that the distribution spikes becomes as distribution in distplot (distplot is a histogram plot in seaborn package), we can see that most of the trips are getting concentrated between these lat-long only. Let's plot them on an empty image and check what kind of a city map we are getting as we can't use gmaps and folium on kaggle kernel for visualizations. \n**Update** - As at that time we didn't have folium in kaggle python docker, Now we have it and I have included beautiful folium visualizations in this kernel."
"## Heatmap of coordinates\n### Let's do basic image processing here \nWe have taken an empty image and make it a color it black so that we can see colors where the lat-longs are falling. To visualize we need to consider each point of this image as a point represented by lat-long, to achieve that we will bring the lat-long to image coordinate range and then take a summary of lat-long and their count, assign a different color for different count range. Running next cell will result in beautiful visualization shown below.\n\nThis is the very low-level implementation of making plots. As in the early stage of competition people using R was making plots on leaflet and we, Python users were missing gmaps and folium in kaggle kernel's python docker, I requested them to get the folium installed but it took some time and I wasn't aware of the ""datashader"" package that time.So, I decided to use the knowledge I gained in last kaggle competition I participated in (Nature Conservancy Fisheries Monitoring) and made such plots using basic image processing. "
"**Findings** - From the heatmap kind of image above -\n- Red points signifies that 1-10 trips in the given data have that point as pickup point\n- Green points signifies that more than 10-50 trips in the given data have that point as pickup point \n- Yellow points signifies that more than 50+ trips in the given data have that point as pickup point\n\nClearly, the whole manhattan is yellow colored and with few green points as well, that shows that in Manhatten most of the trips are getting originated.  This is the basic way in which you can plot large geospatial data in an empty image without being dependent on any package. But if you hate image processing, you can use **datashader**, datashader is a package which is used to show billions of data points on an image, they also use the similar approach with a different color gradient.\nThought I will also show the same plot with a sample data of 1000 trips on pygmaps in next few cell. it will generate an HTML in output and user has to open that HTML in the browser."
"# Features' Exploration (Checking if we have any explainable pattern) -\nEven if we don't visualize these features we can make model and model will predict the trip_duration, then why are we visualizing? - Because - it will give us an explanation of model's output and will give us some pattern which may even guide if we should make multiple models, or one model if we should include that particular variable or there is no sense in including that variable. in short - it will give us new ideas to make a model. \n**Note** - Basically in consulting world, if you can't explain model it won't sell, and without visuals, it's very hard to explain patterns.\n- **1. Let's check the average time taken by two different vendors vs weekday**"
"**Findings** - it's clear that the vendor 1 is taking more time than vendor 2 on all the days of the week, we can also subset data frame based on the month and that will also give us the same results. The difference between the average time taken by vendor 1 is ~250 seconds more than vendor 2. "
"# Identifying Duplicate Questions\n\nWelcome to the Quora Question Pairs competition! Here, our goal is to identify which questions asked on [Quora](https://www.quora.com/), a quasi-forum website with over 100 million visitors a month, are duplicates of questions that have already been asked. This could be useful, for example, to instantly provide answers to questions that have already been answered. We are tasked with predicting whether a pair of questions are duplicates or not, and submitting a binary prediction against the logloss metric.\n\nIf you have any questions or want to discuss competitions/hardware/games/anything with other Kagglers, then join the KaggleNoobs Slack channel [here](https://goo.gl/gGWFXe). We also have regular AMAs with top Kagglers there.\n\n**And as always, if this helped you, some upvotes would be very much appreciated - that's where I get my motivation! :D**\n\nLet's dive right into the data!"
"Looks like we are simply given two files this time round, one for the training set and one for the test set. They are relatively small compared to other recent competitions, weighing in at less than 400MB total.\n\nIt's worth noting that there is a lot more testing data than training data. This could be a sign that some of the test data is dummy data designed to deter hand-labelling, and not included in the calculations, like we recently saw in the [DSTL competition](https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/leaderboard).\n\nLet's open up one of the datasets.\n\n## Training set"
"We are given a minimal number of data fields here, consisting of:\n\n**`id`:** Looks like a simple rowID    \n**`qid{1, 2}`:** The unique ID of each question in the pair    \n**`question{1, 2}`:** The actual textual contents of the questions.    \n**`is_duplicate`:** The **label** that we are trying to predict - whether the two questions are duplicates of each other."
"In terms of questions, everything looks as I would expect here. Most questions only appear a few times, with very few questions appearing several times (and a few questions appearing many times). One question appears more than 160 times, but this is an outlier.\n\nWe can see that we have a 37% positive class in this dataset. Since we are using the [LogLoss](https://www.kaggle.com/wiki/LogarithmicLoss) metric, and LogLoss looks at the actual predicts as opposed to the order of predictions, we should be able to get a decent score by creating a submission predicting the mean value of the label.\n\n## Test Submission"
"Nothing out of the ordinary here. We are once again given rowIDs and the textual data of the two questions. It is worth noting that we are not given question IDs here however for the two questions in the pair.\n\nIt is also worth pointing out that the actual number of test rows are likely to be much lower than 2.3 million. According to the [data page](https://www.kaggle.com/c/quora-question-pairs/data), most of the rows in the test set are using auto-generated questions to pad out the dataset, and deter any hand-labelling. This means that the true number of rows that are scored could be very low.\n\nWe can actually see in the head of the test data that some of the questions are obviously auto-generated, as we get delights such as ""How their can I start reading?"" and ""What foods fibre?"". Truly insightful questions.\n\nNow onto the good stuff - the text data!\n## Text analysis\n\nFirst off, some quick histograms to understand what we're looking at. **Most analysis here will be only on the training set, to avoid the auto-generated questions**"
"We can see that most questions have anywhere from 15 to 150 characters in them. It seems that the test distribution is a little different from the train one, but not too much so (I can't tell if it is just the larger data reducing noise, but it also seems like the distribution is a lot smoother in the test set).\n\nOne thing that catches my eye is the steep cut-off at 150 characters for the training set, for most questions, while the test set slowly decreases after 150. Could this be some sort of Quora question size limit?\n\nIt's also worth noting that I've truncated this histogram at 200 characters, and that the max of the distribution is at just under 1200 characters for both sets - although samples with over 200 characters are very rare.\n\nLet's do the same for word count. I'll be using a naive method for splitting words (splitting on spaces instead of using a serious tokenizer), although this should still give us a good idea of the distribution."
"We see a similar distribution for word count, with most questions being about 10 words long. It looks to me like the distribution of the training set seems more ""pointy"", while on the test set it is wider. Nevertheless, they are quite similar.\n\nSo what are the most common words? Let's take a look at a word cloud."
"## Semantic Analysis\n\nNext, I will take a look at usage of different punctuation in questions - this may form a basis for some interesting features later on."
"# Initial Feature Analysis\n\nBefore we create a model, we should take a look at how powerful some features are. I will start off with the word share feature from the benchmark model."
"Here we can see that this feature has quite a lot of predictive power, as it is good at separating the duplicate questions from the non-duplicate ones. Interestingly, it seems very good at identifying questions which are definitely different, but is not so great at finding questions which are definitely duplicates.\n\n## TF-IDF\n\nI'm now going to try to improve this feature, by using something called TF-IDF (term-frequency-inverse-document-frequency). This means that we weigh the terms by how **uncommon** they are, meaning that we care more about rare words existing in both questions than common one. This makes sense, as for example we care more about whether the word ""exercise"" appears in both than the word ""and"" - as uncommon words will be more indicative of the content.\n\nYou may want to look into using sklearn's [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to compute weights if you are implementing this yourself, but as I am too lazy to read the documentation I will write a version in pure python with a few changes which I believe should help the score."
"# Code Implementation in Tensorflow 2.0\n> Note: The code for this notebook is taken from the [public kernel](https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic/) posted by [akensert](https://www.kaggle.com/akensert)\n\nThis kernel does not explore the data. For that you could check out some of the great EDA kernels: [introduction](https://www.kaggle.com/corochann/google-quest-first-data-introduction), [getting started](https://www.kaggle.com/phoenix9032/get-started-with-your-questions-eda-model-nn) & [another getting started](https://www.kaggle.com/hamditarek/get-started-with-nlp-lda-lsa). This kernel is an example of a TensorFlow 2.0 Bert-base implementation, using TensorFow Hub. "
"**1. Read data and tokenizer**\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"
 \n## Notebook  Content\n1. [Memory issues](#1)\n1. [EDA-briefly](#2)\n1. [Feature Engineering (to be continued) ](#3)\n
 \n#  1-Memory issues
Let us proceede with further interesting **EDA**
This time around we plotted the independently but on the same graph and the conclusion is the same. Right before the problem happens signal increases. Ok what wcan we now say regarding the univariate distribution of the columns themselves?
Very intersting (again for the first 150 000 observations) it seems that (yellow=accoustic_data) follows normal distribution with some outliers. On the other hand time to failure takes on 2(?) very distinct values...
 \n# Import Libraries
"\n#### Exploratory Data Analysis\n- Loading Data \n- Data Disribution\n- Data Visualization\n\nSetup all the param, which we will use in model\n"
"\n# Data Visualization and EDA\n> Data distrubution per class\n\nas per below bar chart, it clearly showing that data set is quite imbalance. And even it's expected in medical domain."
"Histogram is clearing showing that training data is Imbalanced. Because in class ‘No DR’ records are approx. 1750 while in class ‘Severe’ very less. So, may be for balancing data set, we would be requiring data augmentation. \n\nThere are couple of ways to do image data augmentation. We will see down in this kernel.\n"
\n### Train and Test dataset \n- We will use pie chart for showing the size of dataset.
It's showing train and testing data are in 2:1 ratio. Both are quite small data set.
"\n### GrayScale Images\nConverting the Ratina Images into Grayscale. So, we can usnderstand the regin or intest ."
"It's clearly showing, that the image [0,1] has give regin  black around the EYE ball. Which is ust noise, that will not add any value fo model. We need to remove this black area. in my next iteration will work on that to crop black are from image. "
"##  3b. Correlation Matrix & Heatmap\n***\n**Moderate Positively Correlated Features:** \n- projectCount vs evaluation: 0.349333\n- projectCount vs averageMonthlyHours:  0.417211\n- averageMonthlyHours vs evaluation: 0.339742\n\n**Moderate Negatively Correlated Feature:**\n - satisfaction vs turnover:  -0.388375\n\n**Stop and Think:**\n- What features affect our target variable the most (turnover)?\n- What features have strong correlations with each other?\n- Can we do a more in depth examination of these features?\n\n**Summary:**\n\nFrom the heatmap, there is a **positive(+)** correlation between projectCount, averageMonthlyHours, and evaluation. Which could mean that the employees who spent more hours and did more projects were evaluated highly. \n\nFor the **negative(-)** relationships, turnover and satisfaction are highly correlated. I'm assuming that people tend to leave a company more when they are less satisfied. "
"## 3b2. Statistical Test for Correlation\n***\n\n### One-Sample T-Test (Measuring Satisfaction Level)\nA one-sample t-test checks whether a sample mean differs from the population mean. Since satisfaction has the highest correlation with our dependent variable turnover, let's test to see whether the average satisfaction level of employees that had a turnover differs from the those that had no turnover.\n\n**Hypothesis Testing:** Is there significant difference in the **means of satisfaction level** between employees who had a turnover and temployees who had no turnover?\n\n - **Null Hypothesis:** *(H0: pTS = pES)* The null hypothesis would be that there is **no** difference in satisfaction level between employees who did turnover and those who did not..\n\n - **Alternate Hypothesis:** *(HA: pTS != pES)* The alternative hypothesis would be that there **is** a difference in satisfaction level between employees who did turnover and those who did not.."
"### Conducting the T-Test\n***\nLet's conduct a t-test at **95% confidence level** and see if it correctly rejects the null hypothesis that the sample comes from the same distribution as the employee population. To conduct a one sample t-test, we can use the **stats.ttest_1samp()** function:"
"### T-Test Result\n***\nThe test result shows the **test statistic ""t"" is equal to -51.33**. This test statistic tells us how much the sample mean deviates from the null hypothesis. If the t-statistic lies **outside** the quantiles of the t-distribution corresponding to our confidence level and degrees of freedom, we reject the null hypothesis. We can check the quantiles with **stats.t.ppf()**:"
You can access the raw files with:
"\nYou will see that **each word comes with a slash and a label** and unlike normal text, we see that **punctuations are separated from the word that comes before it**, e.g. \n\n> The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n\n\nAnd we also see that the **each sentence is separated by a newline**:\n\n> There/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n> \n> The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n\n\nThat brings us to the next point on **sentence tokenization** and **word tokenization**."
\n# Import libraries\n[Back to Table of Contents](#ToC)
\n# Define useful classes\nNOTE: I use the neural network in [*Siwei Xu's tutorial\n*](https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998) with some proper modifications to adapt to the problem in ConnectX competition and be able to save trained and load pre-trained models.\n\n[Back to Table of Contents](#ToC)
\n# Define helper-functions\n[Back to Table of Contents](#ToC)
\n# Create ConnectX environment\n[Back to Table of Contents](#ToC)
"Our response variable, **diagnosis**, is categorical and has two classes,  'B' (Benign) and 'M' (Malignant). All explanatory variables are numerical, so we can skip data type conversion.\n\nLet's now take a closer look at our response variable, since it is the main focus of our analysis. We begin by checking out the distribution of its classes."
"Out of the 569 observations, 357 (or 62.7%) have been labeled malignant, while the rest 212 (or 37.3%) have been labeled benign. Later when we develop a predictive model and test it on unseen data, we should expect to see a similar proportion of labels.\n\nAlthough our dataset has 30 columns excluding the **id** and the **diagnosis** columns, they are all in fact very closely related since they all contain information on the same 10 key attributes but only differ in terms of their perspectives (i.e., the mean, standard errors, and the mean of the three largest values denoted as ""worst""). \n\nIn this sense, we could attempt to dig out some quick insights by analyzing the data in only one of the three perspectives. For instance, we could choose to check out the relationship between the 10 key attributes and the **diagnosis** variable by only choosing the ""mean"" columns.\n\nLet's quickly scan for any interesting patterns between our 10 ""mean"" columns and the response variable by generating a scatter plot matrix as shown below:"
"There are some interesting patterns visible. For instance, the almost perfectly linear patterns between the **radius**, **perimeter** and **area** attributes are hinting at the presence of multicollinearity between these variables. Another set of variables that possibly imply multicollinearity are the **concavity**, **concave_points** and **compactness**. \n\nIn the coming up section, we will generate a matrix similar to the one above, but this time displaying the correlations between the variables instead of a scatter plot. Let's find out if our hypothesis about the multicollinearity has any statistical support. "
"# 2. The Variables\n---\nAs said earlier, let's take a look at the correlations between our variables. This time however, we will create a correlation matrix with all variables (i.e., the ""mean"" columns, the ""standard errors"" columns, as well as the ""worst"" columns)."
"Looking at the matrix, we can immediately verify the presence of multicollinearity between some of our variables. For instance, the **radius_mean** column has a correlation of 1 and 0.99 with **perimeter_mean** and **area_mean** columns, respectively. This is probably because the three columns essentially contain the same information, which is the physical size of the observation (the cell). Therefore we should only pick one of the three columns when we go into further analysis. \n\nAnother place where multicollienartiy is apparent is between the ""mean"" columns and the ""worst"" column. For instance, the **radius_mean** column has a correlation of 0.97 with the **radius_worst** column. In fact, each of the 10 key attributes display very high (from 0.7 up to 0.97) correlations between its ""mean"" and ""worst"" columns. This is somewhat inevitable, because the ""worst"" columns are essentially just a subset of the ""mean"" columns; the ""worst"" columns are also the ""mean"" of some values (the three largest values among all observations). Therefore, I think we should discard the ""worst"" columns from our analysis and only focus on the ""mean"" columns. \n\nIn short, we will drop all ""worst"" columns from our dataset, then pick only one of the three attributes that describe the size of cells. But which one should be pick?\n\nLet's quickly go back to 6th grade and review some geometry. If we think of a cell as roughly taking a form of a circle, then the formula for its radius is, well, its radius,  *r*. The formulae for its perimeter and area are then **\\(2\pi r\\) ** and **\\(\pi r^2\\) **, respectively. As we can see, a cell's **radius** is the basic building block of its size. Therefore, I think it is reasonable to choose **radius** as our attribute to represent the size of a cell. \n\nSimilarly, it seems like there is multicollinearity between the attributes **compactness**, **concavity**, and **concave points**. Just like what we did with the size attributes, we should pick only one of these three attributes that contain information on the shape of the cell. I think **compactness** is an attribute name that is straightforward, so I will remove the other two attributes. \n\nWe will now go head and drop all unnecessary columns. "
"Are we all set now?\n\nLet's take a look at the correlation matrix once again, this time created with our trimmed-down set of variables."
Looks great! Now let's move on to our model.
## Dependencies 
## Evaluation metric 
## Visually inspecting our network against unlabeled data ##\n\n
"Alright, so we made a couple mistakes, but not too bad actually! \n\nIf you're happy with it, let's compete!"
"\nGoals\nThis kernel hopes to accomplish many goals, to name a few...\n    \n        Learn/review/explain complex data science topics through write-ups.\n        Do a comprehensive data analysis along with visualizations.\n        Create models that are well equipped to predict housing prices.  \n    \n\n\nImporting Necessary Libraries and datasets"
A Glimpse of the datasets.\nSample Train Dataset
Relationship between missing values and Sale Price:
These plots compare the median SalePrice in the observations where data is missing vs the observations where a value is available. As you can see there is a significant difference in median sale price between where missing value exists and where missing value doesn't exist. We are using median here because mean would not direct us towards a better assumption as there are some outliers present. 
"You can see these values are represented in years as we hoped. However, we generally don't use data for example year in their raw format, instead we try to get information from them. Let's look at the ""YrSold"" plot "
"This plot should raise an eyebrows. As the year increases the price of the houses seems to be decreasing, which in real time is quite unusual. Let's see if there is a relationship between year features and SalePrice"
"These charts seems more like a real life situation case. The longer the time between the house was built/remodeled and sold, the lower the sale price. This is likely, because the houses will have an older look and might need repairing. "
All you need to know when tackling Time Series Data!!\n\n\n\n  Table of Contents\n  Fetch the data1\n  Downcasting2\n  Melting the data3\n  Exploratory Data Analysis4 \n  Feature Engineering5\n  Modelling and Prediction6
# 1. Fetch the data
"Below plot shows how much effect downcasting has had on the memory usage of DataFrames. Clearly, we have been able to reduce `sales` & `prices` to less than 1/4th of their actual memory usage. `calendar` is already a small dataframe."
"# 3. Melting the data\nCurrently, the data is in three dataframes: `sales`, `prices` & `calendar`. The `sales` dataframe contains daily sales data with days(d_1 - d_1969) as columns. The `prices` dataframe contains items' price details and `calendar` contains data about the days d.  \n\n3.1 Convert from wide to long format\nHere's an example of conversion of a wide dataframe to a long dataframe.\n"
"# 4. Exploratory Data Analysis\n4.1 The Dataset\nThe M5 dataset, generously made available by Walmart, involves the unit sales of various products sold in the USA, organized in the form of grouped time series. More specifically, the dataset involves the unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated.  The products are sold across ten stores, located in three States (CA, TX, and WI).\n\nI have drawn an interactive visualization showing the distribution of 3049 items across different aggregation levels."
4.2 Item Prices\nHere I'll be studying about the item prices and their distribution. Please note the prices vary weekly. So to study the distribution of prices I have taken their average.
"Below are some of the observations from the above plot:-\n\n  The distribution of item prices is almost uniform for all the stores across Califoria, Texas and Wisconsin.\n  Item HOBBIES_1_361 priced at around 30.5 dollars is the costliest item being sold at walmarts across California.\n  Item HOUSEHOLD_1_060 priced at around 29.875 dollars is the costliest item being sold at walmarts across Texas.\n  Item HOBBIES_1_361 priced at around 30.48 dollars is the costliest item being sold at TX_1 and TX_3 in Texas. While item HOBBIES_1_255 priced at around 30.5 dollars is the costliest at TX_2\n"
"As can be seen from the plot above, food category items are quite cheap as compared with hobbies and household items. Hobbies and household items have almost the same price range."
4.3 Items Sold\nLet's study the sales accross all the stores.
"Below are some of the observations from the above plot:-\n\n  California: CA_3 has sold the most number of items while, CA_4 has sold the least number of items.\n  Texas: TX_2 and **TX_3** have sold the maximum number of items. TX_1 has sold the least number of items.\n  Wisconsin: WI_2 has sold the maximum number of items while, WI_3 has sold the least number of items.\n  USA: CA_3 has sold the most number of items while, CA_4 has sold the least number of items.\n\n\n**Let's study number of items sold over time across all the stores.**"
"4.4 State wise Analysis\n  California1\n  Texas2\n  Wisconsin3\n\nIn this section, I will be studying the sales and revenue of all the stores individually across all the three states: California, Texas & Wisconsin. I have plotted total three plots for each store: CA_1, CA_2, CA_3, CA_4, TX_1, TX_2, TX_3, WI_1, WI_2 & WI_3. Details about the plots are as follows:-\n- First plot shows the daily sales of a store. I have plotted the values separately for SNAP days. Also, SNAP promotes food purchase, I have plotted food sales as well to check if it really affects the food sales.\n- Second plot shows the daily revenue of a store with separate plotting for SNAP days.\n- Third is a heatmap to show daily sales. It's plotted in such a way that it becomes easier to see day wise values.\n\n\nWhat is SNAP?\nThe United States federal government provides a nutrition assistance benefit called the Supplement Nutrition Assistance Program (SNAP).  SNAP provides low income families and individuals with an Electronic Benefits Transfer debit card to purchase food products.  In many states, the monetary benefits are dispersed to people across 10 days of the month and on each of these days 1/10 of the people will receive the benefit on their card.  More information about the SNAP program can be found [here.](https://www.fns.usda.gov/snap/supplemental-nutrition-assistance-program)\n\n\n\nFor the heatmaps, the data is till 16th week of 2016 and datetime.weekofyear of function is returning 1,2 & 3 january of 2016 in 53rd week. Plotly's heatmap is connecting the data gap between the 16th and 53rd week. Still figuring out on how to remove this gap.\n"
Churn is a one of the biggest problem in  the telecom industry. Research has shown that the average monthly churn rate among the top 4 wireless carriers in the US is 1.9% - 2%. 
**Let us read the data file in the python notebook**
1. **Gender Distribution** - About half of the customers in our data set are male while the other half are female
2. **% Senior Citizens** - There are only 16% of the customers who are senior citizens. Thus most of our customers in the data are younger people.\n
"3. **Partner and dependent status**  - About 50% of the customers have a partner, while only 30% of the total customers have dependents. "
"**What would be interesting is to look at the % of customers, who have partners, also have dependents. We will explore this next. **"
"Interestingly,  among the customers who have a partner, only about half of them also have a dependent, while other half do not have any independents. \nAdditionally, as expected, among the customers who do not have any partner, a majority (80%) of them do not have any dependents ."
"I also looked at any differences between the % of customers with/without dependents and partners by gender. There is no difference in their distribution by gender. Additionally, there is no difference in senior citizen status by gender."
"**1. Tenure:**  After looking at the below histogram we can see that a lot of customers have been with the telecom company for just a month, while quite a many are there for about 72 months. This could be potentially because different customers have different contracts. Thus based on the contract they are into it could be more/less easier for the customers to stay/leave the telecom company."
"**2. Contracts:** To understand the above graph, lets first look at the # of customers by different contracts. "
As we can see from this graph most of the customers are in the month to month contract. While there are equal number of customers in the 1 year and 2 year contracts.\n\n
Below we will understand the tenure of customers based on their contract type.
"Interestingly most of the monthly contracts last for 1-2 months, while the 2 year contracts tend to last for about 70 months. This shows that the customers taking a longer contract are more loyal to the company and tend to stay with it for a longer period of time. \n\nThis is also what we saw in the earlier chart on correlation with the churn rate. "
# Import the libraries
# Import the dataset
[Go to Contents Menu](#0.)\n\n# **Visualizer class** 
[Go to Contents Menu](#0.)\n\n# **'ObjectOrientedTitanic' class**  
"### Gender, age and country\n"
"Ouch ! The gender gap is huge! Unfortunately, this is common in the tech industry. Statistics show that** women hold only 25% of computing jobs**, which is already low but what we're having here is worse. 16.71% is too low, **there's 5 times as many male respondents as female respondents.**"
"Instances with 0, 5, 100 years old don't make much sens. Removing those instances here (we'll keep them later on as the age doesn't affect the other properties) would yield more significant results."
The age median is about 30 years old and most participants are between 25 and 37 years old.
"Seems like most Kagglers are either Americans or Indians. More precisely,"
### Formal education and Major\n
"Nearly half of the kagglers who took this survey are Master's graduates, impressive.   \nWhat's more, 80.34% of respondents hold at least a bachelor degree. "
### How did you start learning Data Science ? \n 
"The most popular way to start learning Data Science / Machine Learning is Online Courses. In fact, I was expecting those results because anytime I ask someone *'Please where can I learn X efficiently'* (where X is related to Machine Learning), I end up with a MOOC recommendation.   \nKaggle has a very small proportion but that's natural, people usually come to kaggle to complete their learning once they've got the basics.\n\nPersonally, I started learning data science both at university and with online courses at the same time. Then, couple of months later, I discovered Kaggle and its kernels quickly became my favourite learning tool. So I perfectly identify with the results to this question ! "
### Employment status\n
**2.1 Boxplots of all stats**
**2.2 Customized Boxplots**
# 3. Radar charts\nA radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. The relative position and angle of the axes is typically uninformative.\nSource: [Wikipedia](https://en.wikipedia.org/wiki/Radar_chart)
**3.1 Visualizing stats of single pokemon**
**3.2 Comparing the stats of 2 pokemon**
**4.1 2D Scatterplot(Adding colorscale makes it 3D)**
**4.2 3D scatterplots**
"# 5. Contour Charts\nContour plots (sometimes called Level Plots) are a way to show a three-dimensional surface on a two-dimensional plane. It graphs two predictor variables X Y on the y-axis and a response variable Z as contours. These contours are sometimes called z-slices or iso-response values.\n\nThis type of graph is widely used in cartography, where contour lines on a topological map indicate elevations that are the same. Many other disciples use contour graphs including: astrology, meteorology, and physics. Contour lines commonly show altitude and density.\n\nSource: [http://www.statisticshowto.com/contour-plots/](http://www.statisticshowto.com/contour-plots/)"
**5.1 Contour chart for distribution of bug pokemon**
"**5.2 Contour chart for depicting density(and distribution) of HP, Speed, Sp. Attack, Sp. Defense of different generations of pokemon based on their Attack and Defense**"
"We can combine all the list of words (stopwords, frequent words and rare words) and create a single list to remove them at once.\n\n## Stemming\n\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (From [Wikipedia](https://en.wikipedia.org/wiki/Stemming))\n\nFor example, if there are two words in the corpus `walks` and `walking`, then stemming will stem the suffix to make them `walk`. But say in another example, we have two words `console` and `consoling`, the stemmer will remove the suffix and make them `consol` which is not a proper english word.\n\nThere are several type of stemming algorithms available and one of the famous one is porter stemmer which is widely used. We can use nltk package for the same."
"We can see that words like `private` and `propose` have their `e` at the end chopped off due to stemming. This is not intented. What can we do fort hat? We can use Lemmatization in such cases.\n\nAlso this porter stemmer is for English language. If we are working with other languages, we can use snowball stemmer. The supported languages for snowball stemmer are"
**Importing the libraries**
## Some dogs\n\n> The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world.
"# Insights\n\nCheck this posts:\n\n>[Quick data explanation and EDA](https://www.kaggle.com/witold1/quick-data-explanation-and-eda) by @witold1\n\n> [New Insights](https://www.kaggle.com/c/generative-dog-images/discussion/97863#latest-564673)\n\n- There are pictures with more than one dog (even with  3  dogs);\n- There are pictures with the dog (-s) and person (people);\n- There are pictures with more than one person (even with  4  people);\n- There are pictures where dogs occupy less than  1/5  of the picture;\n- There are pictures with text (magazine covers, from dog shows, memes and pictures with text);\n- Even wild predators included, e.g. African wild dog or Dingo, but not wolves.\n\n**Examples**\n\n\n\n\n\n\n"
"This doesn't run because EPOCH = 0, change it and try ;)"
# Best public training\n- 06/29 [RaLSGAN dogs](https://www.kaggle.com/speedwagon/ralsgan-dogs) V9\n- 06/29 this kernel V5\n- some version of this kernel
**Plot Loss per EPOCH**\n> plot_loss()
**Show generated images**\n> show_generated_img()
- Let's import libraries
- Read csv and see the top 5 instances of the data
### **Univariate Analysis**
- Our features have very close to normal distribution. \n- Solids have slightly right skewness.
"# Outline\n1. Stochastic gradient descent and online learning\n    - 1.1. SGD\n    - 1.2. Online approach to learning\n2. Categorical data processing: Label Encoding, One-Hot Encoding, Hashing trick\n    - 2.1. Label Encoding\n    - 2.2. One-Hot Encoding\n    - 2.3. Hashing trick\n3. Vowpal Wabbit\n    - 3.1. News. Binary classification\n    - 3.2. News. Multiclass classification\n    - 3.3. IMDB reviews\n    - 3.4. Classifying gigabytes of StackOverflow questions\n4. VW and Spooky Author Identification "
"# 1. Stochastic gradient descent and online learning\n##  1.1. Stochastic gradient descent\n\nDespite the fact that gradient descent is one of the first things learned in machine learning and optimization courses, it is hard to overrate one of it's modifications, namely, Stochastic Gradient Descent (SGD).\n\nLets recap that the very idea of gradient descent is to minimize some function by making small steps in the direction of fastest function decreasing. The method was named due to the following fact from calculus: vector $\nabla f = (\frac{\partial f}{\partial x_1}, \ldots \frac{\partial f}{\partial x_n})^T$ of partial derivatives of the function $f(x) = f(x_1, \ldots x_n)$ points to the direction of the fastest function growth. It means that by moving in the opposite direction (antigradient) it is possible to decrease the function value with the fastest rate.\n\n\n\nHere is a snowboarder (me) in Sheregesh, Russian most popular winter resort. I highly recommended it if you like skiing or snowboarding. We place this picture not only for a good view but also for picturing the idea of gradient descent. If you have an aim to ride as fast as possible, you need to choose the way with steepest descent (as long as you stay alive). Calculating antigradient can be seen as evaluating the slope in each particular point."
"1. **int data type variables:** Pclass, SibSp, Parch, and PassengerId.\n2. **float data type variables:** Fare and Age, *Survived (due to concatenation)*\n3. **object (numbers + strings) data type variables:** Name, Sex, Ticket, Cabin, and Embarked.\n\n# 4.Univariate Analysis \nUnivariate analysis separately explores the distribution of each variable in a data set. It looks at the range of values, as well as the central tendency of the values. Univariate data analysis does not look at relationships between various variables (like bivariate and multivariate analysis) rather it summarises each variable on its own. Methods to perform univariate analysis will depend on whether the variable is categorical or numerical. For numerical variable, we would explore its shape of distribution (distribution can either be symmetric or skewed) using histogram and density plots. For categorical variables, we would use bar plots to visualize the absolute and proportional frequency distribution. Knowing the distribution of the feature values becomes important when you use machine learning methods that assume a particular type of it, most often Gaussian. **Let's starts off with categorical variables:**\n\n## 4.1 Categorical Variables  \nTo analyse categorical variables, let's create a custom function to display bar chart in absolute and relative scale of a variable in a subplot."
###  4.1.1 Survived 
"------------------------------------------------------------------\n## What is Autoencoder?\n\n![](https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png)\n\nPicture Credit: https://lilianweng.github.io\n\nThe auto encoder learns to produce the same output as the input as much as possible. Through this learning process, the representation of the input can be effectively compressed in the latent space. In other words, it compresses the dimensions and stores the representation of the input in the latent space."
> This dataset contains the ECG readings of patients.\n> Each row corresponds to a single complete ECG of a patient. Every single ECG is composed of 140 data points(readings).\n>  \n> 1. Columns 0-139 contain the ECG data point for a particular patient. These are floating point numbers.\n> 2. The label which shows whether the ECG is normal or abnormal. It is a categorical variable with value either 0 or 1.
## Checking Target Imblance
**OK! Target distribution is balanced.**
Plot the normal ECG.
Plot the abnormal ECG.
-----------------------------------\n# Modeling
"The `boosting_type` and `is_unbalance` domains are pretty simple because these are categorical variables. For the hyperparameters that must be integers (`num_leaves`, `min_child_samples`), we use `range(start, stop, [step])` which returns a range of numbers from start to stop spaced by step (or 1 if not specified). `range` always returns integers, which means that if we want evenly spaced values that can be fractions, we need to use `np.linspace(start, stop, [num])`.  This works the same way except the third argument is the number of values (by default 100).\n\nFinally, `np.logspace(start, stop, [num = 100], [base = 10.0])` returns values evenly spaced on a logarithmic scale. According to the [the docs](https://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html) ""In linear space, the sequence starts at $base^{start}$ (base to the power of start) and ends with $base ^{stop}$ "" This is useful for values that differ over several orders of magnitude such as the learning rate."
"### Learning Rate Domain\n\nThe learning rate domain is from 0.005 to 0.5. Using a logarithmic uniform distribution allows us to create a domain where there are as many values from 0.005 to 0.05 as from 0.05 to 0.5. In a linear space, there would be far more values from 0.05 to 0.5 because this represents a larger distance in linear space but in logarithmic space each of these two intervals is the same width because they are multiples of 10 of each other. (Think about going from 1 to 10 and then from 10 to 100. On a logarithmic scale, these intervals are the same size, but on a linear scale the latter is 10 times the size of the former). In other words, a logarithmic uniform distribution lets us sample more evenly from a domain that varies over several orders of magnitude. \n\nIf that's a little confusing, perhaps the graph above makes it clearer. We can also do a sanity check to make sure the spacing is correct by counting the number of values in each interval."
"As an example of a simple domain, the `num_leaves` is a uniform distribution. This means values are evenly spaced on a linear scale."
"# Algorithm for selecting next values\n\nAlthough we don't generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random. \n\nWe will implement these algorithms very shortly, as soon as we cover the final part of hyperparameter tuning."
Check the dataset for gaps in the data
There are no missing values!
Let's look at the distribution of the target variable
"We are dealing with an unbalanced sample, where the majority of people are healthy."
Let's analyze numerical variables.
"The analysis showed that only the BMI variable is close to the normal distribution, the rest are close to bimodal."
Let's look at the distribution of the number of people with heart disease from various factors
I propose to look at the distribution of categorical variables depending on gender
 \n## 5-1 Import
 \n## 5-2 version
 \n## 5-3 Setup\n\nA few tiny adjustments for better **code readability**
" \n## 6- EDA\nBy the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n\n1. Data Collection\n1. Visualization\n1. Data Cleaning\n1. Data Preprocessing\n\n\n ###### [Go to top](#top)"
# DataSet & Library Loading
# Making the dataset ready for the model\n\n- let's drop the unnecessary columns\n- encode the categorical (no details)\n- impute the necessary columns (again no details)\n- scale both the train and test data for linear models\n- split the data for the model
# Pytorch Training
# predictions
## 1.3. JOBS
## 1.4. MARITAL
## 1.5. EDUCATION
"## 1.6. DEFAULT, HOUSING, LOAN"
# **Target variable distribution**
## **Numerical Feature Distribution**
## **Categorical Feature Distribution**
## **Correlation of Features**
## **Logging Plots to Weights and Biases**
# **Preprocessing**
"**Age, Fare and cabin has missing values.\nwe will see how to fill missing values next.**"
**Visualizations**\n==============
**we can see that Age and Fare are measured on very different scaling. So we need to do feature scaling before predictions.**
"**Looks like Pclass has got highest negative correlation with ""Survived"" followed by Fare, Parch and Age** "
"Missing Value Imputation\n========================\n\n**Its important to fill missing values, because some machine learning algorithms can't accept them eg SVM.**\n\n*But filling missing values with mean/median/mode is also a prediction which may not be 100% accurate, instead you can use models like Decision Trees and Random Forest which handle missing values very well.*"
"PLOTLY TUTORIAL - 1\n***\n\n*Kaggle ML and Data Science Survey was live from August 7th to August 25th. The median time in the survey was 16.4 minutes. Respondents were allowed to complete the survey at any time.*\n\n*Since Kaggle is one of the best data science community, I would like to share main findings of the survey with interactive plotly library. I hope that recommendations of respondents help data enthusiasts.*\n\n*Let's deep dive into the world of data scientists!*\n\n**PLOTLY TUTORIAL - 2 (2015 Flight Delays and Cancellations):** https://www.kaggle.com/hakkisimsek/plotly-tutorial-2\n\n**PLOTLY TUTORIAL - 3 (S&P 500 Stock Data): **\nhttps://www.kaggle.com/hakkisimsek/plotly-tutorial-3\n\n**PLOTLY TUTORIAL - 4 (Google Store Customer Data): https://www.kaggle.com/hakkisimsek/plotly-tutorial-4**\n\n**PLOTLY TUTORIAL - 5 (Kaggle Survey 2018): https://www.kaggle.com/hakkisimsek/plotly-tutorial-5**\n\n\n\nsource: http://www.timqian.com/star-history/#bokeh/bokeh&plotly/dash"
**The tech world seems still a man's world.**
**Top five countries in 2017:**\n1. USA - 4197 participants\n2. India - 2704 participants\n3. Russia - 578 participants\n4. United Kingdom - 535  participants\n5. China - 471 participants\n\n**Top five countries in 2018:**\n1. USA - 4716 participants\n2. India - 4417 participants\n3. China - 1644 participants\n4. Russia - 879 participants\n5. Brazil - 736 participants
**Age Distribution in 2017 (left) vs 2018 (right):**\n\n* 18-21: ** 7.2% - 12.7%**\n* 22-24: ** 14.9% - 21.5%**\n* 25-29: **25.9% - 25.8%**\n* 30-34: **18.5% - 15.8%**\n* 35-39: **12.6% - 9.4%**\n* 40-44: **7.7% - 5.7%**\n* 44+: **12.9% - 8.4%**
**We can say that online courses (MOOC) are mainstream training platforms of data science.**
"**Coursera seems the leader of MOOCs thanks to Andrew NG's' [amazing machine learning courses](http://www.coursera.org/instructor/andrewng).**\n* **After learning basics of machine learning, people discover the world of Kaggle so although it is the last first learning platform in the above ranking, it is graded as the best learning platform. **\n* **Online courses and Stack & Overflow are preferred to textbooks and university courses. The changing face of education in the 21st century!**"
**Basic laptop is enough to follow data science trends so hardware requirements is no excuse not to discover the world.**
"**The most wondering part of the survey is probably salaries. I think that there is no surprise in the ranking. However, before accepting a job offer,  I would recommend you to check [purchasing power parities](http://data.oecd.org/conversion/purchasing-power-parities-ppp.htm).** \n\n**Note: Although I calculate median salary that is more robust to outliers I dropped rate-adjusted monthly salaries less than 100 dollars more than 500.000 dollars for the following parts.**"
"**If you are a newbie, do not be in a hurry about salary, it increases. Learn as many new technologies as possible and be an active member of the data science community. **\n\n**Since there is a big wage gap between US & non-US countries, I analyze them seperately. Do not mix apples & oranges.**"
**Correlation does not imply causation but it seems that satisfaction and salary go hand in hand.**
**It seems that there is no gender inequality in terms of salary in data science world.**
"**But wait! After 60, there is something weird it could change all the picture. Maybe gender-gap?? Let's check box-plot!**"
"**Yes, we are right to being suspicious. Median wage of male is higher than median wage of female.  \nLet's investigate gender inequality in US vs. Non-US.**"
"**It seems that women are underpaid after 35 and they are out of labor force especially after their 50s in non-US countries!!**\n\n**Let's investigate majors & titles. As expected, computer science, mathematics, statistics and engineering are leading degree majors in data science.**"
\n# 1. \n## Import packages
***
"The **data points** are shown in **light blue** on the left hand side of the grey dashed line. The **orange points** represent the **true future values**, and the **solid dark blue line** shows the **prediction** from the data points. \n\n- When the outliers are left in the model, the **model overfits** and is sensitive to these points. Therefore, it predicts values much higher than the true future values. *This is what we want to avoid.*\n- However, when outliers are removed, it **predicts much more accurately** with a generalised model that splits the distribution of the data points evenly.\n- ***This is very important in Machine Learning because our goal is to create robust models that are able to generalise to future situations.*** If we create a model that is very sensitive and tuned to fit outliers, this will result in a model that over or underfits. If we can create models that are able to cancel out the distractions and noise of outliers, this is usually a better situation.\n\nBy referring to the **Ames Housing Dataset** link provided in the **Acknowledgements**, you'll see that the author outlines there are some outliers that must be treated: \n\n*"" Although all known errors were corrected in the data, no observations have been removed due to unusual values and all final residential sales from the initial data set are included in the data presented with this article. There are five observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will quickly indicate these points). Three of them are true outliers (Partial Sales that likely don’t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these five unusual observations) before assigning it to students. ""*\n\n- First, let's plot the two features stated against one another, to identify the outliers. Then we will remove them. The chart on the left shows the data before removing the outliers, and the chart on the right shows after."
"Here we see that we have 1 remaining feature with missing values, Utilities.\n\n- Let's inspect this closer to see how to treat it."
"This tell us that within the training dataset, Utilities has two unique values: ""AllPub"" and ""NoSeWa"". With ""AllPub"" being by far the most common.\n- However, the test dataset has only 1 value for this column, which means that it holds no predictive power because it is a constant for all test observations.\n\nTherefore, we can drop this column"
## How to compare Regression Models
![winner2.jpg](attachment:6803c413-577f-493f-886d-22179bb4b3e3.jpg)
## 3. 1 Import Libraries
## 3.2 Defining function for regression metrics
\n**Import Libraries**\n\nThere are basically 4 type of libraries which you have to import\n\n1. Pandas :- For reading / writing data\n2. Matplotlib to display images\n3. Tensorflow Keras models :- Need a model to predict right !! \n4. Tensorflow Keras layers :- Every NN needs layers and CNN needs well a couple of layers.\n\nLayers needed by CNN\n1. Conv2D :- Basic Convolutional layer . Here we will be using a 64 neuron layer\n2. Dense :- Dense layer is needed by every neural network to finally output the result however every once in while using a Dense layer helps in making model learn.\n3. MaxPooling :- CNN has a concept of max pooling. After every convoulution we get some values in a kernel. However in max pooling we select max kernel value.\n4. Flatten:- Conv2D layer returns doesn't return a flatten data hence we need Flatten layer before feeding it into final Dense layer\n
"We need to train a model first so we will check training data In the below code we are iterating through all images in train folder and then we will split image name with deliminiter ""."" We have names like dog.0, dog.1, cat.2 etc.. Hence after splitting we are gonna get results like ""dog', ""cat"" as category value of the image. To make this example more easy we will consider dog as ""1"" and cat as ""0""\n\nNow every image is actually a set of pixels so how to get our computer know that. Its simple convert all those pixels into an array. So we are going to use here a **cv2** library to read our image into an array and also it will read as a gray scale image.\n\n>cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n\nNow we have got here images of all sizes . We have landscape, portrait etc etc.. We need to make them all of a single size so it can be analysed pretty easily. How to do that very very simple again. Use cv2\n\n>cv2.resize(img_array, dsize=(80, 80))\n\nOk so we have got image array and its resized but do you believe whatever I just did was correct. Was the resizing of 80 X 80 good or is it bad. Should check it. How can we do that. There is one answer matplotlib. Using the below code we can display the image.\n\n>plt.imshow(new_img_array,cmap=""gray"")\n\nPlease run the below the code to get better understanding. I have applied break here to just display 1 image. You can try out with 50 X 50 or 100 X100 to see the difference.\n\n\n"
"Okay so the above code was more for understanding purpose. Nowe we will get to the real part of coding here.\n\nDeclare your training array X and your target array y. Here X will be the array of pixels and y will be value 0 or 1 indicating its a dog or cat\nWrite convert function to map category ""dog"" or ""cat"" into 1 and 0\n\nCreate a function create_test_data which takes all training images into a loop. Converts into image array.Resize image into 80 X80. Append image into X array. Append category value into y array."
# 2. | Importing Libraries 📚\n\n    👉 Importing libraries that will be used in this notebook.\n\n
# 3. | Color Palettes 🎨\n\n    👉 This section will create some color palettes that will be used in this notebook.\n
"# 4. | Reading Dataset 👓\n\n    👉 After importing libraries, the dataset that will be used will be imported.\n\n"
### 5.1.2 | Item_Fat_Content
"\n    👉 It can be seen that most of the products are categorized as ""Low Fat"" products with a percentage of 59.71%.\n    👉 There are inconsistent values, means that some values have same meaning but in different form. (Ex: ""LF"" and ""low fat"" for Low Fat"", and ""reg"" for Regular).\n"
### 5.1.4 | Outlet_Identifier
"\n    👉 There are ten outlets, with almost all the outlets have the same number of outlets (around 11%).\n    👉 However, ""OUT10"" and ""OUT19"" have the least number of outlets (around 6%).\n"
### 5.1.5 | Outlet_Size
"\n    👉 There are 3 types of outlet size, small, medium, and high size.\n    👉 Medium size becomes the outlet size with the most number (45.69%).\n    👉 However, the smallest number outlet size is High size (only 15.25%).\n"
### 5.1.6 | Outlet_Location_Type
"\n    👉 There are 3 levels of outlet location, tier 1, tier 2, and tier 3.\n    👉 Tier 3 becomes the outlet location with the most number (39.31%).\n    👉 However, the smallest number outlet location is Tier 1 (only 28.02%).\n"
"### **Imputation of Age variable**\n\n- `Age` is a continuous variable. First, we will check the distribution of `age` variable."
"- We can see that the `age` distribution is skewed. So, we will use the median imputation."
"Importing the Essential Libraries, Metrics"
Loading the Data
***Visualizing the correlations between numerical variables***
Feature Selection
***Visualizing the Correlation between the numerical variables using pairplot visualization***
***Visualizing the Correlation between each column and the target variable using jointplot visualization***
"X, y Split"
This simple histogram shows the count of digits in the training data for each number. This graphic is used to visualize if there is an unequal sample size among the digits. The sample size for each digit appear to be comparable. There is no issue of unequal sampling. 
"This graphic plots the first few digits in the training set to show how this pixel data is representing digits, and to also show how the handwriting varies. Not all digits are written the same. For example, there is a lot of variation in how people write 4s and 9s. "
"# PCA\nThere are many features in this data resulting in high dimensionality. PCA is used to compress the features into a small but informative set of features before using the data in a machine learning model. Data is normalized before PCA is applied. This is so the scale of the data does not throw of the PCA, and so the 0's are represented meaningfully.  There is unequal variance in this data, and features with larger variance will influence the PCA more, creating bias. This is why the data is normalized. "
"This plot shows the separation of classes (digits) based on the first PCAs. Theoretically, these PCs should explain most of the variance in the data, enough to show separation in the groups of digits."
#Neural Network
## Import the Data
Now we should create our own network and train it. First we'll want to define the criterion (something like nn.CrossEntropyLoss or nn.NLLLoss) and the optimizer (typically optim.SGD or optim.Adam).\n* Make a forward pass through the network\n* Use the network output to calculate the loss\n* Perform a backward pass through the network with loss.backward() to calculate the gradients\n* Take a step with the optimizer to update the weights
"Oh what just happended there? I'll explain not to worry.\n\n#### Training: \n* I'm looping over the train loader, pulling out the images and labels.\n* Note that I have a line of code optimizer.zero_grad(). When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches.\n* I have named the next variable log_ps because our model gives us back logs of class probabilities, you can take exponent to convert it to normal probabilities which I've done down below for validation. \n* We calculate the loss. Then backpropagate through the network. We then make one optimizer step. Which brings us closer and closer to the global optimum.\n\n#### Validation\n* We turn off the gradients for validation as it is not needed and saves a lot of memory and computation. Note that we should turn it back on after each step of validation.\n* We loop over the test_loader and essentially repeat some steps we have done above. Since it's validation we don't need to backpropagate. \n* The next step - With the probabilities, we can get the most likely class using the ps.topk method. This returns the $k$ highest values. Since we just want the most likely class, we can use ps.topk(1). This returns a tuple of the top-$k$ values and the top-$k$ indices. If the highest value is the fifth element, we'll get back 4 as the index.\n* Then we check if the predicted value is equal to the actual value. \n* We then calculate the percentage of correct predictions, which indeed is using the mean of our top predictions. But you cannot just use torch.mean because topk returns a byte tensor but we need a float tensor to perform torch.mean we do that in the next step.\n\nThe same process is repeated over and over again. The results are printed on each step. With this simple model we're able to get about 98% accuracy on validation which is awesome, isn't it?\n\nHope that made sense. "
## Visualizing Model Performance
This graph looks decent to me. We're doing fairly well for our first model in PyTorch.
___\n* _How many orders we have for each status?_\n___
"By the time this dataset was created, the highest amount of orders went from delivered ones. Only 3% of all orders came from the other status."
"So now we can purpose a complete analysis on orders amount of brazilian e-commerce during the period of the dataset. For that let's plot three graphs using a `GridSpec` with the aim answear the following questions:\n\n    1. Is there any growing trend on brazilian e-commerce?\n    2. On what day of week brazilians customers tend to do online purchasing?\n    3. What time brazilians customers tend do buy (Dawn, Morning, Afternoon or Night)?"
"By the chart above we can conclude:\n\n* E-commerce on Brazil really has a growing trend along the time. We can see some seasonality with peaks at specific months, but in general we can see clear that customers are more prone to buy things online than before.\n* Monday are the prefered day for brazilian's customers and they tend to buy more at afternoons.\n\n_Obs: we have a sharp decrease between August 2018 and September 2018 and maybe the origin of that is related to noise on data. For further comparison between 2017 and 2018, let's just consider orders between January and August in both years_"
___\n* _E-commerce: a comparison between 2017 and 2018_\n___
\n3.2 E-Commerce Around Brazil\n\nGo to TOC
# Prepare for data analysis\n\n## Load packages
## Check the data\n\nWe verify what data is available.
Let's plot the audio frames
Let's zoom in on first 1000 frames
\n### Audio Length\n\nWe shall now analyze the lengths of the audio files in our dataset
"The number of categories is large, so let's check the frame distributions of top 25 categories."
We observe:  \nThe distribution of audio length across labels is non-uniform and has high variance as the previous competition.  \n\nLet's now analyze the frame length distribution in train and test.
We observe:\n- Majority of the audio files are short.\n- There are an `abnormal` length in the train histogram. Let's analyze them.
"# About the dataset\n\nContext\nOur world population is expected to grow from 7.3 billion today to 9.7 billion in the year 2050. Finding solutions for feeding the growing world population has become a hot topic for food and agriculture organizations, entrepreneurs and philanthropists. These solutions range from changing the way we grow our food to changing the way we eat. To make things harder, the world's climate is changing and it is both affecting and affected by the way we grow our food – agriculture. This dataset provides an insight on our worldwide food production - focusing on a comparison between food produced for human consumption and feed produced for animals.\n\nContent\nThe Food and Agriculture Organization of the United Nations provides free access to food and agriculture data for over 245 countries and territories, from the year 1961 to the most recent update (depends on the dataset). One dataset from the FAO's database is the Food Balance Sheets. It presents a comprehensive picture of the pattern of a country's food supply during a specified reference period, the last time an update was loaded to the FAO database was in 2013. The food balance sheet shows for each food item the sources of supply and its utilization. This chunk of the dataset is focused on two utilizations of each food item available:\n\nFood - refers to the total amount of the food item available as human food during the reference period.\nFeed - refers to the quantity of the food item available for feeding to the livestock and poultry during the reference period.\nDataset's attributes:\n\nArea code - Country name abbreviation\nArea - County name\nItem - Food item\nElement - Food or Feed\nLatitude - geographic coordinate that specifies the north–south position of a point on the Earth's surface\nLongitude - geographic coordinate that specifies the east-west position of a point on the Earth's surface\nProduction per year - Amount of food item produced in 1000 tonnes\n\nThis is a simple exploratory notebook that heavily expolits pandas and seaborn"
Let's see what the data looks like...
# Plot for annual produce of different countries with quantity in y-axis and years in x-axis
"Clearly, China, India and US stand out here. So, these are the countries with most food and feed production.\n\nNow, let's have a close look at their food and feed data\n\n# Food and feed plot for the whole dataset"
"So, there is a huge difference in food and feed production. Now, we have obvious assumptions about the following plots after looking at this huge difference.\n\n# Food and feed plot for the largest producers(India, USA, China)"
"Though, there is a huge difference between feed and food production, these countries' total production and their ranks depend on feed production."
"So, cereals, fruits and maize are the most produced items in the last 50 years\n\n# Food and feed plot for most produced items "
"# Now, we plot a heatmap of correlation of produce in difference years"
"So, we gather that a given year's production is more similar to its immediate previous and immediate following years."
# Heatmap of production of food items over years\n\nThis will detect the items whose production has drastically increased over the years
"There is considerable growth in production of Palmkernel oil, Meat/Aquatic animals, ricebran oil, cottonseed, seafood, offals, roots, poultry meat, mutton, bear, cocoa, coffee and soyabean oil.\nThere has been exceptional growth in production of onions, cream, sugar crops, treenuts, butter/ghee and to some extent starchy roots."
## Q1 & Q2. Age/Gender Distribution
You can see that more than half of the total is under 30.
"Obviously, I felt that there was a lot of influx of women, but I can see that there are still few."
## Q4 & Q6. Degree / Experience\n 
"I thought there would be the most bachelors, but there are the most masters."
"About 75% have less than 5 years of experience and 25% have more than 5 years.\n\nThis time, let's look at the two questions at once."
The most common is a 1-2 year bachelor's degree and a 3-5 year graduate.
## Q14. Visualization Library 
"1. The most basic library, matplotlib, is used the most.\n2. Plotly is the most used for interactive visualization.\n3. It is unfortunate that the number of geovisualization libraries is insufficient. \n    - Folium is still less used than Geoplotlib.\n4. There are more d3 users than I think. Is there any reason to use d3 even though it is ML unfriendly?"
"# ART BY GAN\n\n\n\nIn this Notebook, I will build a Generative Adversarial Network  (GAN) to illustrate the workings of a Generative Adversarial Network and to generate images. Generative modelling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data. As GANs work by identifying the patterns in the data, I will be using oil painted portraits. However, glancing over the dataset gives me an idea that it is going to be a long shot. The orientation and poses in the dataset vary vastly. Keeping that in mind I am still willing to give it a try. Only because portraits are my jam. I basically love oil painted portraits. \n\n\n\n TABLE OF CONTENTS   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. DATA LOADING & PREPREPROCESSING](#2)\n    \n* [3. BUILDING GAN](#3)\n    * [3.1 The Generator](#3.1)\n    * [3.2 The Discriminator](#3.2)\n    \n    \n* [4. GAN COMPILATION](#4)  \n    \n* [5. TRAINING THE MODEL](#5) \n      \n* [6. EVALUATING THE MODEL](#6)\n    \n* [7. CONCLUSION](#7)\n    \n* [8. END](#8)\n\n\n\n# IMPORTING LIBRARIES\n    \nThe following Libraries will be used in the project\n    "
"##### \n# DATA LOADING & PREPREPROCESSING\n\nFor this project, I am using .jpg files of images of portraits. The dataset includes various artists. I am loading data as TensorFlow.Dataset,, with a batch size of 64. I have reduced the image size to (64,64), presuming, it will be computationally less taxing on the GPU.\n\nLoading the data"
"Now that I have the dataset loaded, let us have a look at a few images."
"Most of the images are portraits. A portrait is a painting representation of a person, The face is predominantly depicted portraits along with expressions and postures. To represent the personality of the subject. Since our model is relative a smaller GAN we have reduced the size of the image. \n\nPreprocessing the data\n\n**Normalization:** For the data normalization, I will convert the data in the range between 0 to 1. This helps in fast convergence and makes it easy for the computer to do calculations faster. \nEach of the three RGB channels in the image can take pixel values ranging from 0 to 256. Dividing it by 255 converts it to a range between 0 to 1. By doing this we "
"Now that the Generator is framed, let us see what random output our untrained Generator produces to get an idea of the process. "
"Clearly, the output is a random seed containing noise as the Generator is not trained yet. \n\n\n# The Discriminator\n\nIn GANs the Generator works along with the Discriminator. \n\nThe Discriminator network decided whether the data is fake aka created by the Generator or real i.e. from the original input data. To do so it applies a binary classification method using a sigmoid function to get an output in the range of 0 to 1.\n\nBuilding a Discriminator"
"\n# EVALUATING THE MODEL\n\nNow that I have my model trained, let us see how it performs.\nHaving a look at the performance of the model via Learning Curves\n\nPloting the Learning Curves"
This looks alright-ish! \n\nLet us get some portraits done by the GAN and appreciate the art created by this AI. \nTo get the art output I will create a function that saves the output portraits generated. We will be plotting the generated Portraits\n\nAI makes Artwork
"# How To Recommend Anything?\n\n**To support people best possible on their way through life, it is necessary to have an optimal recommendation on hand.**\nWhether you want to introduce people among themselves in your social network, try to recommend a suitable supplement for the shopping basket of your customers or need a hint for yourself which movie to watch in the evening, there are unlimited possibilities to apply recommendation engines/systems around us.\n\nIn this notebook I will explore and compare different algorithms and approaches to recommend anything. I am using the **[netflix movie-dataset](https://www.kaggle.com/netflix-inc/netflix-prize-data/home)** and the **[movies-dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset/home)** for this purpose.\n\nFeel free to suggest suggestions or to comment comments.\n\n+ [1. Import Libraries](#1)\n+ [2. Load Movie-Data](#2)\n+ [3. Load User-Data And Preprocess Data-Structure](#3)\n+ [4. When Were The Movies Released?](#4)\n+ [5. How Are The Ratings Distributed?](#5)\n+ [6. When Have The Movies Been Rated?](#6)\n+ [7. How Are The Number Of Ratings Distributed For The Movies And The Users?](#7)\n+ [8. Filter Sparse Movies And Users](#8)\n+ [9. Create Train- And Testset](#9)\n+ [10. Transform The User-Ratings To User-Movie-Matrix](#10)\n+ [11. Recommendation Engines](#11)\n + [11.1. Mean Rating](#11.1)\n + [11.2. Weighted Mean Rating](#11.2)\n + [11.3. Cosine User-User Similarity](#11.3)\n + [11.4. Cosine TFIDF Movie Description Similarity](#11.4)\n + [11.5. Matrix Factorisation With Keras And Gradient Descent](#11.5)\n + [11.6. Deep Learning With Keras](#11.6)\n + [11.7. Deep Hybrid System With Metadata And Keras](#11.7)\n+ [12. Exploring Python Libraries](#12)\n + [12.1. Surprise Library](#12.1)\n + [12.2. Lightfm Library](#12.2)\n+ [13. Conclusion](#13)\n\n***\n## 1. Import Libraries"
***\n## 2. Load Movie-Data
There are about **24.000.000 different ratings**.\nI loaded only a single file of four to reduce memory footprint and accelerate computation. Keep in mind that this approach could introduce biases in the data.\n\n***\n## 4. When Were The Movies Released?
Many movies on Netflix have been released in this millennial. Whether Netflix prefers young movies or there are no old movies left can not be deduced from this plot.\nThe decline for the rightmost point is probably caused by an **incomplete last year.**\n\n***\n## 5. How Are The Ratings Distributed?
"Netflix movies rarely have a rating lower than three. **Most ratings have between three and four stars.**\nThe distribution is probably biased, since only people liking the movies proceed to be customers and others presumably will leave the platform.\n\n***\n## 6. When Have The Movies Been Rated?"
With beginning of november 2005  a strange decline in ratings can be observed. Furthermore two unnormal peaks are in january and april 2005.\n\n***\n## 7. How Are The Number Of Ratings Distributed For The Movies And The Users?\n\n
The ratings per movie as well as the ratings per user both have nearly a perfect **exponential decay**. Only very few \nmovies/users have many ratings. \n\n***\n## 8. Filter Sparse Movies And Users\n\nTo reduce the dimensionality of the dataset I am filtering rarely rated movies and rarely rating users out.
"***\n## 11. Recommendation Engines\n### 11.1. Mean Rating\n\nComputing the **mean rating for all movies** creates a ranking. The recommendation will be the same for all users and can be **used if there is no information on the user.**\nVariations of this approach can be separate rankings for each country/year/gender/... and to use them individually to recommend movies/items to the user.\n\nIt has to be noted that this approach is **biased and favours movies with fewer ratings**, since large numbers of ratings tend to be less extreme in its mean ratings."
### 11..2. Weighted Mean Rating\n\nTo tackle the problem of the unstable mean with few ratings **e.g. IDMb uses a weighted rating.** Many good ratings outweigh few in this algorithm. \n
"The variable **""m"" can be seen as regularizing parameter.** Changing it determines how  much weight is put onto the movies with many ratings.\nEven if there is a better ranking the RMSE decreased slightly. There is a **trade-off between interpretability and predictive power.**\n\n### 11.3. Cosine User-User Similarity\n\nInterpreting each row of the matrix as a vector, a similarity between all user-vectors can be computed. This enables us to find all similar users and to work on user-specific recommendations. **Recommending high rated movies of similar users** to a specific user seems reasonable.\nSince there are still empty values left in the matrix, we have to use a reliable way to impute a decent value. A simple first approach is to **fill in the mean of each user into the empty values.**\nAfterwards the **ratings of all similar users will be weighted with their similarity score and the mean will be computed.** Filtering for the unrated movies of a user reveals the best recommendations.\nYou can easily adapt this process to find similar items by computing the item-item similarity the same way. Since the matrix is mostly sparse and there are more users than items, this could be better for the RMSE score."
"### 11.4. Cosine TFIDF Movie Description Similarity\n\nIf there is no historical data for a user or there is reliable metadata for each movie, it can be useful to **compare the metadata of the movies to find similar ones.**\nIn this approch I will use the **movie description to create a TFIDF-matrix**, which counts and weights words in all descriptions, and compute a cosine similarity between all of those sparse text-vectors. This can easily be extended to more or different features if you like.\nUnfortunately it is impossible for this model to compute a RMSE score, since the model does not recommend the movies directly.\nIn this way it is possible to **find movies closly related to each other**, but it is **hard to find movies of different genres/categories.**"
"### 11.5. Matrix Factorisation With Keras And Gradient Descent\n\nThe **user-movie rating matrix is high dimensional and sparse**, therefore I am going to reduce the dimensionality to represent the data in a dense form.\n**Using matrix factorisation a large matrix can be estimated/decomposed into two long but slim matrices.** With gradient descent it is possible to adjust these matrices to represent the given ratings. The **gradient descent algorithm finds latent variables which represent the underlying structure** of the dataset. Afterwards these latent variables can be used to reconstruct the original matrix and to predict the missing ratings for each user.\nIn this case the model has not been trained to convergence and is not hyperparameter optimized."
"# OpenVaccine: mRNA Vaccine Degradation Prediction\n\n\n\n\n\nIn this competition, you will be predicting the degradation rates at various locations along RNA sequence.\n\nThere are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: `reactivity`, `deg_Mg_pH10`, and `deg_Mg_50C`."
## Data Exploration\nLets take a look at the data. It's provided in json format.
Lets plot this data for 25 examples.
"# Sample Submission\n\nLets quickly look at the sample submission format. Even though we submit for addional solumns, only three columns are scored: `reactivity`, `deg_Mg_pH10`, and `deg_Mg_50C`"
## signal_to_noise feature
## seq_length\n\nTrain data consists of only 107 sequence length. The test data contains mostly 130 sequence lengths.
# Baseline Submission [0.47840 LB]\n## Predict the average value for each target column\nLets first calculate the average value for the target columns. And then create a 91 length vector as a baseline submission.
## Fill in predictions with the mean value\n
# Exploratory Data Analysis\n\n### Heatmap to check null/missing values
" Let's have a closer look at the distribution of temperature and ph.\n    \nIt is symmetrical and bell shaped, showing that trials will usually give a result near the average, but will occasionally deviate by large amounts. It's also fascinating how these two really resemble each other!"
" A quick check if the dataset is balanced or not. If found imbalanced, we would have to downsample some targets which are more in quantity but so far everything looks good! "
 A very important plot to visualize the diagonal distribution between two features for all the combinations! It is great to visualize how classes differ from each other in a particular space.
"#### During rainy season, average rainfall is high (average 120 mm) and temperature is mildly chill (less than 30'C).\n\n#### Rain affects soil moisture which affects ph of the soil. Here are the crops which are likely to be planted during this season. \n\n-  Rice needs heavy rainfall (>200 mm) and a humidity above 80%. No wonder major rice production in India comes from East Coasts which has average of 220 mm rainfall every year!\n-  Coconut is a tropical crop and needs high humidity therefore explaining massive exports from coastal areas around the country."
#### This graph correlates with average potassium (K) and average nitrogen (N) value (both>50). \n#### These soil ingredients direcly affects nutrition value of the food. Fruits which have high nutrients typically has consistent potassium values.
Let's try to plot a specfic case of pairplot between `humidity` and `K` (potassium levels in the soil.)\n\n#### `sns.jointplot()` can be used for bivariate analysis to plot between humidity and K levels based on Label type. It further generates frequency distribution of classes with respect to features
#### We can see ph values are critical when it comes to soil. A stability between 6 and 7 is preffered
#### Another interesting analysis where Phosphorous levels are quite differentiable when it rains heavily (above 150 mm).
"#### Further analyzing phosphorous levels.\n\nWhen humidity is less than 65, almost same phosphor levels(approx 14 to 25) are required for 6 crops which could be grown just based on the amount of rain expected over the next few weeks."
# DATA PRE-PROCESSING\n\n### Let's make the data ready for machine learning model
**Correlation visualization between features. We can see how Phosphorous levels and Potassium levels are highly correlated.**
"# FEATURE SCALING\n**Feature scaling is required before creating training data and feeding it to the model.**\n\nAs we saw earlier, two of our features (temperature and ph) are gaussian distributed, therefore scaling them between 0 and 1 with MinMaxScaler."
Machine Learning Class\n
Up to outlines
"Also note that one does not have to use only words. In some cases, it is possible to generate N-grams of characters. This approach would be able to account for similarity of related words or handle typos."
"Adding onto the Bag of Words idea: words that are rarely found in the corpus (in all the documents of this dataset) but are present in this particular document might be more important. Then it makes sense to increase the weight of more domain-specific words to separate them out from common words. This approach is called TF-IDF (term frequency-inverse document frequency), which cannot be written in a few lines, so you should look into the details in references such as [this wiki](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). The default option is as follows:\n\n$$ \large idf(t,D) = \log\frac{\mid D\mid}{df(d,t)+1} $$\n\n$$ \large tfidf(t,d,D) = tf(t,d) \times idf(t,D) $$\n\nAnalogs of Bag of Words can be found outside of text problems e.g. bag of sites in the [Catch Me If You Can competition](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking), [bag of apps](https://www.kaggle.com/xiaoml/talkingdata-mobile-user-demographics/bag-of-app-id-python-2-27392), [bag of events](http://www.interdigital.com/download/58540a46e3b9659c9f000372), etc.\n\n![image](https://habrastorage.org/webt/r7/sq/my/r7sqmyj1nmqmzltaftt40zi7-gw.png)\n\nUsing these algorithms, it is possible to obtain a working solution for a simple problem, which can serve as a baseline. However, for those who do not like the classics, there are new approaches. The most popular method in the new wave is Word2Vec, but there are a few alternatives as well (GloVe, Fasttext, etc.).\n\nWord2Vec is a special case of the word embedding algorithms. Using Word2Vec and similar models, we can not only vectorize words in a high-dimensional space (typically a few hundred dimensions) but also compare their semantic similarity. This is a classic example of operations that can be performed on vectorized concepts: king - man + woman = queen.\n\n![image](https://cdn-images-1.medium.com/max/800/1*K5X4N-MJKt8FGFtrTHwidg.gif)\n\nIt is worth noting that this model does not comprehend the meaning of the words but simply tries to position the vectors such that words used in common context are close to each other. If this is not taken into account, a lot of fun examples will come up.\n\nSuch models need to be trained on very large datasets in order for the vector coordinates to capture the semantics. A pretrained model for your own tasks can be downloaded [here](https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models).\n\nSimilar methods are applied in other areas such as bioinformatics. An unexpected application is [food2vec](https://jaan.io/food2vec-augmented-cooking-machine-intelligence/). You can probably think of a few other fresh ideas; the concept is universal enough."
"\n\nWeights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. **Kaggle competitions require fast-paced model development and evaluation**. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.\n\n> ⏳ Lots of components = lots of places to go wrong = lots of time spent debugging \n\nW&B can be useful for Kaggle competition with it's lightweight and interoperable tools:\n\n* quickly track experiments,\n* version and iterate on datasets, \n* evaluate model performance,\n* reproduce models,\n* visualize results and spot regressions,\n* and share findings with colleagues.\n\nTo learn more about Weights and Biases check out this [kernel](https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases)."
"> You can learn more about using W&B in this introduction kernel, [Experiment Tracking with Weights and Biases](https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases)."
### Let's quickly test the function to read the DICOM file.
"> Each MRI image has a resolution of 512x512 pixels. \n\n> There are some images without any useful content, i.e, there's no brainy stuff in the image."
Let's look at gaps
"Great, no missing information!"
Let's look at the quantitative distribution of the target variable
Lets look at continuous columns
\nSTATISTICAL TESTS
" 2.1.2 Correlation Heatmap \n\nCorrelation heatmap measures nullity correlation between columns of the dataset. It shows how strongly the presence or absence of one feature affects the other.\n\nNullity correlation ranges from(-1 to 1):\n- -1 means if one column(attribute) is present, the other is almost certainly absent.\n- 0 means there is no dependence between the columns(attributes).\n- 1 means if one column(attributes) is present, the other is also certainly present.\n\nUnlike in a familiar correlation heatmap, if you see here, many columns are missing. Those columns which are always full or always empty have no meaningful correlation and are removed from the visualization.\n\nThe heatmap is helpful for identifying data completeness correlations between attribute pairs, but it has the limited explanatory ability for broader relationships and no special support for really big datasets."
From above visualization we can easily interpret missingness of attribute rate_of_interest and upfront_charges is dependent on each other(correlation value = 1) means if one will be present another will be present. 
# Exploratory Data Analisys\nIn the following we're gonna see some data analysis on the corpus. \n\nSpecifically:\n- General dataset infos\n    - Number of samples\n    - Class Label Distributiom\n- Text analysis (Done both on 'text' and 'selected_text' for trainin and on 'text' for the test set)\n    - Number of characters in tweets \n    - Number of words in a tweet\n    - Average word lenght in a tweet\n    - Word distribution\n    - Number of unique words\n    - Top Bi-grams and Tri-grams\n    \n
"## Insights\n\nFrom the above analysis, we can say the following:\n1. Both training and test set have a majority of **netrual samples** and a (more or less) equal amount of positive and negative samples. The sentiment distribution of the test set follows the one of the training set.\n2. As expected, positive words like ""good"", ""love"", ""happy"" are in the top of the **most frequent words** for both text and selected_text for the positive class, also the test set seems to follow the same line.\n3. **Negative and Neutral posts** seems to be pretty **high corelated** w.r.t to word, bi-gram and tri-gram analysis. \n4. The **selected_text** seems to have a very good bi-gram, tri-gram **corelation** w.r.t the **text**. Maybe a simple approach based on these features can work.\n5. Neutral posts seem to be longer."
# Importing libraries:
# Importing Data:
# 2.Ngram Analysis: 
## Bi-gram Plots:
# 3. Histogram plot of Number of words
We can observe from above histogram plot that the number of words in train text and test text ranges from 1 to 30.Selected text words mostly fall in range of 1-10. 
# 4.Histogram plots of Number of characters
From above plot we can see that number of characters in test and train set was in same range.In selected text the range flows from 3 to 138 Characters.
## **Null Values**\n\n📌 There are no null values in the dataset
## **Target Distribution**
"# Undersampling and oversampling imbalanced data\n\n## Introduction\n\nOftentimes in practical machine learning problems there will be significant differences in the rarity of different classes of data being predicted. For example, when detecting cancer we can expect to have datasets with large numbers of false outcomes, and a relatively smaller number of true outcomes. \n\nThe overall performance of any model trained on such data will be constrained by its ability to predict rare points. In problems where these rare points are only equally important or perhaps less important than non-rare points, this constraint may only become significant in the later ""tuning"" stages of building the model. But in problems where the rare points are important, or even the _point_ of the classifier (as in a cancer example), dealing with their scarcity is a first-order concern for the mode builder.\n\nTangentially, note that the relative importance of performance on rare observations should inform your choice of error metric for the problem you are working on; the more important they are, the more your metric should penalize underperformance on them. See my previous [Model Fit Metrics](https://www.kaggle.com/residentmario/model-fit-metrics/) and [Log Loss](https://www.kaggle.com/residentmario/log-loss-with-new-york-city-building-sales/) notebooks for slightly more detail on which error metrics do and don't care about this problem.\n\nSeveral different techniques exist in the practice for dealing with imbalanced dataset. The most naive class of techniques is **sampling**: changing the data presented to the model by undersampling common classes, oversampling (duplicating) rare classes, or both.\n\n## Motivation\n\nWe'll motivate why under- and over- sampling is useful with an example. The following visualization shows the radical effect that the relative number of points-per-class in a dataset has on classification as performed by a (linear-kernel) Support Vector Machine (for a quick primer on SVMs check [here](https://www.kaggle.com/residentmario/primer-on-support-vector-machines)), shamelessly stolen from the `imbalanced-learn` documentation [here](http://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/over-sampling/plot_comparison_over_sampling.html)."
"As you can see, when a dataset is dominated by one or few classes, to the exclusion of some other classes, the optimal solution can break down to collapse: a model which simply classifies all or most points in the majority class (as in the first and second visualizations in this grid). However, as the number of observations per point approaches an equal split, the classifier becomes less and less biased.\n\nRe-sampling points that are being fed into the model is the simplest way to fix model errors like this one stemming from rare class problems. Not all datasets have this issue, but for those that due, dealing with this issue is an important early step in modeling the data."
## Getting some sample data\n\nWe'll use the following data for the sake of illustration (taken from the `sklearn` documentation):
"## Raw over- and under- sampling\n\nA group of researchers implemented the full suite of modern data sampling techniques with the `imbalance-learn` contrib module for `sklearn`. This submodule is installed as part of the base `sklearn` install by default, so it should be available to everyone. It comes with its own documentation as well; that is available [here](http://contrib.scikit-learn.org/imbalanced-learn/stable/).\n\n`imblearn` implements over-sampling and under-sampling using dedicated classes."
"In the first graph we have oversampled the dataset, duplicating points from the rare class 2 and the ultra rare class 3 in order to match the common class 1. This results in many points getting ""pasted over"" a huge number of times, as there are just 64 distinct points in class 2, but 4700 of them in class 1.\n\nIn the second graph we have undersampled the dataset. This goes the other way: classes 1 and 2 are reduced in numeric volume until they reach the same number of observations as class 3, at 64.\n\nWhich of these two fields of points are you better off training your classifier on? In extreme cases where the number of observations in the rare class(es) is *really* small, oversampling is better, as you will not lose important information on the distribution of the other classes in the dataset. For example, if there were just five observations in class 3, we'd have an awful time training a classifier on just 15 (5 times 3) undersampled points!\n\nOutside of this case however, the performance of the one or the other will be most indistinguishable. Remember, sampling doesn't introduce new information in the dataset, it (hopefully) merely shifts it around so as to increase the ""numerical stability"" of the resulting models."
"By default the number of observations will be balanced, e.g. each class will appear equally many times. There's also a `ratio` parameter, which allows you to choose the number of observations per class (in terms of integer absolute numbers, e.g. ""60 class 1 observations"") to push into your sample dataset. For example:"
"This resampling technique is useful if the distribution of the classes in your training data differs from the distribution of the classes in the ""real world"" (and you have a sufficiently large amount of data to have the luxury of ""slimming down""). In theory you should not try to work with such data, but in practice this scenario arises often. For example, suppose that you have a dataset of kidney health checks taken from a health clinic, and we are classifying them as health and unhealthy according to some metric. We would naturally expect kidneys from the general population to be healthier, on average, than the one represented in clinical data. For us to succeed in building a generalizable model we would need to resample as a step one!"
"## Sample size learning curves\n\nIn the context of the bias-variance tradeoff, what we hope to achieve by resampling data is to reduce bias, or underfit (recall the clearly one-class-is-the-only-class model from the illustrative example, which is a) more than we increase variance, or overfit (which goes up when decrease the number of input observations or copy-paste points). A way of quantifying this hope is to look at a **learning curve**.\n\nA learning curve illustrates the level of fit of a model as we increase the number of observations we show it. I gave a very brief demo on learning curves in [a previous notebook](https://www.kaggle.com/residentmario/learning-curves-with-zillow-economics-data/), and if you're totally unfamiliar with them I suggest taking a look at that first.\n\nWe can generate a learning curve for any sampling method by training a simple model on an incrementally larger number of sampled points, then scoring the resulting model on the full test set. When the performance of the model becomes stable with respect to the number of observations it was trained on, then we have evidence that that's the number of observations we need to include in the sample in order to represent ""enough"" of the data distribution to not hurt accuracy much.\n\nHere is a simple learning curve built on sampling the data from our synthetic dataset:"
"Here we see that the performance of the model becomes stable around 30 observations per class. This means that the decision boundaries on the Support Vector Machine we train on this sampled data stops moving around (much) some time after the 30-point mark.  which is in turn good evidence that the 64-point `RandomUnderSampler` we've been working with won't introduce significant variance to the model by virtue of having too few points. Hence if under-sampling helps significantly with our rare-point bias problem, it is well worth doing!\n\nBuilding and checking the sample learning curve is easy and well worth doing. Note that for two-dimensional, normally distributed data like this the number of observations necessary to achieve stability will of course be quite small. In the general case, except stability to require potentally quite a lot of points, certainly more than this."
"## Ensemble samplers\n\nThe two basic fitting classes we've introduced so far may be combined to generate arbitrarily sized samples. If you want to oversample one class, then undersample some other class, then by running one sampler after the other you can do just that (using the `ratio` argument to fine-tune).\n\nHowever this can be nitty-gritty. For production `sklearn` provides abstractions on top of these two classes called ""ensemble samplers"". \n\nThe first and more straightforward of these is `EasyEnsemble`. `EasyEnsemble` can be used to resample features in the dataset naively. An application to our synthetic dataset looks like this:"
"An interesting feature is that `EasyEnsemble` will perform this resampling operation ten times, resulting in ten different sample sets. The plot here is actually showing just the first of these ten different samples. You can change the number of times this is done using the `n_subsets` parameter.\n\nAnother thing `EasyEnsemble` allows is sampling with replacement.\n\nBy default `EasyEnsemble` is the same as `RandomUnderSampler`, it reduces the counts of each of the classes down to the count of the smallest constituent class. Since class 3 had just 64 observations, we ended up with a sample which has 64 observations per class. This reduces the shape of our dataset from `(5000, 2)` way down to `(192, 2)` per sample. The API for adjusting this is the same: use the `ratio` parameter, passing in a `dict` with the desired number of observations per class.\n\nHowever, I find the `EasyEnsemble` API hard to use, because it doesn't allow automatically combining oversampling with undersampling, e.g. oversampling one category and undersampling another. There is however one `EasyEnsemble` feature that is not available in the raw sampler methods, which is sampling with replacement (`replacement=True`). I'm dubious as to the utility of this feature, but it exists.\n\nThe other method you can use is `BalanceCascade`. `BalanceCascade` is a computational adaptation of `EasyEnsemble`. You feed it an estimator, and that estimator is used to predict output classes. When observations from the dataset are sampled, observations which are misclassified are replaced in the dataset, and those which aren't are not. The end result is a set of samples biased towards (duplicated) poorly classifiable observations.\n\n`EasyEsnemble` allows you to over/under-sample with or without replacement, `BalanceCascade` allows you to over/under-sample with smart replacement (at least that's what the `sklearn` documentation calls this technique). This is an interesting resampling technique if you are interested in working on problem point performance while working with rare classes. It allows you to, in a sense, test the numerical stability of the classifier you have built, by re-training it on a set of points that it is known to have trouble with (and then checking the difference).\n\nI again find this API difficult-to-use, however."
"## Application\n\nSo far we've motivated the case for sampling your data and introduced under- and over- sampling facilities in `sklearn`. Next let's look at an application. \n\nFor ease of visualization let's initially treat the first two predictor columns, `V1` and `V2`, only."
"If we resampled this data, a more meaningful pattern emerges. It appears that transactions that load heavily in `V2` are almost always fradulent:"
"If we train the SVM on this resampled data, the algorithm will now ""realize"" this and pick a separating hyperplane that takes this fact into account:"
"Suppose you are designing an ""early warning"" system that quarantines transactions which might be fradulent for additional checks in the system. Suppose that after market research and cost analysis, your analysts have determined that a 50:1 ratio of false positives to true positives is ""worth it"". We could implement a custom classification metric to reflect this. We would find that in this (admittedly made-up, but reasonably realistic) cost environment, the price of the post-sampling model is near that of the pre-processing model, even though we're misclassifying a hella lot of legit records:"
Evolution of Mobile Phone\n![](https://s3b.cashify.in/blog/wp-content/uploads/2018/05/mobile-phone-evolution.jpg)
"In this kernel, I analyzed mobile phone features with price range. Also I build a model using support vector machine algortihm.\n\n### CONTENTS:\n\n1. [Read Data](#1)\n\n\n2. [Data Analysis](#2):\n     2. 1.  [Data Visualization](#3) :\n            * [PRICE RANGE AND RAM](#4)\n            * [BATTERY POWER- RAM AND PRICE RANGE](#5)\n            * [INTERNAL MEMORY IN GIGABYTE - RAM AND PRICE RANGE](#6)\n            * [TOUCH SCREEN-RAM AND PRICE RANGE](#7)\n            * [3G-RAM and Price Range](#8)\n            * [4G-RAM and Price Range](#9)\n            * [NUMBER OF CORES OF PROCESOR(n_cores)- RAM and PRICE RANGE](#10)\n            * [Primary Camera mega pixels(pc) - RAM and Price Range](#11)\n            * [Front Camera mega pixels(fc) - RAM and Price Range](#12)\n            * [Weight of mobile phone(mobile_wt)- RAM and Price Range](#13)\n            * [Mobile Depth in cm (m_dep), Pixel Resolution Height(px_height), Pixel Resolution Width(px_width), Clock Speed(clock_speed) , RAM and Price Range](#14)\n            * [Longest time that a single battery charge will last when you are(talk_time),RAM and Price Range](#15)\n            * [WIFI-RAM and PRICE RANGE](#16)\n            * [DUAL SIM- RAM AND PRICE RANGE](#17)\n            * [Screen Height of mobile(sc_h) AND Screen Width of mobile(sc_w)- RAM and Price Range](#18)\n            \n\n3. [Support Vector Machine Algorithm](#19):\n     * [Class Balance Visualization](#20)\n     * [First Model](#21)\n     * [Feature Selection](#22)\n     * [Model with GridSearchCV](#23)\n     * [CV SCORES](#24)\n     * [Building Model](#25)\n     \n4. [Prediction Visualization](#26)"
"In the following table, first 10 entries are shown. "
Now let's check if there are any missing values in the dataset.
 \n## 2- DATA ANALYSIS 
Following heatmap shows correlation values between features.
"As we can see our target price range has highly positive correlation between ram. \n\nAlso \n\n* 3G and 4G\n* pc(Primary Camera mega pixels) and fc(Front Camera mega pixels)\n* px_weight(Pixel Resolution Width) and px_height(Pixel Resolution Height)\n* sc_w(Screen Width of mobile in cm) and sc_h(Screen Height of mobile in cm)\n\nfeatures have highly positive correlation. For example 	as long as sc_w (screen width of mobile) increase, sc_h(screen height of mobile) is increasing.\n"
## 5.1 | Disease Distribution based on Chest Pain Type in Each Gender\n
"\n    From the butterfly chart above and as previously mentioned, typical angina chest pain and female patients have a greater number in the dataset. When viewed more detail, atypical angina, non-anginal pain, and asymptomatic chest pain have more sick patients than healthy male and female patients. In addition, for patients with typical angina chest pain, the ratio of male and female patients with heart disease is almost the same. However, the number of healthy female patients in that chest pain category is higher than healthy male patients.\n\n"
## 5.2 | Maximum Heart Rate vs. Age based on Patients Sickness\n
"\n    The scatter plot above shows that patients with and without heart disease are aged between 40 to 70 years old. In addition, the spread of max. patient's heart rate in the dataset ranges from 140 to 180. When viewed in more detail, patients who tend to get heart disease have max. heart rate over 149 and under 54 years of age. In the scatter plot above, it can also be seen that age and max. heart rate has a negative correlation, especially in patients with heart disease. In addition, heart disease patients have more numbers than healthy ones.\n\n"
## 5.3 | Fasting Blood Sugar Distribution by Resting Electrocardiographic Results\n
"\n    The donut chart above shows resting electrocardiograph types 0 and 1 have almost the same number of patients. However, inversely proportional to resting electrocardiograph type 2, where the number of patients is only four. In addition, resting electrocardiograph types 0 and 1 have patients with fasting blood sugar over 120 mg/dl. Although, resting electrocardiograph type 2 had no patients with fasting blood sugar over 120 mg/dl.\n\n"
## 5.4 | Number of Major Vessles Distribution based on Exercise Induced Angina\n
\n    The waffle charts above show that the proportion between patients who do and do not do exercise-induced angina is almost the same. This can be seen by comparing the total number of patients between major vessels in each exercise.\n\n
## 5.5 | Resting Blood Pressure Distribution based on Slope\n
"\n    The distribution plot and Q-Q plots above show that each slope type's distribution is moderately right-skewed. This is due to outliers (distribution tail) on the right side of the plot. In addition, the skewness value and gap at the upper of Q-Q plots with a 45-degree line also show that the distribution in this column is not normal.\n\n"
" \n# INTRODUCTION\n* **Deep learning:** One of the machine learning technique that learns features directly from data. \n* **Why deep learning:** When the amounth of data is increased, machine learning techniques are insufficient in terms of performance and deep learning gives better performance like accuracy.\n\n* **What is amounth of big:** It is hard to answer but intuitively 1 million sample is enough to say ""big amounth of data""\n* **Usage fields of deep learning:** Speech recognition, image classification, natural language procession (nlp) or recommendation systems\n* **What is difference of deep learning from machine learning:** \n    * Machine learning covers deep learning. \n    * Features are given machine learning manually.\n    * On the other hand, deep learning learns features directly from data.\n\n\nLets look at our data."
" \n# Overview the Data Set\n* We will use ""sign language digits data set"" for this tutorial.\n* In this data there are 2062 sign language digits images.\n* As you know digits are from 0 to 9. Therefore there are 10 unique sign.\n* At the beginning of tutorial we will use only sign 0 and 1 for simplicity. \n* In data, sign zero is between indexes 204 and 408. Number of zero sign is 205.\n* Also sign one is between indexes 822 and 1027. Number of one sign is 206. Therefore, we will use 205 samples from each classes(labels).\n* Note: Actually 205 sample is very very very little for deep learning. But this is tutorial so it does not matter so much. \n* Lets prepare our X and Y arrays. X is image array (zero and one signs) and Y is label array (0 and 1)."
"* In order to create image array, I concatenate zero sign and one sign arrays\n* Then I create label array 0 for zero sign images and 1 for one sign images."
* Up to this point we learn \n    * Initializing parameters (implemented)\n    * Finding cost with forward propagation and cost function (implemented)\n    * Updating(learning) parameters (weight and bias). Now lets implement it.
"* Woow, I get tired :) Up to this point we learn our parameters. It means we fit the data. \n* In order to predict we have parameters. Therefore, lets predict.\n* In prediction step we have x_test as a input and while using it, we make forward prediction."
"# Introduction\n\nA whole host of kernels have been written on the quintessential Iris dataset covering all sorts of topics under the sun. They include implementing different Machine learning models to Exploratory data analysis as well as dimensionality transformation techniques. \n\nThis notebook however will take a somewhat different approach. I will focus solely on one aspect of the dataset, visualising the decision boundary. A decision boundary in a nutshell, is a surface that separates data points belonging to different class lables. Although apparently simple at first glance, there is quite a lot of useful information to be gleaned from visualising a decision boundary, information that will give you an intuitive grasp of learning models. \n\nLet's go."
"# 1. Decision Boundary of Two Classes\n\n1. Before we start on the Iris dataset, as a starter I would like to generate some custom data points so we can have a feel for how the decision boundaries would look like on a two-class dataset (since the Iris set is a three-class set). To do this, we can call Sklearn's very convenient internal datasets by invoking -   **sklearn.datasets** to create datasets in the shapes of circles (make_circles()), crescents (make_moon( ) ) and blobs (make_blobs( ) ) as follows:"
"**Plotting the Decision Surface**\n\nThe best site to obtain Python code for plotting decision surfaces can be found on the Sklearn website. There are multiple excellent examples on the site and here are some links that you may find useful:\n\n 1. [Classifier Comparison][1] \n\n 2. [Plot the Decision Boundaries of a Voting Classifier][2]\n\n  [1]: http://scikit-learn.org/0.15/auto_examples/plot_classifier_comparison.html\n  [2]: http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html\n\nWhen plotting a decision surface, the general layout of the Python code is as follows:\n\n - Define an area with which to plot our decision surface and boundaries. We will use a very handy Numpy method **meshgrid** for this task\n - Extract either the class probabilities by invoking the attribute ""predict_proba"" or the distances between boundaries via the attribute ""decision_function"""
"**Takeaway from the Plots**\n\nAs we can see, our tree-based models are able to capture non-linear boundaries as evinced by the jagged edges and kinks in the decision boundaries to take into account the moon and circle shapes of the custom data-points. Furthermore, we can see that these tree-based models have been able to segregate the data points sufficiently based on a visual inspection of the Decision boundaries where there is clear demarcation between the red and the blue points. "
"## Interactive Plotly visualisations\n\n### 2.1 Overfitting effects on the Decision Boundary\n\nIn the following section, we will use the interactive capabilities of Plotly to visualise the effects of overfitting ones model and how the decision boundary changes due to this. We will plot two Random Forest classifiers, the first being one with reasonable parameters (max_depth = 4) while the second is clearly overfitting (max_depth = 50). \n\nInvoking the Plotly code is very similar to that of Matplotlib when generating the decision surface. We will need a Numpy meshgrid to form the basis of our surface plots as well as the **.predict** method from the learning model which to populate our surface with data."
*PLEASE CLICK AND DRAG THE ABOVE. THESE PLOTS ARE INTERACTIVE. DOUBLE-CLICK IF YOU WANT TO GET BACK TO THE ORIGINAL VIEW.*
Following on from this we plot our Plotly charts in the same vein as follows:
"### 2.3 Probabilities via Decision surfaces\n\nSo far sub-sections 2.1 and 2.2 have been been about visualising the actual data point separations between the three iris classes (or what the model thinks they are ). However, what if we want to visualise the probability of a data point being in either of the three classes?\n\nHandily enough for us, we can plot the class probabilities via Sklearn. Instead of invoking **.predict** in our plots, we use the **.predict_proba** attribute instead. The thing to note when calling the latter instead of the former is that the resulting output is now a matrix of three columns (instead of a vector). These three columns correspond to the three classes in the iris dataset. Therefore when plotting the decision surface for the class probabilities, the standard way of plotting is as follows:"
"Kaggle offers very nice interface to look at the data. Basically what we have here is:\n\n\nsurvival Survival 0 = No, 1 = Yes\n\npclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex: Male or Female\n\nAge: Age in years\n\nsibsp: # of siblings / spouses aboard the Titanic\n\nparch: # of parents / children aboard the Titanic\n\nticket: Ticket number\n\nfare: Passenger fare\n\ncabin: Cabin number\n\nembarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n**Missing Values**\nLet us define a function that will find the missing values and also plot the frequency of them. With this information we can impute the values accordingly.  OR as in the case of Cabin feature delete it, since imputing does not make much sense.\n"
"There is a possibility that test and train datasets do not have sama NaN values, especially if they are small. Let us check:\n"
"**Data imputation**\n\nFirst of all we need to think is it instincitive to do it? For example Cabin variable should be excluded. \nAge variable can be imputed, but which value? First things that come to mind are mean and median imputation. But one should note that they are not automatically correct and that imputing with these two method can lead to more damage than leaving the data untouched. We are going to focus on these two simple method of imputing numerical data, but for more on imputation please consult [](https://en.wikipedia.org/wiki/Imputation_(statistics))\n\n\nChoosing between mean or median imputation we should look for outliers. If there are signs of them then median is the choice since mean will be heavily affected by them. Depending on the context (and that is general rule!) mean could also be unfavorable because is produces(possibly) value in data set that did not exist beforehand. Let us plot our Age variable"
"There are couple of outliers here, we can procede bothways. Here we can learn another lesson regarding modeling. When in doubt try both methods and see what works better.\n"
Before proceeding let us have a look at how our variables interact with each other. (not the correlation/heat matrix)\n
"We are slowly getting our dataset ready. What variables are useless, drop them! and can we create new features?\n"
"One last thing before we start with modeling. We have a lot of variables, but question is, is there a lot of correlation between them. Because if there is some of them are redundant, i.e. using onge of them is waste of resources. (Highly positive or negative correlated).  In the following we are going to show-case heat-map of correlation, to get a view of the correlation. Pearson correlation gives quantifies the relationships.\n"
One possibility is to pair all of the varibales that have coefficient larger or smaller than 0.5 and -0.5 respectively. To keep things simple we are going to skip this step.
Let's check the data for information gaps.
there are big omissions of information in age as well as cabin number. Minor in the port of loading.
"Large omissions of information were found in the age, as well as the cabin number."
\n\n\n\n0  IMPORTS    ⤒
\n\n\n\n1  BACKGROUND INFORMATION    ⤒\n\n---\n
\n\n\n\n\n\n\n    3  HELPER FUNCTION & CLASSES    ⤒\n\n\n---
"**The following functions are for feature identification, matrix manipulations, etc**"
\n\n\n\n\n\n\n    4  DATASET EXPLORATION & PREPROCESSING    ⤒\n\n\n---\n\nLet's investigate the provided information and then plot some images for various things
"\n\n4.4 LET'S LOOK INTO THE CAMERA EXTRINSICS\n\n---\n\nREFERENCE --> https://ksimek.github.io/2012/08/22/extrinsic/\n \nAs discussed previously, the camera's **extrinsic matrix** describes the **camera's location in the world (3D), and what direction it's pointing (orientation)**. It has two components: \n* **a rotation matrix, $\mathbf{R}$ (or $\theta$)**\n* **a translation vector $\mathbf{t}$**\n\nHowever, shocker, these don't exactly correspond to the camera's rotation and translation. \n\nThe extrinsic matrix takes the form of a rigid transformation matrix: \n* a $3\times 3$ rotation matrix in the left-block\n* a $3\times 1$ translation column-vector in the right\n\n$$\n\left [\n    \begin{array}{c|c}\nR & t\n    \end{array}\n\right] = \n\left [\n    \begin{array}{ccc|c}\nr_{1,1} & r_{1,2} & r_{1,3} & t_1 \\\nr_{2,1} & r_{2,2} & r_{2,3} & t_2 \\\nr_{3,1} & r_{3,2} & r_{3,3} & t_3\n    \end{array}\n\right]\n$$\n\n\n\nIt's common to see a version of this matrix with extra row of $(0,0,0,1)$ added to the bottom. This makes the matrix square, which allows us to further decompose this matrix into a rotation followed by translation\n\n\n\n$$\n\begin{align}\n    \left [\n        \begin{array}{c|c} \n            R & \boldsymbol{t} \\\n            \hline\n            \boldsymbol{0} & 1 \n        \end{array}\n    \right ] &= \n    \left [\n        \begin{array}{c|c} \n            I & \boldsymbol{t} \\\n            \hline\n            \boldsymbol{0} & 1 \n        \end{array}\n    \right ] \n    \times\n    \left [\n        \begin{array}{c|c} \n            R & \boldsymbol{0} \\\n            \hline\n            \boldsymbol{0} & 1 \n        \end{array}\n    \right ] \\\n        &=\n\left[ \begin{array}{ccc|c} \n1 & 0 & 0 & t_1 \\\n0 & 1 & 0 & t_2 \\\n0 & 0 & 1 & t_3 \\\n  \hline\n0 & 0 & 0 & 1\n\end{array} \right] \times\n\left[ \begin{array}{ccc|c} \nr_{1,1} & r_{1,2} & r_{1,3} & 0  \\\nr_{2,1} & r_{2,2} & r_{2,3} & 0 \\\nr_{3,1} & r_{3,2} & r_{3,3} & 0 \\\n  \hline\n0 & 0 & 0 & 1\n\end{array} \right] \n\end{align}\n$$\n\n\n\nThis matrix describes how to transform points in world coordinates to camera coordinates. \n* The vector $t$ can be interpreted as the position of the world origin in camera coordinates\n* The columns of $R$ represent represent the directions of the world-axes in camera coordinates.\n\n**The important thing to remember about the extrinsic matrix is that it describes how the world is transformed relative to the camera.** This is often counter-intuitive, because we usually want to specify how the camera is transformed relative to the world. \n\n---\n\nLet's briefly look into each matrix type to get a grasp of what is happening in each in laymens terms**\n\nIn 3D space, rotation can occur about the x, y, or z-axis. Such a type of rotation that occurs about any one of the axis is known as a basic or elementary rotation. Given below are the rotation matrices that can rotate a vector through an angle about any particular axis.\n\n\n\n$$\n\begin{align}\n    R_x = \left [\n        \begin{array}{ccc} \n            1 & 0 & 0 \\\n            0 & \cos(\boldsymbol{\gamma}) & -\sin(\boldsymbol{\gamma}) \\\n            0 & \sin(\boldsymbol{\gamma}) & \cos(\boldsymbol{\gamma}) \n        \end{array}\n    \right ] \n\end{align} \qquad\text{This is also known as ROLL}\quad\text{(CC-rotation of }\gamma\text{ about the x-axis)}\n$$ \n\n\n\n$$\n\begin{align}\n    R_y = \left [\n        \begin{array}{ccc} \n            \cos(\boldsymbol{\beta}) & 0 & \sin(\boldsymbol{\beta}) \\\n            0 & 1 & 0 \\\n            -\sin(\boldsymbol{\beta}) & 0 & \cos(\boldsymbol{\beta})\n        \end{array}\n    \right ]\n\end{align} \qquad\text{This is also known as PITCH}\quad\text{(CC-rotation of }\beta\text{ about the y-axis)}\n$$\n\n\n\n$$\n\begin{align}\n    R_z = \left [\n        \begin{array}{ccc} \n            \cos(\boldsymbol{\alpha}) & -\sin(\boldsymbol{\alpha}) & 0 \\\n            \sin(\boldsymbol{\alpha}) & \cos(\boldsymbol{\alpha}) & 0 \\\n            0 & 0 & 1 \n        \end{array}\n    \right ]\n\end{align} \qquad\text{This is also known as YAW}\quad\text{(CC-rotation of }\alpha\text{ about the z-axis)}\n$$"
\n\n4.5 LOOK AT A SINGLE EXAMPLE W/ COVISIBILITY & CALIBRATION INFORMATION\n\n---\n
# Load Malware Train and Test Data
# Append Timestamps to Data
"# Define EDA Python Functions\nI wrote two EDA Python functions. To see the code, click the show code button. One function visualizes overall density and detection rate per category value. The other visualizes density and detection rate over time. Feel free to fork my kernel and use my functions to explore the data. I've also made my timestamp database public so you can import timestamps."
"In the plots below, solid lines are density and use the left y-axis. Dotted lines are detection rate and use the right y-axis. You can see that the majority of train data are observations in August and September 2018 while test data is October and November 2018. It is interesting that the malware infection rate is correlated with observation density. Perhaps infected computers send more reports to Microsoft. Or Microsoft chose to give us more infected samples during this time interval of interest."
"# Training ROC Curve\nThis competiton's metric is AUC, area under ROC. Let's plot our training ROC and calculate our training AUC. (We should really calculate validation AUC, but since our model is linear, training AUC should be similar.)"
# Predict and Submit to Kaggle
"# Visualize the data\n\nNow that our data has been easily loaded in, the next step is to visualize our images. This helps us understand what is being used as an input for our model. It also serves as a check to see if our images have been loaded in correctly."
"# Feature Engineering\n\nBecause we are working with categorical and noncontinuous data, we want to convert our model into one-hot encodings. One-hot encodings are a way for the model to understand that we're looking at categorial instead of continuous data. Transforming features so that they'll be more understandable is called feature engineering. Learn more about feature engineering [here](https://developers.google.com/machine-learning/crash-course/representation/feature-engineering)."
"# Visualize Model Metrics\n\nLet's graph the ROC AUC metric and loss after each epoch for the training and validation data. Although we didn't use a random seed for our notebook, the results may slightly vary, generally the scores for the validataion data is similar, if not better, than the training dataset."
"# Evaluate the Model\n\nAlthough we used the validatation dataset to continually evaluate the model, we also have a separate testing dataset. Let's prepare the testing dataset."
### a. Target column details:-
### b. Passenger class and gender:-
### c. Age:-
### d. Ticket fare:-
### e. Null valued columns:-
### f. Cabin null inference in training set:-
# 2. | Importing Libraries 📚\n\n    👉 Importing libraries that will be used in this notebook.\n
# 3. | Color Palletes 🎨\n\n    👉 This section will create some color palettes that will be used in this notebook.\n
"# 4. | Reading Dataset 👓\n\n    👉 After importing libraries, the dataset that will be used will be imported.\n"
## 5.1 | Categorical Variable 🔠\n\n    👉 The first type of variable that will be explored is categorical variable.\n\n
## 5.2 | Continuous Variable 🔢\n\n    👉 The second type of variable that will be explored is continuous variable.\n\n
#### 5.2.2.1 | Sepal Length (cm)
"\n    👉 From the histogram and boxenplot, it can be seen that this column is normally distributed. This also proven by skewness value (0.32) of this column.\n    👉 In this column, the kurtosis value is -0.55, which indicates that the column is platikurtic.\n    👉 From the Q-Q plot, the data values tend to closely follow the 45-degree, which means the data is likely normally distributed (as stated previously).\n    📌 If skewness is less than -1 or greater than 1, the distribution is highly skewed. If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed. If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n    \n    📌 Kurtosis values used to show tailedness of a column. The value of normal distribution (mesokurtotic) should be equal to 3. If kurtosis value is more than 3, it is called leptokurtic. Meanwhile, if kurtosis value is less than 3, then it is called platikurtic.\n"
#### 5.2.2.2 | Sepal Width (cm)
"\n    👉 From the histogram and boxenplot, it can be seen that this column is normally distributed. This also proven by skewness value (0.33) of this column.\n    👉 It also can be seen that the median value is closer too Q1 (can be seen from the boxen plot).\n    👉 In this column, the kurtosis value is 0.29, which indicates that the column is platikurtic.\n    👉 From the Q-Q plot, the data values tend to closely follow the 45-degree, which means the data is likely normally distributed (as stated previously).\n"
#### 5.2.2.3 | Petal Length (cm)
"\n    👉 From the histogram and boxenplot, it can be seen that this column is normally distributed. This also proven by skewness value (-1.4) of this column.\n    👉 It also can be seen that the median value is closer too Q3 (can be seen from the boxen plot).\n    👉 In this column, the kurtosis value is -1.4, which indicates that the column is platikurtic.\n    👉 There is a gap between 45-degree line with the upper and bottom part of Q-Q plot.\n"
#### 5.2.2.4 | Petal Width (cm)
"\n    👉 From the histogram and boxenplot, it can be seen that this column is normally distributed. This also proven by skewness value (-0.1) of this column.\n    👉 It also can be seen that the median value is closer too Q3 (can be seen from the boxen plot).\n    👉 In this column, the kurtosis value is -1.3, which indicates that the column is platikurtic.\n    👉 There is a gap between 45-degree line with the upper and bottom part of Q-Q plot.\n"
## 6.1 | Pairplot of Numerical Variables 📐\n
## 6.2 | Jointplot between Sepal Length and Sepal Width 🌹\n
### Categorical Features :\n\n#### Distribution of Categorical Features :
- All the categorical features are **Normally Distributed**.
### Numerical Features :\n\n#### Distribution of Numerical Features :
"- **Age**,**Creatinine_Phosphokinase**,**Ejaction_Fraction** and **Serum_creatinine** have a **rightly** or **positively skewed** data distribution.\n- **Platelets** and **Serum_Sodium** are near about **normally distributed**.\n- **Time's** data distribution is similar to a typical **Time Series Analysis** graph with irregularities present in it. "
### Target Variable Visualization (DEATH_EVENT) : 
"- The dataset is **unbalanced** with very low data points (299)!\n- **2 : 1** ratio for **No Death Event cases : Death Event cases!**\n- Due to this, predictions will be biased towards **No Death Event** cases.\n- Visualizations will also display this bias, thus making it difficult to gain insight."
### Categorical Features vs Target Variable (DEATH_EVENT) :
"- All the graphs near about share the same pattern.\n- According to the graphs, patients with negative cases of **anaemia**, **diabetes**, **high_blood_pressure** and **smoking**  leads to **DEATH_EVENT** more than the positive cases of these medical conditions.\n- There are more cases cases of **male** population confronting a **DEATH_EVENT** due to heart failure than **female** population."
"## 4. Regularization of Linear Regression \n\nThere are situations where we might intentionally increase the bias of the model for the sake of stability i.e. to reduce the variance of the model $\text{Var}\left(\widehat{f}\right)$. One of the conditions of the Gauss-Markov theorem is the full column rank of matrix $\textbf{X}$. Otherwise, the OLS solution $\textbf{w} = \left(\textbf{X}^\text{T} \textbf{X}\right)^{-1} \textbf{X}^\text{T} \textbf{y}$ does not exist since the inverse matrix $\left(\textbf{X}^\text{T} \textbf{X}\right)^{-1}$ does not exist. In other words, matrix $\textbf{X}^\text{T} \textbf{X}$ will be singular or degenerate. This problem is called an  ill-posed problem. Problems like this must be corrected, namely, matrix $\textbf{X}^\text{T} \textbf{X}$ needs to become non-degenerate, or regular (which is why this process is called regularization). Often we observe the so-called multicollinearity in the data: when two or more features are strongly correlated, it is manifested in the matrix $\textbf{X}$ in the form of ""almost"" linear dependence between the columns. For example, in the problem of predicting house prices by their parameters, attributes ""area with balcony"" and ""area without balcony"" will have an ""almost"" linear relationship. Formally, matrix $\textbf{X}^\text{T} \textbf{X}$ for such data is reversible, but, due to multicollinearity, some of its eigenvalues will be close to zero. In the inverse matrix $\textbf{X}^\text{T} \textbf{X}$, some extremely large eigenvalues will appear, as eigenvalues of the inverse matrix are $\frac{1}{\lambda_i}$. The result of this vacillation of eigenvalues is an unstable estimate of model parameters, i.e. adding a new set of observations to the training data will lead to a completely different solution. \nOne method of regularization is Tikhonov regularization, which generally looks like the addition of a new member to the mean squared error:\n\n$$\Large \begin{array}{rcl} \n\mathcal{L}\left(\textbf{X}, \textbf{y}, \textbf{w} \right) &=& \frac{1}{2n} \left\| \textbf{y} - \textbf{X} \textbf{w} \right\|_2^2 + \left\| \Gamma \textbf{w}\right\|^2\\\n\end{array}$$\n\nThe Tikhonov matrix is often expressed as the product of a number by the identity matrix: $\Gamma = \frac{\lambda}{2} E$. In this case, the problem of minimizing the mean squared error becomes a problem with a restriction on the $L_2$ norm. If we differentiate the new cost function with respect to the model parameters, set the resulting function to zero, and rearrange for  $\textbf{w}$, we get the exact solution of the problem.\n\n$$\Large \begin{array}{rcl} \n\textbf{w} &=& \left(\textbf{X}^{\text{T}} \textbf{X} + \lambda \textbf{E}\right)^{-1} \textbf{X}^{\text{T}} \textbf{y}\n\end{array}$$\n\nThis type of regression is called ridge regression. The ridge is the diagonal matrix that we add to the $\textbf{X}^\text{T} \textbf{X}$ matrix to ensure that we get a regular matrix as a result.\n\n\n\nSuch a solution reduces dispersion but becomes biased because the norm of the vector of parameters is also minimized, which makes the solution shift towards zero. On the figure below, the OLS solution is at the intersection of the white dotted lines. Blue dots represent different solutions of ridge regression. It can be seen that by increasing the regularization parameter $\lambda$, we shift the solution towards zero.\n\n\n"
## Support course creators\n\n\nYou can make a monthly (Patreon) or one-time (Ko-Fi) donation ↓\n\n\n\n\n\n\n\n\n\n\n\n    \n
\n\n## 2| IMPORT NECESSARY LIBRARIES
\n\n## 3|LOAD DATASET
\n\n#### 5.1 | Catplot and Barplot
"\n\nChart report  \n    \n**From this graph, it is clear that the number of men in the ""sex"" variable in the dataset is much more than the number of women**"
\n\nChart report  \n    \n\n\n**It is clear from this graph that the presence of anemia is less than the absence**
"\n\nChart report  \n    \n\n\n**Looking at the values in the graph, it is known that the risk of death of men due to heart attack is 2 times higher than that of women.**"
"\n\nChart report  \n    \n\n\n**Looking at the values in the graph, it becomes clear that the mortality rate of smokers is much lower than that of non-smokers. But here you need to be careful. Because it is known to everyone that under normal circumstances, the death rate of people who smoke is high. The problem here is with our dataset. Because the number of smokers in our dataset is high and the number of deaths is low. Hence the inconsistency.**"
# Which Classifier is Should I Choose? \n\nThis is one of the most import questions to ask when approaching a machine learning problem. I find it easier to just test them all at once. Here's 10 of your favorite Scikit-Learn algorithms applied to the leaf data. 
## Data Preparation\n
"# Analysis Time!\n\nOk the short inspection at the beginning give us some hints how should we move from here. I'm going to play with the data we have while analysing the data at the same time. With this way I hope we can get the data in better shape while digging deeper into it.\n\nWe're going to start with basic correlation table here. I dropped the top part since it's just mirror of the other part below. With this table we can understand some linear relations between different features.\n\n#### Observations:\n- There's strong relation between overall quality of the houses and their sale prices.\n- Again above grade living area seems strong indicator for sale price.\n- Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n- There are some obvious relations we gonna pass like total square feet affecting how many rooms there are or how many cars can fit into a garage vs. garage area etc.\n- Overall condition of the house seems less important on the pricing, it's interesting and worth digging.\n"
- **I'm going to merge the datasets here before we start editing it so we don't have to do these operations twice. Let's call it features since it has features only. So our data has 2919 observations and 79 features to begin with...**
"- **That's quite a lot! No need to panic though we got this. If you look at the data description given to us we can see that most of these missing data actually not missing, it's just means house doesn't have that specific feature, we can fix that easily...**"
"### **Ok this is how we gonna fix most of the missing data:**\n\n1. First we fill the NaN's in the columns where they mean 'None' so we gonna replace them with that,\n2. Then we fill numerical columns where missing values indicating there is no parent feature to measure, so we replace them with 0's.\n3. Even with these there are some actual missing data, by checking general trends of these features we can fill them with most frequent value(with mode).\n4. MSZoning part is little bit tricky I choose to fill them with most common type of the related MSSubClass type. It's not perfect but at least we decrease randomness a little bit.\n5. Again we fill the Lot Frontage with similar approach."
Now the same separately for Alice and everybody else.
"Now we definitely see that Alice mostly prefers 4-5 pm for browsing. So let's create features 'morning', 'day' and 'evening' and 'night'. Separators between these times of the day will be almost arbitrary: 0 am, 7 am, 12 am, and 7 pm. However, you can tune this."
### 0.0 Load modules
### 0.1 Load data
## Preparing the Data
"The dataset is divided into separate files, one per weather attribute. To illustrate the encoding of cyclical features we will only be using the temperature data of a single city: Montreal."
"Great, it appears the absolute difference an `hour_sin` before, at and after midnight is now the same! However, if we look at the plot of `hour_sin` (following any flat line intersection with the graph), we can see there is a problem. If we consider just the one dimension, there are two records with exactly the same `hour_sin` values, e.g. records 11 and 25.\n\nThis is why we also need the cosine transformation, to separate these records from each other.\n\nIndeed, if we plot both features together in two dimensions we get the following:"
Exactly what we want: our cyclical data is encoded as a cycle. Perfect for presenting to our deep learning algorithm.
# 2. Importing the necessary libraries📗 
# 3. Reading the train.csv 📚
## Loading Modules
## Loading Datasets\n\nLoading train and test dataset
### Required Libraries
## Dataset
"# Missing Values\n\nWe have small portion of missing values for age and sex I think there is no harm if we impute them with the most frequent ones, meanwhile body parts missing on both datasets, we better be set 'unknown' for missing values for this one..."
"## Checking Variables Before Imputing\n\nJust wanted to check variable distribution before we impute the missing ones. Looks like our assumptions were ok, we can continue with imputing..."
# Imputing Missing Data\n\nLet's fill the missing values with appropriate methods.
"# Body Part Ratio by Gender and Target\n\nLooks like some body parts are more likely to be malignant, head/neck comes first with followed by oral/genital and upper extremity. Scanned body part locations are similar in order between males and females with small differences on distribution."
# A General Look With Sunburst Chart\n\nSunburst chart is pretty cool looking fella I'd say. It also giving lots of basic information to us. Let's see...\n\n- Only 2% of our targets are malignant\n- On malignant images males are dominant with 62% \n- Gender wise benign images are more balance 52-48% male female ratio\n- Malignant image scan locations differs based on the patients gender:\n    - Meanwhile the torso is most common location in males it's almost half of the scans meanwhile in females it's 39%\n    - Lower extremity is more common with female scans than males 18% males vs 26% females\n    - Again upper extremity malignant scans is common with females than males (23- 17%)\n- Benign image scan locations more similar between male and female patients.
"# Age and Scan Result Relations\n\nAge looks pretty decent factor on scan result. Getting malignant scan result with elderly age is more possible than young patients. There is spike for both genders after age of 85, if we look distribution of ages there isn't much of 80+ patients and it can be the reason of this spike but we can safely say it's more likely to be malignant scan after age of 60. We see small bump on age 15-20 for females, again it depends on the scan numbers but still, poor souls..."
"# Age Round Two\n\nWanted to double check age distributions after our previous observations. Age seems evenly distributed on both train and test datasets, we can see small bumps at age 75+ and around 40, these worth investigating.\n\nWe can see again older people are more likely to get malignant scan results. One last thing about age distributions, we see more female patients in younger ages this trend changes with the older patients..."
"# Unique Patients and Their Scan Images\n\nIt looks like we have multiple scan images per patient, actual unique patient counts are much lower than images on both datasets. We can get more information about patients age like when he had his first scan and his last scan. We can get interesting insights like:\n\n- Most of the malignant results are found around first 20 scans. Of course there can be control scans after the diagnosis...\n- Scan numbers are similar in first 100 scans but we have 200+ scan images for **one particular patient** in dataset, it's pretty interesting since we don't have this case in our training data. We should be careful about this and it can effect our model.\n- Most of the malignant cases are under 20 images but in general we can say it's more likely to be malignant result if there are more scan images..."
---------------------------------------------------------------------\n# Loading library and dataset
The effect of each dimension reduction is identified using the MNIST dataset. 
"__________________________________________________________________\n# PCA\nPCA is the most representative method of dimensionality reduction. This is a method of re-axis of multidimensional data in the direction of large variance. The greater the dependence between variables, the smaller the principal component can represent the original data.\nHowever, since it is assumed that each feature follows a normal distribution, it is not appropriate to apply a variable with a distorted distribution to PCA."
"# Truncated SVD\nTruncated SVD is a method of extracting and decomposing only the upper part of the diagonal elements in the sigma matrix, that is, the upper part of the singular values.\nWith this decomposition, the original matrix cannot be accurately restored because it artificially decomposes $U∑V^T$ of smaller dimensions.\nHowever, despite the data information being compressed and decomposed, it is possible to approximate the original matrix to a considerable degree."
"-------------------------------------------------------------------\n# NMF\n\nNMF is a variant of the Low-Rank Approximation method like SVD. However, it must be guaranteed that the values ​​of all elements in the source matrix are positive. NMF decomposes a matrix into W and H matrices. The W matrix indicates how well the values ​​of the latent elements correspond to the source matrix. The H matrix represents how this latent element is composed of sour features."
"------------------------------------------\n# LDA\nLDA is a method of dimensionality reduction in the classification problem of supervised learning. It finds a low-dimensional feature space that can classify the training data well, and reduces the dimensionality by projecting the original features into that space."
"---------------------------------------------\n# t-SNE\nt-SNE is often used for visualization purposes by compressing data on a two-dimensional plane. Points that are close to the original feature space are also expressed in a two-dimensional plane after compression. Since the nonlinear relationship can be identified, the model performance can be improved by adding the compression results expressed by these t-SNEs to the original features. However, since the computation cost is high, it is not suitable for compression exceeding two or three dimensions."
"-------------------------------------------------------\n# UMAP\nUMAP (Uniform Manifold Approximation and Projection), which is faster than t-SNE and separates the data space well, has been proposed for nonlinear dimensionality reduction. In other words, it can process very large datasets quickly and is suitable for sparse matrix data. Furthermore, compared to t-SNE, it has the advantage of being able to embed immediately when new data comes in from other machine learning models."
"## UMAP connectivity plot\n\n> UMAP works by constructing an intermediate topological representation of the approximate manifold the data may have been sampled from. In practice this structure can be simplified down to a weighted graph. Sometimes it can be beneficial to see how that graph (representing connectivity in the manifold) looks with respect to the resulting embedding. It can be used to better understand the embedding, and for diagnostic purposes.\n\nRef: https://umap-learn.readthedocs.io"
------------------------------------------------------------\n# UMAP 3D plot
"The figure above was drawn by reducing the dimension to 3D with UMAP.\n\nIf you compare it with the previous two-dimensional plot, you can see that it is visually complex and the points are distributed sparsely in space.\n**If the dimension is increased further, the above phenomenon will become more severe.**\n\n**It's a small experiment, but we've experienced the curse of a dimension.**"
"Examining the raw data: \n\nTaking the raw data we were given of the 2017 and 2018 seasons, and examining each *unique* NFL athlete, I have compiled their college attended previous to their entry into the NFL, and ranked in order of the density of graduates that are in the NFL.  You see some colleges absolutely dominate.  Which means its EVEN harder to get into the NFL than the statistics shown above, because it looks like your best shot is by attending certain certain dominant colleges, which in turn have less and less room on their roster, lowering the player's chance of getting into the NFL.  Research also seems to indicate that approximately 85% of the NFL players were former **Division I college** students.  \n\nDoes it make more sense now why **101,821** people attended the LSU vs Alabama game (November 9th) ?  \n\nBetter yet, the front runner Heisman Trophy candidate [Joe Burrow](https://twitter.com/LSUfootball/status/1194754755123277824) ?  He attends LSU...\n(UPDATE:  Joe did go on to win the Heisman in an overwhelming fashion)\n\nPlot is interactive, click on a college and you will see its NFL player count and ranking..."
"> It gets worse:\n  * Even if you enter the NFL draft, it also depends on what **position** you play.  They may not need certain players depending on how the season before had gone and other external factors. \n\nThe 256 players that entered the NFL in the year 2018:    *Source: Wikipedia*\n\n"
"Summary of our dataset:\nIt is always important to look at our entire dataset and examine the descriptive statistics:\n\n  **Number of football teams in the NFL:**   32  \n  **Number of unique NFL players in our dataset:**   2,231  \n  **Number of 2017 Season players:**   1,788  \n  **Number of 2018 Season players:**   1,783   \n  **Number of players playing both yrs:**   1,340    \n  **Number of players allowed per team:**   53    \n  **Number of games a team plays in a NFL season:**   16      \n  **Number of weeks in a NFL season:**   17   \n  **Total unique NFL games played per season:**   256  \n  **Number of NFL seasons in the dataset:**   2  \n  **Dataset NFL season years:**   2017 and 2018 Seasons    \n  **Dataset total number of unique NFL games:**   512  \n  **Number of unique run plays in our dataset:**   23,171  \n  **Number of 2017 Season run plays:**   11,900  \n  **Number of 2018 Season run plays:**   11,271  \n  **Number of unique NFL jersey numbers:**   99  \n  **Number of players on roster that never played:**   11  \n  **Size of a typical NFL field (in acres):**   1.32"
It's Gametime...\n
### 4.1.1 Count of Null values in features \n\nzoom in to see the hidden features
There are too many features with null values right now we will just fill those values with mean values.
### 4.4.1 Utility Score vs action with 80% accuracy for train data
We can see that there is positive linear relation ship between action and score which is obvious given the linear metrics\n\n**Note: this graph is just to see relationship of action vs score.\nWe can only use values 0 or 1 for the action.**
# 1) Import important libraries and packages
# 2) Load and clean dataset
# 2. Loading Libraries\n## Importing Libraries
# 2.2 Reading Data\n# Loading Data
# 4.1 Top 5 States With Maximum Number of Donor Cities 
# 4.2 Locality of schools with their counts 
# Preparation & peek at the data structure \n\n## Loading packages and setting 
### Settings
# Exploratory analysis \n\n## What do we know about our data? 
### Insights\n\n1. The number of image patches per patient varies a lot! **This leads to the questions whether all images show the same resolution of tissue cells of if this varies between patients**. \n2. Some patients have more than 80 % patches that show IDC! Consequently the tissue is full of cancer or only a part of the breast was covered by the tissue slice that is focused on the IDC cancer. **Does a tissue slice per patient cover the whole region of interest?**\n3. The **classes of IDC versus no IDC are imbalanced**. We have to check this again after setting up a validation strategy and find a strategy to deal with class weights (if we like to apply them).
### Cancer patches
### Healthy patches
### Insights\n\n* Sometimes we can find artifacts or incomplete patches that have smaller size than 50x50 pixels. \n* Patches with cancer look more violet and crowded than healthy ones. Is this really typical for cancer or is it more typical for ductal cells and tissue?\n* Though some of the healthy patches are very violet colored too!\n* Would be very interesting to hear what criteria are important for a pathologist.\n* I assume that the wholes in the tissue belong to the mammary ducts where the milk can flow through. 
Importing the librarys
Importing the data
Let's start exploring the Funded and and Loan Amount
Cool. We have a normal distribuition to the both values.
Another interesting numerical values is the lender number and the term in months.\n\nI will start exploring further the Lenders_count column
We have a interesting distribuition...Or you have 1 lender or you will have a great chance to chave through 4 ~ 12 lenders in the project
"Curious... In a ""normal"" loan the term almost ever is 6 ~ 12 ~ 24 and so on. \nIt's the first time that I see 8 ~ 14 ~ 20; Very curious.\n"
Let's look through the Sectors to known them
"Very cool graph. It show us that the highest mean values ins't to the most frequent Sectors.... Transportation, Arts and Servies have a low frequency but a high mean. "
Now I will look some values through the sectors.
"The values have an equal distribution through all data, being little small to Personal Use, that make sense.\n\nThe highest Term months is to Agriculture, Education and Health.\n\n"
" Taking advantage of sectors, let's look the Acitivities'"
We can see that the activities with highest mean loan amount aren't the same as more frequent...\nThis is an interesting distribution of Acitivies but it isn't so meaningful... Let's further to understand this.
Now I will explore the activies by the top 3 sectors
Looking the activities by Sector is more insightful than just look through the sectors or activities
### Import Libraries
### Reading Data
"Curiosity Bonus: Top 20 Countries in 2021 Ranking Evolution\n\nLet's look at the top 10 countries in 2021 and their evolution:\n\n* 🎩 For the gentlemen, there have been many more responders from Nigeria and Pakistan, ending up in the top 10 in 2021 from rankings below 20 in 2017.\n* 💃 For the ladies, there has been a huge surge in respondents from Egipt, Indonesia and Nigeria, these countries ending up in the top 10 most responders in 2021.\n\nIt's beautiful to see that more and more people join our community from more diverse backgrounds and countries."
"## 1.4 Education\n\n* The most interesting trend we see is a decrease in the percentage of respondents with Doctoral and Masters Degrees to the detriment of people that have a Bachelor as the highest educational degree so far.\n* This is because, as the pool of young people increased in 2020 and 2021, the number of students that are still in their studies (and not yet finished) increased as well."
"\n  👀 So far, we know that we're looking at a 20:80 ratio between 💃ladies and 🎩gents. We know that Data Science is getting much more traction among the youth in 2021 and more and more people are drawn towards the Kaggle platform, especially students. We know that, although the vast majority of respondents are from India and the US, people from Nigeria, Pakistan, Egypt and Indonesia are increasing in numbers fast, so it's possible that we will see more diversity in the years to come.\n\n\n# 2. Getting up close and personal\n\n## 2.1 Occupation\n\nFor both genders, the majority of respondents are either Students or have a profession within Data Science. However, some interesting differences happen within 3rd most common position:\n\n* 🎩 3rd most common position for gents is Software Engineer.\n* 💃 3rd most common position for ladies is Data Analyst.\n\nThroughout the years, there are no significant changes between these rankings."
"Most used programming languages within Data Science\n\nBoth genders look the same in terms of the distribution of languages and what they use more often in their projects.\n\nA few trends to mention:\n\n* C & C++ have been increasing a few percentage points, meaning that there are a few more respondents that use these 2 languages than in previous years.\n* R has been decreasing in popularity, as we can also see in respondents' recommendations. Still, 💃women tend to use it more (23.4% in 2021) than 🎩men so (19.8% in 2021).\n\n\n  👀 Psst! Hover your mouse over the streamgraphs to see the legend and more information!\n\n\n"
"## 2.3 Environment & Surroundings\n\nThe Environment\n\nRegarding the Data Science setup and what the genders use most often within their Data Science environment, we can see that:\n\n* Visual Studio increased in popularity significantly in 2021, after a slight dip in 2020. 🎩Gents use it more often (49% in 2021) than 💃ladies do (39% in 2021).\n* After many years in which Jupyter has been having the majority of the usage (half of the respondents said they use Jupyter frequently), it has lost its popularity suddenly in 2021, with only ~20% of users still saying that they use it on a regular basis."
"The Surroundings\n\nRegarding the preferences for hosted notebooks, the trends are as follows:\n\n* Kaggle and Colab have been increasing in popularity each year, being the most preferred hosted notebooks out of all available choices in 2021.\n* There is still a big pool of people (around 30% in 2021 for both genders) that still never use any hosted notebook on a regular basis. I assume these are the people that compete frequently in Kaggle competitions (and win) but don't like to post notebooks for these competitions as well. I would also assume that these people use high GPU/TPU power and have local very powerful computers/workstations. I would love to have a chat with them and understand how could I develop these hosted notebooks in order to help them in their Data Science work.\n\nCode Ocean, IBM Watson Studio, Amazon Sagemaker Studio & EMR, Google Cloud Notebooks & Datalab, Databricks Collaborative Notebooks have less than 8% usage between the respondents, so they were not shown within the graphs."
"## 2.4 Special Sauces\n\nI. First Special Sauce: The Hardware\n\nThere is a smaller percentage of users for Workstations and Cloud Platforms and an increase in Personal Laptop's popularity. \n\nI believe this is the case because, as the overall percentage of respondents in 2021 consists of many more young adults (aged 18 to 24), they do not yet have the necessary budget and experience to work with heavy-duty equipment, like specialized workstations and Cloud Platforms with online GPU or TPU."
"We can see that `Fare` contains mainly values of around `0` to `30`, but there's a few really big ones. This is very common with fields contain monetary values, and it can cause problems for our model, because once that column is multiplied by a coefficient later, the few rows with really big values will dominate the result.\n\nYou can see the issue most clearly visually by looking at a histogram, which shows a long tail to the right (and don't forget: if you're not entirely sure what a histogram is, Google ""[histogram tutorial](https://www.google.com/search?q=histogram+tutorial&oq=histogram+tutorial)"" and do a bit of reading before continuing on):"
"To fix this, the most common approach is to take the logarithm, which squishes the big numbers and makes the distribution more reasonable. Note, however, that there are zeros in the `Fare` column, and `log(0)` is infinite -- to fix this, we'll simply add `1` to all values first:"
The histogram now shows a more even distribution of values without the long tail:
"It looks from the `describe()` output like `Pclass` contains just 3 values, which we can confirm by looking at the [Data Dictionary](https://www.kaggle.com/competitions/titanic/data) (which you should always study carefully for any project!) -- "
"# Bank Customer Segmentation\n\nIn this kernel I will perform segmentation of German bank customers. The first step is to read necessary libraries. We will use: \n* [pandas](https://pandas.pydata.org/) - to manipulate data frames\n* [numpy](http://www.numpy.org/) - providing linear algebra\n* [seaborm](https://seaborn.pydata.org/) - to create nice visualizations\n* [matplotlib](https://matplotlib.org/) - basic tools for visualizations\n* [scikit-learn](https://scikit-learn.org/stable/) - machine learning library\n\nFrom sklearn, I will import necessary pre-processing tools and two clustering algorithms: [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [Affinity Propagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html).\n"
Reading the raw data 
**Exploratory Data Analysis**\n\nBelow I will define a function which will generate plots for three numeric variables with stratification by selected categorical column.
At the beginning let’s look at scatter plots our 3 numerical variables stratified by sex.
"The general impression is that women tend to be younger than men, however, the top plot shows that there is no clear difference between men and women in terms of amount and duration of the credit. From visual inspection, it seems that there is some positive correlation between duration and amount of credit, what makes sense. \n\nLet’s check the linear correlation between credit amount and duration"
"The plot above shows a linear correlation with Pearson value of 0.62 and very small p-value. That make’s sense because usually, people take bigger credits for longer periods.  Below I will analyse linear regression plots with various categorisations."
The plot above indicates that there is no significant difference between men and women.
The plot above shows similarly that there is no diference betwen housing categories.\n\nBelow I will show “business” area where granted the biggest amount of credits. 
"> We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. "
# 3. Read in Data 
### Import required libraries
### Load models
"# 1. EDA\n\nAltogether, we are given 7 files.\n\n>Tailoring education to a student's ability level is one of the many valuable things an AI tutor can do. Your challenge in this competition is a version of that overall task; you will predict whether students are able to answer their next questions correctly. You'll be provided with the same sorts of information a complete education app would have: that student's historic performance, the performance of other students on the same question, metadata about the question itself, and more.\n\n>This is a time-series code competition, you will receive test set data and make predictions with Kaggle's time-series API. Please be sure to review the Time-series API Details section closely.\n\nSo we should realize that example_test.csv really is just an example. The submission happens via the API."
"As the train dataset is huge, I am gladly using the pickle that Rohan Rao prepared in this kernel: https://www.kaggle.com/rohanrao/tutorial-on-reading-large-datasets/ (Thanks Rohan!). I actually do this at work all the time, and in this case it reduces the time to load the dataset (with the data types specified in the file description) from close to 9 minutes to about 16 seconds.\n\nAs we can see, we have over 101 million rows the the train set.\n"
"timestamp: (int64) the time in milliseconds between this user interaction and the first event completion from that user. As you can see, most interactions are from users that were not active very long on the platform yet."
"Do we have the full history of all user_id's? Yes, if we filter train on timestamp==0, we get a time 0 for all users."
"# The target: answered_correctly\nAnswered_correctly is our target, and we have to predict to probability for an answer to be correct. Without looking at the lecture interactions (-1), we see about 1/3 of the questions was answered incorrectly."
"I also want to find out if there is a relationship between timestamp and answered_correctly. To find out I have made 5 bins of timestamp. As you can see, the only noticable thing is that users who have registered relatively recently perform a little worse than users who are active longer."
"Let's also check out what the distribution of answered_correctly looks like if we groupby the (10,000 unique) task_container_id's."
"Before we create a Random Forest or Gradient Boosting Machine, we'll first need to learn how to create a *decision tree*, from which both of these models are built.\n\nAnd to create a decision tree, we'll first need to create a *binary split*, since that's what a decision tree is built from.\n\nA binary split is where all rows are placed into one of two groups, based on whether they're above or below some threshold of some column. For example, we could split the rows of our dataset into males and females, by using the threshold `0.5` and the column `Sex` (since the values in the column are `0` for `female` and `1` for `male`). We can use a plot to see how that would split up our data -- we'll use the [Seaborn](https://seaborn.pydata.org/) library, which is a layer on top of [matplotlib](https://matplotlib.org/) that makes some useful charts easier to create, and more aesthetically pleasing by default:"
"Here we see that (on the left) if we split the data into males and females, we'd have groups that have very different survival rates: >70% for females, and <20% for males. We can also see (on the right) that the split would be reasonably even, with over 300 passengers (out of around 900) in each group.\n\nWe could create a very simple ""model"" which simply says that all females survive, and no males do. To do so, we better first split our data into a training and validation set, to see how accurate this approach turns out to be:"
"Alternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work -- here's an example of how we could look at `LogFare`:"
"The [boxenplot](https://seaborn.pydata.org/generated/seaborn.boxenplot.html) above shows quantiles of `LogFare` for each group of `Survived==0` and `Survived==1`. It shows that the average `LogFare` for passengers that didn't survive is around `2.5`, and for those that did it's around `3.2`. So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\n\nLet's create a simple model based on this observation:"
This is unusual as we most of the time see data splits as 80:20 but in our case it is 1:16 which indicates that the competition creators are encouraging participants to generate their own data.
"The target labels; 1 if the data contains the presence of a gravitational wave, 0 otherwise. (Please note the presence of a small number of files labeled -1. Physicists are currently unable to determine the status of these files.)"
"\n  Background\n\nSince 1920 the National Football League has developed the model for a successful modern sports league. The NFL enterprise includes national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country. Last year, 29 of the NFL's 32 teams appeared in the Forbes Top 50 most valuable sports franchises in the world. Football is also regarded as the United States' most popular sport. A 2018 Gallup poll found that among US adults, 37% name football as their favorite sport to watch. The number tops basketaball(11%), baseball(9%) and soccer(7%) by a wide margin. \n\nPlayer safety has always been a concern for football. Over the past few years, concussions sustained during play have become one of the most visible issues affecting players. Concussion incidents rose in 2017 even as the NFL implemented several safety measures. The figure below shows the incidence of concussions since 2012. \n\n Hover over the points to see exact numbers. "
"\nKick and punt plays have historically posed the highest risk of concussion to players. During the 2015-2017 seasons, the kickoff represented only six percent of plays but 12 percent of concussions, making the risk four times greater than running or passing plays. In response, the NFL revised its kicking rules for 2018 to reduce risk during kickoffs. \n\nThe NFL is now increasing the attention given to punt plays. According to NFL executive Jeff Miller, concussion risk during punts is twice that of running or passing plays. In response the NFL is sponsoring this competition as part of an overall effort to make punt plays safer. The goal of the competition is to discover specific rule modifications, supported by data, that may reduce the occurrence of concussions during punt plays."
"The figures and table below show that returned punts are by far the most common outcome for both the absolute number of concussions and the percent of concussions for that play type. Fair catches, by contrast are low on both counts. \n\n\n Hover over the bars to see exact numbers. "
The NFL would most likely see fewer concussions if some returned punts are shifted to other outcomes by changing the rules. This is consistent with my findings in the next section that over half of the recorded concussons occurred near the punt returner.\n\nAnother important consideration here was to see yards gained from returned punts. The figure below shows yards gained on each punt when a returner gets control of the ball with the intent to move forward. It does not include muffed catches. The vertical line in the figure indicates the median of all returns.
" Opportunity Area 2. Less Harmful Contact.\n\nFor this issue I looked at the types of collisions that occurred, where they occurred,  and the players involved. The figure below shows the numerous play flows that result in concussion. Players receiving a concussion appear to the left and primary partners appear to the right. The width of the flowpaths indicates the number of concussions occurring with that combination. For example, offensive linemen from the punting team received 9 concussions while tackling. 8 of those were while the returner was tackled, as one might expect. The 9th was from a defensive lineman blocking the offensive lineman during a tackle.\n\n Hover over flowpaths to see exact values. "
"The above diagram shows a variety of scenarios in which concussions occur. One can see by the green box to the left that 20 offensive linemen sustained concussions from tackling and being blocked. Although punt returners received the most concussions as a single-person role, offensive linemen collectively received over half the concussions during the two-season period. Gunners (typically two per play) also collectively received more concussions than most roles."
## Import Python libraries
### Check file size
# State Feature\n- I will start looking the state column distribuition that might will be our key to understand this dataset
"Very interesting distribution ! \nwe can see that only 35,38% of all projects got sucess.\nMore than 60% have failed or canceled; \n\nI think that the most important category's here is failed and canceled;\n\nMaybe we can build an model to predict if a project would obtain the money or not;\n\n\n\n# Let's start looking our Project values\n- I will start exploring the distribuition logarithmn of these values"
# Goal Distribution\n- Let's plot and analyze the distribution of goal that failed and successful projects.
"Interesting difference between Pledged and Goal distribuition! \n\nGoal seems a normal distribution, so it would be good if we test the normality of it and also, if exists some statistical difference between failed and success projects. \n\nLet start with the normality test of goal"
"# Normality Test\n- Although our data seems normal distributed, its good to do a test and be sure that it is true\n- So, before we go further, lets test if the distribution of or goal is normal distributed"
"Nice. \nBased on the result, we can see that the data isn't normal distributed;"
### Importing Libraries
### Importing Dataset
"### Color coding\n\nThrough this challenge in order to visualize the tasks color coding are adopted, here we show what color corresponds to what number. (code taken from [here](https://www.kaggle.com/nagiss/abstraction-and-reasoning-view-all-data))"
"## Task training/db3e9e38, continue the pattern\n\nLet's try to design a cellular automata for task `training/db3e9e38`. \n         \n  "
"### 1.2 Exploratory Data analysis with dabl\n\ndabl provides a high-level interface that summarizes several common high-level plots. For low dimensional datasets, all features are shown; for high dimensional datasets, only the most informative features for the given task are shown"
* **Initial Model Building with dabl**\nWe can find an initial model for our data. The SimpleClassifier implements the familiar scikit-learn API of fit and predict.
"### 2.2 Bar Chart\n\nAlternatively, you can also plot a barchart to show the missing values\n"
### 2.3 Heatmap\n\n[`missingno.heatmap`](https://github.com/ResidentMario/missingno#heatmap) measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:
\n3. Emot \n\n\n[Emot](https://github.com/NeelShah18/emot) is an Emoji and Emoticons detection package for Python. It can come in real handy when we have to preprocess our text data to get rid of the emoticons.
"This notebook presents two ideas for target enginnering:\n- Training on **cross-sectional daily bins** of `weight` x `resp`\n- Training on **two** targets simultaneiously\n\n# Why Target Engineering?\n\nIn low signal-to-noise prediction problems, it is very difficult to build a robust model. One way to aid in robustness is to engineer the target. In fact, most everyone in this competition is already doing this with something like\n\n```y = (train_data['resp'] > 0).astype(int)```\n\n\nto make a binary target for use in a classifier. Can we do ""better"" than this?\n\nAs noted in the great notebook [The Most Important Model Parameter](https://www.kaggle.com/gkoundry/the-most-important-model-parameter), there is a drift in the mean daily return, `resp`, and this drift is different when  you account for the `weight`:"
"How do we account for this drift in the modeling process? The referenced notebook indicated that one could tune the classifier decision boundary, noting that this is likely just overfitting to the public leaderboard."
"## Cross Sectional Daily Bins\n\nSince `weight` is strictly positive, it doesn't help to engineer the `resp` target simply by multiplying by `weight`. However, one approach to engineer the target and ""solve"" this is to quantile the `resp x weight` per day and then set the target to be the top N bins. This approach focuses the classifer to train on high return **and** high weight trades. It removes, in training, any drift in the dataset."
The weights are log-normally distributed. A simple log transform shows this.
"The Gender Gap\n\nI wanted to start with something simple and important at the same time. So I questioned myself: over the past three years, did the proportion of Men and Women change? I knew there was a gap, but I would like to see it decreasing (and a lot) in the recent years.\n    "
"\n    Unfortunately, there is a considerable gap in professionals participation in Kaggle: 84% of men against 16% of women.\n\nAnd what is worse than that is that women participation did not increase over the past three years. I saw some other notebooks that showed an increase in female students. Maybe this will cause an increase in data professionals next year, but need a lot more women to close this gap.\n\nMaybe Kaggle could host women only competitions, in a bid to attract more of them to the platform (and to Data Science).\n\n\nIf we zoom-in using the same chart, we can see which countries are getting more women into Data over the past few years."
"\n    While most countries increased the gender gap in 2020, India is the country that is closing the gap faster. But remember that we are still talking about 18.5% women against 80.3% of men in India.\n    "
"\n    Ok, there is a huge gap in participation, but the pay gap must be closing, no? We are past 2020 after all.\n\nTo analyse the pay gap I decided to break down the average anual compensation of women and men for education levels. I was hoping to see the gap closing with higher degrees. \n    "
"\n    I have to confess that seeing this chart was very disappointing. First the distance between women and men salaries increased in 2020 for most education levels. The pandemic could partially explain the sharp drop in 2020, with women having to leave their jobs or reduce hours to take care of children at home, for example. However, the gap also slightly increased in 2019, we are in an alarming trend.\n\nAnd the worst news is that  even though the gap closes a little bit for Bachelor's and Master's degrees, it increases again for PhDs (Doctoral)! This was something that I did not expect, and I feel sorry for all women that despite all effort  to achieve the highest education title are still treated unequally to men.\n\nLet's do something to close the gap? Give more opportunities for women to ingress data careers even if they don't have all the required experience. And please, pay women the same you pay men for consistent education level and experience.\n\n    "
**Import libraries**\n====================
**Load train & test data**\n====================
**Heatmap**\n-----------
As we saw above there are few feature which shows high multicollinearity from heatmap.\nLets focus on yellow squares on diagonal line and few on the sides.\n\nSalePrice and OverallQual\n\nGarageArea and GarageCars\n\nTotalBsmtSF and 1stFlrSF\n\nGrLiveArea and TotRmsAbvGrd\n\nYearBulit and GarageYrBlt\n\nWe have to create a single feature from them before we use them as predictors.\n
*Univariate Analysis*\n--------------------\n\nHow 1 single variable is distributed in numeric range.\nWhat is statistical summary of it.\nIs it positively skewed or negatively.
Prices are right skewed and  graph shows some peakedness.
### Loading Metadata
Let's load the metadata of the dateset. 'title' and 'journal' attributes may be useful later when we cluster the articles to see what kinds of articles cluster together.
"### Plot for a sanity check\n\nTo make sure the conversion went as planned, let's make a plot showing the distribution of loan lengths."
"It looks as if there are a number of loans that are unreasonably long. Reading through the discussions, other people had noticed this as well. At this point, we will just leave in the outliers. We also will drop the time offset columns."
#### Bureau Balance\n\nThe bureau balance dataframe has a `MONTHS_BALANCE` column that we can use as a months offset. The resulting column of dates can be used as a `time_index`.
#### Previous Applications\n\nThe `previous` dataframe holds previous applications at Home Credit. There are a number of time offset columns in this dataset:\n\n* `DAYS_DECISION`: number of days before current application at Home Credit that decision was made about previous application. This will be the `time_index` of the data.\n* `DAYS_FIRST_DRAWING`: number of days before current application at Home Credit that first disbursement was made\n* `DAYS_FIRST_DUE`: number of days before current application at Home Credit that first due was suppoed to be\n* `DAYS_LAST_DUE_1ST_VERSION`: number of days before current application at Home Credit that first was??\n* `DAYS_LAST_DUE`: number of days before current application at Home Credit of last due date of previous application\n* `DAYS_TERMINATION`: number of days before current application at Home Credit of expected termination\n\nLet's convert all these into timedeltas in a loop and then make time columns.
#### Previous Credit and Cash\n\nThe `credit_card_balance` and `POS_CASH_balance` each have a `MONTHS_BALANCE` column with the month offset. This is the number of months before the current application at Home Credit of the previous application record. These will represent the `time_index` of the data. 
#### Installments Payments \n\nThe `installments_payments` data contains information on each payment made on the previous loans at Home Credit. It has two date offset columns:\n\n* `DAYS_INSTALMENT`: number of days before current application at Home Credit that previous installment was supposed to be paid\n* `DAYS_ENTRY_PAYMENT`: number of days before current application at Home Credit that previous installment was actually paid\n\nBy now the process should be familiar: convert to timedeltas and then make time columns. The DAYS_INSTALMENT will serve as the `time_index`. 
"# Applying Featuretools\n\nWe can now start making features using the time columns. We will create an entityset named clients much as before, but now we have time variables that we can use. "
\n\nIMPORT NECESSARY LIBRARIES
\n\nLOAD DATASETS
## Step 1 - Load Data ##\n\nThis next hidden cell will import some libraries and set up our data pipeline. We have a training split called `ds_train` and a validation split called `ds_valid`.
Let's take a look at a few examples from the training set.
"## Step 2 - Define Pretrained Base ##\n\nThe most commonly used dataset for pretraining is [*ImageNet*](http://image-net.org/about-overview), a large dataset of many kind of natural images. Keras includes a variety models pretrained on ImageNet in its [`applications` module](https://www.tensorflow.org/api_docs/python/tf/keras/applications). The pretrained model we'll use is called **VGG16**."
"When training a neural network, it's always a good idea to examine the loss and metric plots. The `history` object contains this information in a dictionary `history.history`. We can use Pandas to convert this dictionary to a dataframe and plot it with a built-in method."
"# Conclusion #\n\nIn this lesson, we learned about the structure of a convnet classifier: a **head** to act as a classifier atop of a **base** which performs the feature extraction.\n\nThe head, essentially, is an ordinary classifier like you learned about in the introductory course. For features, it uses those features extracted by the base. This is the basic idea behind convolutional classifiers: that we can attach a unit that performs feature engineering to the classifier itself.\n\nThis is one of the big advantages deep neural networks have over traditional machine learning models: given the right network structure, the deep neural net can learn how to engineer the features it needs to solve its problem.\n\nFor the next few lessons, we'll take a look at how the convolutional base accomplishes the feature extraction. Then, you'll learn how to apply these ideas and design some classifiers of your own."
"# COVID Global Forecast: SIR model + ML regressions\n\nIn the context of the global COVID-19 pandemic, Kaggle has launched several challenges in order to provide useful insights that may answer some of the open scientific questions about the virus. This is the case of the [COVID19 Global Forecasting](https://www.kaggle.com/c/covid19-global-forecasting-week-1), in which participants are encouraged to fit worldwide data in order to predict the pandemic evolution, hopefully helping to determine which factors impact the transmission behavior of COVID-19.\n\n**TABLE OF CONTENTS**\n\n1. [Exploratory data analysis (EDA)](#section1)\n\n    1.1. [COVID-19 global tendency excluding China](#section11)\n    \n    1.2. [COVID-19 tendency in China](#section12)\n    \n    1.3. [Italy, Spain, UK and Singapore](#section13)\n    \n2. [SIR model](#section2)\n\n    2.1. [Implementing the SIR model](#section21)\n    \n    2.2. [Fit SIR parameters to real data](#section22)\n    \n3. [Data enrichment](#section3)\n\n    3.1. [Join data, filter dates and clean missings](#section31)\n    \n    3.2. [Compute lags and trends](#section32)\n    \n    3.3. [Add country details](#section33)\n    \n4. [Predictions for the early stages of the transmission](#section4)\n\n    4.1. [Linear Regression for one country](#section41)\n    \n    4.2. [Linear Regression for all countries (method 1)](#section42)\n    \n    4.3. [Linear Regression for all countries (method 2)](#section43)\n    \n    4.4. [Linear regression with lags](#section44)\n    \n5. [Predictions for the late stages of the transmission](#section5)\n\n    5.1. [Logistic curve fit](#section51)\n    \n    5.2. [Logistic curve fit for all countries](#section52)\n    \n    5.3. [ARIMA](#section53)\n    \n6. [Statement of the author](#section6)\n\n**Disclaimer 1**: this notebook is being updated frequently with the objective of improving predictions by using new models.\n\n**Disclaimer 2**: the training dataset is also updated on a daily basis in order to include the most recent cases. In order to be up to date and prevent data leaking and other potential problems, daily updates on ""filtered dates"" will be applied.\n\n**Disclaimer 3**: the COVID Global Forecasting competition is updated week by week (with a new competition). I'll move the notebook from previous weeks to the new one, so that it only appears in the most recent competition. "
"# 1. Exploratory data analysis (EDA) \n\nFirst of all, let's take a look on the data structure:"
"The dataset covers 163 countries and almost 2 full months from 2020, which is enough data to get some clues about the pandemic. Let's see a few plots of the worldwide tendency to see if we can extract some insights:"
"**Observations**: The global curve shows a rich fine structure, but these numbers are strongly affected by the vector zero country, China. Given that COVID-19 started there, during the initial expansion of the virus there was no reliable information about the real infected cases. In fact,  the criteria to consider infection cases was modified around 2020-02-11, which strongly perturbed the curve as you can see from the figure. "
"## 1.1. COVID-19 global tendency excluding China \n\nSince details of the initial breakthrough strongly interfere with the results, it's recomended to analyze China independently. Let's first see the results without China: "
"**Observations**: In this case the general behavior looks cleaner, and in fact the curve resembles a typical epidemiology model like [SIR](http://mat.uab.cat/matmat/PDFv2013/v2013n03.pdf). SIR models present a large increasing in the number of infections that, once it reaches the maximum of the contagion, decreases with a lower slope. For comparison, a SIR simulation from section [2. SIR model](#section2):\n\n![__results___28_0.png](attachment:__results___28_0.png)"
"## 1.2. COVID-19 tendency in China \n\nSince China was the initial infected country, the COVID-19 behavior is different from the rest of the world. The medical system was not prepared for the pandemic, in fact no one was aware of the virus until several cases were reported. Moreover, China government took strong contention measures in a considerable short period of time and, while the virus is widely spread, they have been able to control the increasing of the infections. "
"**Observations**:\n\n* **Smoothness**. Both plots are less smooth than theoretical simulations or the curve from the rest of the world cumulative\n* **Infected criteria**. The moment in which the criteria to consider an infected case was changed is direclty spotted\n* **Irregularities**. There are some iregularities. I should check the literature in depth to look for evidences, but the reasons may be that both the resources spent to monitor the epidemy and the security measures to stop ot have been changing over time\n* **Plateaux**. It looks like the curve has reached a plateaux, which would imply that China is on their maximum of contagion "
"## 1.3. Italy, Spain, UK and Singapore \n\nBoth Italy and Spain are experiencing the larger increase in COVID-19 positives in Europe. At the same time, UK is a unique case given that it's one of the most important countries in Europe but recently has left the European Union, which has create an effective barrier to human mobility from other countries. The fourth country we will study in this section is Singapore, since it's an asiatic island, is closer to China and its  socio-economic conditions is different from the other three countries."
As a fraction of the total population of each country:
"In order to compare the 4 countries, it's also interesting to see the evolution of the infections from the first confirmed case:"
"**Observations**:\n* **Italy**. With almost 120.000 confirmed cases, Italy shows one of the most alarming scenarios of COVID-19. The infections curve is very steep, and more than 2% of population has been infected\n* **Spain**. Spain has the same number of cumulative infected cases than Italy, near 120.000. However, Spain's total population is lower (around 42 millions) and hence the percentage of population that has been infected rises up to 3%.\n* **United Kingdom**. Despite not being very far from them, the UK shows less cases. This may be due to the number of tests performed, but it's  soon to know for sure. The number of cases is around 40.000, this is, a 0.6 of the total population.\n* **Singapore**. Singapore is relatively isolated given that is an island, and the number of international travels is lower than for the other 3 countries. The number of cases is still very low (>1000), despite the general tendency is to increase. However, the infections started faster in the beginning, but the slope of the infections curve hasn't increased very much in the past weeks. A 0.2% of the population was infected"
"Results obtained for N=world population, only one initial infected case, $\beta=0.3$, $\gamma=0.5$ and a leap pass $h_s = 0.1$ are shown below:"
"**Observations**: \n* The number of infected cases increases for a certain time period, and then eventually decreases given that individuals recover/decease from the disease\n* The susceptible fraction of population decreases as the virus is transmited, to eventually drop to the absorbent state 0\n* The oposite happens for the recovered/deceased case\n\nNotice that different initial conditions and parameter values will lead to other scenarios, feel free to play with these numbers to study the system."
"## 2.2. Fit SIR parameters to real data \n\nThe SIR model is purely theoretical, and we are interested into a real approximation of the COVID-19 expansion in order to extract insights and understand the transmission of the virus. Hence, we need to extract the $\beta$ and $\gamma$ paramaters for each case if we hope to be able to predict the evolution of the system."
"**I'm not happy with the fit of parameters** and I want to work more on this, since I'm not properly reproducing the curves. I'll keep working on this for curiosity, but in the meanwhile I'll develop a data-centric approach to the prediction."
## Import necessary libraries
## Data loading\nLoad all provided datasets and get a feel of the data provided to us
 Prerequisites 
Please Upvote my kernel and keep it in your favourite section if you think it is helpful.
\n4.2 Finding LR\n
\n4.3 Train Model\n
Just a quick look on variables we are dealing with.
"#### A citate from a movie: 'Children and women first'. \n* Sex: Survival chances of women are higher.\n* Pclass: Having a first class ticket is beneficial for the survival.\n* SibSp and Parch: middle size families had higher survival rate than the people who travelled alone or big families. The reasoning might be that alone people would want to sacrifice themselves to help others. Regarding the big families I would explain that it is hard to manage the whole family and therefore people would search for the family members insetad of getting on the boat.\n* Embarked C has a higher survival rate. It would be interesting to see if, for instance, the majority of Pclass 1 went on board in embarked C."
## 1.2 Survival by Sex and Age
"* Survival rate of boys is higher than of the adult men. However, the same fact does not hold for the girls. and between 13 and 30 is lower. Take it into consideration while engineering the variable: we could specify a categorical variable as young and adult.\n* For women the survival chances are higher between 14 and 40 age. For men of the same age the survival chances are flipped."
## 1.3.1 Survival by Class and Embarked
"* As noticed already before, the class 1 passangers had a higher survival rate.\n* All women who died were from the 3rd class. \n* Embarked in Q as a 3rd class gave you slighly better survival chances than embarked in S for the same class.\n* In fact, there is a very high variation in survival rate in embarked Q among 1st and 2nd class. The third class had the same survival rate as the 3rd class embarked C. We will exclude this variable embarked Q. From crosstab we see that there were only 5 passengers in embarked Q with the 1st and 2nd class. That explains large variation in survival rate and a perfect separation of men and women in Q."
## 1.3.2 Fare and class distribution
* It appears that the higher the fare was in the first class the higher survival chances a person from the 1st had.
## 1.3.3 Class and age distribution
"* Interesting note that Age decreases proportionally with the Pclass, meaning most old passangers are from 1st class. We will construct a new feature Age*Class to intefere the this findig. \n* The younger people from 1st had higher survival chanches than older from the same class.\n* Majority (from the 3rd class) and most children from the 2nd class survived."
## 1.4 Survival rate regarding the family members
"Assumption: the less people was in your family the faster you were to get to the boat. The more people they are the more managment is required. However, if you had no family members you might wanted to help others and therefore sacrifice.\n\n* The females traveling with up to 2 more family members had a higher chance to survive. However, a high variation of survival rate appears once family size exceeds 4 as mothers/daughters would search longer for the members and therefore the chanes for survival decrease.\n* Alone men might want to sacrifice and help other people to survive. "
"## Setup program\n\n### Configure imports and eager execution\n\nImport the required Python modules—including TensorFlow—and enable eager execution for this program. Eager execution makes TensorFlow evaluate operations immediately, returning concrete values instead of creating a [computational graph](https://www.tensorflow.org/guide/graphs) that is executed later. If you are used to a REPL or the `python` interactive console, this feels familiar. Eager execution is available in [Tensorlow >=1.8](https://www.tensorflow.org/install/).\n\nOnce eager execution is enabled, it *cannot* be disabled within the same program. See the [eager execution guide](https://www.tensorflow.org/guide/eager) for more details."
"## The Iris classification problem\n\nImagine you are a botanist seeking an automated way to categorize each Iris flower you find. Machine learning provides many algorithms to classify flowers statistically. For instance, a sophisticated machine learning program could classify flowers based on photographs. Our ambitions are more modest—we're going to classify Iris flowers based on the length and width measurements of their [sepals](https://en.wikipedia.org/wiki/Sepal) and [petals](https://en.wikipedia.org/wiki/Petal).\n\nThe Iris genus entails about 300 species, but our program will only classify the following three:\n\n* Iris setosa\n* Iris virginica\n* Iris versicolor\n\n\n  \n    \n  \n  \n    Figure 1. Iris setosa (by Radomil, CC BY-SA 3.0), Iris versicolor, (by Dlanglois, CC BY-SA 3.0), and Iris virginica (by Frank Mayfield, CC BY-SA 2.0). \n  \n\n\nFortunately, someone has already created a [data set of 120 Iris flowers](https://en.wikipedia.org/wiki/Iris_flower_data_set) with the sepal and petal measurements. This is a classic dataset that is popular for beginner machine learning classification problems."
"Notice that like-features are grouped together, or *batched*. Each example row's fields are appended to the corresponding feature array. Change the `batch_size` to set the number of examples stored in these feature arrays.\n\nYou can start to see some clusters by plotting a few features from the batch:"
"To simplify the model building step, create a function to repackage the features dictionary into a single array with shape: `(batch_size, num_features)`.\n\nThis function uses the [tf.stack](https://www.tensorflow.org/api_docs/python/tf/stack) method which takes values from a list of tensors and creates a combined tensor at the specified dimension."
### How many Survived??
"It is evident that not many passengers survived the accident. \n\nOut of 891 passengers in training set, only around 350 survived i.e Only **38.4%** of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn't.\n\nWe will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port Of Embarcation, Age,etc.\n\nFirst let us understand the different types of features."
### The tennis matches data 
"The dataset ""atp_data.csv"" is directly built from the data you can find on http://tennis-data.co.uk/data.php. I selected the columns that will be useful for the betting model, and added the elo rankings of the players at the beginning of each match.\n\nWe have all the matches played on the ATP World Tour from January 2000 to March 2018.\n\nOne row per match. And we have some information about each match :"
# Assessment of some basic strategies 
## Betting on all matches 
**Import the Libraries**
**Import Data**
Correlation matrix between numerical values:
Correlations between numerical variables and Survived aren't so high but it doesn't mean that the other features are not useful.
Embarked vs Survived
##**Missing values**
"## Dataset\n\nBefore beginning the analysis, we will load and view the dataset, and perform some initial cleaning.\n\n* View the dataset info:"
* Clean up column names\n* Transform selected columns to numeric format:\n    - `Income` to float
"#### Outliers\n\n* Identify features containing outliers:\n    - Findings: Multiple features contain outliers (see boxplots below), but the only that likely indicate data entry errors are `Year_Birth <= 1900`"
* Remove rows where `Year_Birth <= 1900`:
## Are there any variables that warrant transformations?\n\n* View data types:\n    - Findings: The `Dt_Customer` column should be transformed to datetime format
"### Do you notice any patterns or anomalies in the data? Can you plot them?\n\n* To identify patterns, we will first identify feature correlations. Positive correlations between features appear red, negative correlations appear blue, and no correlation appears grey in the clustered heatmap below.\n* From this heatmap we can observe the following clusters of correlated features:\n    - The **""High Income""** cluster:\n        - Amount spent ('TotalMnt' and other 'Mnt' features) and number of purchases ('TotalPurchases' and other 'Num...Purchases' features) are positively correlated with 'Income'\n        - Purchasing in store, on the web, or via the catalog ('NumStorePurchases', 'NumWebPurchases', 'NumCatalogPurchases') is positively correlated with 'Income'\n    - The **""Have Kids & Teens""** cluster:\n        - Amount spent ('TotalMnt' and other 'Mnt' features) and number of purchases ('TotalPurchases' and other 'Num...Purchases' features) are negatively correlated with 'Dependents' (with a stronger effect from kids *vs.* teens)\n        - Purchasing deals ('NumDealsPurchases') is positively correlated with 'Dependents' (kids and/or teens) and negatively correlated with 'Income'\n    - The **""Advertising Campaigns""** cluster:\n        - Acceptance of the advertising campaigns ('AcceptedCmp' and 'Response') are strongly positively correlated with each other\n        - Weak positive correlation of the advertising campaigns is seen with the ""High Income"" cluster, and weak negative correlation is seen with the ""Have Kids & Teens"" cluster\n* Anomalies:\n    - Surprisingly, the number of website visits in the last month ('NumWebVisitsMonth') does not correlate with an increased number of web purchases ('NumWebPurchases')\n    - Instead, 'NumWebVisitsMonth' is positively correlated with the number of deals purchased ('NumDealsPurchases'), suggesting that  suggesting that deals are an effective way of stimulating purchases on the website"
"* Plot illustrating the effect of high income on spending:\n\nNote: For the purposes of this plot, limiting income to < 200000 to remove outlier"
* Plot illustrating negative effect of having dependents (kids & teens) on spending:
* Plot illustrating positive effect of having dependents (kids & teens) on number of deals purchased:
"![Alt Text](https://sa.kapamilya.com/absnews/abscbnnews/media/ancx/culture/2019/57/bannerburnout.jpg?ext=.jpg)\n\n About this Kernel \n\nIn this kernel we'll try to understand what are the factos contributing the mental health of a person. This dataset is from a 2014 survey that measures attitudes towards mental health and frequency of mental health disorders in the tech workplace. This kernel is going to be different from my other kernels as together we are going to understand mental health at workplace in a systematic manner. So, what exactly is the first step? The first step towards this is getting the domain knowledge. Lets begin!\n\n\n\nThere are always a lot of great kernels regarding different ways of solving the problems but only a few handful address the problems of domain knowledge and getting started. In this notebook , I will start with complete explanation of everything you need know related to Prostate Cancer and its detection and I will built on that to explain the dataset,perform EDA and then Build a baseline model\n\n Domain Knowledge #101 \n\n> **Q1 What exactly do we mean by Mental Health at workplace?**\n\nMental health affects your emotional, psychological and social well-being. It affects how we think, feel, and act. It also helps determine how we handle stress, relate to others, and make choices. In the workplace, communication and inclusion are keys skills for successful high performing teams or employees. The impact of mental health to an organization can mean an increase of absent days from work and a decrease in productivity and engagement. In the United States, approximately 70% of adults with depression are in the workforce. Employees with depression will miss an estimated 35 million workdays a year due mental illness. Those workers experiencing unresolved depression are estimated to encounter a 35% drop in their productivity, costing employers $105 billion dollars each year.\n\n\n![Alt Text](https://exudeinc-wpengine.netdna-ssl.com/wp-content/uploads/2018/07/mental-health.jpg)\n\n\n> **Q2 What can your employer do about this?**\n\nSo what can employers do? It’s called Mental Health First Aid.\n\nMental Health First Aid teaches participants how to notice and support an individual who may be experiencing a mental health or substance use concern or crisis and connect them with the appropriate employee resources. It teaches employees critical communication and support skills that can influence your organizations bottom line.\n\nResearch shows that employees who go through Mental Health First Aid have an increased awareness of mental health among themselves and their co-workers. It allows them to recognize the signs of someone who maybe struggling and teaches them the skills to know when to reach out and what resources are available. Which in turn creates beneficial intervention that increases engagement and creates an environment of inclusion and support.\n\nEmployers can also offer robust benefit packages to support employees who go through mental health issues. That includes Employee Assistance Programs, Wellness programs that focus on mental and physical health, Health and Disability Insurance or flexible working schedules or time off policies.\n\nOrganizations that incorporate mental health awareness help to create a healthy and productive work environment that reduces the stigma associated with mental illness, increases the organizations mental health literacy and teaches the skills to safely and responsibly respond to a co-workers mental health concern.\n\nIncorporating mental health awareness in the workplace can help lead the way for mental health issues throughout your community by equipping people with the tools they need to start a dialogue so that more people can get the help they need.\n\n Initial Understanding of the data #101 \n"
*1. Loading the data*\n\n*2. Checking the head of the data*\n\n*3. Looking out for the null values*
" To be Noted: \n\nUnknowingly, we have stumbled upon the fact that the number of males in the dataset are 4 times the number of females. Thus, we must keep this in mind and avoid making any faulty assumptions that males are more susceptible to mental health issues etc. \n\nAlternatively, we may conclude that the number of males in the tech industry are much more as compared to the number of females (This research was conducted specifically for the tech industry.).\n"
"\n* There's only one column which is **'work_interfere'** remaining that contains null values. For now we will proceed without any imputation. \n* Actually, there's another column, **'self_employed'** which contains around 18 null values which we failed to notice at first.\n\n> Now let us move forward and perform EDA"
> Let us begin by understanding the target data!
" Inferences: \n\n\nThis is the respondents result of question, **'Have you sought treatment for a mental health condition?'**.\n\n\nThis is our target variable.\nLooking at the first graph, we see that the percentage of respondents who want to get treatment is exactly 50%. Workplaces that promote mental health and support people with mental disorders are more likely to  have increased productivity, reduce absenteeism, and benefit from associated economic gains. If employees enjoy good mental health, employees can:\n\n* Be more productive\n* Take active participation in employee engagement activities and make better relations; both at workplace and personal life.\n* Be more joyous and make people around them happy.\n"
"> After analysing the target variable, we will try to explore the individual columns and what they mean."
"This is respondent's answer to the question, '**Are you self-employed?**'.\n\nWe see that the number of people who are self employed are around 10%. Most of the people who responded to the survey belonged to working class. We also see that though there is a vast difference between people who are self employed or not, the number of people who seek treatment in both the categories is more or less similar. \n\n> Thus, we may conclude that whether a person is self employed or not, does not largely affect whether he may be seeking mental treatment or not. "
" Inference: \n\nThis is the respondents answer to the question, **'Do you have a family history of mental illness?'**.\n\nFrom close to 40% of the respondents who say that they have a family history of mental illness, the plot shows that they significantly want to get treatment rather than without a family history. This is acceptable, remember the fact that people with a family history pay more attention to mental illness. Family history is a significant risk factor for many mental health disorders. \n\n> Thus, this is an important factor that has to be taken under consideration as it influences the behaviour of the employees to a significant extent.\n\n"
" Inference: \n\n\nThis was the respondent's answer to the question, **'If you have a mental health condition, do you feel that it interferes with your work?'.**\n\n* On seeing the first graph we conclude that around 48% of people say that sometimes work interefers with their mental health. Now **'Sometimes'** is a really vague response to a question, and more often than not these are the people who actually face a condition but are too shy/reluctant to choose the extreme category.\n* Coming to our second graph, we see that the people who chose **'Sometimes'** had the highest number of people who actually had a mental condition. Similar pattern was shown for the people who belonged to the **'Often category'*.\n* But what is more surprising to know is that even for people whose mental health **'Never'** has interfered at work, there is a little group that still want to get treatment before it become a job stress. It can be triggered a variety of reasons like the requirements of the job do not match the capabilities, resources or needs of the worker.\n\n\n\n> We will be leaving the 'number_of_employees' category and move forward with the next column which is 'remote_work'.\n"
" Inference: \n\nThis was the respondent's answer to the question, **'Do you work remotely (outside of an office) at least 50% of the time?'.**\n\nAround 70% of respondents don't work remotely, which means the biggest factor of mental health disorder came up triggered on the workplace. On the other side, it has slightly different between an employee that want to get treatment and don't want to get a treatment. \nThe number of people who seek treatment in both the categories is more or less similar and it does not affect our target variable. \n\n> Let's move forward with our next variable which is 'tech_company'."
"\nThis is the respondents answer to the question, **'Is your employer primarily a tech company/organization?'.**\n\n* Although the survey was specifically designed to be conducted in the tech field, there are close to 18% of the companies belonginf to the non tech field. However, looking at the second graph, one may conclude that whether a person belongs to the tech field or not, mental health still becomes a big problem.\n\n* However, on a deeper look we find that the number of employees in the tech sector who want to get treatment is slightly lower than the one's who don't. But in the non-tech field the situation gets reversed."
0  IMPORTS
1  BACKGROUND INFORMATION
3  HELPER FUNCTIONS
4  INFERENCE LOOP
# Imports
# Loading the data
"## **House Prices: EDA to ML (Beginner)**  \n\n**This is my first Kaggle for the House Prices competition.**  \n**It includes the following approaches and techniques:**\n\n* EDA with Pandas and Seaborn\n* Find features with strong correlation to target\n* Data Wrangling, convert categorical to numerical\n* apply the basic Regression models of sklearn \n* use gridsearchCV to find the best parameters for each model\n* compare the performance of the Regressors and choose best one"
"![](https://www.reno.gov/Home/ShowImage?id=7739&t=635620964226970000)\n\n**Competition Description from Kaggle**  \nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n**Data description**  \nThis is a detailed description of the 79 features and their entries, quite important for this competition.  \nYou can download the txt file here: [**download**](https://www.kaggle.com/c/5407/download/data_description.txt)"
**Imports**
"**Settings and switches**\n\n**Here one can choose settings for optimal performance and runtime.**  \n**For example, nr_cv sets the number of cross validations used in GridsearchCV, and**  \n**min_val_corr is the minimum value for the correlation coefficient to the target (only features with larger correlation will be used).** "
### The target variable : Distribution of SalePrice
"As we see, the target variable SalePrice is not normally distributed.  \nThis can reduce the performance of the ML regression models because some assume normal distribution,   \nsee [sklearn info on preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html)  \nTherfore we make a log transformation, the resulting distribution looks much better.  "
### Numerical and Categorical features
# Step 4: Explore Data #\n\nLet's take a moment to look at some of the images in the dataset.
You can display a single batch of images from a dataset with another of our helper functions. The next cell will turn the dataset into an iterator of batches of 20 images.
# Step 6: Training #\n\n## Learning Rate Schedule ##\n\nWe'll train this network with a special learning rate schedule.
"## Fit Model ##\n\nAnd now we're ready to train the model. After defining a few parameters, we're good to go!"
"1. In this problem we have to use 30 different columns and we have to predict the Stage of Breast Cancer M (Malignant)  and B (Bengin)\n 2. This analysis has been done using Basic Machine Learning Algorithm with detailed explanation\n 3. This is good for beginners like as me Lets start.\n \n4.Attribute Information:\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign)\n\n-3-32.Ten real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\n\nb) texture (standard deviation of gray-scale values)\n\nc) perimeter\n\nd) area\n\ne) smoothness (local variation in radius lengths)\n\nf) compactness (perimeter^2 / area - 1.0)\n\ng). concavity (severity of concave portions of the contour)\n\nh). concave points (number of concave portions of the contour)\n\ni). symmetry\n\nj). fractal dimension (""coastline approximation"" - 1)\n\n5  here 3- 32 are divided into three parts first is Mean (3-13),  Stranded Error(13-23) and  Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension) \n\n 6. Here Mean means the means of the all cells,  standard Error of all cell and worst means the worst  cell "
**Import data **
## Data Analysis a little feature selection
"*observation*\n\n - the radius, parameter and area  are highly correlated as expected from their relation*\n    so from these we will use anyone of them *\n - *compactness_mean, concavity_mean and concavepoint_mean are highly correlated so we will use compactness_mean from here *\n - so selected Parameter for use is perimeter_mean, texture_mean, compactness_mean, symmetry_mean*"
"# Introduction\n\nThe Future Sales competition is the final assesment in the 'How to win a Data Science' course in the Advanced Machine Learning specialisation from HSE University, Moscow. The aim is to predict the monthly sales of items in specific shops, given historical data. The sale counts are clipped between 0 and 20."
# Remove outliers
"We'll remove the obvious outliers in the dataset - the items that sold more than 1000 in one day and the item with price greater than 300,000."
# Cleaning Item Data
Clean item names.
### Import Required Packges
### Importing the Data
Drama looks to be the most popular genre followed by Comedy.
Now lets generate a list 'genreList' with all possible unique genres mentioned in the dataset.
# 1. Packages\n\nYou can find the packages below what I used.
# 2. Importing Data
"This feature is highly correlated with sales but first, you are supposed to sum the sales feature to find relationship. Transactions means how many people came to the store or how many invoices created in a day.\n\nSales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n\nThat's why, transactions will be one of the relevant features in the model. In the following sections, we will generate new features by using transactions."
"There is a stable pattern in Transaction. All months are similar except December from 2013 to 2017 by boxplot. In addition, we've just seen same pattern for each store in previous plot. Store sales had always increased at the end of the year."
**Let's take a look at transactions by using monthly average sales!**\n\n We've just learned a pattern what increases sales. It was the end of the year. We can see that transactions increase in spring and decrease after spring.
"When we look at their relationship, we can see that there is a highly correlation between total sales and transactions also. "
"The days of week is very important for shopping. It shows us a great pattern. Stores make more transactions at weekends. Almost, the patterns are same from 2013 to 2017 and Saturday is the most important day for shopping."
"# 4. Oil Price\n\n\nThe economy is one of the biggest problem for the governments and people. It affects all of things in a good or bad way. In our case, Ecuador is an oil-dependent country. Changing oil prices in Ecuador will cause a variance in the model. I researched Ecuador's economy to be able to understand much better and I found an article from IMF. You are supposed to read it if you want to make better models by using oil data.\n\n- https://www.imf.org/en/News/Articles/2019/03/20/NA032119-Ecuador-New-Economic-Plan-Explained\n\n\n\n\n\n\n\nThere are some missing data points in the daily oil data as you can see below. You can treat the data by using various imputation methods. However, I chose a simple solution for that. Linear Interpolation is suitable for this time serie. You can see the trend and predict missing data points, when you look at a time serie plot of oil price."
"**I just said, ""Ecuador is a oil-dependent country"" but is it true? Can we really see that from the data by looking at?**\n\nFirst of all, let's look at the correlations for sales and transactions. The correlation values are not strong but the sign of sales is negative. Maybe, we can catch a clue. Logically, if daily oil price is high, we expect that the Ecuador's economy is bad and it means the price of product increases and sales decreases. There is a negative relationship here.  "
"You should never decide what you will do by looking at a graph or result! You are supposed to change your view and define new hypotheses.\n\nWe would have been wrong if we had looked at some simple outputs just like above and we had said that there is no relationship with oil prices and let's not use oil price data.\n\nAll right! We are aware of analyzing deeply now. Let's draw a scatter plot but let's pay attention for product families this time. All of the plots almost contains same pattern. When daily oil price is under about 70, there are more sales in the data. There are 2 cluster here. They are over 70 and under 70. It seems pretty understandable actually. \n\nWe are in a good way I think. What do you think? Just now, we couldn't see a pattern for daily oil price, but now we extracted a new pattern from it."
"# 5. Sales\n\nOur main objective is, predicting store sales for each product family. For this reason, sales column should be examined more seriously. We need to learn everthing such as seasonality, trends, anomalies, similarities with other time series and so on."
"Most of the stores are similar to each other, when we examine them with correlation matrix. Some stores, such as 20, 21, 22, and 52 may be a little different."
There is a graph that shows us daily total sales below.
"I realized some unnecessary rows in the data while I was looking at the time serie of the stores one by one. If you select the stores from above, some of them have no sales at the beginning of 2013. You can see them, if you look at the those stores 20, 21, 22, 29, 36, 42, 52 and 53. I decided to remove those rows before the stores opened. In the following codes, we will get rid of them."
We'll be taking our data visualization skills to the next level with plotly.  \nplotly is a great package that allows you to create dynamic data visualizations in Python.  \nHere's an example of what we'll be able to create by the end of this tutorial.
The prerequisites for this tutorial are intermediate knowledge of Python and beginner knowledge of pandas and numpy.
"Shoutout to Derek Banas, I created this Kaggle Notebook in part by following his [tutorial on YouTube](https://www.youtube.com/watch?v=GGL6U0k8WYA).  \n"
---\n\n# *Section 1 - Data Exploration*\n\nThe first step is to import needed libraries.
"To begin our analysis, lets take our first look at the dataset. To save some precious time on our Exploratory Data Analysis process, we are going to use 2 libraries: **""pandas_profiling""** and **""autoviz""**."
"## PPS (Predictive Power Score)\n\nYou may have heard about correlation matrices. Basically, correlation matrices are able to identify linear relationships between variables. Because relationships in our data may sometimes be non-linear (most of the times, actually), we can use a PPS (Predictive Power Score) matrix, to figure out **non-linear relations** between columns.\n\nIf you want to understand why PPS is important, I recommend you to read this medium article: https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598\n\nAlso, take a look at the Python PPS implementation used in this notebook: https://github.com/8080labs/ppscore"
"Looking at this PPS matrix, we can see that the best univariate predictor of the **Survived** variable is the column **Ticket**, with 0.19 pps, followed by **Sex**, with 0.13 pps. That makes sense because women were prioritized during the rescue, and ticket is closely related to **Pclass**. The best univariate predictor of the **Parch** variable is the column **Cabin**, with 0.37 pps, and so on."
"---\n\n# *Section 2 - Supervised Learning: Classification*\n\nNow that we have some nice context about the data we are working with, let's dive into the modeling part.\n\nFirst of all, we import the libraries we're going to use."
"## Feature Engineering \n\nTo help us get a better performance, we can create new features based on the original features of our dataset.\n\nSome new features created here were based on the great ideas shown on this brilliant notebook from **Gunes Evitan**: https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial#2.-Feature-Engineering"
### ***What platforms did Data analysist find to be most helpful when they first started studying data-science?***
"- This graphs represents Online courses platforms for learning to take the road map of Data Analysts. This graphs show Online vidoe platforms like Coursera, EDX are best platforms for learning data analysts courses form begnning. "
### ***Programming Languages Data Analyst Use On a Regular Basis:***
- Python and SQL are the Necessary languages for completing data analytics course. This graph shows python and SQL as top rated language.
### ***Do Data Analysts use any of the following hosted notebook products?***
- Noted Books are used for coding practices or projects. these notebooks are also available online for team work or for save or run your coding work in better way. according to this graph colab note books are on the top.
### ***Top 10 Cloud Platforms Used By Data Analyst:***
"- Cloud services are often used for saving your projects, models or creating virtual machines, This graph represent Google cloud platform as best rated.***"
### ***Top 10 Pre-trainned Model Used by Data Analyst:***
- All job roles are mostly depending on Kaggle data sets except for MLops they are using Tensorlfow hub a lot.
# TPU or GPU detection
# Configuration\nThe Flowers dataset is availabe in multiple image sizes. 331x331px is the default. 512x512px will OOM on GPU but works on TPU.
"## Visualization utilities\n\ndata -> pixels, nothing of much interest for the machine learning practitioner in this section."
# Read images and labels from TFRecords
"Let's get to know our data by performing a preliminary data analysis.\n\n#  Part 1. Preliminary data analysis\n\nFirst, we will initialize the environment:"
"You should use the `seaborn` library for visual analysis, so let's set it up too:"
"To make it simple, we will work only with the training part of the dataset:"
It would be instructive to peek into the values of our variables.\n \nLet's convert the data into *long* format and depict the value counts of the categorical features using [`factorplot()`](https://seaborn.pydata.org/generated/seaborn.factorplot.html).
We can see that the target classes are balanced. That's great!\n\nLet's split the dataset by target values: sometimes you can immediately spot the most significant feature on the plot.
"You can see that the target variable greatly affects the distribution of cholesterol and glucose levels. Is this a coincidence?\n\nNow, let's calculate some statistics for the feature unique values:"
"\n\n# 1. Introduction 📜\n\nThis notebook is just me being frustrated on **deep learning** and trying to understand in ""baby steps"" what is going on here. For somebody that starts in this area with no background whatsoever it can be very confusing, especially because I seem to be unable to find code with many explanations and comments.\n\nSo, if you are frustrated just like I was when I started this stuff I hope the following guidelines will help you. I am by no means a teacher, but in this notebook I will:\n1. Share articles/videos I watched that TRULY helped\n2. Explain code along the way to the best of my ability\n\n \nNote: Deep learning coding is VERY different in structure than the usual sklearn for machine learning. In addition, it usually works with images and text, while ML usually works with tabular data. So please, be patient with yourself and if you don't understand something right away, continue reading/ coding and it will all make sense in the end.\n\n\n\n\n# 2. Before we start 📝\n\n> This is my third notebook in the ""series"": **How I taught myself Deep Learning**.\n1. **[How I taught myself Deep Learning: Vanilla NNs](https://www.kaggle.com/andradaolteanu/how-i-taught-myself-deep-learning-1-pytorch-fnn)**\n        * PyTorch and Tensors\n        * Neural Network Basics, Perceptrons and a Plain Vanilla Neural Net model\n        * MNIST Classification using FNN\n        * Activation Functions\n        * Forward Pass\n        * Backpropagation (Loss and Optimizer Functions)\n        * Batching, Iterations and Epochs\n        * Computing Classification Accuracy\n        * Overfitting: Data Augmentation, Weight Decay, Learning Rate, Dropout() and Layer Optimization   \n2. **[Convolutional Neural Nets (CNNs) Explained](https://www.kaggle.com/andradaolteanu/convolutional-neural-nets-cnns-explained)**\n        * Why ConvNets\n        * Convolutions Explained\n        * Computing Activation Maps\n        * Kernels, Padding, Stride\n        * AlexNet\n        * MNIST Classification using Convolutions"
"# 3. RNN with 1 Layer 📘\nRecurrent Neural Networks are very different from [FNNs](https://www.kaggle.com/andradaolteanu/how-i-taught-myself-deep-learning-vanilla-nns) or [CNNs](https://www.kaggle.com/andradaolteanu/how-i-taught-myself-deep-learning-convnet-cnns). \n\nRNNs model **sequential data**, meaning they have **sequential memory**. An RNN takes in different kind of inputs (text, words, letters, parts of an image, sounds, etc.) and returns different kinds of outputs (the next word/letter in the sequence, paired with an FNN it can return a classification etc.).\n\n\n\n**How RNN works**:\n1. It uses previous information to affect later ones\n2. There are 3 layers: *Input*, *Output* and *Hidden* (where the information is stored)\n3. The loop: passes the input forward sequentialy, while *retaining information* about it\n4. This info is stored in the *hidden state*\n5. There are only 3 matrixes (U, V, W) that contain weights as parameters. These *DON'T change* with the input, they stay the same through the entire sequence."
### Target Variable Visualization (Churn) : 
"- The dataset is **unbalanced** in a near about **3 : 1** ratio for **Not-Churn : Churn** customers!\n- Due to this, predictions will be biased towards **Not-Churn** customers.\n- Visualizations will also display this bias!"
"# Introduction: Taxi Fare Prediction\n\nWelcome to another Kaggle challenge. In this contest, the aim is to predict the fare of a taxi ride given the starting time, the starting and ending latitude / longitude, and the number of passengers. This is a __supervised regression__ machine learning task.\n\nIn this notebook, I'll provide you with a solid foundation and leave you with the challenge to better the score. Although the dataset is large, this is an approachable problem and as usual with Kaggle competitions, provides realistic practice for building a machine learning solution. The best way to learn is by doing, so let's work through a complete machine learning problem!\n\nGreat resources for Kaggle competitions are the [discussion forums](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/discussion) and the [kernels](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/kernels) completed by other data scientists. I recommend using, adapting, and building on others' code, especially when you are getting started."
"## Read in 5 million rows and examine data\n\nThroughout this notebook, we will work with only 5 million rows (out of 55 million). The first point for improvement might therefore be to use more data!\n\n* __Potential improvement 1: use more data__\n\nGenerally, performance of a machine learning model increases as the amount of training data increases. However, there might be diminishing returns, and I sample the data here in order to train faster. The data file is randomly sorted by date, so taking the first 5 million rows is a random sample in terms of time. \n\nWhen we read in the data, we tell pandas to treat the `pickup_datetime` as a date. We will also drop the `key` since it is a unique identifier and does not tell us anything about the taxi trip. After reading in the data we'll remove any rows with `nan`  (observations with missing entries). "
"For visualization purposes, I'll create a binned version of the fare. This divides the variable into a number of bins, turning a continuous variable into a discrete, categorical variable."
### Empirical Cumulative Distribution Function Plot\n\nAnother plot for showing the distribution of a single variable is the [empirical cumulative distribution function](https://en.wikipedia.org/wiki/Empirical_distribution_function). This shows the percentile on the y-axis and the variable on the x-axis and gets around some of the issues associated with binning data for histograms or the kernel width of the KDE.\n
"This shows the distribution is heavily right skewed. Most of the fares are below \$20, with a heavy right tail of larger fares.\n\n#### Other Outliers\n\nWe can also remove observations based on outliers in other columns. First we'll make a graph of the passenger counts which seemed to have some suspicious values."
"Based on this graph, we'll remove any passenger counts greater than 6."
Now we can graph the `latitude` and `longitude` columns to see the distribution. We'll just sample 10000 values so the plot doesn't take too long to generate.
"### Rides on Map of NYC\n\nFor a more contextualized representation, we can plot the pickup and dropoff on top of a map of New York. The following code is taken directly from https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration by Kaggle user [breeman](https://www.kaggle.com/breemen). All credit goes to him and please check out the rest of his kernel for more excellent work! \n\nThe map was extracted from OpenStreetMaps (https://www.openstreetmap.org/export#map=12/40.7250/-73.8999)"
\n\n\n\n\n\n4  PREPARE THE DATASET    ⤒\n\nIn this section we prepare the **`tf.data.Datasets`** we will use for training and validation
"4.1 READ TFRECORD FILES - CREATE THE RAW DATASET(S)\n\n---\n\nHere we will leverage **`tf.data.TFRecordDataset`** to read the TFRecord files.\n* The simplest way is to specify a list of filenames (paths) of TFRecord files.\n* It is a subclass of **`tf.data.Dataset`**.\n\nThis newly created raw dataset contains **`tf.train.Example`** messages, and when iterated over it, we get scalar string tensors."
# Setup
# Introduction
"## Motivation\n\nAt a high level, one can observe there is a periodicity to the number of transactions. They vary both over a one day period and potentially over a week.\n\nBelow, we show a histogram of the number of transactions per time interval to display this periodicity."
"Next we use our 'make_day_feature' function to create our new feature. We use the offset parameter to change what we define as the 'start' of the day. \n\nAs we can see, there is not a significant correlation on the day of the week and whether a transaction is fraudulent. This feature on is unlikely to be useful."
Next we create a feature which encodes the (relative) hour of the day: 
"Wow! Clearly the time of day has a strong dependence on whether the transaction is fraudulent and will likely make a good feature for our machine learning models.\n\nPhysically, the higher fraction of fraudulent transactions coincides with when there are a low number of transactions. This likely reflects international fraud: e.g., card details being used by a third-party in a fraudulent way in a country different to where the card was issued.  \n\nGoing forward, this is a feature you can use in your models and it will likely be a good predictor of the target."
\n
"Thus, if we have a model that predicts everything as a background pixels (i.e. y = 0 for any input), we will get an accuracy of **0.5** or **50%**.\n\nThat's a good illustration of the accuracy metric.\n\nWhat if we have less object pixels? \n\n\nIn fact, that's what happens **99% of the time**: the backgournd is the vast majority of the image and the object\nto predict represents only a tiny percentage of the whole (let's say between few percents to 10s of percents).\n\n\nLet's take a more realistic example then and see what happens. "
"Here you can see that we have only few object pixels (in white). \nAround 10 * 20 + 20 * 20 = **600** pixels out of a total of 256 * 256 = **65536**.\n\nThat's **0.90%**, i.e. less than 1%."
"To have a confusion matrix, we need some predictions and some ground truths. As said in the beginning,\nwe will only focus on binary perdictions for now but notice that the concept extends easily to multiclasses.\n\nLet's generate a confusion matrix using [scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py): "
"We can see that we have 4 quadrants: \n    \n    \n- A: 0 vs B: 0\n- A: 1 vs B: 0\n- A: 0 vs B: 1\n- A: 1 vs B: 1\n\n\nIf A are the true labels and B are the predicted labels and if 1 is the positive class (object of interest), then we get the following: \n\n\n- A: 0 vs B: 0 => **true negative** tha will be shortened to TN\n- A: 1 vs B: 0 => **false negative** that will be shortened to FP\n- A: 0 vs B: 1 => **false positive** that will be shortened to FN\n- A: 1 vs B: 1 => **true positive**  that will be shortened to TP\n\nWe can place these labels in the confusion matrix.\n\nNow that we have these 4 numbers (TP, FP, TN, FN) from the **confusion matrix**, we can build some useful metrics. \nLet's start with **precision**."
"\nNext, we define a function to visualize the percentage of missing values in each feature of a DataFrame in the form of a barplot. In this way, depending on the percentage of missing values in each feature, we can decide how to deal with missing values:"
We call ‍‍the __missing_percent_plot__ function on the training data:
"The average age of male customers is lightly higher than female ones (39.8 versus 38.1). Distribution of male age is more uniform than females, where we can observe that the biggest age group is 30-35 years old. Kolgomorov-Smirnov test shows that the differences between these two groups are statistically insignificant."
There are slightly more female customers than male ones (112 vs. 87). Females are 56% of total customers.
There is a negligible correlation between age and annual income of customers for both sex groups.
There are week negative correlations (<0.5) between age and spending score for both sex groups.
There is a negligible correlation between annual income and spending score of customers for both sex groups.
"First let's start by importing the essential libraries to work with dataframes (**pandas**), numeric values (**numpy**) and visualization (**matplotlib.pyplot**)."
Now let's import the csv file with the training dataset. You can download it from [here](https://www.kaggle.com/c/titanic/data).  The explanation of the features (each column from the dataset) is also presented in this link. 
"You may wonder why are we still keeping the **""Name""** column. In fact the name does not seem to have influence, it does not matter if a person is named Owen or William, however this column has the title located after the Surname and the comma (""Mr"", ""Mrs"", ""Miss"", etc.) which can be useful.  \n\nIf we take a look at the table X displayed previously we can see many missing values for the **""Age""** column. Removing these rows with missing values would involve removing 177 rows (which is quite a lot!) and we would have less information to create the model. In some cases, it is acceptable to take the average of the column and replace the null values, nonetheless in this case, it is possible to estimate the age of the person by their title, present in the **""Name""** column.   \n\nTherefore, I will first identify the different titles presented and then average the Age for each title. We can provide this averaged Age found for each title to the people with missing Age values, accordingly to their title in **""Name""**. \n\nAfter using the information in **""Name""** we can drop this column. "
"We can also make feature transformation. For example, we could transform the **""Age""** feature in order to simplify it. We could distinguish the youngsters (age less than 18 years) from the adults.  \n\n"
"# Introduction \n\nThe issue of keeping one's employees happy and satisfied is a perennial and age-old challenge. If an employee you have invested so much time and money leaves for ""greener pastures"",  then this would mean that you would have to spend even more time and money to hire somebody else. In the spirit of Kaggle, let us therefore turn to our predictive modelling capabilities and see if we can predict employee attrition on this synthetically generated IBM dataset. \n\nThis notebook is structured as follows:\n\n 1. **Exploratory Data Analysis** : In this section, we explore the dataset by taking a look at the feature distributions, how correlated one feature is to the other and create some Seaborn and Plotly visualisations\n 2. **Feature Engineering and Categorical Encoding** : Conduct some feature engineering as well as encode all our categorical features into dummy variables\n 3. **Implementing Machine Learning models** : We implement a Random Forest and a Gradient Boosted Model after which we look at feature importances from these respective models"
# 1. Exploratory Data Analysis\n\nLet us load in the dataset via the trusty Pandas package into a dataframe object and have a quick look at the first few rows
"### Correlation of Features\n\nThe next tool in a data explorer's arsenal is that of a correlation matrix. By plotting a correlation matrix, we have a very nice overview of how the features are related to one another. For a Pandas dataframe, we can conveniently use the call **.corr** which by default provides the Pearson Correlation values of the columns pairwise in that dataframe.\n\nIn this correlation plot, I will use the the Plotly library to produce a interactive Pearson correlation matrix via the Heatmap function as follows:"
"**Takeaway from the plots**\n\nFrom the correlation plots, we can see that quite a lot of our columns seem to be poorly correlated with one another. Generally when making a predictive model, it would be preferable to train a model with features that are not too correlated with one another so that we do not need to deal with redundant features. In the case that we have quite a lot of correlated features one could perhaps apply a technique such as Principal Component Analysis (PCA) to reduce the feature space."
### Pairplot Visualisations\n\nNow let us create some Seaborn pairplots and set it against the target variable which is our Attrition column to get a feel for how the various features are distributed vis-a-vis employee attrition
"# 2. Feature Engineering & Categorical Encoding\n\nHaving carried out a brief exploration into the dataset, let us now proceed onto the task of Feature engineering and numerically encoding the categorical values in our dataset. Feature engineering in a nutshell involves creating new features and relationships from the current features that we have. Feature engineering has been quite \n\nTo start off, we shall segregate numerical columns from categorical columns via the use of the dtype method as follows:"
However just by a quick inspection of the counts of the number of 'Yes' and 'No' in the target variable tells us that there is quite a large skew in target as shown
"Therefore we have to keep in mind that there is quite a big imbalance in our target variable. Many statistical techniques have been put forth to treat imbalances in data (oversampling or undersampling). In this notebook, I will use an oversampling technique known as SMOTE to treat this imbalance."
\n## 1. Import the required libraries
\n## 2. Do everything we can to make our results reproducible\n\nThumb rule: **Always set the seed!**
We will read the `monkey_labels.txt` file to extract the information about the labels. We can store this information in a list which then can be converted into a `pandas` dataframe.  
"The labels are `n0, n1, n2, ...`. We will create a mapping of these labels where each class will be represented by an integer starting from 0 to number of classes. We will also create a mapping for the names corresponding to a class. We will be using `Common Name` for the last part"
#  | GRAPH №1
"Сonclusions from the graph №1:\n- Between 1990 and 2000 сhanges are insignificantly\n- Between 2014 and 2020,  military spending around the world did not change  \n- Sharp increase is noted in the period from 2002 to 2012"
In this kernel I speed up feature importance using scikit-learn-intelex on different datasets:
V \n## 2- Import
 \n## 3- Estimator
\n# 1. Import of Libraries
"\n# 2. Data visualization\n\nSpecial thanks and reference: [SIDDHESH PUJARI](https://www.kaggle.com/siddheshpujari) and his [notebook](https://www.kaggle.com/siddheshpujari/forest-cover-eda#Dataset-Info).\n>The dataset includes four wilderness areas located in the Roosevelt National Forest in northern Colorado.\n>The Roosevelt National Forest is a National Forest, located in north central Colorado.\n>Each observation is a 30m x 30m patch.\n>\n>* **Elevation** - Elevation in meters.\n>* **Aspect** - Aspect in degrees azimuth.\n>To study how aspect works , please refer the following website [link](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/how-aspect-works.htm) that explains how it works.\n>\n>* **Slope** - Slope in degrees. To study how the slope works, please refer the following website [link](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/slope.htm).\n>* **Horizontal_Distance_To_Hydrology** - Horz Dist to nearest surface water features.\n>* **Vertical_Distance_To_Hydrology** - Vert Dist to nearest surface water features.\n>* **Horizontal_Distance_To_Roadways** - Horz Dist to nearest roadway.\n>* **Hillshade_9am (0 to 255 index)** - Hillshade index at 9am, summer solstice.\n>To study how aspect works , please refer the following website [link](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/hillshade.htm)\n>\n>* **Hillshade_Noon (0 to 255 index)** - Hillshade index at noon, summer solstice.\n>* **Hillshade_3pm (0 to 255 index)** - Hillshade index at 3pm, summer solstice.\n>* **Horizontal_Distance_To_Fire_Points** - Horz Dist to nearest wildfire ignition points.\n>* **Wilderness_Area** (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation.\n>* **Soil_Type** (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation.\n> Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation.\n>\n> **Seven Types of Forest Cover**:\n>* 1 - Spruce/Fir.\n>* 2 - Lodgepole Pine.\n>* 3 - Ponderosa Pine.\n>* 4 - Cottonwood/Willow.\n>* 5 - Aspen.\n>* 6 - Douglas-fir.\n>* 7 - Krummholz.\n>\n> **Let's read the data first** (I strongly recommend using 'datatable' to for faster data reading):"
> **Now let's plot our target values**:
"> Well **`df_train`** is **extremely class-imbalanced**. The classes 4, 5, 6 and 7 combined form only 1% of all observations.\n>\n> Assumption: If we are to choose folds for training, we are better going with Kfolds.\n>\n> **Next**, let's get 30000 samples and plot it:"
\n# 4. Exploratory Data Analysis\n\n> Let's take a closer look at the distribution of the features:
"> **We have plotted Probability Density Function estimation for each feature. What does it tell us?**\n>* The features are distributed differently;\n>* The data is not perfectly symmetrical. The most of the features either right or left skewed.\n>* Only **Hillshade_3pm** is bell-shaped-like (e.g., Gaussian distribution);\n>* Let's plot the skewness:"
"> Alright, the most of the features are skewed right. \n>\n> Let's see what is the total representation of 'continuous' and 'categorical' features::"
> Let's us take a look at features correlation matrix:
"> If we wish to label the strength of the features association, for absolute values of correlation, 0-0.19 is regarded as very weak (the most of our examples are: -0.20-0.20). **""Elevation""** has a positive correlation with **""Wilderness_Area4""**. **""Wilderness_Area1""** and **""Wilderness_Area3""** are also correlated.\n"
"# Analysis Time!\n\nOk the short inspection at the beginning give us some hints how should we move from here. I'm going to play with the data we have while analysing the data at the same time. With this way I hope we can get the data in better shape while digging deeper into it.\n\nThese inspections are going to be some sort of intuitive but we can create new features depending on our assertions.\n\nWe're going to start with basic correlation table here. I dropped the top part since it's just mirror of the other part below. With this table we can understand some linear relations between different features.\n\n#### Observations:\n- There's strong relation between overall quality of the houses and their sale prices.\n- Again above grade living area seems strong indicator for sale price.\n- Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n- There are some obvious relations we gonna pass like total square feet affecting how many rooms there are or how many cars can fit into a garage vs. garage area etc.\n- Overall condition of the house seems less important on the pricing, it's interesting and worth digging.\n"
- **We're going to merge the datasets here before we start editing it so we don't have to do these operations twice. Let's call it features since it has features only. So our data has 2919 observations and 79 features to begin with...**
"## 3. Preparing the data\n\nBefore we build our model using machine learning algorithms, let's prepare our dataset. We will go through the following stages:\n1. Importing the data\n2. Selecting the feature matrix `X` and the target column `y` (*supervised learning*)\n3. Scaling numeric features\n4. Encoding categorical features\n5. Splitting the whole dataset into training and testing datasets\n\nIn the very beginning don't forget to import the required libraries. =)"
For the purpose of this demonstration we will only use the file `train.csv` that contains the target column.
"Since SGD calculates the gradient only of the random part of the dataset (hence the name ""stochastic""), the accuracy score might decrease after some of the iterations. Nevertheless, the score should be converging. This convergence should always be checked to ensure that the optimum point is reached. On the plot below the model converged after 1500 iterations so this or higher value should be used for the maximum number of iterations. Above we used 2000 iterations so it should suffice.\n\n**Try it yourself:** Scaling of the numeric features helps a lot with the convergence of the SGD classifier. Try to comment the code that does scaling and rerun all the cells. You will see that the convergence doesn't occur. In this case you need to set the parameter `learning_rate='constant'` and manually decrease the parameter `eta0` until the convergence occurs again."
"Now that the convergence is confirmed, let's calculate the confusion matrix of our model. These values were used in Part 1 to explain the problem:\n\n* Correct predictions: 177 (survivors 61, victims 116)\n* Wrong predictions: 46 (survivors 23, victims 23)"
"Let's plot the Precision-Recall curve and find a point with a better precsion. Note that precision, recall and corresponding thresholds are obtained from the training dataset, not the testing dataset, to avoid biased scores."
"The best precision of 1.0 is obtained for thresholds of 0.973211, 0.974678 and 1.0 (see the last 3 rows in the dataframe below). Let's pick up the threshold 0.973211 that corresponds to the highest recall among them. The higher the recall, the more true positives (or survived passengers) are predicted and the more insurances we will sell."
## Introduction \n\nHello! This is my very first Kernel. It is meant to give a grasp of a problem of speech representation. I'd also like to take a look on a features specific to this dataset. \n\nContent:\n* [1. Visualization of the recordings - input features](#visualization)\n   * [1.1. Wave and spectrogram](#waveandspectrogram)\n   * [1.2. MFCC](#mfcc)\n   * [1.3. Sprectrogram in 3d](#3d)\n   * [1.4. Silence removal](#resampl)\n   * [1.5. Resampling - dimensionality reductions](#silenceremoval)\n   * [1.6. Features extraction steps](#featuresextractionsteps)\n* [2. Dataset investigation](#investigations)\n   * [2.1. Number of files](#numberoffiles)\n   * [2.2. Mean spectrograms and fft](#meanspectrogramsandfft)\n   * [2.3. Deeper into recordings](#deeper)\n   * [2.4. Length of recordings](#len)\n   * [2.5. Note on Gaussian Mixtures modeling](#gmms)\n   * [2.6. Frequency components across the words](#components)\n   * [2.7. Anomaly detection](#anomaly)\n* [3. Where to look for the inspiration](#wheretostart)\n\nAll we need is here:
"\n# 1. Visualization \n \n\nThere are two theories of a human hearing - place ( https://en.wikipedia.org/wiki/Place_theory_(hearing) (frequency-based) and temporal (https://en.wikipedia.org/wiki/Temporal_theory_(hearing) )\nIn speech recognition, I see two main tendencies - to input [spectrogram](https://en.wikipedia.org/wiki/Spectrogram) (frequencies), and more sophisticated features MFCC - Mel-Frequency Cepstral Coefficients, PLP. You rarely work with raw, temporal data.\n\nLet's visualize some recordings!\n\n## 1.1. Wave and spectrogram:\n \n\nChoose and read some file:"
"Frequencies are in range (0, 8000) according to [Nyquist theorem](https://en.wikipedia.org/wiki/Nyquist_rate).\n\nLet's plot it:"
"If we use spectrogram as an input features for NN, we have to remember to normalize features. (We need to normalize over all the dataset, here's example just for one, which doesn't give good *mean* and *std*!)"
"## 1.3. Spectrogram in 3d\n \n\nBy the way, times change, and the tools change. Have you ever seen spectrogram in 3d?"
(Don't know how to set axis ranges to proper values yet. I'd also like it to be streched like a classic spectrogram above..)
"We can agree that the entire word can be heard. It is impossible to cut all the files manually and do this basing on the simple plot. But you can use for example *webrtcvad* package to have a good *VAD*.\n\nLet's plot it again, together with guessed alignment of* 'y' 'e' 's'* graphems"
"## 1.5. Resampling - dimensionality reduction\n \n\nAnother way to reduce the dimensionality of our data is to resample recordings.\n\nYou can hear that the recording don't sound very natural, because they are sampled with 16k frequency, and we usually hear much more. However, [the most speech related frequencies are presented in smaller band](https://en.wikipedia.org/wiki/Voice_frequency). That's why you can still understand another person talking to the telephone, where GSM signal is sampled to 8000 Hz.\n\nSummarizing, we could resample our dataset to 8k. We will discard some information that shouldn't be important, and we'll reduce size of the data.\n\nWe have to remember that it can be risky, because this is a competition, and sometimes very small difference in performance wins, so we don't want to lost anything. On the other hand, first experiments can be done much faster with smaller training size.\n\nWe'll need to calculate FFT (Fast Fourier Transform). Definition:\n"
"### When we plot character counts we can see some linear relation there. With increasing number of character counts our target score decreases, interesting..."
"## Word Counts\n\n### Word Counts distribution looks more evenly (not normally!) not sure what it means for now, let's take a closer look with scatterplot..."
"# Yeah... Doesn't look much meaningful to me, there's slight decrease in scores when the word count increses. Well let's check other meta features then..."
## Word Lenghts\n\n### Does length of the words affect readability score? Let's check how's the distribution: It looks like decently spread with little bit of right skew...
### Oh that's interesting... There's strong linear relation between mean word lenghts and readability. Looks like longer/complicated words decreases the score.
## Number of Digits\n\n### We can observe that having lots of digits slightly decreasing readability score...
# Common N-Grams\n\n### Here we check some common words. They're not directly giving us info but they can help us summarizing what kind of texts we have here and understand them.
"Now for a little practice! We want to solve the problem of binary classification of IMDB movie reviews. We have a training set with marked reviews, 12500 reviews marked as good, another 12500 bad. Here, it's not easy to get started with machine learning right away because we don't have the matrix $X$; we need to prepare it. We will use a simple approach: bag of words model. Features of the review will be represented by indicators of the presence of each word from the whole corpus in this review. The corpus is the set of all user reviews. The idea is illustrated by a picture\n\n"
"**To get started, we automatically download the dataset from [here](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) and unarchive it along with the rest of datasets in the data folder. The dataset is briefly described [here](http://ai.stanford.edu/~amaas/data/sentiment/). There are 12.5k of good and bad reviews in the test and training sets.**"
"## INTRODUCTION\n- It’s a Python based scientific computing package targeted at two sets of audiences:\n    - A replacement for NumPy to use the power of GPUs\n    - Deep learning research platform that provides maximum flexibility and speed\n- pros: \n    - Interactively debugging PyTorch. Many users who have used both frameworks would argue that makes pytorch significantly easier to debug and visualize.\n    - Clean support for dynamic graphs\n    - Organizational backing from Facebook\n    - Blend of high level and low level APIs\n- cons:\n    - Much less mature than alternatives\n    - Limited references / resources outside of the official documentation\n- I accept you know neural network basics. If you do not know check my tutorial. Because I will not explain neural network concepts detailed, I only explain how to use pytorch for neural network\n- Neural Network tutorial: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners \n- The most important parts of this tutorial from matrices to ANN. If you learn these parts very well, implementing remaining parts like CNN or RNN will be very easy. \n\n**Content:**\n1. [Basics of Pytorch](#1)\n    - Matrices\n    - Math\n    - Variable\n1. [Linear Regression](#2)\n1. [Logistic Regression](#3)\n1. [Artificial Neural Network (ANN)](#4)\n1. [Concolutional Neural Network (CNN)](#5)\n1. Recurrent Neural Network (RNN)\n    - https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch\n1. Long-Short Term Memory (LSTM)\n    - https://www.kaggle.com/kanncaa1/long-short-term-memory-with-pytorch"
" \n## Basics of Pytorch\n### Matrices\n- In pytorch, matrix(array) is called tensors.\n- 3*3 matrix koy. This is 3x3 tensor.\n- Lets look at array example with numpy that we already know.\n    - We create numpy array with np.numpy() method\n    - Type(): type of the array. In this example it is numpy\n    - np.shape(): shape of the array. Row x Column"
" \n### Linear Regression\n- Detailed linear regression tutorial is in my machine learning tutorial in part ""Regression"". I will not explain it in here detailed.\n- Linear Regression tutorial: https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners\n- y = Ax + B.\n    - A = slope of curve\n    - B = bias (point that intersect y-axis)\n- For example, we have car company. If the car price is low, we sell more car. If the car price is high, we sell less car. This is the fact that we know and we have data set about this fact.\n- The question is that what will be number of car sell if the car price is 100."
- Now this plot is our collected data\n- We have a question that is what will be number of car sell if the car price is 100$\n- In order to solve this question we need to use linear regression.\n- We need to line fit into this data. Aim is fitting line with minimum error.\n- **Steps of Linear Regression**\n    1. create LinearRegression class\n    1. define model from this LinearRegression class\n    1. MSE: Mean squared error\n    1. Optimization (SGD:stochastic gradient descent)\n    1. Backpropagation\n    1. Prediction\n- Lets implement it with Pytorch
"- Number of iteration is 1001.\n- Loss is almost zero that you can see from plot or loss in epoch number 1000.\n- Now we have a trained model.\n- While usign trained model, lets predict car prices."
" \n### Logistic Regression\n- Linear regression is not good at classification.\n- We use logistic regression for classification.\n- linear regression + logistic function(softmax) = logistic regression\n- Check my deep learning tutorial. There is detailed explanation of logistic regression. \n    - https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n- **Steps of Logistic Regression**\n    1. Import Libraries\n    1. Prepare Dataset\n        - We use MNIST dataset.\n        - There are 28*28 images and 10 labels from 0 to 9\n        - Data is not normalized so we divide each image to 255 that is basic normalization for images.\n        - In order to split data, we use train_test_split method from sklearn library\n        - Size of train data is 80% and size of test data is 20%.\n        - Create feature and target tensors. At the next parts we create variable from these tensors. As you remember we need to define variable for accumulation of gradients.\n        - batch_size = batch size means is that for example we have data and it includes 1000 sample. We can train 1000 sample in a same time or we can divide it 10 groups which include 100 sample and train 10 groups in order. Batch size is the group size. For example, I choose batch_size = 100, that means in order to train all data only once we have 336 groups. We train each groups(336) that have batch_size(quota) 100. Finally we train 33600 sample one time.\n        - epoch: 1 epoch means training all samples one time.\n        - In our example: we have 33600 sample to train and we decide our batch_size is 100. Also we decide epoch is 29(accuracy achieves almost highest value when epoch is 29). Data is trained 29 times. Question is that how many iteration do I need? Lets calculate: \n            - training data 1 times = training 33600 sample (because data includes 33600 sample) \n            - But we split our data 336 groups(group_size = batch_size = 100) our data \n            - Therefore, 1 epoch(training data only once) takes 336 iteration\n            - We have 29 epoch, so total iterarion is 9744(that is almost 10000 which I used)\n        - TensorDataset(): Data set wrapping tensors. Each sample is retrieved by indexing tensors along the first dimension.\n        - DataLoader(): It combines dataset and sample. It also provides multi process iterators over the dataset.\n        - Visualize one of the images in dataset\n    1. Create Logistic Regression Model\n        - Same with linear regression.\n        - However as you expect, there should be logistic function in model right?\n        - In pytorch, logistic function is in the loss function where we will use at next parts.\n    1. Instantiate Model\n        - input_dim = 28*28 # size of image px*px\n        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n        - create model\n    1. Instantiate Loss \n        - Cross entropy loss\n        - It calculates loss that is not surprise :)\n        - It also has softmax(logistic function) in it.\n    1. Instantiate Optimizer \n        - SGD Optimizer\n    1. Traning the Model\n    1. Prediction\n- As a result, as you can see from plot, while loss decreasing, accuracy(almost 85%) is increasing and our model is learning(training).    "
"# **Introduction**\n\nThis is an intial Explanatory Data Analysis for the [Mercari Price Suggestion Challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge#description) with matplotlib and Plot.ly - a visualization tool that creates beautiful interactive plots and dashboards.  The competition is hosted by Mercari, the biggest Japanese community-powered shopping app with the main objective to predict an accurate price that Mercari should suggest to its sellers, given the item's information. \n\n***Update***: The abundant amount of food from my family's Thanksgiving dinner has really energized me to continue working on this model. I decided to dive deeper into the NLP analysis and found an amazing tutorial by Ahmed BESBES. The framework below is  based on his source code at: https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html. It provides guidance on pre-processing documents and  machine learning techniques (K-means and LDA) to clustering topics.  So that this kernel will be divided into 2 parts: \n\n1. Explanatory Data Analysis \n2. Text Processing  \n    2.1. Pre-processing: Tokenizing and  tf-idf algorithm  \n    2.2. K-means Clustering  \n    2.3. Latent Dirichlet Allocation (LDA)  \n "
"# **Exploratory Data Analysis**\nOn the first look at the data, besides the unique identifier (item_id), there are 7 variables in this model. This notebook will sequentially go through each of them with a brief statistical summary. \n\n1. **Numerical/Continuous Features**\n    1. price: the item's final bidding price. This will be our reponse / independent variable that we need to predict in the test set\n    2. shipping cost     \n \n1. **Categorical Features**: \n    1. shipping cost: A binary indicator, 1 if shipping fee is paid by seller and 0 if it's paid by buyer\n    2. item_condition_id: The condition of the items provided by the seller\n    1. name: The item's name\n    2. brand_name: The item's producer brand name\n    2. category_name: The item's single or multiple categories that are separated by ""\"" \n    3. item_description: A short description on the item that may include removed words, flagged by [rm]"
### Wordcloud of all comments
"In the wordcloud above, we can see the most common words in the comments. These words include ""wikipedia"", ""page"", and ""article"" among other words. More offensive words like ""f**k"" seem to occur less often, indicating that toxic, insulting comments are seen less frequently than non-toxic comments. "
### English vs. Non-English
"We can see that English comments dominate the training data, with a total of 220636 comments written in English and a mere 2913 comments written in languages other than English. There is a heavy imbalance in the language of comments in the training data."
### Bar chart of non-English languages
"We can see that German, Scots, and Danish are the most common non-English languages featuring in the dataset, with more than 100 comments in each language. Spanish, Persian, and Arabic are not far behind. We can thus conclude that Europe and the middle-east are the most represented regions in the dataset."
### Pie chart of non-English languages
"From the pie chart above, we can once again see that German, Danish, and Scots with more than 15% of the pie belonging to each of these three languages."
### World plot of non-English languages
"From the word plot above, we can see that western Europe and the middle-east are the most represented regions in the dataset. Africa, Asia, and eastern Europe are relatively under-represented."
## 1. 모듈 불러오기 (Import module)\n \n#### 제가 가장 자주 쓰는 모듈들을 불러올 예정입니다.\n#### It will load all the modules I use the most.
## 2. 데이터 불러오기(Read Dataset)
\n\n\n\n1  SETUP    ⤒\n\n---\n
\n\n4.2 TRAINING THE MODEL\n\n[REF]\n\n---
\n\n4.3 VALIDATE AND VISUALIZE\n\n---
\n\n\n\n\n\n\n    5  MODEL INFERENCE    ⤒\n\n\n---\n\n**Only run this when things are done as masks will be deleted**
## 1. Import libraries \n\n[Back to Table of Contents](#0.1)
## 2. Download datasets \n\n[Back to Table of Contents](#0.1)
"![Next Generation Weapon](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/pytorch-logo-flat-300x210.png)\n\n\n---\n10 Minute of Pytorch\n---\n---\n* [Pytorch Introduction](#Pytorch-Introduction)\n    * [PyTorch provides two high-level features](#PyTorch-provides-two-high-level-features)\n* [Environment Configuration](#Environment-Configuration)\n    * [Install pytorch on Windows Snapshop Tutorial](https://www.superdatascience.com/pytorch/)\n* [**Section : 1. Pytorch Basic Foundation**](#Section-1-:-Pytorch-Basic-Foundation)\n    * [Tensor](#Tensor)\n        * [Construct a 5x3 matrix uninitialized](#Construct-a-5x3-matrix-uninitialized)\n        * [Convert to numpy](#Convert-to-numpy)\n        * [Size of tensor](#Size-of-tensor)\n        * [From Numpy to tensor](#From-Numpy-to-tensor)\n    * [Tensor Operation](#Tensor-Operation)\n        * [Random similar to numpy](#Random-similar-to-numpy)\n        * [Construct a matrix filled zeros and of dtype long](#Construct-a-matrix-filled-zeros-and-of-dtype-long)\n        * [Construct a tensor directly from data](#Construct-a-tensor-directly-from-data)\n        * [Create tensor based on existing tensor](#Create-tensor-based-on-existing-tensor)\n        * [Basic Tensor Operation](#Basic-Tensor-Operation)\n    * [Variable](#Variable)\n    * [Activation Function](#Activation-Function)\n        * [Generate Fake Data](#Generate-Fake-Data)\n        * [Popular Activation Function](#Popular-Activation-Function)\n        * [Activation Function plot from data](#Activation-Function-plot-from-data)\n        \n* [**Section : 2. Neural Network**](#Section-:-2.-Neural-Network)\n     * [Linear Regression](#Linear-Regression)\n     * [Relationship Fitting Regression Model](#Relationship-Fitting-Regression-Model)\n     * [Distinguish type classification](#Distinguish-type-classification)\n     * [Easy way to Buid Neural Network](#Easy-way-to-Buid-Neural-Network)\n     * [Save and Reload Model](#Save-and-Reload-Model)\n     * [Train on Batch](#Train-on-batch)\n     * [Optimizers](#Optimizers)\n \n* [**Section : 3. Advance Neural Network**](#Section-:-3.-Advance-Neural-Network)\n    * [CNN](#CNN)\n    * [RNN-Classification](#RNN-Classification)\n    * [RNN-Regression](#RNN-Regression)\n    * [AutoEncoder](#AutoEncoder)\n    * [DQN Reinforcement Learning](#DQN-Reinforcement-Learning)\n    * [A3C Reinforcement Learning](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2) [(Code)](https://github.com/nailo2c/a3c/blob/master/tutorial.ipynb)\n    * [Generative Adversarial Network](#Generative-Adversarial-Network)\n    * [Conditional GAN](#Conditional-GAN)\n    \n    \n    \n\n\n---\nPytorch Introduction\n---\n---\nPyTorch is an open source machine learning library for Python, based on [Torch](https://en.wikipedia.org/wiki/Torch_(machine_learning),used for applications such as natural language processing.It is primarily developed by Facebook's artificial-intelligence research group, and Uber's ""Pyro"" software for probabilistic programming is built on it.\n\n#### PyTorch provides two high-level features:\n\n1. *Tensor computation (like NumPy) with strong GPU acceleration*\n1. *Deep Neural Networks built on a tape-based autodiff system*\n\n---\nEnvironment Configuration\n---\n---\n![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/3-768x368.png)\n\n* [Install pytorch on Windows Snapshop Tutorial](https://www.superdatascience.com/pytorch/)\n"
Section-1 : Pytorch Basic Foundation\n---
"In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization -- including a generalization bound that we prove here -- we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that **SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.**\n\n*Sharpness-Aware Minimization for Efficiently Improving\nGeneralization* : https://arxiv.org/abs/2010.01412\n\n![](https://github.com/davda54/sam/raw/main/img/loss_landscape.png)"
Helper Function\n\nConverts the activation function for the entire network
#### Trends:\n \nAmazon's stock price is showing signs of upper trend yearly. \n Amazon's stock price show upper trend signs during January (December Sales tend to give a boost to Amazon's stock price)\nThere is no weekly trend for stock prices. \n
"Technical Analysis: \nIn this section we will go into basic technical concepts when dealing with stock investing. This are simple theories however, we shouldn't solely rely on these concepts to maximize profits as it is the case with patterns related to moving averages. Before going into this concepts, I will like to show how OHLC and Candlesticks are interpreted.\n\nOpen High Low Close (OHLC): \n\n\nCandleSticks: \n\n\nMoving Averages: \n"
"## 2. Simplifying the Observation Space\n\nLux S2 is fully observable which means you can see everything on the map, the opponents units etc. However, this is very high dimensional and not necessarily easy to learn from due to the curse of dimensionality (again!). We want to simplify this observation space in a way that contains sufficient information to learn a good policy but is also easy to learn from.\n\nFor this tutorial, we will create a state-based observation space (no image like features e.g. the rubble, ice, ore maps) with some feature engineering that includes useful information such as the distance to the closest factory and ice tile. The wrapper we provide below will use the `gym.ObservationWrapper` interface. Note that since we are focusing on just controlling one heavy robot, the observation wrapper is written to only support one heavy robot (and returns 0 if there are none).\n\n\nMore advanced solutions can look into using the full set of observations and designing the appropriate neural net architecture to process them. One idea would be to use convolutional neural networks to process board features like images. See [Season 1's solution by ToadBrigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) and our previous [research paper: Emergent Collective Intelligence from Massive-Agent Cooperation and Competition](https://arxiv.org/abs/2301.01609) for example architectures and feature engineering choices.\n"
"## 3. Transforming Lux S2 into a Single Phase\n\nNormally RL frameworks like Stable Baselines 3, RLlib, Tianshou etc. expect the action space and observation space to be consistent throughout an episode. Lux S2 does not conform to this as we add some additional complexity like bidding and factory placement phases. A simple way to get around this is to **upgrade the reset function.**\n\nPreviously we saw that `env.reset()` resets an environment to a clean slate. We will upgrade this function by building a environment wrapper that not only resets to the clean slate, but also handles the bidding and factory placement phases so effectively agents that are learning start from game states with factories already placed.\n\nBelow will build a wrapper that works with the SB3 package. To do this, we want to provide the wrapper a bidding policy and factory placement policy which will be used by all teams to handle the first two phases in the reset function. The code below does just that by overriding the environment's reset function in the wrapper. \n\nFurthermore, we want to use the Controller we defined earlier, so that is also an argument to the SB3Wrapper and we use it to transform actions inside the `env.step` function"
"### Defining a Bid and Factory Placement policy\n\nTo test the code above, we can program some heuristic bid and factory placement policies"
"So **without the wrapper**, when we reset the environment it looks like this:"
"**With the wrapper**, when we reset the environment it looks like this:"
"Success! Our upgraded reset function makes the environment now start from the start of the normal game phase, meaning the action space can be consistently the same throughout the game."
"## 3. Training with RL\n\nIn the previous tutorial, we saw how to train an agent with SB3 in single-agent environments. Handling true multi-agent via training separate or shared policies to control all agents requires a few extra things so instead, for the purpose of a tutorial we will treat Lux S2 like a single agent environment by training a policy for one team and letting the other team simply do nothing.\n\nMoreover, we want to define our own reward function to encourage our robots to seek ice, dig it, and return to a factory so it can generate water and survive longer. To do this all, we will just create a custom environment wrapper.\n\n\n"
### 3.1 Defining the Environment and using Wrappers
# Import libraries 📚
"\n\nI will be integrating W&B for visualizations and logging artifacts!\n\n> [Kaggle ML & DS Survey W&B Dashboard](https://wandb.ai/ruchi798/kaggle-survey?workspace=user-ruchi798)🏋️‍♀️\n>\n> - To get the API key, an account is to be created on the [website](https://wandb.ai/home) first.\n> - Next, use secrets to use API Keys more securely 🤫"
\n\nLibraries
\n\nGlobal Config
## **Combining Image with Mask** 
---
## **Organ** 
## **Data Source** 
"\n## Python Libraries\n* In this section, we import used libraries during this kernel."
"\n## Data Content\n1. **ID number**\n1. **Diagnosis (M = malignant, B = benign)**\n1. **radius (mean of distances from center to points on the perimeter)**\n1. **texture (standard deviation of gray-scale values)**\n1. **perimeter**\n1. **area**\n1. **smoothness (local variation in radius lengths)**\n1. **compactness (perimeter^2 / area - 1.0)**\n1. **concavity (severity of concave portions of the contour)**\n1. **concave points (number of concave portions of the contour)**\n1. **symmetry**\n1. **fractal dimension (""coastline approximation"" - 1)**\n\n* The mean, standard error and ""worst"" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n* All feature values are recoded with four significant digits.\n* Missing attribute values: none\n* Class distribution: 357 benign, 212 malignant"
Before violin and swarm plot we need to normalization or standirdization. Because differences between values of features are very high to observe on plot. I plot features in 3 group and each group includes 10 features to observe better.
"Lets interpret the plot above together. For example, in **texture_mean** feature, median of the *Malignant* and *Benign* looks like separated so it can be good for classification. However, in **fractal_dimension_mean** feature,  median of the *Malignant* and *Benign* does not looks like separated so it does not gives good information for classification."
"In order to compare two features deeper, lets use joint plot. Look at this in joint plot below, it is really correlated.\n Pearsonr value is correlation value and 1 is the highest. Therefore, 0.86 is looks enough to say that they are correlated. \nDo not forget, we are not choosing features yet, we are just looking to have an idea about them."
"What about three or more feauture comparision ? For this purpose we can use pair grid plot. Also it seems very cool :)\nAnd we discover one more thing **radius_worst**, **perimeter_worst** and **area_worst** are correlated as it can be seen pair grid plot. We definetely use these discoveries for feature selection."
"Up to this point, we make some comments and discoveries on data already. If you like what we did, I am sure swarm plot will open the pub's door :) "
Check the distribution of features below to try to fill not so big NULL valued columns
"Let's fill the Embarked column with more frequently value ""S"".   \nThe column Fare fill with a mean value"
# 2. | Importing Libraries 📚\n\n    👉 Installing and importing libraries that will be used in this notebook.\n
"# 4. | Reading Dataset 👓\n\n    👉 After importing libraries, the datasets that will be used will be imported.\n"
## 6.1 | Total Survival based on Gender and Title 🚻🎓\n
"\n    👉 The majority of Titanic passengers were males. In more detail, the titanic male passengers are dominated by men over 18. Meanwhile, titanic female passengers are dominated by females aged less than 18 years.\n    👉 From the plot above, it is clear that most Titanic survivor are dominated by female passengers.\n    👉 Men over 18 years old (Mr.) had more survivors than men under 18. The opposite happened for women, where the number of female survivors under 18 years old (Miss) has more digits than women over 18 years old.\n    👉 Two males and one female out of 7 passengers with ""Dr."" title survived the events of the Titanic.\n    👉 None of the men who have the title ""Rev"" survived 😢.\n\n"
## 6.2 | Cabin and Class Survival Distributions 🪙🛳️😇\n
"\n    👉 From the chart, majority of passengers have an unidentified cabin code.\n    👉 Passengers in the middle cabin (B, C, D, and E) had more survivors than passengers in other cabins.\n\n"
## 6.4 | Embarked Survival Rate and Fare Distributions ⚓💰\n
## 6.4 | Passengers Family Sizes Distributions 👨‍👩‍👧‍👦\n
## 6.5 | Age Distribution based on Survival 🧓😇\n
"\n    👉 From the chart, it can be seen that not survived distribution is leptokurtic, while other is platikurtic. Furthermore, there are outliers on the right side of both plots.\n\n"
"# CommonLit Readability 📝 A complete Analysis\n\n![nlp-header.png](attachment:4a916c95-e05d-4636-98fa-3e82cb1066ce.png)\n\n**Natural Language Processing or NLP** is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way.\n\n**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include Positive, Neutral, and Negative, Review Ratings and Happy, Sad. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject.\n\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence."
\n# 1. Loading Data 💎\n\nJust load the dataset and global variables for colors and so on.
"\n## 2.1 Missing values\n\nAs we can see, the only missing values are in: `url_legal` and `license`. For now, we are going to do an analysis based on the `excerpt` text so we can go ahead."
\n## 2.2 Target and Std_err Distributions 📸\n
"# 3. Visualize data\nOne of the biggest parts of the notebook. Here we can look through some variables and see some dependencies. Firstly, let's check the **dependency of the oil from the date**:"
"As we have so much rows in out dataset, it will be easier to group data, as example, by week or month. The aggregation will be made by **mean**."
"# 3.1. Linear Regression\nAfter that, we can build some more plots. **Linear regression** is widely used in practice and adapts naturally to even complex forecasting tasks. The linear regression algorithm learns how to make a weighted sum from its input features."
"# 3.2 Lag feature\nTo make a lag feature we shift the observations of the target series so that they appear to have occured later in time. Here we've created a 1-step lag feature, though shifting by multiple steps is possible too. So, firstly, we should **add lag** to our data:"
"### 📌 Notes:\n\n> - `cp` {Chest Pain} : People with cp equl to 1, 2, 3 are more likely to have heart disease than people with cp equal to 0.\n> - `restecg` {resting electrocardiographic results} : People with value 1 (signals non-normal heart beat, can range from mild symptoms to severe problems) are more likely to have heart disease.\n> - `exang` {exercise induced angina} : People with value 0 (No ==> exercice induced angina) have heart disease more than people with value 1 (Yes ==> exercice induced angina)\n> - `slope` {the slope of the peak exercise ST segment} : People with slope value equal to 2 (Downslopins: signs of unhealthy heart) are more likely to have heart disease than people with slope value equal to 0 (Upsloping: better heart rate with excercise) or 1 (Flatsloping: minimal change (typical healthy heart)).\n> - `ca` {number of major vessels (0-3) colored by flourosopy} : the more blood movement the better so people with ca equal to 0 are more likely to have heart disease.\n> - `thal` {thalium stress result} : People with thal value equal to 2 (fixed defect: used to be defect but ok now) are more likely to have heart disease."
### 📌 Notes:\n> - `trestbps` : resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n> - `chol` {serum cholestoral in mg/dl} : above 200 is cause for concern.\n> - `thalach` {maximum heart rate achieved} : People how acheived a maximum more than 140 are more likely to have heart disease.\n> - `oldpeak` ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more
# Importing Libraries
"## Data Preparation\n* As we are working with four different datasets, so i will be creating a dataframe storing all emotions of the data in dataframe with their paths.\n* We will use this dataframe to extract features for our model training."
First let's plot the count of each emotions in our dataset.
We can also plot waveplots and spectograms for audio signals\n\n* Waveplots - Waveplots let us know the loudness of the audio at a given time.\n* Spectograms - A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. It’s a representation of frequencies changing with respect to time for given audio/music signals.
#### 1. Simple Audio
#### 2. Noise Injection
We can see noise injection is a very good augmentation technique because of which we can assure our training model is not overfitted
#### 3. Stretching
#### 4. Shifting
# 1.2 - Null Values\n\nLet's explore the issue of missing values in the dataset to see if there are systemic problems with data representation.
The good news is that we don't have any null values when it comes to CSF observations with the peptide and protein data. We do however see null values when it comes to the clinical data. Let's see what we are missing and why. We'll start with the clinical data.
"We are starting to see some interesting trends. Let's break this down by null counts.\n\n### Rows with 1 Null Value\n\nWhen there is a single null value in the row, it usually corresponds to the feature `upd23b_clinical_state_on_medication`. Valid responses are one of `On` or `Off`. Null values within the field are of interest, as it is uncertain whether they indicate that the patient was `Off` of medication, or if the assessment failed to capture the medication status of the patient. In the other two instances of null value counts, they occur 7 times in `updrs_3` and 21 times in `updrs_4`. According to Goetz et al (2008), part 3 of the UPDRS assessment concerns motor assessment, and has a minimum score of 0. Part 4 of the UPDRS assessment concerns motor complications, and again has a minimum score of 0. Null values in either of those columns suggest that the assessment was not performed. This is important, as a score of 0 indicates that the patient was assessed and was deemed to have normal responses. \n\n### Rows with 2 Null Values\n\nWhen there are two null values in the row, they usually correspond to `updrs_4` and `upd23b_clinical_state_on_medication`. As mentioned previously, valid responses are `On` or `Off`, thus a null value here is of interest as we cannot be certain whether the assessment failed to capture medication status. The majority of the other null value fields occur with `updrs_4`, which concerns motor complications. Other null values occur infrequently in the `updrs_3`, and `updrs_2` fields. Again, UPDRS part 3 concerns motor assessment, and null values here cannot assumed to be 0 scores, given that 0 indicates normal function. With UPDRS part 2, the assessment concerns motor experiences of daily living, and again, null values here may indicate that the assessment was not performed. \n\n### Rows with 3 Null Values\n\nThere are 10 instances where rows contain 3 null values. In each instance, the row is missing information from `updrs_3`, `updrs_4`, and `upd23b_clinical_state_on_medication`. Again missing values cannot be assumed to be 0.\n\n### Rows with 4 Null Values\n\nOnly a single instance of a row with 4 null values occurs. It appears that only the UPDRS part 3 assessment was performed at the visit. Again, null values cannot be interpreted as being 0, given that 0 based scores indicate a normal response. "
We need to also look at the supplemental information for null values.
"Again, similar patterns emerge. When there is a single null value in a row, it either appears in `updrs_4` and `upd23b_clinical_state_on_medication`. The same trend continues when there are two null values. When there are three null values, the trend differs slightly from the clinical data. In this case, the supplemental data is more likely to be missing `updrs_1` and `updrs_2` when compared to the clinical data. Finally, when there are four missing values, we see the same missing values in `updrs_1`, `updrs_2`, `updrs_4` and `upd23b_clinical_state_on_medication`. There is a significantly higher number of 4 null value rows in the supplemental data than in the clinical data.\n\n### Key Observations About Null Values\n\n* No null values are missing from peptide and protein data.\n* Null values occur in the clinical data and supplemental data. General trends are:\n    * Single null values occur most frequently in the `updrs_4` and `upd23b_clinical_state_on_medication` features.\n    * Two null values occur most frequently in the `updrs_4` and `upd23b_clinical_state_on_medication` features.\n    * Three null values occur most frequently in the:\n        * `updrs_3`, `updrs_4`, and `upd23b_clinical_state_on_medication` features for the clinical data.\n        * `updrs_1`, `updrs_2`, and `upd23b_clinical_state_on_medication` features for the supplemental data.\n    * Four null values occur most frequently in the `updrs_1`, `updrs_2`, `updrs_4`, and `upd23b_clinical_state_on_medication` features.\n* The supplemental data has many more examples of rows with four null values when compared to the clinical data.\n* Care must be taken when dealing with null values:\n    * It is not apparent whether the null values should be used to indicate a missed assessment, or whether they can be set to another value.\n        * For UPDRS assessments, it may be erroneous to set the value to 0 as that would indicate a ""normal"" result.\n        * For the `upd23b_clinical_state_on_medication` feature, the only valid settings are `On` or `Off`, thus the impact of null is undefined."
"# 1.3 - Duplicated Rows\n\nWe should next check to see if we have any duplicated values in our various datasets. Duplicates may impact our learning methods, resulting in prediction bias toward the duplicate information. "
"### Key Observations About Duplicated Rows\n\n* In terms of raw duplicates:\n    * In the clinical data:\n        * There are 2 instances where the same row of data appears twice.\n        * Duplicates account for 0.15% of all clinical data.\n    * In the supplemental data:\n        * There are 7 instances where the same row of data appears twice.\n        * Duplicates account for 0.63% of all supplemental data.\n    * In the protein data:\n        * There are 400 instances where the same row of data appears twice.\n        * Duplicates account for 0.35% of all protein data.\n    * In the peptide data:\n        * There are 1,765 instances where the same row of data appears twice.\n        * There are 2 instances where the same row of data appears 3 times.\n        * Duplicates account for 0.36% of all peptide data.\n* Overall, with the clinical and supplemental data, duplicates are likely to have little or no impact."
"# 1.4 - Statistical Breakdown\n\nLet's take a closer look at some of the statistical properties of the continuous features. \n\n## 1.4.1 - Clinical vs Supplemental Data\n\nTo begin, let's compare the clinical data to the supplemental data to see what kind of differences we have.\n\n### Clinical Data"
### Supplemental Data
"Supplemental data appears to have visits that occur mainly between 0 and 36 months, while clinical data shows visits occurring between 0 and 108 months. We can confirm this by looking at kernel density estimates for the months of the various visits."
"As we can see, the supplemental data is focused around 0 month visits, and ends at 36 months, while the clinical data spans a much longer time-frame. We can also do a quick visual check to see if there are differences between the clinical and supplemental data when it comes to UPDRS scores. For the figures below, the trend lines are kernel density estimates, thus differences in raw counts are taken into consideration with the trend lines."
There are a few interesting observations:\n\n* UPDRS Part 1 and 4 scores appear to be fairly similar in their distribution between the clinical and supplemental data sources.\n* UPDRS Part 2 and 3 scores have a much higher proportion of 0 based scores in the clinical data when compared to the supplemental data source.
**Preprocessing**
"There seems to be 2 extreme outliers on the bottom right, really large houses that sold for really cheap. More generally, the author of the dataset recommends removing 'any houses with more than 4000 square feet' from the dataset.  \nReference : https://ww2.amstat.org/publications/jse/v19n3/decock.pdf"
**1* Linear Regression without regularization**
"RMSE on Training set shows up weird here (not when I run it on my computer) for some reason.  \nErrors seem randomly distributed and randomly scattered around the centerline, so there is that at least. It means our model was able to capture most of the explanatory information."
"**2* Linear Regression with Ridge regularization (L2 penalty)**\n\nFrom the *Python Machine Learning* book by Sebastian Raschka :  Regularization is a very useful method to handle collinearity, filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights.  \n\nRidge regression is an L2 penalized model where we simply add the squared sum of the weights to our cost function."
"We're getting a much better RMSE result now that we've added regularization. The very small difference between training and test results indicate that we eliminated most of the overfitting. Visually, the graphs seem to confirm that idea.  \n\nRidge used almost all of the existing features."
"**3* Linear Regression with Lasso regularization (L1 penalty)**\n\nLASSO stands for *Least Absolute Shrinkage and Selection Operator*. It is an alternative regularization method, where we simply replace the square of the weights by the sum of the absolute value of the weights. In contrast to L2 regularization, L1 regularization yields sparse feature vectors : most feature weights will be zero. Sparsity can be useful in practice if we have a high dimensional dataset with many features that are irrelevant.  \n\nWe can suspect that it should be more efficient than Ridge here."
"RMSE results are better both on training and test sets. The most interesting thing is that Lasso used only one third of the available features. Another interesting tidbit : it seems to give big weights to Neighborhood categories, both in positive and negative ways. Intuitively it makes sense, house prices change a whole lot from one neighborhood to another in the same city.  \n\nThe ""MSZoning_C (all)"" feature seems to have a disproportionate impact compared to the others. It is defined as *general zoning classification : commercial*. It seems a bit weird to me that having your house in a mostly commercial zone would be such a terrible thing."
\n\n### 2| IMPORT NECESSARY LIBRARIES
\n\n### 3| LOAD DATASETS
# Importing all the libraries needed
"# Context \n\n\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help"
## Correlation Matrix\n### Its necessary to remove correlated variables to improve your model.One can find correlations using pandas “.corr()” function and can visualize the correlation matrix using plotly express.\n- Lighter shades represents positive correlation\n- Darker shades represents negative correlation
"Here we can see Heart Disease has a high negative correlation with ""MaxHR"" and somewhat negative correlation wiht ""Cholesterol"", where as here positive correatlation with ""Oldpeak"",""FastingBS"" and ""RestingBP"""
"To plot multiple pairwise bivariate distributions in a dataset, you can use the pairplot() function. This shows the relationship for (n, 2) combination of variable in a DataFrame as a matrix of plots and the diagonal plots are the univariate plots."
### Now to check the linearity of the variables it is a good practice to plot distribution graph and look for skewness of features. Kernel density estimate (kde) is a quite useful tool for plotting the shape of a distribution.
# Outliers\n\nA box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables.The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution.The box plot (a.k.a. box and whisker diagram) is a standardized way of displaying the distribution of data based on the five number summary:\n- Minimum\n- First quartile\n- Median\n- Third quartile\n- Maximum.\n\nIn the simplest box plot the central rectangle spans the first quartile to the third quartile (the interquartile range or IQR).A segment inside the rectangle shows the median and “whiskers” above and below the box show the locations of the minimum and maximum.
# Import Libraries\nImport the usual libraries for pandas and plotting. You can import sklearn later on.
# Get the Data\n\n**Use pandas to read loan_data.csv as a dataframe called loans.**
"# Exploratory Data Analysis\n\n* **Let's do some data visualization!**\n\n* **We'll use seaborn and pandas built-in plotting capabilities, but feel free to use whatever library you want.** \n\n\n* **Create a histogram of two FICO distributions on top of each other, one for each credit.policy outcome.**"
"\n**Create a similar figure, except this time select by the not.fully.paid column.**"
"**Create a countplot using seaborn showing the counts of loans by purpose, with the color hue defined by not.fully.paid.**"
* **Let's see the trend between FICO score and interest rate.** \n* **Recreate the following jointplot.**
**Create the following lmplots to see if the trend differed between not.fully.paid and credit.policy.** 
# Setting up the Data\n\n**Let's get ready to set up our data for our Random Forest Classification Model!**\n\n**Check loans.info() again.**
\n# Imports:
\n# Read the data:
## Importing libraries
## Exploratory Data Analysis
Plotting the same graph but with ratio instead.
The Sex variable seems to be a discriminative feature. Women are more likely to survive.
"Now, visualizing survival based on the fare."
"Passengers with cheaper ticket fares are more likely to die. Put differently, passengers with more expensive tickets, and therefore a more important social status, seem to be rescued first."
\n## Table of Contents\n* [Introduction](#top_section)\n    - [Well... What do we have here?](#section1)\n* [Exploring the Data](#section2)\n    - [Categorical Features](#section3)\n    - [Numerical Features](#section4)\n    - [Missing Values](#section5)    \n* [Building the Feature Engineering Machine](#section6)\n    - [Data Merger](#section7)\n    - [Family Assembler](#section8)\n    - [Family Survival Detector](#section9)   \n    - [Title Extractor](#section10)\n    - [Title Encoder](#section11)\n    - [Age Filler](#section12)\n    - [Age Grouper](#section13)\n    - [Fare Imputer](#section14)\n    - [Fare Encoder](#section15)\n    - [Scaler](#section16)\n    - [Embarked Processor](#section17)\n    - [Deck Finder](#section18)\n    - [Gender Mapper](#section19)\n    - [Pclass Sorter](#section20)\n    - [Ticket Cleaner](#section21)\n    - [Housekeeping](#section22)\n    - [Feeding the Machine](#section23)\n* [Double Check](#section24)\n    - [Correlation Matrix](#section25)\n* [Modelling](#section26)\n    - [Model Selection](#section27)\n    - [Cross-Validate Models](#section28)\n    - [Model Results](#section29)\n    - [ROC'S of the Models](#section30)\n    - [Learning Curves of the Models](#section31)\n* [Feature Selection](#section31.1)\n    - [Feature Importances](#section32)\n    - [Decision Trees](#section33)    \n    - [Feature Selection by Recursive Feature Elimination](#section34)\n    - [Dimension Reduction by Principal Component Analysis](#section35)\n    - [Reduced Dimension Model Results with Cross-Validation](#sectioncv)\n* [Plotting Decision Boundaries](#section36)\n* [Plotting Decision Regions](#section37)\n* [Submission & Some Last Words](#sectionlst)\n\n\n
"\n# Well... What do we have here?\n\nOk we have two sets(train and test) data and in total 1301 observations 11 features. Our target is Survived column which is not present on the test set(duh!)... With basic inspection I'd say 'PassengerId' has effect on survival, but... Maybe we can use it on feature engineering part? For the rest we gonna inspect them individually soon but  generally speaking they look mostly categorical data with some continuous values like Fare and Age.\n\nI'm going to use different sets of variables for visualizing and feature engineering so let's start with assigning visualizing ones. I'm also saving idx variable for future use.\n\n### [Back To Table of Contents](#toc_section)\n"
"\n## Missing Values\n\nOn both datasets Cabin feature is missing a lot, it looks this feature not useful for modelling but we might give it a chance with feature engineering later.\n\nAgain, Age feature has many missing values, we can impute them with some logical way to use later...\n\nThere are little number of missing values on Embarked and Fare, I think we can impute them without taking much risk.\n\n### [Back To Table of Contents](#toc_section)"
"\n# Building the Feature Engineering Machine\n\nHere comes to part where we are going to play with the data we have. I wanted to build functions instead of moving one by one, I feel like with this way I can change my build up for the model more easily. Since there are some changes might get better results but there are some changes might badly effect outcome of the model by increasing complexity. This is a trial and error process so with this approach I feel like I can control them easily.\n\nI called this part machine but if you are more traditional you can say this is our toolbox too :)\n\n### [Back To Table of Contents](#toc_section)"
# Topic 2. Visual data analysis in Python\n## Part 1. Visualization: from Simple Distributions to Dimensionality Reduction
## Article outline\n\n1. [Dataset](#1.-Dataset)\n2. [Univariate visualization](#2.-Univariate-visualization)\n    * 2.1 [Quantitative features](#2.1-Quantitative-features)\n    * 2.2 [Categorical and binary features](#2.2-Categorical-and-binary-features)\n3. [Multivariate visualization](#3.-Multivariate-visualization)\n    * 3.1 [Quantitative vs. Quantitative](#3.1-Quantitative-vs.-Quantitative)\n    * 3.2 [Quantitative vs. Categorical](#3.2-Quantitative-vs.-Categorical)\n    * 3.3 [Categorical vs. Categorical](#3.3-Categorical-vs.-Categorical)\n4. [Whole dataset visualizations](#4.-Whole-dataset-visualizations)\n    * 4.1 [Naive approach](#4.1-A-naive-approach)\n    * 4.2 [Dimensionality reduction](#4.2-Dimensionality-reduction)\n    * 4.3 [t-SNE](#4.3-t-SNE)\n5. [Assignments](#5.-Assignments)\n6. [Useful resources](#6.-Useful-resources)
"## 1. Dataset\n\nBefore we get to the data, let's initialize our environment:"
"In the first article, we looked at the data on customer churn for a telecom operator. We will reload the same dataset into a `DataFrame`:"
"## 2. Univariate visualization\n\n*Univariate* analysis looks at one feature at a time. When we analyze a feature independently, we are usually mostly interested in the *distribution of its values* and ignore other features in the dataset.\n\nBelow, we will consider different statistical types of features and the corresponding tools for their individual visual analysis.\n\n#### 2.1 Quantitative features\n\n*Quantitative features* take on ordered numerical values. Those values can be *discrete*, like integers, or *continuous*, like real numbers, and usually express a count or a measurement.\n\n##### Histograms and density plots\n\nThe easiest way to take a look at the distribution of a numerical variable is to plot its *histogram* using the `DataFrame`'s method [`hist()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html)."
"A histogram groups values into *bins* of equal value range. The shape of the histogram may contain clues about the underlying distribution type: Gaussian, exponential, etc. You can also spot any skewness in its shape when the distribution is nearly regular but has some anomalies. Knowing the distribution of the feature values becomes important when you use Machine Learning methods that assume a particular type (most often Gaussian).\n\nIn the above plot, we see that the variable *Total day minutes* is normally distributed, while *Total intl calls* is prominently skewed right (its tail is longer on the right).\n\nThere is also another, often clearer, way to grasp the distribution: *density plots* or, more formally, *Kernel Density Plots*. They can be considered a [smoothed](https://en.wikipedia.org/wiki/Kernel_smoother) version of the histogram. Their main advantage over the latter is that they do not depend on the size of the bins. Let's create density plots for the same two variables:"
"It is also possible to plot a distribution of observations with `seaborn`'s [`distplot()`](https://seaborn.pydata.org/generated/seaborn.distplot.html). For example, let's look at the distribution of *Total day minutes*. By default, the plot displays both the histogram with the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) on top."
The height of the histogram bars here is normed and shows the density rather than the number of examples in each bin.\n\n##### Box plot\n\nAnother useful type of visualization is a *box plot*. `seaborn` does a great job here:
"Let's see how to interpret a box plot. Its components are a *box* (obviously, this is why it is called a *box plot*), the so-called *whiskers*, and a number of individual points (*outliers*).\n\nThe box by itself illustrates the interquartile spread of the distribution; its length is determined by the $25th \, (\text{Q1})$ and $75th \, (\text{Q3})$ percentiles. The vertical line inside the box marks the median ($50\%$) of the distribution. \n\nThe whiskers are the lines extending from the box. They represent the entire scatter of data points, specifically the points that fall within the interval $(\text{Q1} - 1.5 \cdot \text{IQR}, \text{Q3} + 1.5 \cdot \text{IQR})$, where $\text{IQR} = \text{Q3} - \text{Q1}$ is the [interquartile range](https://en.wikipedia.org/wiki/Interquartile_range).\n\nOutliers that fall outside of the range bounded by the whiskers are plotted individually as black points along the central axis.\n\nWe can see that a large number of international calls is quite rare in our data.\n\n##### Violin plot\n\nThe last type of distribution plots that we will consider is a *violin plot*.\n\nLook at the figures below. On the left, we see the already familiar box plot. To the right, there is a *violin plot* with the kernel density estimate on both sides."
"The difference between the box and violin plots is that the former illustrates certain statistics concerning individual examples in a dataset while the violin plot concentrates more on the smoothed distribution as a whole.\n\nIn our case, the violin plot does not contribute any additional information about the data as everything is clear from the box plot alone.\n\n##### describe()\n\nIn addition to graphical tools, in order to get the exact numerical statistics of the distribution, we can use the method [`describe()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html) of a `DataFrame`:"
##### Bar plot\n\nThe bar plot is a graphical representation of the frequency table. The easiest way to create it is to use the `seaborn`'s function [`countplot()`](https://seaborn.pydata.org/generated/seaborn.countplot.html). There is another function in `seaborn` that is somewhat confusingly called [`barplot()`](https://seaborn.pydata.org/generated/seaborn.barplot.html) and is mostly used for representation of some basic statistics of a numerical variable grouped by a categorical feature.\n\nLet's plot the distributions for two categorical variables:
"While the histograms, discussed above, and bar plots may look similar, there are several differences between them:\n1. *Histograms* are best suited for looking at the distribution of numerical variables while *bar plots* are used for categorical features.\n2. The values on the X-axis in the *histogram* are numerical; a *bar plot* can have any type of values on the X-axis: numbers, strings, booleans.\n3. The *histogram*'s X-axis is a *Cartesian coordinate axis* along which values cannot be changed; the ordering of the *bars* is not predefined. Still, it is useful to note that the bars are often sorted by height, that is, the frequency of the values. Also, when we consider *ordinal* variables (like *Customer service calls* in our data), the bars are usually ordered by variable value.\n\nThe left chart above vividly illustrates the imbalance in our target variable. The bar plot for *Customer service calls* on the right gives a hint that the majority of customers resolve their problems in maximum 2-3 calls. But, as we want to be able to predict the minority class, we may be more interested in how the fewer dissatisfied customers behave. It may well be that the tail of that bar plot contains most of our churn. These are just hypotheses for now, so let's move on to some more interesting and powerful visual techniques."
"## 3. Multivariate visualization\n\n*Multivariate* plots allow us to see relationships between two and more different variables, all in one figure. Just as in the case of univariate plots, the specific type of visualization will depend on the types of the variables being analyzed.\n\n#### 3.1 Quantitative vs. Quantitative\n\n##### Correlation matrix\n\nLet's look at the correlations among the numerical variables in our dataset. This information is important to know as there are Machine Learning algorithms (for example, linear and logistic regression) that do not handle highly correlated input variables well.\n\nFirst, we will use the method [`corr()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) on a `DataFrame` that calculates the correlation between each pair of features. Then, we pass the resulting *correlation matrix* to [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) from `seaborn`, which renders a color-coded matrix for the provided values:"
"From the colored correlation matrix generated above, we can see that there are 4 variables such as *Total day charge* that have been calculated directly from the number of minutes spent on phone calls (*Total day minutes*). These are called *dependent* variables and can therefore be left out since they do not contribute any additional information. Let's get rid of them:"
# dist1 & dist2\nPlotting with logx to better show the distribution. Possibly this could be the distance of the transaction vs. the card owner's home/work address. This is just a guess.
"# C1 - C14\nBecause we are provided many numerical columns, we can create a pairplot to plot feature interactions. I know these plots can be hard to read, but it is helpful for gaining intution about potential feature interactions and if certain features have more variance than others."
"We will plot the Recency Distribution and QQ-plot to identify substantive departures from normality, likes outliers, skewness and kurtosis."
"From the first graph above we can see that sales recency distribution is ***skewed***, has a **peak** on the left and a long tail to the right. It **deviates from normal distribution** and is **positively biased**.\n\nFrom the **Probability Plot**, we could see that **sales recency** also does **not align with the diagonal  red line** which represent normal distribution. The form of its distribution confirm that is a skewed right. \n\nWith ***skewness positive of 1.25***, we confirm the **lack of symmetry** and indicate that sales recency  are **skewed right**, as we can see too at the Sales Distribution plot, skewed right means that the right tail is **long relative to the left tail**. The skewness for a normal distribution is zero, and any symmetric data should have a skewness near zero. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n\n**Kurtosis** is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers, and **positive** kurtosis indicates a **heavy-tailed distribution** and **negative** kurtosis indicates a **light tailed distribution**. So, with 0.43 of positive kurtosis **sales recency** are heavy-tailed and has some **outliers**."
"## Visualization utilities\ndata -> pixels, nothing of much interest for the machine learning practitioner in this section."
# Datasets
"The data has been split into two groups:\n\ntraining set (train.csv)\ntest set (test.csv)\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n\n### Data Dictionary\nVariable	Definition	Key\n\nsurvival	Survival	0 = No, 1 = Yes\n\npclass	Ticket class	1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex	Male or Female\n\nAge	Age in years	\n\nsibsp	# of siblings / spouses aboard the Titanic	\n\nparch	# of parents / children aboard the Titanic	\n\nticket	Ticket number	\n\nfare	Passenger fare	\n\ncabin	Cabin number	\n\nembarked	Port of Embarkation	C = Cherbourg, Q = Queenstown, S = Southampton\n\nVariable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fiancés were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them"
### Identifying Missing Value 
### Target Variable Visualization (AveragePrice) : 
- Distribution of **AveragePrice** that is not resampled is pretty much a **normally distributed** curve. It highlights small double peaks but we will allow it in this case.\n- Distribution of **AveragePrice** of the resampled data displays a much better **normally distribution** curve.\n- We can clearly observe a positive trend in **AveragePrice** w.r.t **Date**. Repetitive 3 peaks at consistent intervals of time can be observed.\n- **AveragePrice** drops around the months of December / January and rises to it's highest value for the months September - November.
### Categorical Features vs Target Variable (AveragePrice) :
"- **AveragePrice** of Conventional(0) avocados is less than those of Organic(1).\n- **AveragePrice** of Avocados is near about the same for the **years** 2015, 2016 and 2018. A rise in **AveragePrice** can be clearly seen for the year 2017."
"- Distributions of the non-resampled data were understandable, hence we visulize the distributions of resampled data.\n- **Total Volume**, **4046** & **4225** kind of display a **normally distributed** curve. Remaining numerical features display either a **Double Peak Distribution** or **Right / Positive Skewed Distribution**."
# Models
## KNN-XGB-SVC Ensemble
\nDescriptive Statistics
📌  It is clearly seems that there are ouliters in Quantity and UnitPrice that have to be handled \n📌  There are negative values in UnitPrice and Quantity because of **cancelled orders**.  \n📌  Missing values in Customer ID and Description. \n📌  Quantity and Unit Price should be multiplied in order to create **Total Price**.               \n
\nSegmentation Map
\nModel Evaluation
# Libraries
# Data
**Target distribution**
"The target is highly balanced, so we luckily don't have to consider techniques like under/over-sampling."
**Continuous features**
"*Notes:*\n* 0-18 year olds were **more** likely to be transported than not.\n* 18-25 year olds were **less** likely to be transported than not.\n* Over 25 year olds were about **equally** likely to be transported than not.\n\n*Insight:*\n* Create a new feature that indicates whether the passanger is a child, adolescent or adult."
"*Notes:*\n* Most people don't spend any money (as we can see on the left).\n* The distribution of spending decays exponentially (as we can see on the right).\n* There are a small number of outliers.\n* People who were transported tended to spend less.\n* RoomService, Spa and VRDeck have different distributions to FoodCourt and ShoppingMall - we can think of this as luxury vs essential amenities. \n\n*Insight:*\n* Create a new feature that tracks the total expenditure across all 5 amenities.\n* Create a binary feature to indicate if the person has not spent anything. (i.e. total expenditure is 0).\n* Take the log transform to reduce skew."
**Categorical features**
*Notes:*\n* VIP does not appear to be a useful feature; the target split is more or less equal. \n* CryoSleep appears the be a very useful feature in contrast.\n\n*Insights:*\n* We might consider dropping the VIP column to prevent overfitting.
"## CSV to DataFrame\n\nCSV files can be loaded into a dataframe by calling `pd.read_csv` . After loading the training and test files, print a `sample` to see what you're working with."
## Visualizing Data\n\nVisualizing data is crucial for recognizing underlying patterns to exploit in the model. 
"**Understanding the Respondents:**\n\nLet us first try to understand the respondents who took the survey. We know that most of the respondents were found primarily through Kaggle channels, like our email list, discussion forums and social media channels.\n\nBut let us try to understand more about their demographics from the aspects of age, gender, locality, experience etc\n\n**Gender:**"
A high number of respondents are males compared to females\n\n**Employment Status:**
Employed full time forms the major chunk followed by people looking for work\n\n**Age:**\n\nLet us look at the histaogram of age.
"Majority of the people have age between 18 to 60. Clearly we can see some increments is round values like 20, 25, 30 and so on. \n\nAlso looks like we have few respondents with age 0 and 100 ;) \n\n**Country:**\n\nLet us look at the top countries from where we get the responses."
"Highest number of respondents are from US followed by India. Others is a category where if a country or territory received less than 50 respondents, Kaggle grouped them into a group named “Other” for anonymity.\n\n**Formal Education:**\n\nLet us look at the formal education status of the respondents"
Looks like most of the respondents have Masters degree followed by Bachelors and then Doctoral. Let us now look at the education of the parents.\n\n**Parents Education:**
"In parents education, bachelors comes first followed by masters and then high school. There seems to be an improvement in the highest level of education over years.\n\n**Tenure:**\n\nTenure represents how long the respondents have been writing code to analyze the data. Let us look at it."
Majority of them have less than 5 years of experience.!\n\n**Students Analysis:**\n\n**Student Status:**\n\nThis field tells whether the respondent has currently enrolled as a student at a degree granting school or not. This question is asked only for the people who are not working.
Majority of the students are from degree granting school. Now let us check whether the student respondents are learning data science skills formally or informally. Hopfully we will see a lot of yes since the survey mostly caters to Kaggle community.\n\n**Students Learning Datascience:**
As expected most of the students are looking to learn DS.\n\n**Working Professionals Analysis:**\n\nNow let us look at the working professionals data. First let us start with how many people want to switch to DS career.\n\n**Looking to Switch Jobs:**
"About 70% of the repondents are looking for a career switch.\n\n**Current Job Title:**\n\nSince most of the people are looking for a career switch, let us look a the job title of working professionals."
"Majority of the respondents have title as ""Data Scientist"" followed by ""Software Developer"".\n\nNow let us see how many people believe that the title fits what they have been doing."
Most of the people feel that the title is fine. Around 1700 people feel that the title fits poorly with what they do.!\n\nLet us now look how many people write codes to analyze data of the respondents.
## 4.1.3. Code Example:
# >> 4.2. DBSCAN clustering algorithm
## 4.2.3. Code Example:
# >> 4.3. Gaussian Mixture Model algorithm
## 4.3.3. Code Example :
# > 4.4. BIRCH algorithm\n\n**The Balance Iterative Reducing and Clustering using Hierarchies (BIRCH) algorithm works better on large data sets than the k-means algorithm.**\nIt breaks the data into little summaries that are clustered instead of the original data points. The summaries hold as much distribution information about the data points as possible.\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20200612004451/BIRCH.png)\n\nThis algorithm is commonly used with other clustering algorithm because the other clustering techniques can be used on the summaries generated by BIRCH.\nThe main downside of the BIRCH algorithm is that it only works on numeric data values. You can't use this for categorical values unless you do some data transformations.
"## Considerations\n\nAs we can see here, the pixel array data is stored as a Numpy array, a powerful numeric Python library for handling and manipulating matrix data (among other things). In addition, it is apparent here that the original radiographs have been preprocessed for us as follows:\n\n* The relatively high dynamic range, high bit-depth original images have been rescaled to 8-bit encoding (256 grayscales). For the radiologists out there, this means that the images have been windowed and leveled already. In clinical practice, manipulating the image bit-depth is typically done manually by a radiologist to highlight certain disease processes. To visually assess the quality of the automated bit-depth downscaling and for considerations on potentially improving this baseline, consider consultation with a radiologist physician.\n\n* The relativley large original image matrices (typically acquired at >2000 x 2000) have been resized to the data-science friendly shape of 1024 x 1024. For the purposes of this challenge, the diagnosis of most pneumonia cases can typically be made at this resolution. To visually assess the feasibility of diagnosis at this resolution, and to determine the optimal resolution for pneumonia detection (oftentimes can be done at a resolution *even smaller* than 1024 x 1024), consider consultation with a radiogist physician.\n\n## Visualizing An Example\n\nTo take a look at this first DICOM image, let's use the `pylab.imshow()` method:"
"# Exploring the Data and Labels\n\nAs alluded to above, any given patient may potentially have many boxes if there are several different suspicious areas of pneumonia. To collapse the current CSV file dataframe into a dictionary with unique entries, consider the following method:"
"# Visualizing Boxes\n\nIn order to overlay color boxes on the original grayscale DICOM files, consider using the following  methods (below, the main method `draw()` requires the method `overlay_box()`):"
"As we saw above, patient `00436515-870c-4b36-a041-de91049b9ab4` has pnuemonia so let's take a look at the overlaid bounding boxes:"
\n\n\n\n\n\n\n    4  DATASET CREATION AND EXPLORATION    ⤒\n\n\n---
QUICK DOUBLECHECK ON RLE DECODE FUNCTION\n* https://www.kaggle.com/namgalielei/which-reshape-is-used-in-rle
4.2 INVESTIGATE THE TRAIN DATAFRAME\n\n---\n
4.3 VISUALIZE & INVESTIGATE THE LIVECell DATA\n\n---\n\nLet's review the LIVECell dataset\n
# Importing all the necessary libraries
# Reading the comma separated values file into the dataframe
"### Imports\n\n> We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. \n"
"# 3. Read in Data\n Top\n\nFirst, we can list all the available data files. There are a total of 6 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 4 other files containing additional information about energy types based on historic usage rates and observed weather. . "
- Let's import the libraries
- Let's look at the data.
"In this notebook, let us try and explore the data given for Zillow prize competition. Before we dive deep into the data, let us know a little more about the competition.\n\n**Zillow:**\n\nZillow is an online real estate database company founded in 2006 - Wikipedia\n\n**Zestimate:**\n\n“Zestimates” are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property. And, by continually improving the median margin of error (from 14% at the onset to 5% today),\n\n**Objective:**\n\nBuilding a model to improve the Zestimate residual error.\n\nThe competition is in two stages. This public competition will go on till Jan 2018 and has $50,000 in prize. Please make sure to read about the [Prize details][1] and [Competition overview][2] since it is quite different in this one.\n\nLet us first import the necessary modules.\n\n\n  [1]: https://www.kaggle.com/c/zillow-prize-1#prizes\n  [2]: https://www.kaggle.com/c/zillow-prize-1#Competition%20Overview"
Let us list the files present in the input folder.
"**Logerror:**\n\nTarget variable for this competition is ""logerror"" field. So let us do some analysis on this field first. "
This looks nice with some outliers at both the ends.! \n\nLet us remove the outliers and then do a histogram plot on the same.
Wow. nice normal distribution on the log error.\n\n**Transaction Date:**\n\nNow let us explore the date field. Let us first check the number of transactions in each month. 
"As we could see from the data page as well\n*The train data has all the transactions before October 15, 2016, plus some of the transactions after October 15, 2016.*\n\nSo we have shorter bars in the last three months. \n\n**Parcel Id:**"
There are so many NaN values in the dataset. So let us first do some exploration on that one. 
Let us explore the latitude and longitude variable to begin with.
"From the data page, *we are provided with a full list of real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016.*\n\nWe have about 90,811 rows in train but we have about 2,985,217 rows in properties file. So let us merge the two files and then carry out our analysis. "
"**Univariate Analysis:**\n\nSince there are so many variables, let us first take the 'float' variables alone and then get the correlation with the target variable to see how they are related."
The correlation of the target variable with the given set of variables are low overall. \n\nThere are few variables at the top of this graph without any correlation values. I guess they have only one unique value and hence no correlation value. Let us confirm the same.
# Loading base packages\n\nNothing too surprising here. Collections deque data structure will help us keep track of past data.
# Load data\n\nLoading a pickle file. Check this notebook [pickling](https://www.kaggle.com/quillio/pickling) if you haven't pickled your data set yet. Check this notebook [one liner to halve your memory usage](https://www.kaggle.com/jorijnsmit/one-liner-to-halve-your-memory-usage) if you want to reduce memory usage before pickling.
"Let's import some libraries. First, we will need the [statsmodels](http://statsmodels.sourceforge.net/stable/) library, which has many statistical modeling functions, including time series. For R afficionados who had to move to Python, `statsmodels` will definitely look more familiar since it supports model definitions like 'Wage ~ Age + Education'."
"As an example, let's look at real mobile game data. Specifically, we will look into ads watched per hour and in-game currency spend per day:"
"Unfortunately, we cannot make predictions far in the future -- in order to get the value for the next step, we need the previous values to be actually observed. But moving average has another use case - smoothing the original time series to identify trends. Pandas has an implementation available with [`DataFrame.rolling(window).mean()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). The wider the window, the smoother the trend. In the case of very noisy data, which is often encountered in finance, this procedure can help detect common patterns."
Let's smooth by the previous 4 hours.
There is no pattern visible in this plot and values are distributed equally around zero. So Linearity assumption is satisfied
Here the residuals are normally distributed. So normality assumption is satisfied
### Feature Engineering\n**Credit:** This amazing [notebook](https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977) by [Martin Kovacevic Buvinic](https://www.kaggle.com/ragnar123)
### Configurations & Setup
"### The Competition's Metric\n\nThe evaluation metric, *M* for this competition is the mean of two measures of rank ordering: Normalized Gini Coefficient, *G*, and default rate captured at 4%, *D*.\n\n$$M = 0.5 \cdot ( G + D )$$\n\nThe default rate captured at 4% is the percentage of the positive labels (defaults) captured within the highest-ranked 4% of the predictions, and represents a Sensitivity/Recall statistic.\n\nFor both of the sub-metrics *G* and *D*, the negative labels are given a weight of 20 to adjust for downsampling.\n\nThis metric has a maximum value of 1.0."
![](https://miro.medium.com/max/700/1*oB3S5yHHhvougJkPXuc8og.gif)
# Objective\n\nThe aim of this kernel is to provide all the tips and tricks required to train image classification model on any given image dataset in a single page.This kernel will hold almost all steps and steps required to implement image classification algorithm using SOTA such as ResNET on any given dataset.It could be a great time saver for you.Just utilize it anytime when you are working on Image Classification.\n\nI have learned them from [FastAI](https://docs.fast.ai/)
\n2.2 Library Import
\n3.1 Setting up path for training data
## 3. Age
#### This graph shows the density of people who belong to the 3 class along with the age.
# Initial Overview
"# What are the happiest countries in the world?\n\n'Happiness' to me seems like an individual metric, something that is hard to generalise. However, some countries perform consistently well on the happiness index rankings. \n\nWe've also noted that 9 of the top 10 are in Europe, and that 7 of the bottom 10 are in Africa.\n\nLet's see which countries top the list currently, and those that are at the bottom."
Let's now view the top 10 and bottom 10 ...
Let's bring the top 10 & bottom 10 side-by-side now for an alternative view
"At a glance, we see that many of the happiest countries in the world are indeed in Europe. \n\nAn additional observation is that the countries in Europe in the top 10 are Northern European."
"# Is this the case often?\n\nI will explore temporal change more in depth later, but for now, let's look at the **top 20 countries over the years**.\n\nThis plot shows all scores from 2005 through to the present for the top 20 countries, with their Mean score and their 2021 scores hihglighted specifically.\n\nIt's remarkable that many countries 2021 score is higher than their mean, despite the pandemic.\n\nAlthough the scores do vary, they still remain relatively high."
"# Why are there differences?\n\nWe now understand that Northern European nations top the list, and have done for some time.\n\nLet's explore these differences between Europe and the rest of the world a little more closely."
"Happier countries tend to be those with longer life expectancies, and a higher GDP. This is also most of Western Europe.\n\nLet's explicitly highlight Africa now..."
"By and large, African countries have lower life expectancy, a lower GDP, and ultimately, lower happiness index scores.\n\n# Other factors\n\nSo GDP & Life expactancy are factors. What else can be considered?"
"As I have noted in the plot, Freedom & Corruption are inversely related: higher corruption tends to be accompanied by lower freedom.\n\nHowever, it is interesting to note that several European nations have high percieved levels of corruption too.\n\n\n# A Continental view\n\nLet's wrap the countries up in to their respective continents to see if we can learn more.\n\nOf course we expect Western Europe to be high, but are there any other continents that perform particularly well or poorly in the happiness rankings?"
There are **three clusters** of continents that are clear to see. **More on this later...**\n\nSub-Saharan Africa & South Asia have the lowest scores. Whilst Western Europe and North America & ANZ are far ahead at the top.
"# Differences between those above & below the mean happiness level\n\nLet's plot all many features at once, split by the mean happiness level. The happiest countries are, as always, shown in green."
"The plots above confirm what we saw earlier, with some notable features such as social support.\n\nThat genorisity is percieved as higher in unhappier countries is very interesting."
# A global view\n\nWe've now seen clear differences between countries based on severla factors. \n\nLet's see this globally
"This plot confirms what we discovered above, with South Asia and Africa being in the red.\n\nBut it also highlights areas we can investigate. For exmaple, China & India, both in the red, have **populations over 1 billion.** Can we investigate population and happiness levels?"
"## 3. Practice with Facebook Prophet\n\n### 3.1 Installation in Python\n\nFirst, you need to install the library. Prophet is available for Python and R. The choice will depend on your personal preferences and project requirements. Further in this article we will use Python.\n\nIn Python you can install Prophet using PyPI:\n```\n$ pip install fbprophet\n```\n\nIn R you can find the corresponing CRAN package. Refer to the [documentation](https://facebookincubator.github.io/prophet/docs/installation.html) for details.\n\nLet's import the modules that we will need, and initialize our environment:"
"### 3.2 Dataset\n\nWe will predict the daily number of posts published on [Medium](https://medium.com/).\n\nFirst, we load our dataset (download it from [here](https://drive.google.com/file/d/1G3YjM6mR32iPnQ6O3f6rE9BVbhiTiLyU/view?usp=sharing) and place in the '../../data' folder if you'd like to reproduce the following code):"
### Example of a picture in CelebA dataset\n178 x 218 px
"### Distribution of the Attribute\n\nAs specified before, this Notebook is an imagine recognition project of the Gender. There are more Female gender than Male gender in the data set. This give us some insight about the need to balance the data in next steps."
"## Step 2: Split Dataset into Training, Validation and Test\n\nThe recommended partitioning of images into training, validation, testing of the data set is: \n* 1-162770 are training\n* 162771-182637 are validation\n* 182638-202599 are testing\n\nThe partition is in file list_eval_partition.csv\n\nDue time execution, by now we will be using a reduced number of images:\n\n* Training 20000 images\n* Validation 5000 images\n* Test 5000 Images\n"
### 3.1. Let's start with an example: Data Augmentation\n\nThis is how an image will look like after data augmentation (based in the giving parameters below).
"The result is a new set of images with modifications from the original one, that allows to the model to learn from these variations in order to take this kind of images during the learning process and predict better never seen images."
"**For absoulte beginners, do check the notebook**\n\n# [Beginners Notebook with EDA](https://www.kaggle.com/harshkothari21/beginners-notebook-90-accuracy-with-eda)"
# Table of content \n\n- EDA\n- Handle Missing Values\n- Feature Engineering\n- linear Regression\n- Logistic Regression\n- Scalling\n- KNN Classifier\n- Support Vector Machine(SVM)\n- Kernelize SVM\n- Decision Tree\n- Random Forest
**Let's look at target feature first**
"**So the plot says we have more number of non-survived people and females are more likely to survived than male!. so, 'Sex' looks like a very strong explanatory variable, and it can be good choice for our model!**"
**Let's first vizualize null values on our training set on graph**
**We will be dealling with null values later on.**
**Let's analysize Pclass**
looking at some satistical data!
**It's Time to look at the Age column!**
**Most Important thing when plotting histograms : Arrange Number of Bins**
**Age column has non-uniform data and many outliers**\n\n**Outlier** : An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.
"**Age by surviving status**\n\nDid age had a big influence on chances to survive?\nTo visualize two age distributions, grouped by surviving status I am using boxlot and stripplot showed together:"
**Let's look at Number of siblings/spouses aboard**
\n## 3. Import Libraries 📚 
\n## 4. Read and Explain Dataset 🧾  
# 1. Import Required Libraries
# 2. Loading the dataset 
# 4. Data Visualization\n## Here we are going to plot :-\n- Count Plot :- to see if the dataset is balanced or not\n- Histograms :- to see if data is normally distributed or skewed\n- Box Plot :- to analyse the distribution and see the outliers\n- Scatter plots :- to understand relationship between any two variables\n- Pair plot :- to create scatter plot between all the variables
### **Conclusion** :- We observe that number of people who do not have diabetes is far more than people who do which indicates that our data is imbalanced.
### **Conclusion** :- We observe that only glucose and Blood Pressure are normally distributed rest others are skewed and have outliers
"Outliers are unusual values in your dataset, and they can distort statistical analyses and violate their assumptions. Hence it is of utmost importance to deal with them. In this case removing outliers can cause data loss so we have to deal with it using various scaling and transformation techniques."
**Pearson's Correlation Coefficient** : Helps you find out the relationship between two quantities. It gives you the measure of the strength of association between two variables. The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.\n\nA heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information.
"### **CONCLUSION** :- Observe the last row 'Outcome' and note its correlation scores with different features. We can observe that Glucose, BMI and Age are the most correlated with Outcome. BloodPressure, Insulin, DiabetesPedigreeFunction are the least correlated, hence they don't contribute much to the model so we can drop them. Read more about this here :- https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e I have used 3'rd technique method mentioned here."
"# EDA - Ion Switching\n\n> Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\n>\n> When ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.\n\n\n------\n\n**I'll update this EDA notebook in the following days/weeks. Stay tuned!**"
"# Train data\n\n> In this competition, you will be predicting the number of open_channels present, based on electrophysiological signal data.\n>\n> **IMPORTANT**: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.\n\n## `time`\nWe have 5M rows in the train dataset. According to the data description we have 50 seconds long 10kHz samples (500,000 rows per batch).\nSo we have 10 batches. The data in a batch is continuous, but discontinuous between batches.\n\n## `signal`\n\n\n## `open_channels`\nPredictions have 11 possible values of open_channels: 0-10."
# Target distribution
## Target distribution in different batches
# Statistics
# Loading packages and data \n\nBefore we can start we need to import some packages and the data of course:
**Peek at the data**
# Helper methods \n\nWe will use some helper methods that we collect in this chapter to reduce amount of code during analysis:\n\n* get_outliers\n* make_word_cloud\n* split_data_by_nan\n* scale_and_log
# The Gaussian Mixture Model 
"# Feature scaling and transformation \n\nAfter these steps our dataset is cleaned and optimized for our analysis, but because we want to use the Gaussian Mixture Model we still have to scale and transform the data a bit. Looking closer on how our model learns, we can see two problems in the M-Step:\n\n$$\mu_{k} = \frac{1}{N_{k}} \sum_{n=1}^{N} \gamma_{nk} x_{n} $$\n\n$$\Sigma_{k} =  \frac{1}{N_{k}}  \sum_{n=1}^{N}  \gamma_{nk} (x_{n} - \mu_{k}) (x_{n} - \mu_{k})^{T}$$ \n\n### Why should we transform features?\n\nThe cluster center $\mu_{k}$ and the covariance matrices $\Sigma_{k}$ are influenced by the feature values $x_{n}$ themselves. As we have skewed feature distributions with high outliers the cluster center are shifted towards higher values during each update step. This is caused by the weighted mean. Even though the responsibilities as weights are able to soften the effect of outliers, they can still contribute very much. In this case our model builds clusters during learning that try to match the structure of outliers. The cluster center would not explain the majority of its cluster members and could be even located in regions with no data spots at all. To solve this problem we transform our features using boxcox transformation. Doing so we like to expand the majority of data points to uncover hidden substructures and compress the outliers.\n\nEven this sounds easy, this is the most toughest part of all! \n\n### Why should we scale them?\nEven with transformed features there is still a problem left: So far all our features have positive values and the responsibilities are positive as well as they are probabilities between 0 and 1.  This will still cause a shift in the center and covariance updates as the assignments will be lead by the higher values. To correct this we want to shift our transformed feature distributions such that it is evenly distributed around 0. For this purpose we subtract the mean and scale to unit variance. \n\n### How does to boxcox knead our features?\nUsing the boxcox transformation we are able to knead the feature distributions as we like them to be. But with this this great power comes also great responsibility ;-) . Consequently we should try to understand what boxcox can do with our values!"
"Looking at the power parameter $\lambda$ of the boxcox transformation you can see that we are able to compress outliers more and more by chosing lower values of $\lambda$. In addition we obtain a ""negative"" stretching of low original values lower than one. If we fix $\lambda =0.5$ and vary the constant $c$ we can observe only slight differences of the high values compression. On the other hand we can see that the stretching of low values is even stronger of constants c close to zero. \n\n#### Take-away (not that simple but still good)\n\n* If you like to compress outliers - chose a low $\lambda$\n* If you like to stretch low values - chose a low $c$ close to zero.\n\n\n### Transform! \n\nOk, let's go! If you like, fork and chose other values. You will see that the resulting clusters highly depend on these hyperparameters!"
### Visual comparison\n\nLet's have a look at our transformed features:
Many of them look far better than before but the problem of setting optimal hyperparameters is still very difficult.
# Objective\nThe objective of this notebook is to detect missing values and then go over some of the methods used for imputing them.\n\n\n\n# Data\n\nThere are two publically available datasets which will be used to explain the concepts:\n\n* 1. [Titanic Dataset](https://www.kaggle.com/c/titanic) for Non Time Series problem\n* 2. [Air Quality Data in India (2015 - 2020)](https://www.kaggle.com/rohanrao/air-quality-data-in-india) for Time Series problem\n\n\n# Loading necessary libraries and datasets
## Reading in the dataset\n* Reading in the Titanic Dataset.
"## Detecting missing data visually using Missingno library\n\n>To graphically analyse the missingness of the data, let's use a library called [Missingno](https://github.com/ResidentMario/missingno) It is a package for graphical analysis of missing values. To use this library, we need to import it as follows: `import missingno as msno`"
">The bar chart above gives a quick graphical overview of the completeness of the dataset. We can see that Age, Cabin and embarked columns have missing values. Next,it would make sense to find out the locations of the missing data."
## Finding reason for missing data using a Heatmap 
The heatmap function shows that there are no strong correlations between missing values of different features. This is good; low correlations further indicate that the data are MAR.
## Preparing environment and uploading data\n### Import Packages
### Load Datasets
"### Total Rooms above Ground and Living Area\nFrom a previews experience with Boston data set, you probably main expect to much from the total rooms above ground, as its 'RM' feature (the average number of rooms per dwelling), but here is not the same scenario. Our common sense make to think that live area maybe has some correlation to it and probably we can combine this two features to produce a better predictor. Let's see.\n![image](https://www.housing.iastate.edu/sites/default/files/imported//images/floorplans/Frederiksen-4BR.gif)"
"As we can see, the interaction between the two features did not present a better correlation than that already seen in the living area, include it improves to 0.74 with the cut of the outliers.\n\nOn the other hand, the ***multiplication*** not only demonstrated the living area **outliers** already identified, but it still **emphasized another**. If the strategy is to ***drop the TotRmsAbvGrd***, we should also ***exclude this additional outlier***."
"### Total Basement Area Vs 1st Flor Area\nIn our country it is not common to have Basement, I think we thought it was a little spooky. So I looked a bit more ""carefully"" at this variable...\n![image](https://lparchive.org/Scooby-Doo-Mystery/Update%2002/46-Fusion_2012-08-28_01-59-20-18.png)\nI noticed that in Ames has a lot of variation, but the predictive effect is very small, so I decided to study its composition with the first floor."
"[![image](https://www.wcibasementrepair.com/wp-content/themes/wci/images/home-popupdots.png)](https://www.manta.com/cost-basement-waterproofing-ames-ia)\nSimilar to what we saw in the garage analysis, we again have a better correlation by multiplying the variables, but now we don't have a significant gain with outliers exclusion. So let's continue with the multiplication strategy and remove only the two original metrics that have high correlation with each other."
"### Year Built Vs Garage Year Built\nOf course when we buy a property the date of its construction makes a lot of difference as it can be a source of great headaches. Depending on the age and conditions there will be need for renovations and very old houses there may be cases where the garage has been built or refit after the house itself.\n![image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcROzLQY3lcdYJimrBS7fHjLE0vhecqf1HTCfBANuDX5_5ZGBv0b)\nWell, I'd be more worried about the plumbing, the electricity, ... the garage is only for car and trunk, or is it not? Is that so? it will be?\n\nSo, let's see the graphs below, and confirm that this two features are highly correlated, but as expect is not easy to find a good substitute by iteration."
"However, by making the year of construction of the garage an indicator of whether it is newer, it becomes easiest to identify a pattern of separation. \n\nAnd more, note that we have a rising price due to the lower age. Maybe the old cars had the garage would only be for themselves...\n![image](https://www.corsia.us/wp-content/uploads/2016/05/cars-period-1909-taft-white-steam-car-1-800x533-538x218.jpg)\n..., or put it in the barn. Today we must have other more usable uses for garage, right...?\n![image](http://bonjourmini.com/wp-content/uploads/2018/05/garage-man-cave-how-to-create-a-man-cave-garage-more-best-flooring-for-garage-man-cave.jpg)"
"But note that although we have a rising price the newer the house, the growth rate is very smooth, even with the rate gain with a newer garage. This makes sense, given that the prices of these regressors are meeting with the mean price of each year."
Let's analyze the gaps in the data.
"There are no gaps in the data, great!"
There are no categorical features. Let us analyze the distribution of features.
"Normal distribution, everyone who deals with statistics loves it :)"
Let's look at the mutual relationship between features.
"Conclusions from the presented graphs: with the growth of the store area, the number of items sold increases. The obvious conclusion is that no one will use the area of ​​the store to accommodate fewer goods)"
"Let's analyze the presence of outliers in the data. According to the unspoken rule, no more than 2% of the data should be deleted (no more than 20 lines of the dataset in this case)."
There are minor outliers in the data. We'll remove them later.
"# Tables of Content:\n\n**1. [Introduction](#Introduction)** \n**2. [Univariate Distribution](#Univariate)** \n**3. [Multivariate Distribution](#Multivariate)** \n	- 3.1 Categorical Variable by Categorical Variable\n	- 3.2 Continuous Variable by Categorical Variable\n	- 3.3 Continuous Variables  on Continuous Variables\n	- 3.4 Percentage Standardize Distribution Plots\n    \n**4. [Multivariate Analysis](#Multianalysis)** \n**5. [Working with Text](#Text)** \n	- 5.1 Text Pre-Processing\n	- 5.2 Sentiment Analysis\n**5. [Sentiment Analysis](#Sentiment Analysis)** \n**6. [Word Distribution and Word Cloud](#Word Distribution and Word Cloud)** \n**7. [N Grams by Recommended Feature](#NGRAM)** \n**8. [Supervised Learning](#Supervised Learning)** \n	- 8.1 Naive Bayes\n\n# **1. Introduction:**  \nThis notebook is concerned with using the Python programming language and Natural Language Processing technology to explore trends in the customer reviews from an anonymized women’s clothing E-commerce platform, and extract actionable plans to improve its online e-commerce. The data is a collection of 22641 Rows and 10 column variables. Each row includes a written comment as well as additional customer information. This analysis will focus on using Natural Language techniques to find broad trends in the written thoughts of the customers. The total number of unique words in the dataset is 9810. \n\nMy goal is to get to understand what it is the customers appreciate and dislike about their purchases. To reach this goal, I conduct an observational study of this sizable dataset, first by understanding the characteristics of individual features, and ramping the complexity of the analysis once a proper target is envisioned. \n\n[Notebook Counterpart: In-Depth Simple Linear Regression](https://www.kaggle.com/nicapotato/in-depth-simple-linear-regression)"
"**Code Explanation and Reasoning:** \nThese packages are separated in four categories: *General, Visualization, Pre-Processing, and Modeling*.\n\nThe General category includes the basic data manipulation tools for scientific computation (`numpy`), dataframes (`pandas`), Natural Language Processing (`NLTK`), path directory manipulation (`os`), and image saving (`PIL`).\n\nThe Visualization section enables the creation of simple graphics (`matplotlib`, `seaborn`), as well as `wordcloud`'s text frequency visualization.\n\nThe Pre-Processing section extracts more specialized modules from the NLTK package such as tokenizers and stemmers to enable the preparation of text data for mathematical analysis.\n\nThe Modeling section includes `nltk`’s sentiment analysis module, which can determine the mood of text, NLTK’s N-grams, and `gensim.models`’s word2vec. It also includes `statsmodels.api` which offers an array of linear models."
Just an overview. I want to explore these numbers using visualizations.\n\n***\n\n**Age and Positive Feedback Count Distributions:**
"**Code Explanation:** \nUsing seaborn, a simple variable frequency bar/density plot is created. In the log positive feedback count plot, I had to add 0.0001 to all values so that the logarithm of previously zero values can be taken. Matplotlib's subplots function is employed through assign each plot the **AX** argument.\n\n**Distribution of Age:** \nMy a priori expectation was that the biggest group of reviewing customers would be young, tech savvy women between the age of 18 and 34. However, this plot would say otherwise, since it appears that not only is the 34 to 50 year old age most engage in reviewing products, they also appear to be the most positive reviewers, since they proportionately give higher more reviews of 5. Before making insight about these point, it would be wise to gather further data on the age distribution of shoppers. Nevertheless, this trend suggest that the core market segment for this clothing brand is women between 34 and 50. With its single peak and slight right tail, the distribution of age is more or less normal.\n\n**Distribution of Positive Feedback Count:** \nThis kind of distribution is common for network effect phenomenon, where popularity has an exponential effect on response, and most individuals receive no attention. This phenomenon is also known as the *Cumulative-Advantage Effect / Matthew Effect* or the Pareto Principle.\n\n**Cumulative-Advantage Effect / Matthew Effect:** \nCoined by Robert K. Merton in 1968, this [states that once a social agent gains a small advantage over other agents, that advantage will compound over time into an increasingly larger advantage.](http://www.thwink.org/sustain/glossary/CumulativeAdvantagePrinciple.htm) Here is the passage from the New Testament:\n\n>""For to everyone who has will more be given, and he will have abundance; but from him who has not, even what he has will be taken away.""\n> Matthew 25:29\n\nThis tendency effects any system with a positive feedback loop, which compounds. This effect turns out to be quite common among competing agents, and what we end up with, is a the Pareto Distribution.\n\n**Pareto Distribution:** \nAlso known as the 80/20 rule. Often used to describe the distribution of wealth, 20% of the population hold 80% of the wealth. I wonder how accurate this rule of thumb applies to the distribution of Positive Feedback."
"**Interpretation:** \nIn this case, the 80/20 rule applies pretty closely. Nevertheless do not take this rule as granted, since sometimes the proportion of inequality may be much higher! Since the Pareto Principle is a power law, it is fundamentally embedded in itself. However, notice the green vertical line, where 47% of reviews received *no* feedback at all.\n\nSince I am on the topic of inequaliy, I want to quickly touch the Gini Coefficient.\n\nNext, lets see what happens when we look at the top 20% of the top 20%..."
"**Interpretation:** \nOh look it didn't hold up. Well think about, the compounding influence of the Network Effect as as whole accounts for the sizable portion of the population that receive nothing.\n\n***\n\n**Division Name and Department Name Distribution:**"
"**Code Explanation:** \nEnumerating the loop enables the loop iteration to coincide with the matplotlib subplot ax.\n\n**Distribution of Division Name** \nThis high level feature describes had three categories: General, Petite, and Intimates. This offers some insight into the clothing sizes of the customers leaving reviews.\n\n**Distribution of Department Name** \nIt is notable to observe that *Tops and Dresses* are the most commonly reviewed products. It would be interesting to investigate the motivation of leaving a review in the first place.\n\n***\n**Distribution of Clothing ID to Understand Product Popularity**"
"**Code Explanation** \nSince they are around one thousand unique *Clothing IDs*, I used boolean operators to only select the top 60 most popular cloth items, then optimizing notebook real estate by splitting them in two plot columns.\n\n**Interpretation** \nIt appears like there are around three products that receive a small magnitude more reviews than others. I follow up on these findings by observing the descriptive statistics of the top three items. These items received an average rating of ~4.2, and an average recommendation rate of 81%. Furthermore, it appears that these products are predominately normal sized dresses.\n\nThese observations make me wonder about the nature of review popularity and rating performance. A question that could shed light on the customer's motivation to leave a review.\n\n***\n**Distribution of Class Name**"
"**Interpretation:** \nExploring the class variable suggests that the most popular clothing types are: Petite and Anthro, Dresses, Blouses, and Cut and Sew Knits. The distribution of reviews is fairly constant, suggesting that there are not negative nor positive outliers. This statement has been further verified by taking the mean of the label by class group. The results show that no class falls above .80, and the majority rest at .90. Casual bottoms and Chemises scored the highest in this criteria with a 100% positive review rate, however upon investigation this is because only 4 reviews were made in these categories.\n\n***\n**Distribution of Rating, Recommended IND, and Label:**"
"**Code Explanation:**\nYet another way to iterate plots, where I both loop over the index position of cat_dtypes and subplot ax at the same time with range of the length of cat_dtypes.\n\n**Distribution of Rating:** \nThe vast majority of reviews were highly positive, with a score of five out of five. This suggests that this retail store is performing fairly well, but then again, I am not familiar with the industry benchmark. Competitor reviews may be scraped and analyzed. It is important to note that these reviews are subjective, and some negative reviews may a outcome of a bad day, instead of constructive feedback. In the plot below, the Label plot is the binary classification of 1 = good, and 0= bad.\n\n**Distribution of Recommended IND:** \nThis variable mirrors the positivity of the Rating distribution, but as mentioned earlier, I believe that it provides variation of positivity which is social, rather than personal.\n\n**Distribution of Label:** \nI am surprised to see that products are rated 3 and over, than are recommended by the customer. I am eager to see the multivariate interaction between Rating and Recommended.\n\nI find these three variables especially promising in the quest of finding how customers express dislike. In the multivariate section, I shall explore the interplay between these variables.\n\n***\n**Word and Length:**"
"**Interpretation:** \n- Review Character and Word Count are highly correlated.\n- I suspect that the retailer has a maximum word limit of 500, which caused the long tail to receive compression and spike. \n\n***\n\n## 3. Multivariate Distribution \n### 3.1 Categorical Variable by Categorical Variable\nIn this section, I utilize heatmaps to visualize the percentage occurrence pivot table. Note that I heavily utilized the technique of normalizing the proportion between variables classes by converting frequency into percentages. This technique is very fruitful because the relation upon which the percentage can be explored by aggregate, by index, and by column, each of which providing its own unique insight.\n\n**Division Name by Department Name:**"
"**How to Interpret:** \nFor the second heatmap on the right, the percentages occurrence is in relation to the whole.\n\n**Interpretation:** \nEvidently, the most common product is a normal sized top."
"**How to Interpret:** \nAlthough these two heatmaps use the same features, they different in the relation in which the percentage is taken. For the first plot on the left, the percentages add up to 100% by **column**, while the plot on the right has is **normalized into percentages by row**.\n\n**Interpretation:** \nThe dominance of the *General* size is consistent across the various categories within **Department Name**. There a notable overall between *General Petite* and *Department Name*.\n\n***\n\n**Class Name by Department Name:**"
\n📝 Note: The price range of the thrid class looks a bit suspicious compared to the 2. class. We will have a closer look at that later.  
\n📝 Note: The Fare feature contains outliers in each Pclass. This looks a bit strange.
\n💭 Thougts:  We can't see a direct connection between Age and Survived. Let's have a closer look...
\n💡 Idea: We should consider a closer look at very young passengers.
\n📝 Note: We will use one hot encoder later on. Otherwise the natural order of those numbers could irritate the ML algorithm.
"\n📝 Note: We see strong correlations between the features Survived and Pclass, Sex, Fare and Embarked."
Let's do some visualization.
We can see from training set that almost all people with Age higher than 63 years didn't survive. Can use these information in modeling post processing.
"The game points are 3 for win, 1 for draw and 0 for lose and are different than the FIFA rank points that are already in the database. Also, it's supposed that FIFA Rank points and FIFA Ranking of the same team are negative correlated, and we should use only one of them to create new features. This supposition is checked below:"
"Now, we create columns that will help in the creation of the features: ranking difference, points won at the game vs. team faced rank, and goals difference in the game. All features that are not differences should be created for the two teams (away and home)."
\n# **1. Importing Libraries and Packages**\nWe will use these packages to help us manipulate the data and visualize the features/labels as well as measure how well our model performed. Numpy and Pandas are helpful for manipulating the dataframe and its columns and cells. We will use matplotlib along with Seaborn to visualize our data.
"\n# **2. Loading and Viewing Data Set**\nWith Pandas, we can load both the training and testing set that we wil later use to train and test our model. Before we begin, we should take a look at our data table to see the values that we'll be working with. We can use the head and describe function to look at some sample data and statistics. We can also look at its keys and column names."
We take a look at the distribution of the Age column to see if it's skewed or symmetrical. This will help us determine what value to replace the NaN values.
"Looks like the distribution of ages is slightly skewed right. Because of this, we can fill in the null values with the median for the most accuracy. \n> **Note:** We do not want to fill with the mean because the skewed distribution means that very large values on one end will greatly impact the mean, as opposed to the median, which will only be slightly impacted."
**Gender**
> **Note:** The numbers printed above are the proportion of male/female survivors of all the surviviors ONLY. The graph shows the propotion of male/females out of ALL the passengers including those that didn't survive.
"Machine Learning and Data Science Survey 2021 Analysis\nWhile much of the world closed down during the COVID-19 pandemic, the gates opened wide for financing early and late-stage startups in the data and AI space.[1]\n In the past year, there’s been less headline-grabbing discussion of futuristic applications of AI (self-driving vehicles, etc.), and a bit less AI hype as a result. Regardless, data and ML/AI-driven application companies have continued to thrive, particularly those focused on enterprise use trend cases. Meanwhile, a lot of the action has been happening behind the scenes on the data and ML infrastructure side, with entirely new categories (data observability, reverse ETL, metrics stores, etc.) appearing or drastically accelerating.[2]\nInsights from Kaggle’s annual user survey focused on working data scientists. This is the 5th year conducting an in-depth user survey & publicly sharing the results. Over 25,000 data scientists and ML engineers submitted responses on their backgrounds and day to day experience – everything from educational details to salaries to preferred technologies and techniques. This notebook is data visualization and analysis of the annual user survey result."
# Country\nIn which country do you currently reside?
"India is obviously the number one country of Kaggle's users (28.6%). Second position is the USA (10.2%) with a big gap almost 20%, then another big gap to the 3rd place filled by users from Japan, China, Brazil, Russia and Nigeria with about the same percentage around 3%. Users from Europe are (including) Russia, UK, Germany, Spain, France and Italy, with 2.8%, 2.1%, 1.8%, 1.7%, 1.5% and 1.2% respectively. Users from the South East Asia just Indonesia (1.7%) in the top 20 countries list."
# Higher Education\nWhat is the highest level of formal education that you have attained or plan to attain within the next 2 years?
"More 77.1% of the users have master's and bachelor's degree, 12.2% have doctoral degree, 1.4% professinal doctorate, 8.3% does not have a formal degree, and 1.6% has high school education."
# Gender\nWhat is your gender?
"In 2021 Kaggle's users are dominantly by man (79.3%), and woman (18.8%). Hopefully tha gap will be smaller in the near future."
# Age\nWhat is your age (# years)?
"Kaggle's users are young, below 30 years old are more than half of population (56%). Interestingly users above 50 years old are also quite a lot, further more there are 0.5% above 70 years old. We will see on the next chart how it looks between man and woman."
"Overall, man and woman are spread equally over age range. However if we look more deeper, woman that are 18-21 years old age range, is much higher than other age range. \nOn the other hand, man in 25-29 years old age range are much higher than woman on the same age.\nUser age between 30 to 50 years old are declining sharply in population. Think that five years ago, they were in the five bars on the left. In fact these people are the one who were early adopters of the modern Machine Learning and Data Science knowledge."
# Programming Language\nWhat programming languages do you use on a regular basis?
"It is boldly showed that Python is the swiss army of programming languange, loved by all developers including data scientist. Python is easy to learn, can do almost anything. Second language is SQL (16.3%). SQL is the one talking to the data. In data science world SQL is a must have knowledge. And yes, this notebook is of course written in Python."
# Job Title\nSelect the title most similar to your current role (or most recent title if retired)
"Kaggle users are mostly students (28.9%). Half of it the second job role is Data Scientist (15.3%). The third is about same percentage are software engineer and data analysit, 10.4% and 9.8% respectively."
# Yearly Compensation\nWhat is your current yearly compensation (approximate $USD)?
"Most of users, 21.9% respondents answer that their yearly compensation is less than 1,000. This is an unbelievable number. If we see the data, the reason is many users (40%) didn't answer this question, and second, most users are students. The data is may be a bit bias here.\n\nWe can devide compensation ranges into three groups. First group is 0-15K, second is 15-40K, 40-125K, and 125K-1M, and >1M. We see the professionals with compensation range above 10K, which I think is still underpaid for a data scientist even in poor countries. 4.8% of users is in 30K-39K range, and about the same percentage 4.7 got 100-125K yearly. Rest of the group (7.9%) got a decent yearly compensation amount, above 125K to more than 1M. Interesting group that need to be explored more is the most right part group with more than 125K yearly compensation."
# Writting Code Experience\nFor how many years have you been writing code and/or programming?
"Kaggler's users are mostly having 1-3 years of writting code experience, dominantly by man (24.3%), while woman (6.1%). Much less users have writting code experience more than 3 years. Interesting that there are 6.5% man and 0.6% woman with experience more than 20 years. These people are the gurus of data science and machine learning."
"# Data Science Notebooks\nThe realm of machine learning is that of data science, since after all, we’re trying to derive higher value insights from big data. The primary environment for data science is the “notebook”, which is a collaborative, interactive, document-style environment that combines aspects of coding, data engineering, machine learning modeling, data visualization, and collaborative data sharing. \n[3]\nWhile data science notebooks can be used to develop models of any type, they are primarily used during the experimentation and iteration phases of model development, since data science notebooks are optimized for that sort of iterative experimentation versus being focused on organization-wide aspects of management and deployment. [4]"
"Colab and Kaggle notebooks is the top two winners with only 1% difference, 32.9% and 31.9% respectively. The third and fourth place is still Google products, Google Cloud Notebooks 6.9%, the fourth Google Cloud Datalab 6%. Binder JupyterHub on the 5th, 5.9%. Sixth place is shared by IBM Watson Studio and Azure Notebooks, 3.6%, and Amazon Sagemaker Studio Notebooks 2.5%. [5]"
The goal of this section is to gain an understanding of our data in order to inform what we do in the feature engineering section.  \n\nWe begin our exploratory data analysis by loading our standard modules.
"We then load the data, which we have downloaded from the Kaggle website ([here](https://www.kaggle.com/c/titanic/data) is a link to the data if you need it)."
\n##  🛳  Description of the Dataset \n\n\nThe dataset contains ten variables and 891 passenger details. Survived is the response variable for the study. \n\n\n\n\n  \n    Variable Name \n    Description\n    Type\n  \n  \n    survival\n    Did Survive the incident?\n    Categoricol\n  \n  \n    pclass \n    Class of the ticket\n    Categoricol\n  \n  \n    sex \n    Gender \n    Categoricol\n  \n  \n    Age \n    Age of the passenger\n    Numeric\n  \n  \n    sibsp \n    no of siblings / spouses aboard the Titanic \n    Numeric\n  \n  \n    parch\n    no of parents / children aboard the Titanic\n    Numeric\n  \n  \n    ticket\n    Unique ticket number\n    Categoricol\n  \n  \n    fare\n    Passenger fare \n    Numeric\n  \n  \n    cabin\n    cabin number \n    Categoricol\n  \n  \n    Embarked\n    Port of Embarkation\n    Categoricol\n  \n\n
#### 🚤 Loading the ... dataset 
#### checking missing values in the training set.
 There are some missing values in the dataset. So we need to remove or impute them.
##   📊 Descriptive Analysis 
#### Univariate Analysis
 The majority of the passenegers from southampton and least number of passengers from Queenstown.
 There were 64.8% male passengers on the ship which is significantly greater than female passengers. 
 Age of the passengers are positively skewed and also there were some missing values in this variable.
 The distribution of fare is also skewed. we can use log transformation if we want to noramlize these positively skewed distributions.
 The majority of the passengers had bought class 3 tickets and class 2 tickets are the less. 
####  Bivariate analysis 
"# Convolutional Neural Networks (CNN)\n\nContent: \n* [Loading the Data Set](#1)\n* [Normalization, Reshape and Label Encoding ](#2)\n* [Train Test Split](#3)\n* [Convolutional Neural Network](#4)\n    * [What is Convolution Operation?](#5)\n    * [Same Padding](#6)\n    * [Max Pooling](#7)\n    * [Flattening](#8)\n    * [Full Connection](#9)\n* [Implementing with Keras](#10)\n    * [Create Model](#11)\n    * [Define Optimizer](#12)\n    * [Compile Model](#13)\n    * [Epochs and Batch Size](#14)\n    * [Data Augmentation](#15)\n    * [Fit the Model](#16)\n    * [Evaluate the Model](#17)\n* [Deep Learning Tutorial for Beginners](https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners)\n* [Artificial Neural Network with Pytorch](https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers)\n* [Convolutional Neural Network with Pytorch](https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers)\n* [Recurrent Neural Network with Pytorch](https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch)\n* [Conclusion](#18)\n"
\n## Loading the Data Set\n* In this part we load and visualize the data.
# **Exploratory Data Analysis**\n\n[Source1](https://www.kaggle.com/ambrosm/tpsjan22-01-eda-which-makes-sense) [Source2](https://www.kaggle.com/vad13irt/tps-jan-2022-exploratory-data-analysis)\n\n📌 Histograms in the below graph are skewed with outliers . Hence choosing log(num_sold over num_sold is preferred .\n\n
📌 The peaks in the below graph indicates lot of sales happens during January .
📌 Norway has the highest sales followed by Sweden and Finland\n
📌 KaggleRama has higher sales compared to KaggleMart\n\n
📌 KaggleHat has the highest sales followed by KaggleMug and KaggleStickers
📌 Monthly trend in the below graph shows the seasonal variations in sales across products
"# **Feature Engineering**\n\nNew features can be created from the date column like month , year , weekend or weekday ."
The snapshot of the artifact created is below\n\n![](https://drive.google.com/uc?id=16biHK189-q2mhyZAhE-cAvxHb3BIAfFq)
Here's a [cheat sheet I made in a google sheet](https://docs.google.com/spreadsheets/d/1woVi7wq13628HJ-tN6ApaRGVZ85OdmHsDBKLAf5ylaQ/edit?usp=sharing) to help folks keep the options straight. \n\nLet's set things up and start making some distributions!
# Original Distributions
"If we put this on the same plot as the original distributions, you can't even see the earlier columns."
"The new, high-value distribution is way to the right. And here's a plot of the values."
MinMaxScaler subtracts the column mean from each value and then divides by the range.
"Notice how the shape of each distribution remains the same, but now the values are between 0 and 1."
"## 1. Dataset\n\nFirst, we will set up our environment by importing all necessary libraries. We will also change the display settings to better show plots."
"Now, let's load the dataset that we will be using into a `DataFrame`. I have picked a dataset on video game sales and ratings from [Kaggle Datasets](https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings).\nSome of the games in this dataset lack ratings; so, let's filter for only those examples that have all of their values present."
"## 2. DataFrame.plot\n\nBefore we turn to Seaborn and Plotly,  discuss the simplest and often most convenient way to visualize data from a `DataFrame`: using its own `plot()` method.\n\nAs an example, we will create a plot of video game sales by country and year. First,  keep only the columns we need. Then, we will calculate the total sales by year and call the `plot()` method on the resulting `DataFrame`."
Note that the implementation of the `plot()` method in `pandas` is based on `matplotlib`.
"Using the `kind` parameter, you can change the type of the plot to, for example, a *bar chart*. `matplotlib` is generally quite flexible for customizing plots. You can change almost everything in the chart, but you may need to dig into the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) to find the corresponding parameters. For example, the parameter `rot` is responsible for the rotation angle of ticks on the x-axis (for vertical plots):"
"## 3. Seaborn\n\nNow, let's move on to the `Seaborn` library. `seaborn` is essentially a higher-level API based on the `matplotlib` library. Among other things, it differs from the latter in that it contains more adequate default settings for plotting. By adding `import seaborn as sns; sns.set()` in your code, the images of your plots will become much nicer. Also, this library contains a set of complex tools for visualization that would otherwise (i.e. when using bare `matplotlib`) require quite a large amount of code.\n\n#### pairplot()\n\nLet's take a look at the first of such complex plots, a *pairwise relationships plot*, which creates a matrix of scatter plots by default. This kind of plot helps us visualize the relationship between different variables in a single output."
"As you can see, the distribution histograms lie on the diagonal of the matrix. The remaining charts are scatter plots for the corresponding pairs of features.\n\n#### distplot()\n\nIt is also possible to plot a distribution of observations with `seaborn`'s `distplot()`. For example, let's look at the distribution of critics' ratings: `Critic_Score`. By default, the plot displays a histogram and the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation)."
"#### jointplot()\n\nTo look more closely at the relationship between two numerical variables, you can use *joint plot*, which is a cross between a scatter plot and histogram. Let's see how the `Critic_Score` and `User_Score` features are related."
#### boxplot()\n\nAnother useful type of plot is a *box plot*. Let's compare critics' ratings for the top 5 biggest gaming platforms.
"It is worth spending a bit more time to discuss how to interpret a box plot. Its components are a *box* (obviously, this is why it is called a *box plot*), the so-called *whiskers*, and a number of individual points (*outliers*).\n\nThe box by itself illustrates the interquartile spread of the distribution; its length determined by the $25\% \, (\text{Q1})$ and $75\% \, (\text{Q3})$ percentiles. The vertical line inside the box marks the median ($50\%$) of the distribution. \n\nThe whiskers are the lines extending from the box. They represent the entire scatter of data points, specifically the points that fall within the interval $(\text{Q1} - 1.5 \cdot \text{IQR}, \text{Q3} + 1.5 \cdot \text{IQR})$, where $\text{IQR} = \text{Q3} - \text{Q1}$ is the [interquartile range](https://en.wikipedia.org/wiki/Interquartile_range).\n\nOutliers that fall out of the range bounded by the whiskers are plotted individually.\n\n#### heatmap()\n\nThe last type of plot that we will cover here is a *heat map*. A heat map allows you to view the distribution of a numerical variable over two categorical ones. Let's visualize the total sales of games by genre and gaming platform."
"## 4. Plotly\n\nWe have examined some visualization tools based on the `matplotlib` library. However, this is not the only option for plotting in `Python`. Let's take a look at the `plotly` library. Plotly is an open-source library that allows creation of interactive plots within a Jupyter notebook without having to use Javascript.\n\nThe real beauty of interactive plots is that they provide a user interface for detailed data exploration. For example, you can see exact numerical values by mousing over points, hide uninteresting series from the visualization, zoom in onto a specific part of the plot, etc.\n\nBefore we start,  import all the necessary modules and initialize `plotly` by calling the `init_notebook_mode()` function."
"#### Line plot\n\nFirst of all,  build a *line plot* showing the number of games released and their sales by year."
"`Figure` is the main class and a work horse of visualization in `plotly`. It consists of the data (an array of lines called `traces` in this library) and the style (represented by the `layout` object). In the simplest case, you may call the `iplot` function to return only `traces`.\n\nThe `show_link` parameter toggles the visibility of the links leading to the online platform `plot.ly` in your charts. Most of the time, this functionality is not needed, so you may want to turn it off by passing `show_link=False` to prevent accidental clicks on those links."
"As an option, you can save the plot in an html file:"
"## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. "
"## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *scikit* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load."
"## 3.2 Meet and Great Data\n\nThis is the meet and great step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this tasks, we first import our data. Next we use the info() and sample () function, to get a quick and dirty overview of variable datatypes (i.e. qualitative vs quantitative). Click here for the [Source Data Dictionary](https://www.kaggle.com/c/titanic/data).\n\n1. The *Survived* variable is our outcome or dependent variable. It is a binary nominal datatype of 1 for survived and 0 for did not survive. All other variables are potential predictor or independent variables. **It's important to note, more predictor variables do not make a better model, but the right variables.**\n2. The *PassengerID* and *Ticket* variables are assumed to be random unique identifiers, that have no impact on the outcome variable. Thus, they will be excluded from analysis.\n3. The *Pclass* variable is an ordinal datatype for the ticket class, a proxy for socio-economic status (SES), representing 1 = upper class, 2 = middle class, and 3 = lower class.\n4. The *Name* variable is a nominal datatype. It could be used for [feature engineering](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/) to derive the gender from title, family size from surname, and SES from titles like doctor or master; however these variables already exist. For the first model iteration, this variable will be excluded from analysis. It can be used during subsequent iterations to evaluate if more complexity improves the base model accuracy. \n5. The *Sex* and *embarked* variables are a nominal datatype. They will be converted to dummy variables for mathematical calculations.\n6. The *Age* and *fare* variable are continuous quantitative datatypes.\n7. The *SibSp* represents number of siblings/spouse aboard and *Parch* represents number of parents or children aboard. Both are dicrete quantitative datatypes. This can be used for feature engineering to create a family size or is alone variable.\n8. The *cabin* variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occurred and SES from deck levels. However, since there are many null values, it does not add value and thus is excluded from analysis."
"## 5.1 Model Optimization\n\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~85% accuracy. If this were a college course, that would be a B-grade. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of model optimization. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So, keep that in mind.\n\nFor model optimization, we have a couple options: 1) find a ""better"" algorithm, 2) tune our current algorithm parameters, 3) feature engineer new variables to find new signals in the data, or 4) we can go back to the beginning and determine if we asked the right questions, got the right data, and made the right decisions along the process.\n\n### Data Science 101: Determine a Baseline Accuracy ###\nBefore we decide how-to make our model better, let's determine if ~85% is good enough. To do that, we have to go back to the basics of data science 101. We know this is a binary problem, because there are only two possible outcomes; passengers survived or died. So, think of it like a coin flip. If you have a fair coin and you guessed heads or tail, then you have a 50-50 chance of guessing right. So, let's set 50% as an F grade, because if your model accuracy is any worse than that, then why do I need you when I can just flip a coin?\n\nOkay, so with no information about the dataset, we can always get 50% with a binary problem. But we have information about the dataset, so we should be able to do better. We know that 1,502/2,224 or 67.5% of people died. Therefore, If I just guessed that 100% of people died, then I would be right 67.5% of the time. So, let's set 68% as a D grade, because again, if your model accuracy is any worst that that, then why do I need you, when I can just assume if you were on the Titanic that day you died and have a 68% accuracy.\n\n### Data Science 101: How-to Create Your Own Model ###\nOur accuracy is increasing, but can we do better? Are there any signals in our data? To illustrate this, we're going to build our own decision tree model, because it is the easiest to conceptualize and requires simple addition and multiplication calculations. When creating a decision tree, you want to ask questions that gives you better information about your outcome by segregated the survived/1 from the dead/0. This is part science and part art, so let's just play the 21-question game to show you how it works. If you want to follow along on your own, download the train dataset and import into Excel. Create a pivot table with survival in the columns, count and % of row count in the values, and the features described below in the rows.\n\nRemember, the name of the game is to create subsets using a decision tree model to get survived/1 in one bucket and dead/0 in another bucket. Our rule of thumb will be the majority rules. Meaning, if the majority or 50% or more survived, then everybody in our subgroup survived/1, but if 50% or less survived then if everybody in our subgroup died/0. Also, we will stop if the subgroup is 10% of our total dataset or 9 cases and/or our model accuracy plateaus or decreases. Got it? Let's go!\n\n***Question 1: Were you on the Titanic?*** If Yes, then majority (62%) died. Note our sample survival is different than our population of 68%. Nonetheless, if we assumed everybody died, our sample accuracy is 62%.\n\n***Question 2: Are you male or female?*** Male, majority (81%) died. Female, majority (74%) survived. Giving us an accuracy of 79%.\n\n***Question 3A (going down the female branch with count = 344): Are you in class 1, 2, or 3?*** Class 1, majority (97%) survived and Class 2, majority (92%) survived. Since are dead group is less than 9, we will stop going down this branch. Class 3, is even at a 50-50 split. No new information to improve our model is gained.\n\n***Question 4A (going down the female class 3 branch with count = 144): Did you embark from port C, Q, or S?*** We gain a little information. C and Q, the majority still survived, so no change. Also, the dead subgroup is 9 or less, so we will stop. S, the majority (63%) died. So, we will change females, class 3, embarked S from assuming they survived, to assuming they died. Our model accuracy increases to 81%. \n\n***Question 5A (going down the female class 3 embarked S branch with count = 88):*** So far, it looks like we made good decisions. Adding another level does not seem to gain much more information. This subgroup 55 died and 33 survived, since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. We can play with our features. One I found was fare 0-8, majority survived. It's a small sample size 11-9, but one often used in statistics. We slightly improve our accuracy, but not much to move us past 82%. So, we'll stop here.\n\n***Question 3B (going down the male branch with count = 577):*** Going back to question 2, we know the majority of males died. So, we are looking for a feature that identifies a subgroup that majority survived. Surprisingly, class or even embarked didn't matter, title does and gets us to 82%. Guess and checking other features, none seem to push us past 82%. So, we'll stop here for now.\n\nSo, with very little information, we get to 82% accuracy. We'll give that a grade of a C for average or our baseline. But can we do better? By making a better decision tree, new features, etc. Before we do, let's code what we just wrote above.\n"
"# Credits\nProgramming is all about ""borrowing"" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \n\n* Müller, Andreas C.; Guido, Sarah. Introduction to Machine Learning with Python: A Guide for Data Scientists. O'Reilly Media.\n\n"
--------------------------------------\n# Setting Up
-------------------------------------------------------------------------------\n# Checking Target Imbalance\n\n
-----------------------------------\n# Checking the data type of features
" Observation:\n\n* All features are numerical! So, there is no need to consider categorical feature engineering!    "
"In various features, it is determined that skewness greater than 1 is skewed, and only these features are subjected to nonlinear scaling."
 Observation:\n\n* Only Amount feature is met the condition!
"# Titanic Project Example Walk Through \nIn this notebook, I hope to show how a data scientist would go about working through a problem. The goal is to correctly predict if someone survived the Titanic shipwreck. I thought it would be fun to see how well I could do in this competition without deep learning. \n\n*The accompanying video is located here:* https://www.youtube.com/watch?v=I3FBJdiExcg\n\n**Best results : 79.425 % accuracy (Top 12%)**\n\n## Overview \n### 1) Understand the shape of the data (Histograms, box plots, etc.)\n\n### 2) Data Cleaning \n\n### 3) Data Exploration\n\n### 4) Feature Engineering \n\n### 5) Data Preprocessing for Model\n\n### 6) Basic Model Building \n\n### 7) Model Tuning \n\n### 8) Ensemble Modle Building \n\n### 9) Results "
"Here we import the data. For this analysis, we will be exclusively working with the Training set. We will be validating based on data from the training set as well. For our final submissions, we will make predictions based on the test set. "
"## Project Planning\nWhen starting any project, I like to outline the steps that I plan to take. Below is the rough outline that I created for this project using commented cells. "
From the above plot we see that gender has no direct relation to segmenting customers. That's why we can drop it and move on with other features which is why we will X parameter from now on.
"Elbow method tells us to select the cluster when there is a significant change in inertia. As we can see from the graph, we can say this may be either 3 or 5. Let's see both results in graph and decide.\n\n###  Creating the Visual Plots"
By judging from the plots we could say that 5 cluster seems better than the 3 ones. As this is a unsupervised problem we can't really know for sure which one is the best in real life but by looking at the data it's safe to say that 5 would be our choice. \n\nWe can analyze our 5 clusters in detail now:\n\n- `Label 0` is low income and low spending\n- `Label 1` is high income and high spending\n- `Label 2` is mid income and mid spending\n- `Label 3` is high income and low spending\n- `Label 4` is low income and high spending\n\nAlso let's see them more clearly with swarmplot:
"We can clearly see our clusters as we indicated before.\n\n## Hierarchical Clustering\n\n## Agglomerative\n\nWe will be looking at a clustering technique, which is Agglomerative Hierarchical Clustering. Agglomerative is the bottom up approach which is more popular than Divisive clustering.  \nWe will also be using Complete Linkage as the Linkage Criteria. \n\nThe  Agglomerative Clustering  class will require two inputs:\n\n     n_clusters: The number of clusters to form as well as the number of centroids to generate. \n     linkage: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. \n     \n         Value will be: 'complete'  \n         Note: It is recommended that try everything with 'average' as well \n    \n"
"\n### Dendrogram Associated for the Agglomerative Hierarchical Clustering\nRemember that a distance matrix contains the  distance from each point to every other point of a dataset . \nWe can use the function  distance_matrix,  which requires two inputs. \nRemember that the distance values are symmetric, with a diagonal of 0's. This is one way of making sure your matrix is correct. "
"Using the  linkage  class from hierarchy, pass in the parameters:\n\n     The distance matrix \n     'complete' for complete linkage \n"
"A Hierarchical clustering is typically visualized as a dendrogram as shown in the following cell. Each merge is represented by a horizontal line. The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where cities are viewed as singleton clusters. \nBy moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering. "
"We used __complete__ linkage for our case, let's change it to __average__ linkage to see how the dendogram changes."
"## Density Based Clustering (DBSCAN)\n\nMost of the traditional clustering techniques, such as k-means, hierarchical and fuzzy clustering, can be used to group data without supervision. \n\nHowever, when applied to tasks with arbitrary shape clusters, or clusters within cluster, the traditional techniques might be unable to achieve good results. That is, elements in the same cluster might not share enough similarity or the performance may be poor.\nAdditionally, Density-based Clustering locates regions of high density that are separated from one another by regions of low density. Density, in this context, is defined as the number of points within a specified radius.\n\nIn this part, the main focus will be manipulating the data and properties of DBSCAN and observing the resulting clustering.\n\n### Modeling\nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. This technique is one of the most common clustering algorithms  which works based on density of object.\nThe whole idea is that if a particular point belongs to a cluster, it should be near to lots of other points in that cluster.\n\nIt works based on two parameters: Epsilon and Minimum Points  \n__Epsilon__ determine a specified radius that if includes enough number of points within, we call it dense area  \n__minimumSamples__ determine the minimum number of data points we want in a neighborhood to define a cluster."
"As we can see DBSCAN doesn't perform very well because the density in our data is not that strong. Label -1 means outliers so it will appear most as outliers. We may have performed better if we had had a bigger data.\n\n## Mean Shift Algorithm\n\nMeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\n\nThe algorithm automatically sets the number of clusters, instead of relying on a parameter bandwidth, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided estimate_bandwidth function, which is called if the bandwidth is not set."
## Wrap Up All in One Place\n\nLet's visualize all the algorithms we used so far and see their clustering distributions.
## Conclusions\n\nThe Scikit learn official website provides the math behind the algorithms and if you're not sure which algorithm you want to use check their usage [here](https://scikit-learn.org/stable/modules/clustering.html)\n\nDon't forget to upvote if you like my kernel :)
# **6. Import libraries** \n\n[Table of Contents](#0.1)\n
### Ignore warnings\n
"\n# 1. Importing the necessary libraries\n\nIncase you fork the notebook, make sure to keep the Internet in `ON` mode."
# 2. Reading the Image datasets
## Location of imaged site w.r.t gender
## Age Distribution of patients
# 2. Importing Libraries 📚\n👉 Importing libraries that will be used in this notebook.
"# 3. Reading Data Set 👓\n👉 After importing libraries, we will also import the dataset that will be used."
"# Categorical Feature Encoding:\n\n## Introduction:\n\nIn most data science problems, our datasets will contain categorical features. Categorical features contain a finite number of discrete values. How we represent these features will have an impact on the performance of our model. Like in other aspects of machine learning, there are no silver bullets. Determining the correct approach, specific to our model and data is part of the challenge.\n\nThis tutorial aims to cover a few of these methods. We begin by covering a straight-forward technique before tackling more complicated lesser-known approaches.\n\n**List of methods covered**:\n1. One-Hot Encoding\n2. Feature Hashing\n3. Binary Encoding\n4. Target Encoding\n5. Weight of Evidence"
"For this tutorial, we will be using the '[Amazon.com Employee Access Challenge](https://www.kaggle.com/c/amazon-employee-access-challenge)' dataset. This binary classification dataset is made up of strictly categorical features, which are already converted into numerals, making it a particularly suitable choice to explore various encoding techniques. To simplify things we will only be using a subset of the features for this demonstration."
"Since ""Age"" is (right) skewed, using the mean might give us biased results by filling in ages that are older than desired.  To deal with this, we'll use the median to impute the missing values. "
"There are only 2 missing values for ""Embarked"", so we can just impute with the port where most people boarded."
"By far the most passengers boarded in Southhampton, so we'll impute those 2 NaN's w/ ""S""."
### 2.4 Final Adjustments to Data (Train & Test)
## Import Libraries\n\n** Import the libraries you usually use for data analysis.**
## Get the Data
\nIt's time to create some data visualizations!\n\n** Create a scatterplot of Grad.Rate versus Room.Board where the points are colored by the Private column. **
**Create a scatterplot of F.Undergrad versus Outstate where the points are colored by the Private column.**
"** Create a stacked histogram showing Out of State Tuition based on the Private column. Try doing this using [sns.FacetGrid](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.FacetGrid.html). If that is too tricky, see if you can do it just by using two instances of pandas.plot(kind='hist'). **"
**Create a similar histogram for the Grad.Rate column.**
** Notice how there seems to be a private school with a graduation rate of higher than 100%.What is the name of that school?**
Loading the libraries
## 1.2. Read data 
Missing values : \n* Insulin = 48.7% - 374\n* SkinThickness = 29.56% - 227\n* BloodPressure = 4.56% - 35\n* BMI = 1.43% - 11\n* Glucose = 0.65% - 5
"OK, all missing values are encoded with NaN value"
**To fill these Nan values the data distribution needs to be understood against the target**. 
A **correlation matrix** is a table showing correlation coefficients between sets of variables. Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj). This allows you to see which pairs have the highest correlation.
"\n\n\nThe distribution of the `num_sold` column seems to be right skewed with some few outliers selling more than 800 books. It is extremely typical for skewed distributions to have one tail that is significantly longer or dragged out compared to the other tail. A distribution is said to be ""skewed right"" if the tail is to the right. Lower or upper boundaries on the data frequently cause skewed data. In other words, data with a lower bound are frequently skewed right, whereas data with an upper bound are typically biased left. Start-up effects can also cause skewness. For instance, some processes in reliability applications could experience a high rate of first failures, which could result in left skewness. On the other side, a dependability process can have a protracted startup phase where failures are uncommon and the data would be right-skewed. \n\nThe following recommendations should be followed if the histogram shows that the data set is right-skewed:\n        Compute and publish the sample mean, sample median, and sample mode to quantitatively summarize the data.\n        Consider a normalizing transformation such as the Box-Cox transformation\n        \n        Choose the best-fit distribution (rightly skewed) from the\n            \n                Weibull family (for the maximum)\n                Gamma Family\n                Chi-square family\n                Lognormal family\n                Power lognormal family\n                \n        \n    \n\n\nFor more in-depth information about this, You can visit this [link](https://www.itl.nist.gov/div898/handbook/eda/section3/eda33e6.htm#:~:text=For%20skewed%20distributions%2C%20it%20is,is%20on%20the%20left%20side.).\n"
"A violin plot, which depicts data peaks, is a cross between a box plot and a kernel density plot. It is used to show how numerical data is distributed. Violin plots provide summary statistics as well as the density of each variable, unlike box plots, which can only show summary statistics. In the violin plot, there are more outsude points within the Kaggle Mart feature compared to the Kaggle Rama. It can be also noted that the former has a wider Inter Quartile Range (IQR) compared to the latter. \n\n![Violin Plots](https://miro.medium.com/max/780/1*TTMOaNG1o4PgQd-e8LurMg.png)\n    \n\nYou can read about an in-depth explanation of violin plots in this link."
\n    📌 Hover your mouse in the graph to examine the data distributions.\n
\n    📌 Drag the slider or press the play button to see the different graphs per country.\n
### Feature importance\nLet us also take a very quick look at the feature importance too:
"Where here the `F score` is a measure ""*...based on the number of times a variable is selected for splitting, weighted by the squared improvement to the model as a result of each split, and averaged over all trees*."" [1] Note that these importances are susceptible to small changes in the training data, and it is much better to make use of [""GPU accelerated SHAP values""](https://www.kaggle.com/carlmcbrideellis/gpu-accelerated-shap-values-jane-street-example), incorporated with version 1.3 of XGBoost.\n### Links\n* XGBoost: [documentation](https://xgboost.readthedocs.io/en/latest/index.html), [GitHub](https://github.com/dmlc/xgboost).\n* LightGBM: [documentation](https://lightgbm.readthedocs.io/en/latest/index.html), [GitHub](https://github.com/microsoft/LightGBM).\n* CatBoost: [documentation](https://catboost.ai/docs/), [GitHub](http://https://github.com/catboost).\n\n### Videos\nFor those who enjoy learning via videos, Josh Starmer on his YouTube channel [StatQuest](https://www.youtube.com/c/joshstarmer) has created some very accessible material:\n* [Gradient Boost Part 1 (of 4): Regression Main Ideas](https://youtu.be/3CC4N4z3GJc)\n* [Gradient Boost Part 2 (of 4): Regression Details](https://youtu.be/2xudPOBz-vs)\n* [Gradient Boost Part 3 (of 4): Classification](https://youtu.be/jxuNLH5dXCs)\n* [Gradient Boost Part 4 (of 4): Classification Details](https://youtu.be/StWY5QWMXCw)\n* [XGBoost Part 1 (of 4): Regression](https://youtu.be/OtD8wVaFm6E)\n* [XGBoost Part 2 (of 4): Classification](https://youtu.be/8b1JEDvenQU)\n* [XGBoost Part 3 (of 4): Mathematical Details](https://youtu.be/ZVFeW798-2I)\n* [XGBoost Part 4 (of 4): Crazy Cool Optimizations](https://youtu.be/oRrKeUCEbq8)\n\n### Related kaggle notebooks\n* [""GPU accelerated SHAP values with XGBoost""](https://www.kaggle.com/carlmcbrideellis/gpu-accelerated-shap-values-jane-street-example)\n* [""Automatic tuning of XGBoost with XGBTune""](https://www.kaggle.com/carlmcbrideellis/automatic-tuning-of-xgboost-with-xgbtune)\n* [""20 Burning XGBoost FAQs Answered to Use Like a Pro""](https://www.kaggle.com/bextuychiev/20-burning-xgboost-faqs-answered-to-use-like-a-pro) written by [BEXGBoost](https://www.kaggle.com/bextuychiev)\n* [""A Guide on XGBoost hyperparameters tuning""](https://www.kaggle.com/prashant111/a-guide-on-xgboost-hyperparameters-tuning) by [Prashant Banerjee](https://www.kaggle.com/prashant111)\n\n### References\n\n[1] [J. Elith, J. R. Leathwick, and T. Hastie ""*A working guide to boosted regression trees*"", Journal of Animal Ecology **77** pp. 802-813 (2008)](https://doi.org/10.1111/j.1365-2656.2008.01390.x)\n\n### Appendix: The RMSLE evaluation metric\nFrom the competition [evaluation page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) we see that the metric we are using is the root mean squared logarithmic error (RMSLE), which is given by\n\n$$ {\mathrm {RMSLE}}\,(y, \hat y) = \sqrt{ \frac{1}{n} \sum_{i=1}^n \left(\log (1 + \hat{y}_i) - \log (1 + y_i)\right)^2} $$\n\nwhere $\hat{y}_i$ is the predicted value of the target for instance $i$, and $y_i$\nis the actual value of the target for instance $i$.\n\nIt is important to note that, unlike the RMSE, the RMSLE is asymmetric; penalizing much more the underestimated predictions than the overestimated predictions. For example, say the correct value is $y_i = 1000$, then underestimating by 600 is almost twice as bad as overestimating by 600:"
A visual inspection of the number of rows:
"1. Why small datasets lead to overfitting?\n\nThe goal of a machine learning model is to **generalize** patterns in training data so that you can correctly predict new data that has never been presented to the model. Overfitting occurs when a model adjusts excessively to the training data, seeing patterns that do not exist, and consequently performing poorly in predicting new data:\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/under_over.png)\nSource: https://medium.com/@shubhapatnim86/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5\n\n\n\nThe fewer samples for training, the more models can fit our data. In an extreme example (a), for just one training point, any model will be able to ""explain"" it, however simple or complex the model may be. As we get to have more samples (b, c), fewer models are able to explain them:\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/few_samples.png)\nSource: https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d\n\n\n\nThat way, for a dataset with only 250 samples, we need to be very careful not to be fooled by overfitting. In this kernel we will see some tips that can help.\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/meme1.jpeg)"
We can plot a chart of the importances:
"In the next example, we will try to identify the most important features by successively training a model and recursively eliminating those that do not contribute to a good final solution (according to the selected model).\n\nWe will use the recursive feature elimination RFE from the scikit-learn library and the XGBClassifier model:"
5. Balance the dataset with synthetic samples (SMOTE)\n\n\n\nLet's look at the distribution of target values:
"In addition to being extremely small, our training dataset has the unbalanced target binary variable, which can undermine some models' predictability. We will perform an oversampling, which consists of creating new samples to increase the 0 minority class. For this we will use the SMOTE technique.\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n\n![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)\nSource: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n\n\n\nWe'll use the SMOTE implementation from the library imbalanced-learn, with the parameter ratio='minority' to resample the minority class:"
"6. Combine models for the final submission\n\nCombine the prediction of several models or the same model with different values of hyperparameters reduces variance and enhances generalization.\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/combine.jpg)\nSource: https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89\n\n\n\nOften, combining weak models that are poorly correlated with each other can lead to superior results than a strong individual model. There are several ways to do this. The simplest is to perform a weighted average of the various predictions:"
### ⬇️😉 Custom Functions Below 
# 2. The Data\n\n> Let's observe the structure of the data first:\n\n
"So now, our distribution would look like this:"
"Ok, now let's look at the `standard_error` in terms of segmentation.\n\nYou can observe that indeed the error decreases a little for medium complexity. However, we can state that usually **we'll encounter a `standard_error` of 0.5 on a normal rating**."
You can see how important rank features are in this competition.\n\n## 3. Feature importances of Tree models\n\nTree models can output feature importances.
"There are several options for measuring importance like ""split"" (How many times the feature is used to split), ""gain"" (The average training loss reduction gained when using a feature for splitting).\n\nHowever, sometimes it doesn't represent the actual contribution.\n\n\n\n> To our dismay we see that the feature importance orderings are very different for each of the three options provided by XGBoost! For the cover method it seems like the capital gain feature is most predictive of income, while for the gain method the relationship status feature dominates all the others. This should make us very uncomfortable about relying on these measures for reporting feature importance without knowing which method is best.\n\n\n    Interpretable Machine Learning with XGBoost\n\n\nThis is the background of Interpretable machine learning which is a field receiving a lot of attention recently. You can find papers and libraries here: [lopusz/awesome-interpretable-machine-learning](https://github.com/lopusz/awesome-interpretable-machine-learning)"
Snapshot of the artifacts created  \n\n![](https://drive.google.com/uc?id=16ROHOYdW3ewGESfCwewUWW8X3mvNbFKT)
Logging plots to W&B dashboard
# **Frequency Distribution of Categorical Features**\n\n
# **Distribution of Target Variable - Pressure**
# **Numerical Variables Vs Target**
# **Categorical Variables Vs Target**
# **Analysis for single breath_id**
Look at a random face image:
## The dataset and data loaders
"To plot the image, we need to unnormalize it and also permute it from (3, 224, 224) to (224, 224, 3). "
"To use the PyTorch data loader, we need to create a `Dataset` object.\n\nBecause of the class imbalance (many more fakes than real videos), we're using a dataset that samples a given number of REAL faces and the same number of FAKE faces, so it's always 50-50."
We see that in the training data only around 38.4% of the passengers managed to survive the disaster: this is an important value that we have to keep in mind.\n# Feature analysis and creation\nThe goal of this section is to gain a general understanding of our data to perform a more precise feature selection in the modeling part.  \nWe will thus explore one feature at a time in order to determine its importance in predicting if a passenger survived or not.\n## Sex\nWe see that around 65% of the passengers were male while the remaining 35% were female.  \nThe important thing to notice here is that the survival rate for women was four times the survival rate for men and this makes `Sex` one of the most informative features.  \nIt is not a case that the gender submission on its own scores 0.76555!
"## Pclass\nThere were three classes on the ship and from the plot we see that the number of passengers in the third class was higher than the number of passengers in the first and second classes combined.  \nHowever, the survival rate by class is not the same: more than 60% of first-class passengers and around half of the second class passengers were rescued, whereas 75% of third class passengers were not able to survive the disaster.  \nFor this reason, this is definitely an important aspect to consider."
"## Age\nDespite this column contains a lot of missing values, we see that in the training data the average age was just under 30 years.  \nHere is the plot of the age distribution in general compared to the one for the survivors and the deads."
"At a first look, the relationship between `Age` and `Survived` appears not to be very clear: we notice for sure that there is a peak corresponding to young passengers for those who survived, but apart from that the rest is not very informative.  \nWe can appreciate this feature more if we consider `Sex` too: now it is clearer that a good number of male survivors had less than 12 years, while the female group has no particular properties."
"Another interesting thing to look at is the relation between `Age`, `Pclass` and `Survived`.  \nWe see the influence of `Pclass` is the important one as there are no super clear horizontal patterns.  \nAlso, we note that there were not many children in the first class."
"After all these plots I am not sure about the importance of `Age` in a model: I guess we will see later, even though I am thinking of not using it.\n## Fare\nFrom the description, we see that the `Fare` distribution is positively skewed, with 75% of data under 31 and a maximum of 512.  \nJust to understand better this feature, the simplest idea here could be creating fare ranges using quartiles.  \nAt a first look, we notice that the higher the fare, the higher the possibility of surviving."
"However, when it came down to modeling, these fare categories did not help at all as they underfit quite substantially.  \nLooking at the more detailed plot below, we also see for example that all males with fare between 200 and 300 died: that is not what we would expect.  \nFor this reason, I left the `Fare` feature as it is in order to prevent losing too much information: at deeper levels of a tree, a more discriminant relationship might open up and it could become a good group detector."
"After seeing Erik's kernel [here](https://www.kaggle.com/erikbruin/titanic-2nd-degree-families-and-majority-voting), it reminded me I had not analyzed this feature deep enough.  \nWhen I printed the description, I should have also noticed that the minimum value for `Fare` is zero and that is a bit strange.  \nIs this information correct? Let's see who these passengers are."
### Tip 1.1. Import the most popular and useful main Python libraries
### Tip 1.2. Warnings - ignore all
# INTRODUCTION\n\nWe have a data which classified if patients have heart disease or not according to features in it. We will try to use this data to create a model which tries predict if a patient has this disease or not. We will use logistic regression (classification) algorithm.
## Read Data
"We'll take a look at the learning curves as always, and also inspect the best values for the loss and accuracy we got on the validation set. (Remember that early stopping will restore the weights to those that got these values.)"
# Your Turn #\n\nUse a neural network to [**predict cancellations in hotel reservations**](https://www.kaggle.com/kernels/fork/11887335) with the *Hotel Cancellations* dataset.
"## How Autoencoders work - Understanding the math and implementation\n\n### Contents \n\n\n1. Introduction\n\n    1.1 What are Autoencoders ? \n    1.2 How Autoencoders Work ? \n\n2. Implementation and UseCases\n\n    2.1 UseCase 1: Image Reconstruction \n    2.2 UseCase 2: Noise Removal \n    2.3 UseCase 3: Sequence to Sequence Prediction \n\n\n\n\n\n## 1. Introduction\n## 1.1 What are Autoencoders \n\nAutoencoders are a special type of neural network architectures in which the output is same as the input. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input (in other words, model the identity function). These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low-dimensional code that can be used by the decoder to reproduce the original input.\n\nA typical autoencoder architecture comprises of three main components: \n\n- **Encoding Architecture :** The encoder architecture comprises of series of layers with decreasing number of nodes and ultimately reduces to a latent view repersentation.  \n- **Latent View Repersentation :** Latent view repersents the lowest level space in which the inputs are reduced and information is preserved.  \n- **Decoding Architecture :** The decoding architecture is the mirro image of the encoding architecture but in which number of nodes in every layer increases and ultimately outputs the similar (almost) input.  \n\n![](https://i.imgur.com/Rrmaise.png)\n\nA highly fine tuned autoencoder model should be able to reconstruct the same input which was passed in the first layer. In this kernel, I will walk you through the working of autoencoders and their implementation.  Autoencoders are widly used with the image data and some of their use cases are: \n\n- Dimentionality Reduction   \n- Image Compression   \n- Image Denoising   \n- Image Generation    \n- Feature Extraction  \n\n\n\n## 1.2 How Autoencoders work \n\nLets understand the mathematics behind autoencoders. The main idea behind autoencoders is to learn a low level repersenation of a high level dimentional data. Lets try to understand the encoding process with an example.  Consider a data repersentation space (N dimentional space which is used to repersent the data) and consider the data points repersented by two variables : x1 and x2. Data Manifold is the space inside the data repersentation space in which the true data resides. "
"To repersent this data, we are currently using 2 dimensions - X and Y. But it is possible to reduce the dimensions of this space into lower dimensions ie. 1D. If we can define following : \n\n- Reference Point on the line : A  \n- Angle L with a horizontal axis  \n\nthen any other point, say B, on line A can be repersented in terms of Distance ""d"" from A and angle L.  "
"But the key question here is with what logic or rule, point B can be represented in terms of A and angle L. Or in other terms, what is the equation among B, A and L. The answer is straigtforward, there is no fixed equation but a best possible equation is obtained by the unsupervised learning process. In simple terms, the learning process can be defined as a rule / equation which converts B in the form of A and L. Lets understand this process from a autoencoder perspective. \n\nConsider the autoencoder with no hidden layers, the inputs x1 and x2 are encoded to lower repersentation d which is then further projected into x1 and x2. \n\n![](https://i.imgur.com/lfq4eEy.png)\n\n\n**Step1 : Repersent the points in Latent View Space**   \n\nIf the coordinates of point A and B in the data representation space are: \n\n- Point A : (x1A, x2A)  \n- Point B : (x1B, x2B)   \n\nthen their coordinates in the latent view space will be:   \n\n(x1A, x2A) ---> (0, 0)  \n(x1B, x2B) ---> (u1B, u2B)  \n\n- Point A : (0, 0)  \n- Point B : (u1B, u2B)   \n\nWhere u1B and u2B can be represented in the form of distance between the point and the reference point  \n\nu1B = x1B - x1A  \nu2B = x2B - x2A\n\n**Step2 : Represent the points with distance d and angle L **    \n\nNow, u1B and u2B can represented as a combination of distance d and angle L. And if we rotate this by angle L, towards the horizontal axis, L will become 0. ie.  \n\n**=> (d, L)**     \n**=> (d, 0)**   (after rotation)   \n\nThis is the output of the encoding process and repersents our data in low dimensions.  If we recall the fundamental equation of a neural network with weights and bias of every layer, then \n\n**=> (d, 0) = W. (u1B, u2B)**    \n==> (encoding)    \n\nwhere W is the weight matrix of hidden layer.  Since, we know that the decoding process is the mirror image of the encoding process. \n\n**=> (u1B, u2B) = Inverse (W) . (d, 0)**    \n==> (decoding)  \n\nThe reduced form of data (x1, x2) is (d, 0) in the latent view space which is obtained from the encoding architecture. Similarly, the decoding architecture converts back this representation to original form (u1B, u2B) and then (x1, x2). An important point is that Rules / Learning function / encoding-decoding equation will be different for different types of data. For example, consider the following data in 2dimentional space.  \n\n\n## Different Rules for Different data\n\nSame rules cannot be applied to all types of data. For example, in the previous example, we projected a linear data manifold in one dimention and eliminated the angle L. But what if the data manifold cannot be projected properly. For example consider the following data manifold view. "
"In this type of data, the key problem will be to obtain the projection of data in single dimention without loosing information. When this type of data is projected in latent space, a lot of information is lost and it is almost impossible to deform and project it to the original shape. No matter how much shifts and rotation are applied, original data cannot be recovered. \n\nSo how does neural networks solves this problem ? The intution is, In the manifold space, deep neural networks has the property to bend the space in order to obtain a linear data fold view. Autoencoder architectures applies this property in their hidden layers which allows them to learn low level representations in the latent view space. \n\nThe following image describes this property: \n\n![](https://i.imgur.com/gKCOdiL.png)\n\nLets implement an autoencoder using keras that first learns the features from an image, and then tries to project the same image as the output.  \n\n## 2. Implementation\n\n## 2.1 UseCase 1 : Image Reconstruction\n\n1. Load the required libraries\n"
"### 2. Dataset Prepration \n\nLoad the dataset, separate predictors and target, normalize the inputs."
Lets plot the original and predicted image\n\n**Inputs: Actual Images**
**Predicted : Autoencoder Output**
"So we can see that an autoencoder trained with 20 epoochs is able to reconstruct the input images very well. Lets look at other use-case of autoencoders - Image denoising or removal of noise from the image.  \n\n## 2.2 UseCase 2 - Image Denoising\n\nAutoencoders are pretty useful, lets look at another application of autoencoders - Image denoising. Many a times input images contain noise in the data, autoencoders can be used to get rid of those images. Lets see it in action. First lets prepare the train_x and val_x data contianing the image pixels. \n\n![](https://www.learnopencv.com/wp-content/uploads/2017/11/denoising-autoencoder-600x299.jpg)"
Before adding noise
After adding noise
"Lets now create the model architecture for the autoencoder. Lets understand what type of network needs to be created for this problem. \n\n**Encoding Architecture:**   \n\nThe encoding architure is composed of 3 Convolutional Layers and 3 Max Pooling Layers stacked one by one. Relu is used as the activation function in the convolution layers and padding is kept as ""same"". Role of max pooling layer is to downsample the image dimentions. This layer applies a max filter to non-overlapping subregions of the initial representation.  \n\n**Decoding Architecture:**   \n\nSimilarly in decoding architecture, the convolution layers will be used having same dimentions (in reverse manner) as the encoding architecture. But instead of 3 maxpooling layers, we will be adding 3 upsampling layers. Again the activation function will be same (relu), and padding in convolution layers will be same as well.  Role of upsampling layer is to upsample the dimentions of a input vector to a higher resolution / dimention. The max pooling operation is non-invertible, however an approximate inverse can be obtained by recording the locations of the maxima within each pooling region. Umsampling layers make use of this property to project the reconstructions from a low dimentional feature space.   \n\n"
Plot statistics for number of kernels:
### 2. How does number of views and number of comments affect number of votes?
"Find out correlation between number of views, number of comments and number of votes:"
"We can see that votes, comments and views are __highly correlated__. So my first assumption would be that __we should get as many views of the kernels as possible to gain votes__.\n Let's try to visualize dependency between views and votes:"
"At first let's look at correlations between TotalKernelVotes, TotalDatasetVotes and	TotalDatasetDownloads."
As we can see there is no correlation between number of kernel votes and number of votes or downloads for a dataset.\n Let's also make scatter plots:
"**Reading a CT Scan**\n-----------------\nThe input folder has three things, one is the sample_images folders which has the sample CT Scans. The `stage1_labels.csv` contains the cancer ground truth for the stage 1 training set images and `stage1_sample_submission.csv` shows the submission format for stage 1. "
"Each 3D CT Scan consists of many slices, whose number depends on the resolution of the scanner and each slice has a Instance Number associated with it which tells the index of the slice from the top. All the dicom files for a CT Scan are inside one folder having the CT Scan's name. Now we will read all the dicom slices for a scan and then stack them with respect to their Instance Number to get the 3D Lung CT Scanned Image."
"## Segmentation of Lungs ##\nAfter reading the CT Scan, the first step in preprocessing is the segmentation of lung structures because it is obvious that the regions of interests lies inside the lungs. It is visible that the lungs are the darker regions in the CT Scans. The bright region inside the lungs are the blood vessels or air. A threshold of 604(-400 HU) is used at all places because it was found in experiments that it works just fine. We segment lung structures from each slice of the CT Scan image and try not to loose the possible region of interests attached to the lung wall. There are some nodules which may be attached to the lung wall.\n\nI will first explain a common method using simple Image Processing and Morphological operations to segment the lungs and then will give references and summaries to good links of papers. "
The `get_segmented_lungs` function segments a 2D slice of the CT Scan. I have outputted the slice after all steps for better visualisation and understanding of the code and applied operations.
"After reading the 3D CT Scans, we will first segment the lungs and then generate the binary mask of nodule regions. This will be done by the `create_nodule_mask` function.  The `draw_circle` function is used to mark the nodule regions in the binary mask. 'cands' are the list of nodule points with the radius given in the `annotation.csv` file of LUNA16 dataset. At the end we save the resized CT Scan with its segmented lungs and binary mask of nodules."
"After preprocessing the dataset, the next thing is to train the model for segmentation. The model is written in keras in the `unet_model` function. It takes a 2D slice as input and returns a 2D slice of the same size as output. There are few things to be kept in mind while training\n\n - We wont use the slices that has no nodule region in the mask for training.\n - Dataset augmentation is very important because are nodules are generally circular or spherical in shape and are of different radius. \n - Since the nodule regions are very less, the dataset is skewed. Thus, we should weight the loss function accordingly.\n - The model may overfit on the training dataset. Thus, Dropout or Spatial Dropout are used to avoid overfitting."
**Import all required libraries**\n===============================
**Load Train and Test data**\n============================
Lets plot 10th label.
Oh its 3 !
**We can also visualize the partial dependence of two features at once using 2D Partial plots.**
"# 3. SHAP Values\n\nSHAP which stands for **SH**apley **A**dditive ex**P**lanation, helps to break down a prediction to show the impact of each feature. It is based on Shapley values, a technique used in game theory to determine how much each player in a collaborative game has contributed to its success[¹](https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80). Normally, getting the trade-off between accuracy and interpretability just right can be a difficult balancing act but SHAP values can deliver both.\n\nSHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we’d make if that feature  took  some baseline value.\n\nShap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature). Let’s say we wanted to know what was the prediction when the insulin level was 150 instead of some fixed baseline no. If we are able to answer this, we could perform the same steps for other features as follows:\n\n`sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values`"
 Titanic - Machine Learning from Disaster\n\n*Top 3% Titanic solution - For this last version best model score is 0.81100*.\n![](https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/2-rms-titanic-ship-plans-jose-elias-sofia-pereira.jpg)
"Table Of Content\n* [1. EDA & Feature Engeneering](#1_bullet)\n    * [ 1.1 Passengers location analisys](#1.1_bullet) - Survival for different Deck / Cabin numbers\n    * [ 1.2 Groups and family bonds analisys](#1.2_bullet) - Ticket numbers analisys, Names / Surnames\n    * [ 1.3 Personal features analisys](#1.3_bullet) - Age / Status analisys\n* [2. Data preparation](#2_bullet)\n    * [ 2.1 Filling None values](#2.1_bullet)\n    * [ 2.2 Encoding features and droping unnecessary](#2.2_bullet)\n* [3. Model development](#3_bullet)\n    * [ 2.1 Catboost baseline](#2.1_bullet)\n    \n"
"Now we gonna plot some developed ""coordinates"" of the passangers, to find some patterns in survival."
"We can see some patterns in the data:\n  - For example, all of the passangers on the ""1"" side of the Deck D survived.\n  - For only some of decks the ""closer"" location to zero may cause the better survival (maby those passangers was closer to the ladders)"
"In the end - we have to fillna for ""Side"" feature with 0 - refering to unknown side of the ship"
"## Convolutional Neural Networks\nIf you want to apply machine learning to image recognition, convolutional neural networks (CNN) is the way to go. It has been sweeping the board in competitions for the last several years, but perhaps its first big success came in the late 90's when [Yann LeCun][1] used it to solve MNIST with 99.5% accuracy. I will show you how it is done in Keras, which is a user-friendly neural network library for python.\n\nMany other notebooks here use a simple fully-connected network (no convolution) to achieve 96-97%, which is a poor result on this dataset. In contrast, what I will show you here is nearly state-of-the-art. In the Kernel (<20 minutes training) we will achieve 99%, but if you train it overnight (or with a GPU) you should reach 99.5. If you then ensemble over several runs, you should get close to the best published accuracy of 99.77% . (Ignore the 100% results on the leaderboard; they were created by learning the test set through repeat submissions)\n\nHere goes:\n\n\n  [1]: http://yann.lecun.com/exdb/lenet/"
"If you don't already have [Keras][1], you can easily install it through conda or pip. It relies on either tensorflow or theano, so you should have these installed first. Keras is already available here in the kernel and on Amazon deep learning AMI.\n\n  [1]: https://keras.io/"
"Each data point consists of 784 values. A fully connected net just treats all these values the same, but a CNN treats it as a 28x28 square. Thes two graphs explain the difference: It's easy to understand why a CNN can get better results."
"We now reshape all data this way. Keras wants an extra dimension in the end, for channels. If this had been RGB images, there would have been 3 channels, but as MNIST is gray scale it only uses one.\n\nThis notebook is written for the tensorflow channel ordering. If you have Keras installed for Theano backend, you might start seeing some error message soon related to channel ordering. This can easily be [solved][1].\n\n\n  [1]: https://keras.io/backend/#set_image_dim_ordering"
The survival probability visualization for each pclass using the above method is as follows.
## Awesome Heatmap\n\n- A **heat map (or heatmap)** is a data visualization technique that shows magnitude of a phenomenon as color in two dimensions.\n\n---\n\n### Simple Explanation\n\n- (tip) `mask` (remove symetric)\n- (tip) `square` (to make x-y scale same)\n- (tip) `colormap (diverging colormap)\n- (tip) text as watermark\n
### 2.2 Import Libraries
### 2.3 Import Data
### 2.4 Some Visualisations
### 2.5 Feature engineering
"# 1. Introduction + Set-up\n\nMachine learning has a phenomenal range of applications, including in health and diagnostics. This tutorial will explain the complete pipeline from loading data to predicting results, and it will explain how to build an X-ray image classification model from scratch to predict whether an X-ray scan shows presence of pneumonia. This is especially useful during these current times as COVID-19 is known to cause pneumonia.\n\nThis tutorial will explain how to utilize TPUs efficiently, load in image data, build and train a convolution neural network, finetune and regularize the model, and predict results. Data augmentation is not included in the model because X-ray scans are only taken in a specific orientation, and variations such as flips and rotations will not exist in real X-ray images. For a tutorial on image data augmentation, check out this [tutorial](https://www.kaggle.com/amyjang/tensorflow-data-augmentation-efficientnet).\n\nRun the following cell to load the necessary packages. Make sure to change the Accelerator on the right to `TPU`."
"We need a Google Cloud link to our data to load the data using a TPU. While we're at it, we instantiate constant variables. It is generally better practice to use constant variables instead of hard-coding numbers."
Define the method to show the images in the batch.
"As the method takes in numpy arrays as its parameters, call the numpy function on the batches to return the tensor in numpy array form."
"# Lets optimize\n\n> The objective of this NB is to showcase some **optimization (Processing & Memory optimization)** while working with Pandas \n\n> The NB includes optimization techniques involving the use of **Pandas** and **Numpy** mainly\n\n> I am also planning to add **Python optimization** techniques & maybe something on **ML & NLP models optimization** depending on the feedback this NB receives\n\n-----------------\n\n\n\n I am no expert in optimizations, the whole point of the NB is to optmize the code especially building data pipelines\n    \nPlease feel free to share any feedback or share any corrections  \n    \nFor a few usecases I have used TPS March 2022 and  Ubiquant market prediction data  & others Just to make sure these optimizations work for real data as well \n\n\n\n\n\nTricks with emojis must not be missed\n"
"#  📌Datatypes\n\n\nWe generally dont give much importance to datatypes in datasets, but the fact is using correct datatypes can save us a lot of memory and time which working/loading the datasets\n\n\nFollowing tricks can be used to optimize using correct datatypes\n\n- Float & Int datatype\n- datetime vs string\n- object vs category\n--------------\n\nWill use `ubiquant-market-prediction` dataset for this section\nhttps://www.kaggle.com/c/ubiquant-market-prediction"
# 8. Use interactive plots   [↑](#top)\n\nInteractive plots are way better than static plots. [Plotly](https://plotly.com/) provides a huge set of interactive plots that can make your presentation much more appealing.
You could also use [matplotlib](https://matplotlib.org/) but the `%matplotlib notebook` magic is not working in Kaggle.
# 9. Use CSS styling inside HTML  [↑](#top)\n\nEvery markdown supports HTML and thus you can customize your cells using CSS. Look around my `style` inside the next markdown to get a reference. You can research a little more about HTML and CSS to improve your markdowns.
"You will use the `seaborn` library for visual analysis, so let's set that up too:"
We can see that the target classes are balanced. That's great!\n\nLet's split the dataset by target values. Can you already spot the most significant feature by just looking at the plot?
"You can see that the distribution of cholesterol and glucose levels great differs by the value of the target variable. Is this a coincidence?\n\nNow, let's calculate some statistics for the feature unique values:"
# Table of Contents\n1. [Understanding Data](#sec1)\n    * [1.2 Univariate Analysis](#sec1.2)  \n    * [1.3 Bi-variate Analysis](#sec1.3)  \n2. [Data Preprocessing](#sec2)  \n    * [2.1 Removing redundant features](#sec2.1)\n    * [2.2 Dealing with Outliers](#sec2.2)\n    * [2.3 Filling Missing Values](#sec2.3)\n3. [Feature Engineering](#sec3)\n4. [Modeling](#sec4)\n    * [4.1 Scaling of features](#sec4.1)\n    * [4.2 Ensemble Algorithms](#sec4.2)
\n# [1. Understanding Data](#sec1)
"### Numeric Features\n\nFor numerical features, we are always concerned about the **distribution** of these features, including the **statistical characteristics** of these columns e.g mean, median, mode. Hence  we will usually use **Distribution plot** to visualize their data distribution. **Boxplots** are also commonly used to unearth the statistical characteristics of each feature. More often than not, we use it to look for any outliers that we might need to filter out later on during the preprocessing step."
"Some of the Variables with mostly 1 value as seen from the plots above:\n1. BsmtFinSF2\n2. LowQualFinSF\n3. EnclosedPorch\n4. 3SsnPorch\n5. ScreenPorch\n6. PoolArea\n7. MiscVal  \n\nAll these features are highly skewed, with mostly 0s. Having alot of 0s in the distribution doesnt really add information for predicting Housing Price. Hence, we will remove them during our preprocessing step"
"### Categorical Features\n\nIn the case of categorical features, we will often use countplots to visualize the count of each distinct value within each features. We can see that some categorical features like **Utilities, Condition2** consist of mainly just one value, which does not add any useful information. Thus, we will also remove them later on."
"Univariate Analysis helps us to understand all the features better, on an individual scale. To further deepen our insights and uncover potential pattern in the data, we will also need to find out more about the relationship between all these features with one another, which brings us to our next step in our analysis - Bivariate Analysis"
### Correlation Matrix
**Highly Correlated variables**:\n* GarageYrBlt and YearBuilt\n* TotRmsAbvGrd and GrLivArea\n* 1stFlrSF and TotalBsmtSF\n* GarageArea and GarageCars\n\nFrom the correlation matrix we have identified the above variables which are highly correlated with each other. This finding will guide us in our preprocessing steps later on as we aim to remove highly correlated features to avoid performance loss in our model
"### Scatterplot\nUsing scatterplot can also help us to identify potential linear relationship between Numerical features. Although scatterplot does not provide quantitative evidence on the strength of linear relationship between our features, it is useful in helping us to visualize any sort of relationship that correlation matrix could not calculate. E.g Quadratic, Exponential relationships. "
"\n# [2. Data Processing](#sec2)\n\nNow that we have more or less finished analysing our data and gaining insights through the various analysis and visualization, we will have to leverage on these insights to guide our preprocessing decision, so as to provide a clean and error-free data for our model to train on later on.  "
"## 2.1 Data related work across fields\nSurely not all professions are made equally, so I wondered **which fields tend to have more professionals involved in data analysis activities?** The below graph shows the percentage of individuals within each field that work in data-related roles on a daily basis. **Additional details are included in the hover text.**\n\n    Source: Kaggle 2020 DS and ML Survey - Q23. Select any activities that make up an important part of your role at work.\n"
"Unsurprisingly fields like **Data Scientists**, with higher percentages of individuals working with data and contributing towards the bulk of the data professionals. **Data Analystst, Business Analysts, ML Engineers and Data Engineers are a close second** with high percentages of data professionals, however due to the fewer number of respondents in these categories they tend to contribute less towards our sum total."
"## 2.4 Data roles in different fields\n\nOne thing I always wondered with all these different roles, what exactly do they work on, on a day-to-day basis - It can't all be the same regardless of the job title right?\n\nAnother question that has always bothered me(and I'm sure many of you too) - ""**What's the difference between the roles data scientist, data engineer, data analyst and business analyst?**"" Lets see if we can answer this in the following graph of data roles.\n\n\n    Source: Kaggle 2020 DS and ML Survey - Q23. Select any activities that make up an important part of your role at work.\n"
"As for the answer to our previous question:\n- For all the four titles(first 4 bar charts), the **main focus remains analysing data for business decisions**.\n- **Data analysts and Business Analysts seem to be almost identical** with respect to the role distribution and average number of roles per individual.\n- Data engineers seem to be similar to the others, except for their significant **focus on building infrastructure for the data management and analysis**.\n- Finally data scientists also have a higher focus on building ML prototypes in a new areas as well as have a relatively higher number of roles on average. Both these traits make it **very similar to Machine Learning Engineers**(except for the focus on impacting business decisions). \n\nAnother interesting detail is machine learning engineers, true to their name, show higher numbers across the board for all the ML related roles. Even their work in research on state of the art ML methods, is only second to research scientists themselves."
## 2.5 Ages of data professionals\n\nLets see if the age data will produce any interesting findings. \n\n\n    Source: Kaggle 2020 DS and ML Survey - Q1. What is your age (# years)?\n
"Some interesting points to note:\n- 70% of students in the age group 18-21 are **pursuing a Bachelor's degree**.\n- In the **22-24 age group, the focus shifts towards Masters**, having a marked jump from 11% to 45% in this age group. Meanwhile the percentage of Bachelors degree holders drops to below 40% for the first time.\n- In later age, groups the proportion Masters still remains around the 40-60% mark, with Doctoral degrees as a close second, while the numbers for bachelors slowly decreases.\n- On the data professional side of things, we see that most fields have a **majority of their respondents in the 25-29 age group**. \n- The **sole exception to this is Project Managers, which peaks at 35-39**. Most likely because these are positions you would only enter after a fair deal of experience. This also helps explain their relatively higher pays that we noted in the previous section."
"## 2.6 Education required\n\nWith all the disciplines that go into data analysis and machine learning, I always wondered what education qualifications were the norm in these professions and possibly what education I should pursue to thrive in these areas. Let's see how the fields stack up in terms of their educational make-up.\n\n\n    Source: Kaggle 2020 DS and ML Survey - Q4. What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\n"
"The general trend seems to be that more than half of the individuals in each field seem to have either a **Master's or Bachelor's degree** in almost all fields. A notable exception being Research Scientists where more than half the respondents have doctoral degrees and when you include Masters and Professional degree holders this number comprises of 93% of the total professionals in their field.\n\nSome additional points of interest:\n- In most fields **Masters degree seems to be the way to go** with 30-55% of respondents having one.\n- A **bachelors degree** seems to be a close second with numbers around the 20-30% mark.\n- **Professional degrees** seem to be a less common choice with around 4-5% having one. Hovewer they do help to get jobs because professional degree holders have the **second lowest fraction of unemployed individuals**, after Doctoral degree holders."
## 2.7.1 Handling Regional bias in pays\n\nSalaries aren't really comparable by simply converting to USD and then drawing conclusions from this data. A major factor in this is the **cost of living** in different countries - Companies situated in places where costs of living are higher pay their employees more to compensate for this. A simple example would be how 100$ would have **different purchasing power in different places**.\n\nI'll handle this disparity by dividing each respondents salary by their **country's cost of living index** and then converting this value to an equivalent amount in USD. A simple demonstration of how this affects reported salaries is shown below\n\n\n    Source: Countries Dataset 2020 - Numbeo: Cost of living index\n
"We see how a country like India, where the cost of living is much lower than that of the US, has the majority of its individuals towards the lower end of the graph. However on normalising with the cost of living data, we see how a much larger portion of Indians enter the higher pay brackets. In fact the salaries in India are **almost tripled to get their corrected values**.\n\nWhile this won't be a 100% accurate it gives us a much **better basis on which we may compare salaries**. And in this case we would probably be safe to assume that  in general, companies in the US just do pay their data experts more."
"## 2.8 Machine Learning practices\n\nSo.. you want to work in machine learning? Well, lets give you a better idea of the type of ML-related work you might encounter in certain fields. \n\n\n    Sources: Kaggle 2020 DS and ML Survey - \n     Q22. Does your current employer incorporate machine learning methods into their business?\n    \n"
"In case you are unclear on what exactly ""production"" means I'll give you a little background here - Traditionally, when a data scientist would create an ML model they would often do so in a sandbox or development environment. This is **great for experimenting and reiterating quickly**, however in order for the consumers to use it, the model must be deployed so that it **scales well with increased usage volumes and is always available** for use by others. This deployment however, requires a skill-set different than what most data scientists possess and often this **burden falls on the Software Engineers**. \n\nIn order for a ML product to be put into production both teams must work closely and this cooperation is what a lot of companies dealing in ML products strive for. This back-and-forth forms the basis by which they manage to build and maintain well established ML models. (You can read more about this [here](https://stackoverflow.blog/2020/10/12/how-to-put-machine-learning-models-into-production/).)\n\nNow back to the graph - an interesting observation is how **Machine Learning Engineers** ranked second here in terms of well-established ML production practices, this is the exact situation where ML Engineers shine - they are a **mix of both Software Engineers and Data Scientists** whose main role is **to take machine learning models and make them scalable in production**.\n\nThe left-most bar also shows us what proportion of the companies don't utilise ML methods in their daily activities."
"# High level insight on genetic variations\nNote: As this is my first published Kernel, am open to suggestions. If this helped you, some upvotes would be very much appreciated.\n\n### Library and Settings\nImport required library and define constants"
### Files
Lets have a look at some genes that has highest number of occurrences in each class. 
Some points we can conclude from these graphs:\n 1. BRCA1 is highly dominating Class 5 \n 2. SF3B1 is highly dominating Class 9\n 3. BRCA1 and BRCA2 are dominating Class 6
As we can see there are some entries without any text data. \nNow let us get distribution of text count for each class
Distribution looks quite interesting and now I am in love with violin plots.\nAll classes have most counts in between 0 to 20000. Just as expected. \nThere should be some 
Frequently occurring terms for each class
We need to know more about text. Tf-idf is known as one good technique to use for text transformation and get good features out of text for training our machine learning model. [Here][1] you can find more details about tf-idf and some useful code snippets. \n\n\n  [1]: https://buhrmann.github.io/tfidf-analysis.html
Lets plot out some top features we got using Tf-Idf for each class
"For EDA on image datasets I think one should at least examine the label distribution, the images before preprocessing and the images after preprocessing. Through examining these three aspects we can get a good sense of the problem. Note that the distribution on the test set can still vary wildly from the training data."
We will visualize a random image from every label to get a general sense of the distinctive features that seperate the classes. We will take this into account and try to enhance these features in our preprocessing. For these images there some to be increasingly more spots and stains on the retina as diabetic retinopathy worsens.
## Preprocessing 
# 2 | MAIN DATA CLASS \n\n\n    2.1 | Class Functions\n\n\nI've combined different functions one might use for visualisation of design & response parameters in a single class TS.\n\n### GET SUBSET OF DATA\n\n- get_id : Get result data for one specific design\n- get_aoasubset : Return a list of unique data subsets \n\n### CREATE PLOTS\n\n- plot_X_clcd : Plot Design Variable against Two Response Variables \n- plot_par_coord : Create Parallel Coordinates Plot\n- plot_polar : Create Drag Polar Plot\n- plot_angle_polar : Create Drag Polar Plot with Angle Hue Variation\n- plot_scat_mat : Create Scatter Matrix Plot
"\n    2.2 | Understanding our features\n\n\n- Pathway to the CSV file; n2412_ts, which contains a summary of all CFD simulations we tested\n- Each simulation, gives us specific metrics cd & cl, as well as a performance metric performance (not used)\n- Each CFD simulation slightly alters the geometry of the airfoil maxcamber, maxcamberposition, thickness\n- For each design, we have another variable for which we create more conditions aoa\n- In total, this creates **480 different test cases** (we ran 480 simulations) to obtain our data"
"\n    3.3 | Missing Data\n \n\n- Whilst missing data in for field values can exist & we don't have any missing data for these sort of features, so we can move on to some visualisation"
"# 4 | DESIGN PARAMETER RELATION TO RESPONSE VARIABLES \n\n\n    4.1 | Airfoil Geometry Parameter\n \n\n### GEOMETRIC PARAMETERS\n\n- 3/4 design parameters are related to the geometry of the airfoil; **maxcamber**, **maxcamberposition** & **thickness**\n- Let's plot the relation of the design parameters to the response parameters; lift & drag coefficients\n- With the addition of an interpoaltion model, we can better understand the general trend of the relationship between design and response variables tested\n\n### TREND OBSERVATIONS\n\n- As **maxcamber** is increased, the lift generated tends to go up. The drag resistance also tends goes up.\n- **Maxcamberposition** relation to lift is quite nonlinear, we can observe a peak at about the half way point (50%) on the airfoil, higher and lower values tend to give smaller values of lift. Drag tends to be lower near this point as well.\n- **Thickness** has the most nonlinear relation of all the other design parameters when it comes to lift. There tends to be a few values at which lift is maximised. On the other hand variations in drag is quite minimal."
### Libraries
\n  \n
### Min Max and Counts
### Distributions
"**Observations:**\n\n* The distribution of the original dataset does not follow the synthetic one closely especially for the two last features above.\n* Although the train and test distributions are pefectly identical.\n\nA synthetic dataset is a type of dataset created by generating new data that mimics the original data using various techniques. However, it is possible that the synthetic dataset features may not closely follow the original dataset distribution (our case). This can occur due to a variety of factors, such as using a different sampling technique, applying different data transformations, or introducing new features that were not present in the original dataset. When the synthetic dataset features do not closely follow the original dataset distribution, it can affect the performance of machine learning models trained on the origin data, as the models may not accurately capture the underlying patterns and relationships in the original data. Therefore, it is important to carefully evaluate the quality of both datasets before using them.\n\nLet's take a look at the train dataset features against the target and target itself:"
### Correlations
**Notes:**\n\n* There are many highly correlated features (> |0.8|). We might end up dropping some of them or create simple interations.\n
### Training Summary
"**Note**:\n* By increasing the number of iterations and reducing the learning rate, the model stopped finishing the training too early.\n* It might be a good idea to increase the number of iterations or decrease `early_stopping_rounds`."
"# Introduction\n\nAutocorrelation analysis is an important step in the Exploratory Data Analysis (EDA) of time series. **The autocorrelation analysis helps in detecting hidden patterns and seasonality and in checking for randomness.**\nIt is especially important when you intend to use an ARIMA model for forecasting because the autocorrelation analysis helps to identify the AR and MA parameters for the ARIMA model.\n\n**Overview**\n* [Fundamentals](#Fundamentals)\n    * [Auto-Regressive and Moving Average Models](#[Auto-Regressive-and-Moving-Average-Models])\n    * [Stationarity](#Stationarity)\n    * [Autocorrelation Function and Partial Autocorrelation Function](#Autocorrelation-Function-and-Partial-Autocorrelation-Function)\n    * [Order of AR, MA, and ARMA Model](#Order-of-AR-MA-and-ARMA-Model)\n* [Examples](#Examples)\n    * [AR(1) Process](#AR(1%29-Process)\n    * [AR(2) Process](#AR(2%29-Process)\n    * [MA(1) Process](#MA(1%29-Process)\n    * [MA(2) Process](#MA(2%29-Process)\n    * [Periodical](#Periodical)\n    * [Trend](#Trend)\n    * [White Noise](#White-Noise)\n    * [Random-Walk](#Random-Walk)\n    * [Constant](#Constant)\n* [🚀 Cheat Sheet](#🚀-Cheat-Sheet)\n* [Case Study](#Case-Study)\n    * [Bitcoin](#Bitcoin)  \n    * [Ethereum](#Ethereum) \n    * [Discussion on Random-Walk](#Discussion-on-Random-Walk) \n    \nIf you need some introduction to or a refresher on the ACF and PACF, I recommend the following video:\n"
"# Fundamentals\n\n## Auto-Regressive and Moving Average Models\n\n### Auto-Regressive (AR) Model\n\n$\hat{y}_t = \alpha_1 y_{t-1} + \dots + {\alpha_p}y_{t-p}$\n\nThe AR model assumes that the current value ($y_t$) is **dependent on previous values** ($y_{t-1}, y_{t-2}, y_{t-3},...$). Because of this assumption, we can build a **linear** regression model.\n\nTo figure out the order of an AR model, you would use the **PACF**.\n\n### Moving Average (MA) Model\n\n$\hat{y}_t = \epsilon_t + \beta_1 \epsilon_{t-1} + \dots + \beta_{q} \epsilon_{t-q}$\n\nThe MA model assumes that the current value ($y_t$) is **dependent on the error terms** including the current error ($\epsilon_{t}, \epsilon_{t-1}, \epsilon_{t-2}, \epsilon_{t-3},...$). Because error terms are random, there is **no linear** relationship between the current value and the error terms.\n\nTo figure out the order of an MA model, you would use the **ACF**.\n\n## Stationarity\n\nACF and PACF assume stationarity of the underlying time series.\nStaionarity can be checked by performing an **Augmented Dickey-Fuller (ADF) test**:\n\n> - p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n> - p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n>\n> [...] We can see that our [ADF] statistic value [...] is less than the value [...] at 1%.\nThis suggests that we can reject the null hypothesis with a significance level of less than 1% (i.e. a low probability that the result is a statistical fluke).\nRejecting the null hypothesis means that the process has no unit root, and in turn that the time series is stationary or does not have time-dependent structure. - [Machine Learning Mastery: How to Check if Time Series Data is Stationary with Python](https://machinelearningmastery.com/time-series-data-stationary-python/)\n\nIf the time series is stationary, continue to the next steps.\n**If the time series is not stationary, try differencing the time series** and check its stationarity again.\n"
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1)\n\n| | AR($p$) | MA($q$) | ARMA($p$, $q$) |\n|-|-|-|-|\n|ACF|Tails off (Geometric decay) |Significant at lag $q$ / Cuts off after lag $q$  |Tails off (Geometric decay) |\n|PACF|  Significant at each lag $p$ / Cuts off after lag $p$ |Tails off (Geometric decay) |Tails off (Geometric decay) |\n\n-> We can use an **AR(1) model** to model this process.\n\nSo that for AR(1), we would model the AR(p) formula\n$\hat{y}_t = \alpha_1 y_{t-1} + \dots + {\alpha_p}y_{t-p}$\nto the following:\n\n$\hat{y}_t = \alpha_1 y_{t-1}$\n"
### 3. Modelling
"As you can see, the AR(1) model fits an $\alpha_1 = 0.4710$, which is quite close to the `alpha_1 = 0.5` which we have set. However, the predicted values seem to be quite off in this case."
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1) and near-adjacent (lag = 2) observations\n\n| | AR($p$) | MA($q$) | ARMA($p$, $q$) |\n|-|-|-|-|\n|ACF|Tails off (Geometric decay) |Significant at lag $q$ / Cuts off after lag $q$  |Tails off (Geometric decay) |\n|PACF|  Significant at each lag $p$ / Cuts off after lag $p$ |Tails off (Geometric decay) |Tails off (Geometric decay) |\n\n-> We can use an **AR(2) model** to model this process.\n\nSo that for AR(2), we would model the AR(p) formula\n$\hat{y}_t = \alpha_1 y_{t-1} + \dots + {\alpha_p}y_{t-p}$\nto the following:\n\n$\hat{y}_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} $\n"
"As you can see, the AR(2) model fits $\alpha_1 = 0.5191$ and $\alpha_2 = -0.5855$, which is quite close to the `alpha_1 = 0.5` and `alpha_2 = -0.5` which we have set. However, the predicted values seem to be quite off as well in this case - similarly to the AR(1) case."
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1)\n\n| | AR($p$) | MA($q$) | ARMA($p$, $q$) |\n|-|-|-|-|\n|ACF|Tails off (Geometric decay) |  Significant at lag $q$ / Cuts off after lag $q$   |Tails off (Geometric decay) |\n|PACF| Significant at each lag $p$ / Cuts off after lag $p$ |Tails off (Geometric decay) |Tails off (Geometric decay) |\n\n-> We can use an **MA(1) model** to model this process.\n\nSo that for MA(1), we would model the MA(q) formula\n$\hat{y}_t = \epsilon_t + \beta_1 \epsilon_{t-1} + \dots + \beta_{q} \epsilon_{t-q}$\nto the following:\n\n$\hat{y}_t = \epsilon_t + \beta_1 \epsilon_{t-1}$\n"
"Overview\n\nThe High Definition-Advanced Imaging Technology (HD-AIT) system files supplied in this contest range in size from 10MB to over 2GB per subject.  With just under 1200 examples, we'll need to figure out how to make or find more and almost any approach will have to substantially reduce the size of the 512x660x16 image data to be fed into a machine learning model.  In the instructions, the organizers suggest that one may even be able to win the contest with one of the smaller image suites. So in this notebook, I take a whack at preprocessing the lowest res images we have and providing some basic building blocks for the preprocessing pipeline.\n\nA quick disclaimer, I'm not an expert on these systems or the related scans.  If you see something I've misunderstood or you think I've made an error, let me know and I'll correct it.  This contest is aimed at a critical problem.  I'm in this to help us get better at threat detection.  The community can definitely improve the predictive veracity of these scans.  Anyway, I'm excited to get going so let's jump in.  \n\nTo begin I collect all of the imports used in the notebook at the top.  It makes it easier when you're converting to a preprocessing script."
Next I collect the constants.  You'll need to replace the various file name reference constants with a path to your corresponding folder structure.
"Analyzing Threat Zones\n\nThe scans to be analyzed in this contest all segment the body into numbered ""threat zones"".  Your modeling and results will have to give probability of contraband within a given threat zone.  The contest sponsors included a  visualization of the threat zones and the corresponding numbering scheme.  (Note that you will need to uncomment the code in the next block for it to run in your own environment)"
*Output removed by request of DHS.  Run in your own environment to review threat zones.*
"Viewing and Selecting Images\n\nI always like to actually see the data before I start messing with it.  So this next function prints a nice, but small 4x4 matrix of the 16 images.  Since I did this originally in Google Datalab, there is an output size constraint that this function hits.  So I used a cv2.resize to get it under the wire.  Since there is no data on Kaggle it doesn't matter for this notebook.  But watch out for this if you run it yourself.  If your environment does not have the same size constraint, show the images full size.  Its good grounding.  After you've seen them once, you can comment out the unit test.  Note also that this only applies the unit test and visualization.\n"
*Output removed by request of DHS.  Uncomment the unit test and run in your own environment.*
"Digging into the Images\n\nWhen we get to running a pipeline, we will want to pull the scans out one at a time so that they can be processed.  So here's a function to return the nth image.  In the unit test, I added a histogram of the values in the image."
Here's the histogram:\n![Raw Histogram](https://storage.googleapis.com/kaggle-datasets-jbf/tsa_datasets/raw_hist.png)\n\n*Image output removed at the request of DHS.  Uncomment the unit test and run in your own environment.*
Rescaling the Image\n\nMost image preprocessing functions want the image as grayscale.  So here's a function that rescales to the normal grayscale range.
Here's the histogram:\n![Grayscale Histogram](https://storage.googleapis.com/kaggle-datasets-jbf/tsa_datasets/grayscale_hist.png)\n\n*Image output removed at the request of DHS.  Uncomment the unit test and run in your own environment.*
# Import The Necessary Libraries & Define Data Access Variables
# Create Train Image Batches & Output Feature Variable
"**Bitcoin Time Series Forecasting**\n\nBitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus ""chained"" together, serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow.  If you don't know what Bitcoin is , then get some knowledge about Bitcoin [here](https://www.coindesk.com/information/what-is-bitcoin) .\n\nThis Kernel is divided into two parts:-\n\n* Data Exploration\n* Time Series Analysis\n\nAnd further for the **Time Series Forecasting:**-\n\n*  Time Series forecasting with **LSTM**\n* Time Series forecasting with **XGBoost**\n* Time Series forecasting with Facebook **Prophet**\n* Time Series forecasting with **ARIMA**\n\nThis kernel takes inspiration from the following kernels,\n* [Time Series forecasting with Prophet by Rob Mulla](https://www.kaggle.com/robikscube/tutorial-time-series-forecasting-with-prophet)\n* [Time Series forecasting with XGBoost by Rob Mulla ](https://www.kaggle.com/robikscube/tutorial-time-series-forecasting-with-xgboost)\n* [Bitcoin Price. Prediction by ARIMA by Артём](https://www.kaggle.com/myonin/bitcoin-price-prediction-by-arima)"
**Data Exploration**\n\nIn this section we just explore the Data i.e the Historic Bitcoin Prices and try to find some insights. We will be using the Coinbase dataset as it is one of the mostly used Bitcoin Exchange/Wallet in the world.
Lets visualize Historical Bitcoin Prices (2015-2018)
Lets visualize Historical Bitcoin Market Volume (2015-2018)
## Examples\n### Fisher's iris dataset\n\nLet's start by uploading all of the essential modules and try out the iris example from the `scikit-learn` documentation. 
Now let's see how PCA will improve the results of a simple model that is not able to correctly fit all of the training data:
"Let's start by visualizing our data. Fetch the first 10 numbers. The numbers are represented by 8 x 8 matrixes with the color intensity for each pixel. Every matrix is flattened into a vector of 64 numbers, so we get the feature version of the data."
"Our data has 64 dimensions, but we are going to reduce it to only 2 and see that, even with just 2 dimensions, we can clearly see that digits separate into clusters."
"Indeed, with t-SNE, the picture looks better since PCA has a linear constraint while t-SNE does not. However, even with such a small dataset, the t-SNE algorithm takes significantly more time to complete than PCA."
"In practice, we would choose the number of principal components such that we can explain 90% of the initial data dispersion (via the `explained_variance_ratio`). Here, that means retaining 21 principal components; therefore, we reduce the dimensionality from 64 features to 21."
"## 2. Clustering\n\nThe main idea behind clustering is pretty straightforward. Basically, we say to ourselves, ""I have these points here, and I can see that they organize into groups. It would be nice to describe these things more concretely, and, when a new point comes in, assign it to the correct group."" This general idea encourages exploration and opens up a variety of algorithms for clustering.\n\n*The examples of the outcomes from different algorithms from scikit-learn*\n\nThe algorithms listed below do not cover all the clustering methods out there, but they are the most commonly used ones.\n\n### K-means\n\nK-means algorithm is the most popular and yet simplest of all the clustering algorithms. Here is how it works:\n1. Select the number of clusters $k$ that you think is the optimal number.\n2. Initialize $k$ points as ""centroids"" randomly within the space of our data.\n3. Attribute each observation to its closest centroid.\n4. Update the centroids to the center of all the attributed set of observations. \n5. Repeat steps 3 and 4 a fixed number of times or until all of the centroids are stable (i.e. no longer change in step 4).\n\nThis algorithm is easy to describe and visualize. Let's take a look."
"This kernel is intended to be a tutorial on Keras around image files handling for Transfer Learning using pre-trained weights from ResNet50 convnet.\n\nThough loading all train & test images resized (224 x 224 x 3) in memory would have incurred ~4.9GB of memory, the plan was to batch source image data during the training, validation & testing pipeline. Keras ImageDataGenerator supports batch sourcing image data for all training, validation and testing. Actually, it is quite clean and easy to use Keras ImageDataGenerator except few limitations (listed at the end).\n\nKeras ImageDataGenerator expects labeled training images to be available in certain folder heirarchy, 'train' data was manually split into 10k for training & 2.5k for validation and re-arranged into the desired folder hierarchy. Even 'test' images had to rearranged due to a known issue in flow_from_directory."
### Global Constants
"# Introduction\n\n**I have carefully analyzed a lot of notebooks** and I have been working on this competition for some time. My goal in this notebook, is to bring you the best pieces of all required parts of this challenge.\n\nWe will start by loading data, EDA, Modelling and finally, evaluation and submission file.\n\n**Please upvote the notebook if it helps you.**\n\nEnjoy and be safe!"
"**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows"
**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n
# Reading the csv file
# Distrubution of types of articles
# Unigrams and bigrams 
# WordCloud of articles
# Articles including images vs Label
"This new Lyft competition is tasking us, the participants, to predict the motion of external cars, cyclists, pedestrians etc. to assist self-driving cars. This is a step ahead from last year's competition, where we were tasked with detecting three-dimensional objects, like stop signs, to teach AVs how to recognize these. "
"This is apparently the **largest collection of traffic agent motion data.** The files are stored in the .zarr file format with Python, which we can easily load using the Level 5 Kit (l5kit for the pip package). Within our training ZARRs, we have the agents, the masks for agents, frames and scenes (which you might recollect from last year) and traffic light faces.\n\nThe test ZARR however is almost practically the same format, but the only exclusion is that of the data masks. for the agents. "
"**UPDATE: Finally got GPU to work by manually installing everything, adding utility scripts did not help at all. Took a painfully long time to get myself to realize that everything needs to be done in the kernel or things will break.**"
Here we go using some helpful functions to visualize the data.
"Now, let's get a sense of the configuration data. This will include metadata pertaining to the agents, the total time, the frames-per-scene, the scene time and the frame frequency."
"Now, however it's time for us to look at the scenes and analyze them in depth. Theoretically, we could create a nifty little data-loader to do some heavy lifting for us."
"So, there's a lot of information in this one image. I'll try my best to point everything out, but do notify me if I make any errors. OK, let's get started with dissecting the image:\n+ We have an intersection of four roads over here.\n+ The green blob represents the AV's motion, and we would require to predict the movement of the AV in these traffic conditions as a sample."
"I don't exactly know what other inferences we can make without more detail on this data, so let's try a satellite-format viewing of these images. "
"Yes! This allows for far more detail than a simple plot without detail. I'd haphazard an educated guess, and make the following inferences:\n+ Green still represents the autonomous vehicle (AV), and blue is primarily all the other cars/vehicles/exogenous factors we need to predict for.\n+ My hypothesis is that the blue represents the path the vehicle needs to go through.\n+ If we are able to accurately predict the path the vehicles go through, it will make it easier for an AV to compute its trajectory on the fly."
"Now, how about from the agent perspective? This would be quite interesting to consider, as we're modeling from principally the agent perspective in most public notebooks so far."
"So yes, I probably should save these as a GIF to visualize the agent movements. Let's try a simpler form of this and use the semantic view for the agent dataset."
#### To fill these Nan values the data distribution needs to be understood
### Aiming to impute nan values for the columns in accordance with their distribution
## Plotting after Nan removal 
## Skewness\n\nA ***left-skewed distribution*** has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That’s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA ***right-skewed distribution*** has a long right tail. Right-skewed distributions are also called positive-skew distributions. That’s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\n\n![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2014/02/pearson-mode-skewness.jpg)\n\n\n#### to learn more about skewness\nhttps://www.statisticshowto.datasciencecentral.com/probability-and-statistics/skewed-distribution/
#### Pair plot for clean data
***Pearson's Correlation Coefficient***: helps you find out the relationship between two quantities. It gives you the measure of the strength of association between two variables. The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.\n\nA heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information.
#### Heatmap for unclean data
#### Heatmap for clean data
"## Scaling the data \ndata Z is rescaled such that μ = 0 and 𝛔 = 1, and is done through this formula:\n![](https://cdn-images-1.medium.com/max/800/0*PXGPVYIxyI_IEHP7.)\n\n\n#### to learn more about scaling techniques\nhttps://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc\nhttps://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/"
"You can see that Keras will keep you updated on the loss as the model trains.\n\nOften, a better way to view the loss though is to plot it. The `fit` method in fact keeps a record of the loss produced during training in a `History` object. We'll convert the data to a Pandas dataframe, which makes the plotting easy."
"Notice how the loss levels off as the epochs go by. When the loss curve becomes horizontal like that, it means the model has learned all it can and there would be no reason continue for additional epochs."
"# **12. Implement a Movie Recommender System in Python** \n\n\n[Table of Contents](#0.1)\n\n\n- In this section, we will develop a very simple movie recommender system in Python that uses the correlation between the ratings assigned to different movies. Thus, we will find the similarity between the movies.\n\n\n- The dataset that we are going to use for this problem is the [MovieLens Dataset](https://www.kaggle.com/ayushimishra2809/movielens-dataset).\n\n\n- Let's import the basic libraries and import the data."
- We can see that there are 2 files in the dataset - `ratings` and `movies`. Let's explore them.
\n        \n             3 ) Exploratory Data Analysis (EDA):\n        \n\n\n
"We saw before that only 338 (38%) of the passengers survived, We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn't.\n\n"
\n        \n            Discovering the features correlation with Survived:\n        \n\n
#### Sex Vs Survived:
 \n# **Loading Library and Dataset**\n
"By Changing the data type of each column, I reduced memory usages by 75%. By taking the minimum and the maximum of each column, the function assigns which numeric data type is optimal for the column and change the data type. If you want to know more about how it works, I suggest you to read [Eryk's article](https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4)! "
\n\n\n\n3  IMPORTS    ⤒
\n\n\n\n4  SETUP AND HELPER FUNCTIONS    ⤒
"\n\n5.7 CALCULATE RANKS OF IMPORTANT COLUMNS\n\n---\n\nFor the important columns we determine the rankings (from largest to smallest for instance) and store those as new columns. The important columns are as follows (the +/- indicates if the ranking is ascending or descending)\n* **`+`**`ddg`\n* **`+`**`sub_matrix_score`\n* **`-`**`b_factor`\n* **`-`**`b_factor_matrix_score_adjusted`\n\nTo do this we will use the **`scipy.stats`** library, specifically the **`scipy.stats.rankdata`** method."
"\n\n5.8 COMBINE THE RANKS OF THE IMPORTANT COLUMNS\n\n---\n\nThis is weird... because we seemingly ignore the calculations we just performed (prior to ranking) in favour of the non-normalized values. \n\nBasically the combination is the following equation\n\n$$\nf(x,y,z) = (x*y*z)^{\dfrac{1}{3}}\n$$\n\n\n\n\n\n"
"**Load Python modules:** The list of modules grows step by step by adding new functionality that is useful for this project. A module could be defined further down once it is needed, but I prefer to have them all in one place to keep an overview."
**Load input data.** And combine the available features of train and test data sets. *test* of course doesn't have the column that indicates survival.
"OK, let's go through the features one by one to see what we find. Here we will see how the distributions of survivors and non-survivors compare. Personally, I like histograms for a first look at comparing two or more populations in case of scaled features. For categorical features we will use barplots plus standard deviation bars, to better judge the significance."
"Above we are creating a kind of summary dashboard, where we collect relevant visualisations to study the distributions of the individual features. We use the matplotlib *subplot* tool to line up the individual plots in a grid. We use overlapping histograms for ordinal features and barplots for categorical features. The barplots show the fraction of people (per group) who survived. There's a lot going on in this figure, so take your time to look at all the details.\n\n**We learn** the following things from studying the individual features:\n\n- *Age:* The medians are identical. However, it's noticeable that fewer young adults have survived (ages 18 - 30-ish) whereas **children younger than 10-ish had a better survival rate.** Also, there are no obvious outliers that would indicate problematic input data. The highest ages are well consistent with the overall distribution. There is a notable shortage of teenagers compared to the crowd of younger kids. But this could have natural reasons.\n\n- *Pclass:* There's a clear trend that **being a 1st class passenger gives you better chances of survival**. Life just isn't fair.\n\n- *SibSp & Parch:* **Having 1-3 siblings/spouses/parents/children on board (SibSp = 1-2, Parch = 1-3) suggests proportionally better survival numbers than being alone (SibSp + Parch = 0) or having a large family travelling with you.**\n\n- *Embarked:* Well, that does look more interesting than expected.  **Embarking at ""C"" resulted in a higher survival rate than embarking at ""S""**. There might be a correlation with other variables, here though.\n\n- *Fare:* This is case where a linear scaling isn't of much help because there is a smaller number of more extreme numbers. A natural choice in this case is to transform the values logarithmically. For this to work we need to adjust for the zero-fare entries.  The plot tells us that the **survival chances were much lower for the cheaper cabins**. Naively, one would assume that those cheap cabins were mostly located deeper inside the ship, i.e. further away from the life boats."
A little follow up: For *SibSp* we see in the plot that most of the differences are not very significant (overlapping error bars). Another way of checking the actual numbers are through *cross tables*:
"Passengers with more than 3 children+parents on board had low survival chances. However the corresponding number are not very large. For SibSp we have 15 vs 3, 5 vs 0, and 7 vs 0.\n\nRandom outcomes with 2 possibilities (like *heads or tails* when flipping a coin) follow the [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution). We can use a *binomial test* to estimate the probability that 5 non-survivors out of a total 5 passengers with SibSp = 5 happened due to chance assuming the overall 38% survival chance for the entire sample."
**Data Visualization**
**Normalization**
## Effect of parameters on Heart Disease based on Age
## Creating Dummy Variables
"\n\n1.1 BASIC COMPETITION INFORMATION\n\n---\n\nPRIMARY TASK DESCRIPTION\n\nIn this competition, you’ll create a model to automatically segment the stomach and intestines on MRI scans. The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment. You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care.\n\nBASIC BACKGROUND INFORMATION\n\nIn 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks. Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, oncologists are able to visualize the daily position of the tumor and intestines, which can vary day to day. \n\nIn these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines. This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate—unless deep learning could help automate the segmentation process. A method to segment the stomach and intestines would make treatments much faster and would allow more patients to get more effective treatment.\n\nCOMPETITION HOST INFORMATION\n\nThe UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015. UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin. The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens.\n\nVISUAL EXPLANATION\n\n\n\nThe tumor above (pink thick line) is close to the stomach (red thick line). High doses of radiation are directed to the tumor while avoiding the stomach. Dose levels are represented by colour. Higher doses are represented by red and lower doses are represented by green.\n\n\n\nMRI is an excellent imaging modality for visualization of soft tissues. This is particularly useful for tumors of the abdomen, such as pancreatic cancer shown below.  The left image shows the patient’s anatomy during exhale, while the image on the right shows the anatomical change during a maximum inspiration breath hold (MIBH). In the MIBH image we can see motion of nearly all the soft tissue, providing us superior ability to align the tumor during our treatment delivery. We are analyzing the clinical impact of using these treatment planning and delivery techniques and our patient’s ability to comply with self-guided breathing maneuvers.[REF]\n\nCOMPETITION IMPACT STATEMENT\n\nCancer takes enough of a toll. If successful, you'll enable radiation oncologists to safely deliver higher doses of radiation to tumors while avoiding the stomach and intestines. This will make cancer patients' daily treatments faster and allow them to get more effective treatment with less side effects and better long-term cancer control."
"\n\n1.3 DATASET OVERVIEW\n\n---\n\nGENERAL INFORMATION\n\nIn this competition we are segmenting organs cells in images. \n\nThe training **annotations are provided as RLE-encoded masks**, and the images are in **16-bit**, **grayscale**, **PNG format**.\n\nEach case in this competition is represented by multiple sets of scan slices\n* Each set is identified by the day the scan took place\n* Some cases are split by time\n    * early days are in train\n    * later days are in test\n* Some cases are split by case\n    * the entirety of the case is in train or test\n\nThe goal of this competition is to be able to generalize to both partially and wholly unseen cases.\n\nNote that, in this case, the test set is entirely unseen.\n* It is roughly 50 cases\n* It contains a varying number of days and slices, (similar to the training set)\n\nFILE INFORMATION\n\n**`train.csv`** \n- IDs and masks for all training objects.\n- **Columns**\n    * **`id`**\n        * unique identifier for object\n    * **`class`**\n        * the predicted class for the object\n    * **`EncodedPixels`**\n        * RLE-encoded pixels for the identified object\n\n\n\n**`sample_submission.csv`**\n- A sample submission file in the correct format\n\n\n\n**`train/`**\n- a folder of case/day folders, each containing slice images for a particular case on a given day.\n\n\n\n\n    ⚠️   NOTE   ⚠️The image filenames include 4 numbers (ex. 276_276_1.63_1.63.png).These four numbers are representative of:slice height (integer in pixels)slice width (integer in pixels)heigh pixel spacing (floating point in mm)width pixel spacing (floating point in mm)The first two defines the resolution of the slide. The last two record the physical size of each pixel.\n\n\n \n\n"
"\n\n4.1 INVESTIGATE THE OCCURENCE SEGMENTATION MAP TYPES\n\n---\n\nIt's quite apparent that not all images have segmentation maps for the various regions (stomach, large-bowel, small-bowel), so we will identify the frequency for which these occur independently... as well as the frequency for which these maps co-occur.\n\nOBSERVATIONS\n\n* There are **38,496** total examples.\n* It can be observed that more than half of the given examples have no annotations present!\n    * There are **21,906** (56.9046%) examples with no annotations/masks/segmentation present\n    * Inversely there are **16,590** (43.0954%) examples with one or more annotations present\n* There are **2,468** (6.41%) examples with **one annotation present**. \n* It can be observed that the vast majority of single mask annotations are **Stomach**!\n    * Of these annotations, **2286** (~92.6%) are **Stomach**\n    * Of these annotations, **123** (~4.98%) are **Large Bowel**\n    * Of these annotations, **59** (~2.39%) are **Small Bowel**\n* There are **10,921** (28.37%) examples with **two annotations present**. \n* It can be observed, in contrast to the single annotation examples, that the majority of annotations do NOT include stomach i.e. **'Large Bowel, Small Bowel'**!\n    * Of these annotations, **7781** (~71.3%) are **'Large Bowel, Small Bowel'**\n    * Of these annotations, **2980** (~27.3%) are **'Large Bowel, Stomach'**\n    * Of these annotations, **160** (~1.47%) are **'Small Bowel, Stomach'**\n* Finally, there are **3,201** (8.32%) examples with **all three annotations present**. \n\n"
"\n\n4.2 INVESTIGATE THE IMAGE SIZES\n\n---\n\nIt's observable that not all images have the same size... however, given that, there is not that much variation between image slice sizes.\n\nOBSERVATIONS\n* Remember, there are **38,496** total examples.\n* Globally, we can see that 3 of the image shapes are **square** while one is **rectangular** and they all fall within a fairly tight distribution of relatively small sizes\n* Of these there are **4** unique sizes:\n    * $234 \times 234$\n        * **Least frequent** image size\n        * **Smallest** image size\n        * Only **144** of the 38,496 occurences are this size (0.37%)\n    * $266 \times 266$\n        * **Most frequent** image size\n        * **Second smallest** image size\n        * **25,920** of the 38,496 occurences are this size (67.33%)\n    * $276 \times 276$\n        * **Second least frequent** image size\n        * **Second largest** image size\n        * **1,200** of the 38,496 occurences are this size (3.12%)\n    * $310 \times 360$\n        * **Second most frequent** image size\n        * **Largest** image size\n        * **11,232** of the 38,496 occurences are this size (29.17%)\n\n\n\n"
"\n\n4.3 INVESTIGATE THE PIXEL SPACING\n\n---\n\nIt's observable that not all images have the same pixel spacing... however, given that, there is not that much variation between pixel spacing.\n\nOBSERVATIONS\n* Remember, there are **38,496** total examples.\n* Globally, we can see that all of the pixel spacings are **square** and that the vast majority are $1.50mm \times 1.50mm$\n* There are only **2** unique sets of pixel spacings:\n    * $1.50mm \times 1.50mm$\n        * **Most frequent** pixel spacing\n        * **Smallest** pixel spacing (barely)\n        * **37,296** of the 38,496 occurences are this size (96.88%)\n    * $1.63mm \times 1.63mm$\n        * **Least frequent** image size\n        * **Largest** pixel spacing (barely)\n        * **1,200** of the 38,496 occurences are this size (3.12%)"
"\n\n4.4 INVESTIGATE CASE IDS\n\n---\n\nHere's the host description of **`case_id`**\n\n> ""Each case in this competition is represented by multiple sets of scan slices (each set is identified by the day the scan took place). Some cases are split by time (early days are in train, later days are in test) while some cases are split by case - the entirety of the case is in train or test. The goal of this competition is to be able to generalize to both partially and wholly unseen cases.""\n\nI don't really observe any oddities associated with any particular **`case_id`** values. I would probably attempt to group them when stratifying/creating-folds... however, they don't seem to perpetrate an obvious bias.\n\nWhen we colour by **day**, we can see that all cases are made up (mostly) of groups of **144**, or less frequently, **80**, images from different days."
## **4.1 Load libraries** \n\n[Table of Contents](#0.1)
## **4.2 Read dataset** \n\n[Table of Contents](#0.1)
It isn't surprising that kagglers usually have (or plan to get) higher education degree. Master degree is the most common one (though in India Bachelor degree is more wide-spread).\n\nIt is quite interesting that the rate of having a higher degree (master and doctoral) is higher for women than for men.
### Major
"After defining the callback, add it as an argument in `fit` (you can have several, so put it in a list). Choose a large number of epochs when using early stopping, more than you'll need."
"And sure enough, Keras stopped the training well before the full 500 epochs!\n\n# Your Turn #\n\nNow [**predict how popular a song is**](https://www.kaggle.com/kernels/fork/11906770) with the *Spotify* dataset."
"# Multiome Quickstart With Sparse Matrices\n\nThis notebook is mostly for demonstrating the utility of sparse matrices in this competition. (Especially for the Multiome dataset).\n\nAs the Multiome dataset is  very sparse (about 98% of cells are zeros), it benefits greatly from being encoded as sparse matrices. \n\nThis notebook is largely based on [this notebook](https://www.kaggle.com/code/ambrosm/msci-multiome-quickstart) by AmbrosM. It is a nice first attempt at handling Multiome data, and I thought it would informative for kagglers to be able to contrast directly the performances of sparse vs dense representations. \n\nMostly, the differences with AmbrosM's notebooks are:\n- We use a representation of the data in sparse CSR format, which let us load all of the training data in memory (using less than 8GB memory instead of the >90GB it would take to represent the data in a dense format)\n- We perform PCA (actually, TruncatedSVD) on the totality of the training data (while AmbrosM's notebook had to work with a subset of 6000 rows and 4000 columns). \n- We keep 16 components (vs 4 in AmbrosM's notebook)\n- We apply Ridge regression on 50000 rows (vs 6000 in AmbrosM's notebook)\n- Despite using much more data, this notebook should run in a bit more than 10 minutes (vs >1h for AmbrosM's notebook)\n\nThe competition data is pre-encoded as sparse matrices in [this dataset](https://www.kaggle.com/datasets/fabiencrom/multimodal-single-cell-as-sparse-matrix) generated by [this notebook](https://www.kaggle.com/code/fabiencrom/multimodal-single-cell-creating-sparse-data/).\n\nSince we will only generate the multiome predictions in this notebook, I am taking the CITEseq predictions from [this notebook](https://www.kaggle.com/code/vuonglam/lgbm-baseline-optuna-drop-constant-cite-task) by VuongLam, which is the public notebook with the best score at the time I am publishing.\n"
"# The scoring function (from AmbrosM)\n\nThis competition has a special metric: For every row, it computes the Pearson correlation between y_true and y_pred, and then all these correlation coefficients are averaged."
# Preprocessing and cross-validation\n\nWe first load all of the training input data for Multiome. It should take less than a minute.
"## PCA / TruncatedSVD\nIt is not possible to directly apply PCA to a sparse matrix, because PCA has to first ""center"" the data, which destroys the sparsity. This is why we apply `TruncatedSVD` instead (which is pretty much ""PCA without centering""). It might be better to normalize the data a bit more here, but we will keep it simple."
# Predicting
# Creating submission\n\nWe load the cells that will have to appear in submission.
##  Imports : \n
⬆️Back to Table of Contents ⬆️
Let's also look at one image in more detail:
"## Load data into DataBunch\nNow that we have the right folder structure and images inside of the folders we can continue. Before training a model in fast.ai, we have to load the data into a [DataBunch](https://docs.fast.ai/basic_data.html#DataBunch), in this case, we use a ImageDataBunch, a special version of the DataBunch. Fast.ai offers different functions to create a DataBunch. We will use the from_folder method of the ImageDataBunch class to create the dataset.\nThere are different hyperparameters we can tweak to make the model perform better:\n\n- [valid_pct](#What-are-Train,-Test-and-Validation-datasets?)\n- [size](#What-image-size-should-I-choose?)\n- [num_workers](#What-is-multiprocessing?)\n- [ds_tfms](#What-are-transforms-and-which-transforms-should-I-use?)\n- [bs (batch size)](#What-is-the-batch-size?)"
We observe:\n1. The distribution of audio length across labels is non-uniform and has high variance.\n\nLet's now analyze the frame length distribution in Train and Test.
We observe:\n1. Majority of the audio files are short.\n1. There are four `abnormal` length in the test histogram. Let's analyze them.
#### Some sssential imports
\n#### Configuration
## Datasets distribution\n\n### Competition data
### 2019 competition data
### Dataset oversampled
"### Learning rate schedule\n\nWe are going to use a `cosine learning rate schedule with a warm-up phase`, this may be a good idea since we are using a pre-trained model, the warm-up phase will be useful to avoid the pre-trained weights degradation resulting in catastrophic forgetting, during the schedule the learning rate will slowly decrease to very low values, this helps the model to land on more stable weights."
# Model
![](https://i.imgur.com/cYkCWRg.png)
# **Visualizing Random Samples**
# **Visualize Segmentation Masks using W&B**\n
# **Visualize a case day-wise using W&B Tables**\n
![](https://i.imgur.com/GVsxPGK.png)
"We're going to try to create a model to enter the Dogs vs Cats competition at Kaggle. There are 25,000 labelled dog and cat photos available for training, and 12,500 in the test set that we have to try to label for this competition. According to the Kaggle web-site, when this competition was launched (end of 2013): ""State of the art: The current literature suggests machine classifiers can score above 80% accuracy on this task"". So if we can beat 80%, then we will be at the cutting edge as of 2013!"
Here we import the libraries we need. We'll learn about what each does during the course.
### Changes by year\n\nAll items and stores seem to enjoy a similar growth in sales over the years.
### Changes by month\n\nAll items and stores seem to share a common pattern in sales over the months as well.\n
### Changes by day of the week\n\nAll items and stores also seem to share a common pattern in sales over the days of the week as well.
"### Are these patterns degenerate?\n\nThis is an important question. Not checking for degeneracies in the data can lead to missing important trends in complex datasets. For example, when looking at the monthly patterns, we average over all days of the month, years and either items or stores. But what if sales have a multi-dimensional dependence on two of these parameters that isn't easily separable? So, always check for degeneracies in the data!"
"In this case, however, there don't seem to be any sneaky degeneracies. We can effectively treat the ""month"", ""year"", ""day of the week"", ""item"" and ""store"" as completely independent modifiers to sales prediction. This leads to a *very very simple* prediction model.\n\n""Relative sales"" in the plots above are the sales relative to the average. Since there are very regular patterns in the ""month"", ""day of week"", and ""year"" trends. All we have to do is simply memorize these trends and apply them to our predictions by multiplying them to the expected average sales. We get the expected average sales for an item at a store from the historical numbers in the training set."
### What about the item-store relationship?
"Same here. Just a constant pattern and no degeneracies. So, you just need a model for how items sell at different stores, which is easily captured by an average sales look-up table or yet another ""relative sales"" pattern model.\n\n> *Aside: Based on the extremely regularity of the data, how neat it is, and how few degeneracies there are - I am fairly confident this is probably simulated data.*"
"## Writing the ""slightly better predictor""\n\nWe just need an item-store average sale look-up table, and then the ""day of week"", ""monthly"", ""yearly"" models."
"We can do a simple linear regression on the yearly growth datapoints. But if you look carefully, you can tell that the growth is slowing down. The quadratic fit works better since it better captures the curvature in the growth curve. Since we only have 5 points, this is the highest degree polynomial fit you should do to avoid overfitting."
"These statements are contradictory, and the label shows that.\n\nLet's look at the distribution of languages in the training set."
## Preparing Data for Input
"## 9.2. Visualizing Distributions of X,y"
"## 9.3. Train Test, Holdout sets"
## 9.3. Plot the training set data
# 10. Plotting with Simple Regression
"\n\n Weights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. Kaggle competitions require fast-paced model development and evaluation. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.\n\n> ⏳ Lots of components = Lots of places to go wrong = Lots of time spent debugging\n\nW&B can be useful for Kaggle competition with it's lightweight and interoperable tools:\n\n* Quickly track experiments,\n* Version and iterate on datasets, \n* Evaluate model performance,\n* Reproduce models,\n* Visualize results and spot regressions,\n* Share findings with colleagues.\n\nTo learn more about Weights and Biases check out this kernel."
# Training Configuration ⚙️
# Training Function
# Validation Function
# **Distribution of features Vs Target**
Snapshot of the artifacts created  \n\n![](https://drive.google.com/uc?id=1w8g5VUO34Wy6Mi3y2M6-Yu6yOdz7qXqA)
"# **Tensorflow Decision Forests**\n\nSource : https://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html\n\n![](https://drive.google.com/uc?id=1u8C0iutX50ajnYPdvnoTyCrtNNECvI_R)\n\nDecision forests are a family of machine learning algorithms with quality and speed competitive with (and often favorable to) neural networks, especially when you’re working with tabular data. They’re built from many decision trees, which makes them easy to use and understand - and you can take advantage of a plethora of interpretability tools and techniques that already exist today.\n\nTF-DF brings this class of models along with a suite of tailored tools to TensorFlow users:\n\nBeginners will find it easier to develop and explain decision forest models. There is no need to explicitly list or pre-process input features (as decision forests can naturally handle numeric and categorical attributes), specify an architecture (for example, by trying different combinations of layers like you would in a neural network), or worry about models diverging. Once your model is trained, you can plot it directly or analyse it with easy to interpret statistics.\nAdvanced users will benefit from models with very fast inference time (sub-microseconds per example in many cases). And, this library offers a great deal of composability for model experimentation and research. In particular, it is easy to combine neural networks and decision forests.\nIf you’re already using decision forests outside of TensorFlow, here’s a little of what TF-DF offers:\n\nIt provides a slew of state-of-the-art Decision Forest training and serving algorithms such as random forests, gradient-boosted trees, CART, (Lambda)MART, DART, Extra Trees, greedy global growth, oblique trees, one-side-sampling, categorical-set learning, random categorical learning, out-of-bag evaluation and feature importance, and structural feature importance.\nThis library can serve as a bridge to the rich TensorFlow ecosystem by making it easier for you to integrate tree-based models with various TensorFlow tools, libraries, and platforms such as TFX.\nAnd for users new to neural networks, you can use decision forests as an easy way to get started with TensorFlow, and continue to explore neural networks from there.\n"
# **Visualize the Output**
# **References**\n\nhttps://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html\n\nhttps://www.kaggle.com/subinium/tps-oct-simple-eda\n\nhttps://www.kaggle.com/craigmthomas/tps-oct-2021-eda\n\n
 Outline : \n1) [Brief Overview](#brief_overview) \n\n2) **Analysis by Type of Group**\na) [Gender Analysis](#gender_analysis) \nb) [Age Groups](#age_groups)\nc) [Wealth Analysis](#wealth_analysis) \n\n3) **Correlations and Purposes of Loans**\na) [Correlations](#correlations)\nb) [Loan Purpose](#purpose_loans)\n\n4) **Modeling**\na) [Predictive Modelling](#predictive_modelling)\n\n\n
"## Brief Overview: \n\nThe first phase of this project is to see what is our data made about. Which variables are numerical or categorical and which columns have ""Null"" values, which is something we will address in the feature engineering phase.\n\n## Summary:\n\n We have four numeric and four categorical features. \n The average age  of people in our dataset is 35.54\n The average credit amount borrowed is 3271 \n\n\n"
"## Drivers\n\n\nDrivers are abstraction for the process of executing a policy in an environment for a specified number of steps  during data collection, evaluation and generating a video of the agent.The data encountered by the driver at each step like observation , action , reward , current and next step is saved in Trajectory and broadcast to a set of observers such as replay buffers and metrics. \n\n\nImplementations for drivers are available both in Python and TensorFlow\n\n**Python Drivers :**\n\nThe PyDriver class takes a python environment, a python policy and a list of observers to update at each step. "
"The code below runs a random policy on the CartPole environment, saving the results to a **replay buffer**.\n"
"**Note:**  Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will ""assume"" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
"**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future."
" Scaling and Distributing \n\nIn this phase of our kernel, we will first scale the columns comprise of Time and Amount . Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n\n What is a sub-Sample?\nIn this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n\n Why do we create a sub-Sample?\nIn the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe  will cause the following issues:\n\nOverfitting: Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. \nWrong Correlations: Although we don't know what the ""V"" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. \n\n\nSummary:  \n\n Scaled amount  and  scaled time  are the columns with scaled values. \n There are 492 cases  of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe. \nWe concat the 492 cases of fraud and non fraud, creating a new sub-sample. \n"
"##  Equally Distributing and Correlating: \n\nNow that we have our dataframe correctly balanced, we can go further with our analysis and data preprocessing."
" Correlation Matrices \nCorrelation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample)  in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n\n### Summary and Explanation: \n\nNegative Correlations: V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.  \n  Positive Correlations:  V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction. \n BoxPlots:   We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions. \n\n\n\n**Note: ** We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe."
# Getting the data
# EDA
**Observation:** \n\n* The number of people belonging to sex category 0 are 96 whereas 1 are 206.\n* The number of people in one category are more than double than the zero.
"**Observation:**\n\n* cp : Chest Pain type chest pain type\n\n    * Value 0: typical angina\n    * Value 1: atypical angina\n    * Value 2: non-anginal pain\n    * Value 3: asymptomatic\n    \n* People of chest pain category '0' have the highest count, whereas of count of chest pain '3' is the lowest"
**Observation:** People of fbs category 1 are less than 25% of people of fbs category 0.
**Observation:** Thall count is maximum for type 2 ( 165 ) and min for type 0 ( 2 ) .\n
"**Observation:** \n\n* ECG count is almost the same for type 0 and 1. \n* Also, its almost negligible for type 2 in comparision to type 0 and 1."
"**Observation:**\n\n* This swarmplot gives us a lot of information.\n* Accoring to the figure, people belonging to caa category '0' , irrespective of their age are highly prone to getting a heart attack.\n* While there are very few people belonging to caa category '4' , but it seems that around 75% of those get heart attacks.\n* People belonging to category '1' , '2' and '3' are more or less at similar risk."
Menu Reference
"You probably already saw this menu on some Kaggle notebooks, I am very happy that it was a little useful!!"
**OBSERVATIONS**: There are many things that can be inferred from the above 3 plots\n* First thing you can see that in the early stages there are no prior_explanation. \n* In the final stages you can see almost all had prior_explanation.\n\n\n* Notice that in starting time there are a lot of question that are answered incorrectly (marked by black x)\n* In the middle time session the questions that did not have prior explanation were answered wrong (look bottom of chart-2)\n* Final stages had nearly all answers correct
All user_answers are evenly distributed except user_answer=2 which is slightly lesser than others which we saw before
Now as we have seen each column of interest let us move to compare 2 different columns
What do we understand from the above plots? \nAns: NOTHING\n\nThis is just to show that there is no correlation when you compare ID columns with other columns
Interesting... \n\nWe contradict ourselves from the previous plots. \nWhen we add details to our ID columns we can see some trends. \nSimilar is the above plot... \n\nUntil `task_container_id` =3000 there are no orange points and above 3000 there are no yellow points. \nthis means `User_ID` in certain range has `task_container_id` also in certain range.
**OBSERVATIONS**: \n* From the above plot we can see most of the questions had an explanation. Also we can see near right bottom some points in groups. This maybe because a large number of students took their test at the same time.\n* Another thing that we can notice is a faint blue line along the y-axis where x is 0. This is where the timestamp is 0 and there were no prior explanations.
"\n    From the above Analysis, Most of the kagglers are Men (77%) and Female Kagglers are (22%) \n"
\n\n    \n        WHICH COUNTRY KAGGLERS BELONGS TO?\n    \n    
"\n    From the above Analysis, Most of the kagglers are from India and USA \n"
\n\n    \n        WHAT IS THE AGE OF THE KAGGLERS?\n    \n    
"\n    From the above analysis, Most of the kagglers are in 18-21, 25-29 and 22-24 Age groups\n"
\n\n    \n        PLATFORMS USED BY KAGGLERS FOR DATA SCIENCE COURSES\n    \n    
\n\n    \n        PLATFORMS HELPED TO KAGGLERS WHEN THEY ARE GETTING STATRED INTO DATASCIENCE\n    \n    
\n\n    \n        DID KAGGLERS PUBLISHED ANY RESEARCH PAPERS?\n    \n    
\n\n    \n        DID THE RESEARCH MAKE USE OF ML?\n    \n\n\n    Theoretical Research - the research made advances related to some novel machine learning method\n    Applied Research - the research made use of machine learning as a tool \n
\n\n    \n        HOW LONG KAGGLERS HAVE BEEN CODING?\n    \n
\n\n    \n        WHAT PROGRAMMING LANGUAGES USED BY KAGGLERS?\n    \n
\n\n    \n        HOW MUCH EXPERIENCE DOES KAGGLERS HAVE IN ML?\n    \n
## Import The Necessary Libraries & Define Data Access Variables 
## Create Test Image Batches
In the dataset we have both categorical and numerical columns. Let's look at the values of categorical columns first.
### Numerical columns exploration
Now let's look at the numerical columns' values. The most convenient way to look at the numerical values is plotting histograms.
"We can see that numerical columns have outliers (especially 'pdays', 'campaign' and 'previous' columns). Possibly there are incorrect values (noisy data), so we should look closer at the data and decide how do we manage the noise.\n Let's look closer at the values of 'campaign', 'pdays' and 'previous' columns:"
"### EDA & VISUALIZATION\n\nBefore working with any kind of data it is important to understand them. A crucial step to this aim is the ***Exploratory data analysis (EDA)***: a combination of visualizations and statistical analysis (uni, bi, and multivariate) that helps us to better understand the data we are working with and to gain insight into their relationships. So, let's explore our target variable and how the other features influence it."
"In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis. Looking at the plot, we can clearly see how the distribution does not seem to be normal, but highly right-skewed. The non-normality of our distribution is also supported by the Shapiro test for normality (p-value really small that allows us to reject the hypotesis of normality). Despite that, let's leave it like that for now, we'll deal with that later in the notebook. "
The correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable.
Now that we know which features correlates most with our target variable we can investigate them more in depth.
"there is only one value for `R`, one value for `C` for the `breath_id`. \n\nLet us visualize `u_in`, `u_out` and `pressure` with respect to the `time_step`:"
"# All breaths\nWhat values do we have for `R`, which represents how restricted the airway is (in cmH2O/L/S)."
"\n# **1. Load Your Data, Load your Libraries**\n\nWell, lets get started loading a bunch of libraries that we will be showing off with the data\n\n"
\n# **2. Navigating your data**\nThe following section is a primer for loading data into **dataframes** which are essentially very powerful data grids that have a lot of great functionality for manipulating data. These can also be thought of as super powerful excel sheets. Below we will show some basic syntax usage of getting around your dataframe. \n\n\n#### Dataset size\n\n- **Train**: 1460\n- **Test**: 1458\n\n### **reading in CSV's from a file path**
# The State of Affairs Today - 2019 \n##  Top 10 Countries with the majority of respondents \nLet's begin by analyzing the 2019 survey's dataset to get a big picture. We shall begin by importing the dataset and the necessary libraries for the analysis.\n\n
"> ** 📌 Points to note :** \n> * The majority of the respondents(both male and female) are from India followed closely by the U.S. In fact, these countries together make up more than 50% of the entire population.\n> * If a country or territory received less than 50 respondents,[they have been grouped into a group named ""Other""](https://www.kaggle.com/c/kaggle-survey-2019/data) for anonymity.\n\n\n## Comparison of number of respondents w.r.t the previous surveys"
"> **📌 Points to note :**\n> * Whereas the number of responses in 2018 was considerably higher than in 2017, the year 2019 has seen a decline. The people need to be sensitized about the importance of surveys. The data obtained from surveys like these can help to understand the Data Science Scenario around the world better.\n> * To encourage more people to take part and hence, complete surveys, specific innovative steps can be taken. People usually do not like lengthy surveys, especially which expects them to write detailed answers.\n\nLet's now focus our analysis on the six key areas of Gender, Country, Age, Education, Professional Experience and Salary."
"# 1. Gender\n![](https://cdn-images-1.medium.com/max/800/1*LUQEhTYkzHRxwaRir6eZBg.png)\n*Photo Credits : [rudall30](https://www.vectorstock.com/royalty-free-vectors/vectors-by_rudall30)*\n> Barack Obama once said that where women are full participants in a country's politics or economy, societies are more likely to succeed. This is indeed true. Let's see if the same is affirmed by the data.\n\n## 1.1 The Great Gender Divide\nThe following plot analyses the gender distribution in the 2019 survey."
"> **📌 Key Points :**\n* There is a staggering difference between men and women respondents in the survey. Around 82% of the respondents are men while only 16% are women.\n* Clearly, the above results indicate that the gender imbalance that affects the tech sector extends to data science and AI, as well.\n\n## 1.2 Gender Distribution over the years\nLet's see how gender distribution varies for the previous two surveys."
"\n> **📌 Points to note :**\n> * Well, the pattern almost remains the same. Women respondents have been consistently low over the years. With this low participation, it gets really for our ideas and suggestions to be incorporated in the industry. \n**Common girls!! we need more participation**\n> * The challenge of getting and keeping women in tech and data science goes well beyond educational exposure and will require some concrete steps from the establishments. Gender shouldn't limit accomplishments and opportunities for anyone."
## 2.2 Top 20 Countries of Female Respondents\nLet us now dive further into the data to determine the top twenty countries of the female respondents. Let's see which countries made the cut.
"> **📌 Points to note :**\n* Maximum female respondents are from India followed by U.S. This pattern has been observed in the overall population also. \n* It is heartening to see that few females from Turkey, Nigeria, and Pakistan have also responded to the survey, albeit their percentage is very small.\n\n## 2.3 The Indian & the U.S Female Respondents over the years\nSince India and U.S have the maximum percentage of respondents, let's analyse and find out if the scenario was the same in the last two years also."
"> ** 📌 Points to note :**\n* The number of female respondents in the U.S was considerably higher than in India for the years 2017 and 2018. However, the year 2019 saw a growth in Indian female respondents and their percentage outpaced the U.S females.\n* This is a good sign for India as a country since the percentage of participation was more than last year. On the underhand, steps could be taken to analyze why the participation of U.S women declined, who were leading from the past two years.\n\n## 2.4 Daunting obstacles remain in Africa\nAs we saw in section 2,1, Africa needs special attention. Let's look at the participants from the African subcontinent."
"> ** 📌 Points to note :**\n* The number of African females who responded to the survey in 2019 has increased as compared to the previous years. Around 150 females responded to the survey in 2019 whereas, for the previous years, the numbers were even less than 100. However, there is a lot of progress to be still made.\n\n## 2.5 Algerian females make an appearance\nIt will be also interesting to find out the reason for the increased participation from the African subcontinent.Let's dig in further for more details."
"> ** 📌 Points to note :**\n* Interestingly, female respondents from Algeria also took the survey for the first time in 2019.\n* There has been a sharp spike in the Nigerian female respondents as compared to the previous two years.\n* Both the above factors have contributed to the better performance of African females in the 2019 survey."
# 3. Age Distribution\n![](https://cdn-images-1.medium.com/max/800/1*hmOJeowjdAZudATFTHIoLw.png)    \n*PC : www.freepik.com  *  \n    \nAge is an important attribute of any demographic analysis. It'll be interesting to see the age distribution of all the female respondents.\n    \n## 3.1 The Young Brigade dominates\nLet's look at the age distribution of the female respondents in 2019.\n
"> ** 📌 Points to note :**\n* The majority of the female respondents are in the (25 -29) age group followed closely by (22–24) age group. Thus we can say that most of the women lie between the 20 to 30 age bracket. \n* The (20–30) age group can comprise of both students(undergraduate and postgraduate) and professionals. \n* Interestingly, females greater than 70 years of age have also responded. Well, as it is said - Age is only a number !!.\n\n## 3.2 Age distribution pattern over the years\nWas the age distribution pattern same for the last two years too? let's analyze and find out.   "
"> ** 📌 Points to note :**\n* No notable change in the pattern in 2018 was observed.\n* in 2017 however, there were more responses from the (22–24) age group as compared to other age brackets.\n* Overall, the 20–30 group dominates.\n\n ## 3.3 Country wise Age distribution\nWe know that the young generation dominates the current Data Science landscape. Let's also see how are they distributed geographically."
# 4. Education\n![](https://cdn-images-1.medium.com/max/800/1*JEEn6yzozl_ye9-oEUIMxQ.png)\n*PC: www.freepik.com*\n    \nIt has been rightly said that educated females form the backbone of society. Let's see the qualification status of the female respondents in 2019.\n## 4.1 Educational qualifications of the female respondents in 2019
"> **📌 Points to note :**\n* The education status of the females is impressive with the majority (~46%)having a Master's degree followed closely by a Bachelor's degree(27%). There are also 16% PhDs who answered the survey. \n* The analysis also reveals that there is a certain proportion who have had no formal education past high school. In spite of this, they took the survey which in itself is a commendable thing. They should be encouraged to complete their education either full time or through part-time courses.\n\n## 4.2 Educational qualifications of the female respondents, country wise"
# Color
"It is incredible how we can apply data science to different areas of our life. Even in sports, data science can help to make the game safer for the athletes.\n\nThe main goal of this analysis is to find out factors, which lead to injury. In particular, find out if the effects that synthetic turf versus natural turf can have on player movements and the factors that may contribute to lower extremity injuries."
## Load Data
### Target Exploration
### Corralation between features (variables)
### Numeric Features Exploration
### Categorical (Ordinal) Features Exploration
### Categorical (Nominal) Features Exploration
### Please upvote if my notebook helped you in any way :)
"In the first article, we demonstrated how polynomial features allow linear models to build nonlinear separating surfaces. Let's now show this visually.\n\nLet's see how regularization affects the quality of classification on a dataset on microchip testing from Andrew Ng's course on machine learning. We will use logistic regression with polynomial features and vary the regularization parameter $C$. First, we will see how regularization affects the separating border of the classifier and intuitively recognize under- and overfitting. Then, we will choose the regularization parameter to be numerically close to the optimal value via (`cross-validation`) and (`GridSearch`)."
"Let's load the data using `read_csv` from the `pandas` library. In this dataset on 118 microchips (objects), there are results for two tests of quality control (two numerical variables) and information whether the microchip went into production. Variables are already centered, meaning that the column values have had their own mean values subtracted. Thus, the ""average"" microchip corresponds to a zero value in the test results.  "
"As an intermediate step, we can plot the data. Orange points correspond to defective chips, blue to normal ones."
Let's define a function to display the separating curve of the classifier.
"We define the following polynomial features of degree $d$ for two variables $x_1$ and $x_2$:\n\n$$\large \{x_1^d, x_1^{d-1}x_2, \ldots x_2^d\} =  \{x_1^ix_2^j\}_{i+j=d, i,j \in \mathbb{N}}$$\n\nFor example, for $d=3$, this will be the following features:\n\n$$\large 1, x_1, x_2,  x_1^2, x_1x_2, x_2^2, x_1^3, x_1^2x_2, x_1x_2^2, x_2^3$$\n\nDrawing a Pythagorean Triangle would show how many of these features there will be for $d=4,5...$ and so on.\nThe number of such features is exponentially large, and it can be costly to build polynomial features of large degree (e.g $d=10$) for 100 variables. More importantly, it's not needed. \n"
Let's train logistic regression with regularization parameter $C = 10^{-2}$.
"We could now try increasing $C$ to 1. In doing this, we weaken regularization, and the solution can now have greater values (in absolute value) of model weights than previously. Now the accuracy of the classifier on the training set improves to 0.831."
"Then, why don't we increase $C$ even more - up to 10,000? Now, regularization is clearly not strong enough, and we see overfitting. Note that, with $C$=1 and a ""smooth"" boundary, the share of correct answers on the training set is not much lower than here. But one can easily imagine how our second model will work much better on new data."
"To discuss the results, let's rewrite the function that is optimized in logistic regression with the form:\n\n$$\large J(X,y,w) = \mathcal{L} + \frac{1}{C}||w||^2,$$\n\nwhere\n\n- $\mathcal{L}$ is the logistic loss function summed over the entire dataset\n- $C$ is the reverse regularization coefficient (the very same $C$ from `sklearn`'s implementation of `LogisticRegression`)"
"To see how the quality of the model (percentage of correct responses on the training and validation sets) varies with the hyperparameter $C$, we can plot the graph. "
"Finally, select the area with the ""best"" values of $C$."
"# Missing Values\n\nLooks like our external data has some missing values, which is expected. Body part missing numbers increased around three times, missing ages increased about four times and gender missing rates didn't change much but small increase... [2020 values here](https://www.kaggle.com/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble#Missing-Values)."
"# Distribution Differences\n\nMeanwhile age and gender distributions seems pretty similar with external data anatom site part has some differences: palms/soles, oral/genital and head/neck parts increased a lot in train data we might have to check these further..."
"# Imputing Missing Values\n\nThis time I decided to replace missing values with fixed values instead of replacing them with most frequent ones, since we have higher miss ratio..."
# Body Part Ratio by Gender and Target\n\nWe have very different malignant ratio for external data it seems. You can check [2020 values here](https://www.kaggle.com/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble#Body-Part-Ratio-by-Gender-and-Target). This is big difference but might be useful for unseen test data...
"# Sunburst Chart\n\n\n- We almost doubled our malignant examples with upsampling and external data,\n- Malignant images became more balanced 56% male to 44% female. It was 62% vs 38% in 2020 data,\n- Gender wise benign images has same ratio as 2020,\n- Malignant image scan locations differs based on the patients gender:\n    - Torso still most common location in males even with 3% decrease; meanwhile female malignant torso scans decreased like 6% with external data,\n    - Lower extremity still more common with female scans than males 12% males vs 26% females (it was 18% vs 26% in 2020 data)\n    - Again upper extremity malignant scans are more common with females than males with 15% males vs 20% females (used to be 17% - 23%)\n    - Head/Neck malignant scans increased with external data on both genders."
"# Age and Scan Result Relations\n\nAge distribution seems little bit different with external data added. In general we have older patients, benign scan age distributions seems more closer to each other than 2020 data, meanwhile malignant difference stay at same levels between genders. We can say generally examples which missing gender values are also missing age values too. Lastly we still have age effect on malignant scans with the difference visible on age below 0 because of the imputing method we choose."
"# Age \n\nAge distribution seems similar with external data, you can [you can compare them here](https://www.kaggle.com/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble#Age-Round-Two)."
"# Image Resolutions \n\nWe had decent observations in the previous notebook about image sizes so wanted to check them here with external data again. With the new examples we increased variety of the image sizes (with high number of 1024x1024 images coming in) but we can see 1920x1080 images in test set still seperated, so we should be really careful about image sizes in our models..."
**1-2: Visualization Libraries**
**1-3: Machine Learning Models**
# AMEX EDA which makes sense ⭐️⭐️⭐️⭐️⭐️\n\nThis EDA analyzes the data and gives some insight which is useful for designing a machine learning pipeline and selecting a model.
"# The labels\n\nWe start by reading the labels for the training data. There are neither missing values nor duplicated customer_IDs. Of the 458913 customer_IDs, 340000 (74 %) have a label of 0 (good customer, no default) and 119000 (26 %) have a label of 1 (bad customer, default).\n\nWe know that the good customers have been subsampled by a factor of 20; this means that in reality there are 6.8 million good customers. 98 % of the customers are good; 2 % are bad.\n\n**Insight:**\n- The classes are imbalanced. A StratifiedKFold for cross-validation is recommended.\n- Because the classes are imbalanced, accuracy would be a bad metric to evaluate a classifier. The [competition metric](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327464) is a mix of area under the roc curve (auc) and recall."
Now we can count how many rows (credit card statements) there are per customer. We see that 80 % of the customers have 13 statements; the other 20 % of the customers have between 1 and 12 statements.\n\n**Insight:** Our model will have to deal with a variable-sized input per customer (unless we simplify our life and look only at the most recent statement as @inversion suggests [here](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327094) or at the average over all statements).
"Let's find out when these customers got their last statement. The histogram of the last statement dates shows that every train customer got his last statement in March of 2018. The first four Saturdays (March 3, 10, 17, 24) have more statements than an average day.\n\nThe test customers are split in two: half of them got their last statement in April of 2019 and half in October of 2019. As was [discussed here](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327602), the April 2019 data is used for the public leaderboard and the October 2019 data is used for the private leaderboard."
"**Insight:** Although the data are a kind of time series, we cannot cross-validate with a TimeSeriesSplit because all training happens in the same month.\n\nFor most customers, the first and last statement is about a year apart. Together with the fact that we typically have 13 statements per customer, this indicates that the customers get one credit card statement every month."
"If we color every statement (i.e. row of train or test) according to the dataset it belongs (training, public lb, and private lb), we see that every dataset covers thirteen months. Train and test don't overlap, but public and private lb periods overlap."
"Now we'll look at the distribution of missing values over time. B_29 is most interesting. Given the each of the three datasets has almost half a million customers, we see that until May of 2019 fewer than a tenth of the customers have a value for B_29. The other nine tenths are missing. Starting in June of 2019, we have B_29 data for almost every customer. \n\n**Insight:** The distribution of the missing B_29 differs between train and test datasets. Whereas in the training and public leaderboard data >90 % are missing, during the last five months of private leaderboard, we have B_29 data for almost every customer. If we use this feature, we should be prepared for surprises in the private leaderboard. Is it better to drop the feature?"
"# The categorical features\n\nAccording to the [data description](https://www.kaggle.com/competitions/amex-default-prediction/data), there are eleven categorical features. We plot histograms for target=0 and target=1. For the ten features which have missing values, the missing values are represented by the rightmost bar of the histogram.\n"
**Insight:**\n- Every feature has at most eight categories (including a nan category). One-hot encodings are feasible.\n- The distributions for target=0 and target=1 differ. This means that every feature gives some information about the target.\n
# The binary features\n\nTwo features are binary:\n- B_31 is always 0 or 1.\n- D_87 is always 1 or missing.
"**Insight:** If you impute missing values for D_87, don't fall into the trap of imputing the mean - the feature would become useless..."
# Load Libraries 
## GPU 
## Plot 
## Seed All
We can visualize our models feature space to analyze class seperation. 
"We can use of learned masks to determine local and global feature importances or attributions for the model. I don't this the local attributions follow the same sensitivity and interprettations of other model agnostic or tree-based methods but are, in previous experiment, similar to those of common Boosting approaches. "
"## Unsupervised Pretraining\nThe TabNet authors see their approach particularly valuable in unsupervised or self-supervised learning applications where models can be pre-trained across large amounts of unlabelled data and then fine-tuned on labelled examples. To allow for such as approach, they define a decoder architures which takes in the encoders feature space of the encoder model and passes this input through a number of step of Feature Tansformer Blocks and Dense Layers. This decoder then returns the original feature input of the encoder model as output for use in training. "
# Import Libraries
# Define Functions
# Data Understanding
\n\n## 2| IMPORT NECESSARY LIBRARIES 🎬
###  Some colors used in this notebook were selected from the following color collection: 
\n\n## 3|LOAD DATASET 📥
\n\n#### 5.1 | Histplot
"\n\nDistribution  \n\nAnalyzing the graphs here, it turns out that the values of the variable 'fixed_acidity' are relatively normally distributed (but a bit left skewed). But there are two peaks in the distributions of other 'volatile_acidity' and 'citric_acid' variables."
"\n\nDistribution  \n\nAnalyzing the graphs here, it turns out that the distributions of these variables are not normal"
### 1. Library
### 2.Data generate
### 3.plot data
### 4.Define data in place holder
 \n# 4. Exploratory Data Analysis
`Potential` tends to fall as you grow old
"## Introduction\n\n* **Natural Language Processing (NLP):** The discipline of computer science, artificial intelligence and linguistics that is concerned with the creation of computational models that process and understand natural language. These include: making the computer understand the semantic grouping of words (e.g. cat and dog are semantically more similar than cat and spoon), text to speech, language translation and many more\n\n* **Sentiment Analysis:** It is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis allows organizations to identify public sentiment towards certain words or topics.\n\nIn this notebook, we'll develop a **Sentiment Analysis model** to categorize a tweet as **Positive or Negative.**\n\n\n## Table of Contents\n1. [Importing dependencies](#p1)\n2. [Importing dataset](#p2)\n3. [Preprocessing Text](#p3)\n4. [Analysing data](#p4)\n5. [Splitting data](#p5)\n6. [TF-IDF Vectoriser](#p6)\n7. [Transforming Dataset](#p7)\n8. [Creating and Evaluating Models](#p8)\n    * [BernoulliNB Model](#p8-1)\n    * [LinearSVC Model](#p8-2)\n    * [Logistic Regression Model](#p8-3)\n9. [Saving the Models](#p9)\n10. [Using the Model](#p10)\n\n## Importing Dependencies"
"## Importing dataset\nThe dataset being used is the **sentiment140 dataset**. It contains 1,600,000 tweets extracted using the **Twitter API**. The tweets have been annotated **(0 = Negative, 4 = Positive)** and they can be used to detect sentiment.\n \n*[The training data isn't perfectly categorised as it has been created by tagging the text according to the emoji present. So, any model built using this dataset may have lower than expected accuracy, since the dataset isn't perfectly categorised.]*\n\n**It contains the following 6 fields:**\n1. **sentiment**: the polarity of the tweet *(0 = negative, 4 = positive)*\n2. **ids**: The id of the tweet *(2087)*\n3. **date**: the date of the tweet *(Sat May 16 23:58:44 UTC 2009)*\n4. **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n5. **user**: the user that tweeted *(robotickilldozr)*\n6. **text**: the text of the tweet *(Lyx is cool)*\n\nWe require only the **sentiment** and **text** fields, so we discard the rest.\n\nFurthermore, we're changing the **sentiment** field so that it has new values to reflect the sentiment. **(0 = Negative, 1 = Positive)**"
"## Preprocess Text\n**Text Preprocessing** is traditionally an important step for **Natural Language Processing (NLP)** tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n\n**The Preprocessing steps taken are:**\n1. **Lower Casing:** Each text is converted to lowercase.\n2. **Replacing URLs:** Links starting with **""http"" or ""https"" or ""www""** are replaced by **""URL""**.\n3. **Replacing Emojis:** Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. *(eg: "":)"" to ""EMOJIsmile"")*\n4. **Replacing Usernames:** Replace @Usernames with word **""USER""**. *(eg: ""@Kaggle"" to ""USER"")*\n5. **Removing Non-Alphabets:** Replacing characters except Digits and Alphabets with a space.\n6. **Removing Consecutive letters:** 3 or more consecutive letters are replaced by 2 letters. *(eg: ""Heyyyy"" to ""Heyy"")*\n7. **Removing Short Words:** Words with length less than 2 are removed.\n8. **Removing Stopwords:** Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. *(eg: ""the"", ""he"", ""have"")*\n9. **Lemmatizing:** Lemmatization is the process of converting a word to its base form. *(e.g: “Great” to “Good”)*"
### Word-Cloud for Negative tweets.
### Word-Cloud for Positive tweets.
## Splitting the Data\nThe Preprocessed Data is divided into 2 sets of data: \n* **Training Data:** The dataset upon which the model would be trained on. Contains 95% data.\n* **Test Data:** The dataset upon which the model would be tested against. Contains 5% data.
### Evaluate Model Function
### BernoulliNB Model
"So we've **gained some understanding** on the distributiona of our numeric variables, **but we can add more information** to this plot. \n\nLet's see how the distribution of our numeric variables is different for **those that have strokes, and those that do not.**\n\nThis could be important for modelling later on"
"# Insight\n\nBased on the above plots, it seems clear that **Age is a big factor** in stroke patients - the older you get the more at risk you are.\n\nThough less obvious, there are also differences in Avg. Glucose Levels and BMI.\n\nLet's explore those variables further..."
"As we suspected, Age is a big factor, and also has slight relationships with BMI & Avg. Glucose levels.\n\nWe might understand intuitively that **as Age increases, the risk of having a stroke increases too, but can ve visualise this?**"
"This confirms what our intuitions told us. The older you get, the more at risk you get.\n\nHowever, you may have notices the low risk values on the y-axis. This is because the **dataset is highly imbalanced**.\n\nOnly 249 strokes are in our dataset which totals 5000 - around 1 in 20. "
"This needs to be considered when modelling of course, but also when formulating risk.\n\nStrokes are still relatively rare, we are not saying anything is guaranteed, just that risk is increasing."
"# General Overview\n\nWe've assessed a few variables so far, and gained some powerful insights. \n\nI'll now plot several variables in one place, so we can spot interesting trends or features.\n\nI will split the data in to 'Stroke' and 'No-Stroke' so we can see if these two populations differ in any meaningful way."
"# Insights\n\nThe plots above are quite enlightening.\n\nAs discussed earlier, we again note the importance of Age, amongst other things."
"### Visualizing masks (using matplotlib)\n\nGiven that the masks are just integer matrices, you can also use other packages to display the masks. For example, using matplotlib and a custom color map we can quickly visualize the different cancer regions:"
"### Overlaying masks on the slides\n\nAs the masks have the same dimension as the slides, we can overlay the masks on the tissue to directly see which areas are cancerous. This overlay can help you identifying the different growth patterns. To do this, we load both the mask and the biopsy and merge them using PIL.\n\n**Tip:** Want to view the slides in a more interactive way? Using a WSI viewer you can interactively view the slides. Examples of open source viewers that can open the PANDA dataset are [ASAP](https://github.com/computationalpathologygroup/ASAP) and [QuPath](https://qupath.github.io/). ASAP can also overlay the masks on top of the images using the ""Overlay"" functionality. If you use Qupath, and the images do not load, try changing the file extension to `.vtif`."
Standard imports for data science work. The LightGBM library is used for the gradient boosting machine.
"* `train_bureau` is the training features built manually using the `bureau` and `bureau_balance` data\n* `train_previous` is the training features built manually using the `previous`, `cash`, `credit`, and `installments` data\n\nWe first will see how many features we built over the manual engineering process. Here we use a couple of set operations to find the columns that are only in the `bureau`, only in the `previous`, and in both dataframes, indicating that there are `original` features from the `application` dataframe. Here we are working with a small subset of the data in order to not overwhelm the kernel. This code has also been run on the full dataset (we will take a look at some of the results)."
## Utility functions
"Get mean, sum and variance of image features over all 30 images."
"\n\n4.2 EXPLORE THE `PROTEIN_SEQUENCE` COLUMN\n\n---\n\n\n\nHost Description\n* Amino acid sequence of each protein variant. \n* The stability (as measured by $t_m$) of protein is determined by its protein sequence. \n\n\n\n---\n\n\n\nObservations\n* The frequency of some amino acids respective to the length of the AA sequence has a low, but non-zero correlation with the target $t_m$ (melting point)\n    * The fractional frequency appears slightly more informative\n* Some amino acid sequences are REALLY long... especially considering all of our test AA sequences are relatively short ~240 vs more than 32,000\n* While some amino acids are more common than others, all are relatively common (Sequences on average contain 1.5% to 10% of each amino acid respectively).\n\n\n\n---\n\n"
\n\n4.3 EXPLORE THE `PH` COLUMN\n\n---\n\n\n\nHost Description\n* The scale used to specify the acidity of an aqueous solution under which the stability of protein was measured.\n* Stability of the same protein can change at different pH levels.\n\n\n\n---\n\n\n\nObservations\n* All training pH values fall within the range from 0.0-11.0 \n\n\n\n---\n\n
\n\n4.4 EXPLORE THE `DATA_SOURCE` COLUMN\n\n---\n\n\n\nHost Description\n* Source where the data was published\n\n\n\n---\n\n\n\nObservations\n* High cardinality\n* All data source is Novozymes in test dataset... so maybe useless?\n\n\n\n---\n\n
"\n\n4.5 EXPLORE THE `TM` COLUMN\n\n---\n\n\n\nHost Description\n* The target column. \n* Since only the Spearman Correlation Coefficient will be used for the evaluation, the correct prediction of the relative order is more important than the absolute tm values.\n    * Higher tm means the protein variant is more stable.\n\n\n\n\n---\n\n\n\nObservations\n* -1 to 130\n* Float\n\n\n\n---\n\n"
\n\n\n\n\n\n\n    5  TEST DATASET EXPLORATION    ⤒\n\n\n---\n\n\n\n**GENERAL OBSERVATIONS**\n* fadgda
"So, for 23 series let's create a 6 by 4 grid which will be resulted in 24 slots and fill it with the plot of our series."
It seems like there are pretty much similar time series such as ```MRTSSM44000USS``` and ```RETAILMSA``` or ```MRTSSM7221USN``` and ```MRTSSM44611USN```.
## 2. 2. Preprocessing\n\nBefore we start analyzing let's check if our data is uniform in length.
"After handling missing values, the other issue is the scale of the series. Without, normalizing data the series that looks like each other will be seen so different from each other and will affect the accuracy of the clustering process. We can see the effect of the normalizing in the following images.\n\n"
"Note that we normalized each time series by their own values, not the values of other time series."
\n\n\n\n4  IMPORTS    ⤒
\n\n\n\n5  SETUP AND HELPER FUNCTIONS    ⤒
"\n\n5.3 SHOW OFF THE SENSOR ARRAY AS A 3D SCATTER PLOT\n\n---\n\nWe can easily plot the sensor array as we are provided with the coordinates for each sensor as (x,y,z) points. Note that, the **azimuth and zenith** angles are determined with respect to a spherical coordinate system centered within the sensor array."
\n\n5.4 CHECK DATA ACCESS BY EXAMINING THE PROVIDED ILLUSTRATION\n\n---\n\nLet's find and plot this particular example:\n\n\n\n
Credits to Leonardo's Kernel : \nhttps://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt
Ploting Transaction Amount Values Distribution
The Product Feature
"Visualizing Card 1, Card 2 and Card 3 Distributions"
Card 4 - Categorical Feature
Card 6 - Categorical
Exploring M1-M9 Features
"With the background details out of the way, let's get started with Bayesian optimization applied to automated hyperparameter tuning! "
The code below reads in the data and creates a smaller version for training and a set for testing. We can only use the training data __a single time__ when we evaluate the final model. Hyperparameter tuning must be done on the training data using cross validation!
We can visualize the learning rate by drawing 10000 samples from the distribution.
The number of leaves on the other hand is a discrete uniform distribution.
"### Conditional Domain\n\nIn Hyperopt, we can use nested conditional statements to indicate hyperparameters that depend on other hyperparameters. For example, the ""goss"" `boosting_type` cannot use subsampling, so when we set up the `boosting_type` categorical variable, we have to set the subsample to 1.0 while for the other boosting types it's a float between 0.5 and 1.0."
### 4.2 Summary Statistics:\n\nSpark DataFrames include some built-in functions for statistical processing. The describe() function performs summary statistics calculations on all numeric columns and returns them as a DataFrame.
Look at the minimum and maximum values of all the (numerical) attributes. We see that multiple attributes have a wide range of values: we will need to normalize your dataset.
## 👨‍⚕️ Main findings of descriptive analysis 
 There are no missing values in this dataset. 
\nImporting Necessary Libraries
\nTraining Dataset
\n   \n# Q1 Age Distribution\n\nAt first Let's plot the distribution of developers age to see if there is special relation between age and Developing data science
*Data science is mostly done by the young developers belowe 35 years old*
"\n    \n    \n# Q1 , Q2 Age and Gender\n\nNow let's plot the distribution of Developers for different Age and Gender\n"
*In general Men are leading the Data science field especially those of age 20s*
\n    \n    \n# **Q3 Country of residence**\n\nLet's see the developers distribution across the countries 
*It's obvious how developers in india are the most with more than double of those in usa*
\n    \n    \n# **Q4 Education Background**
*Most of Data scientists have Master's and Bachelor's degrees*
\n\n    \n    \n# **Q5 Current Role Title**
"*Here we can notice that Data science is an ongoing educational process and that's why most of data scientists call them sleves students, so we shouldn't ever stop learning and exploring new subjects.*"
\n\n    \n    \n# **Q6 Experience Years**
"*Well about 70% of users have experience lower than 5 years, so it is never too late to start a data science career*"
"Data science can be applied using more than 10 langauges and each langauge has different corresponding tools like libraries and IDEs, so let's analyze the Programming languages used and their relation to different variables as gender, age, role title and education background"
Here we will analyze the prgramming languages used for each gender
"*Whatever your prefered programming language you will be able to code in data science with more than 10 languages to start with. But still Python, SQL,and R languages are the black horses in the field*"
"Data Science in 2021 : Adaptation or Adoption?\n\nUnderstanding the Current State of AI / Data Science Adoption\n\nThe *COVID-19* pandemic transformed the way organizations work, collaborate, and operate. In just a few months, the crisis brought about years of change in the ways different companies, sectors, and regions do business. In many industries, the changes were damaging, while in others, they were promising and beneficial. The healthcare/pharma sector, for example, became aggressive, supply chains witnessed major disruptions, and the e-commerce sector boomed up. On the other hand, the local business, travel, and tourism industries were critically affected.\n\nFrom a technology perspective, organizations felt a need for more stabilization and more innovation in which the *adoption of* / *adapting towards* Data Science and Artificial Intelligence played an important role. To stay competitive, ambitious, and operate just like normal times, organizations were required to make strategic shifts, alter their long-term views and develop a data-first / technology-first mindset. Usually, when organizations need to adjust their technology landscape, they follow one of the following two types of approaches:\n\n""The Adapt Approach"" or ""The Adopt Approach""\n\n- The Adapt approach is a conventional idea where companies need to continuously modify their core technology (including the data and the models) to meet the business objectives. This approach only offers a short-term solution which is also inefficient, expensive, and innovation-killing.\n\n- The Adopt Approach enables the organizations to incorporate long-term solutions with best practices and design principles. Companies need to invest in building specialized teams and start adopting a data-first / technology-first culture. \n\nThe more incorporation of the ""adoption approach"" can assist the organizations to accelerate their innovation. Several studies conducted in the pandemic year suggest that digital adoption has taken a quantum leap and the Covid-19 crisis has accelerated the adoption of analytics and AI, This momentum is likely to continue in the 2020s. \n\nBut a vital question that needs to be answered is: Are the organizations and governments ready for a data-first culture and AI Adoption? \n\nAs part of our regular job as customer-facing data scientists, we interact with several organizations - across different countries and multiple industries. This involves companies of varying sizes, different revenues, and different data maturity levels. Some are at a very early stage, some are highly advanced. A common observation is that ""many organizations are still at very early stages in their AI adoption"".  If we have to categorize them in different groups, it can be analogous to  stages of plant growth : \n\n\n\n\nThis scale ranks the entities (organizations, industries, or countries) at different levels of data science and artificial intelligence adoption. \n\n\n \n     Seedling \n     Vegetative \n     Budding \n     Flowering \n     Ripening \n \n\n    Stages\n    Roots Development\n    Stems Growth\n    Plants Formation\n    Flowers/Fruits\n    More Nutrition\n\n\n    Sector / Country\n    Lack of goodof Data\n    Building a data culuture\n    Supporting the Startups with Talent, Fundings\n    Major Investmentsin AI Initiatives\n    Qualified ProfessionalsBetter JobsEducation Degrees\n\n\n    Organizations\n    Establishing the needof Data / AI\n    Focus on data procurement\n    Minimal analysis,and basic modelling\n    Use of data, predictive modelling,and machine learning\n    Models in Production  with Greater ROI\n\n\n\n\n2021 Kaggle Machine Learning & Data Science Survey is the most comprehensive dataset available on the state of ML and data science. By blending this data with several external datasets, we have analyzed the state of data science and artificial intelligence adoption in 2021. We utilized some of the questions directly related to this topic such as: \n\n- Q23: Does your current employer incorporate machine learning methods into their business?\n- Q22: How many individuals are responsible for data science workloads at your place of business?\n- Q26: How much money have you or your team spent on machine learning/data science?  \n\nand many indirect questions that reflect the state of AI/data science: \n\n- Q16-19: Do you use ML, NLP, Computer Vision, etc.?  \n- Q33: Which big data products (relational database, data warehouse, data lake, or similar) do you use?\n- Q38: Do you use tools to help manage machine learning experiments?\n\n\nMethodology\n\nWe aggregated the survey dataset by regions and industries. We then measured the mean, count, diversity among the following: education qualifications, job roles, data science techniques, cloud usage, machine learning platforms usage, data team size, incorporation of machine learning by employers, coding and machine learning experiences, big data tools, auto ml tools, etc. Using the aggregated information, we defined a unified data science adoption index that reflects the current state of maturity levels. We used the simple mean aggregation to derive the adoption index. We then visualized the adoption index, current machine learning incorporation levels by various internal and external data attributes such as - Global innovation index, competitive data science rankings, Industry revenues, job postings by countries, etc. This analysis helped us to identify which groups are leading while which ones are lagging. \n\n**Validation:** To make sure that our methodology, insights, and findings are aligned with industry trends, we manually cross-checked a number of online references. This includes online reports, articles, and surveys published by organizations such as IBM, Mckinsey, Dataiku, Stanford, World Intellectual Property Organization, Element AI, etc. The references are added in the last section.  \n\n**Visual Theme:** We developed a common visualization theme of the plots - HotSpot charts (bubble charts with hotness as the color theme), where something higher in quantity is colored as hot (red), and something lesser is colored as cold (blue). We selected the seed-plant growth stages analogy to reflect different stages of adoption levels. \n\n**External Datasets Used:**\n- **[Kaggle Analytics 2021 Survey Data](https://www.kaggle.com/c/kaggle-survey-2021):** We used the entire data of kaggle survey respondents but removed the 'Students', 'Currently Not Employed', from industries - 'Other' and 'Academics/Education', from countries - 'Other', 'I do not wish to disclose my location. For some parts of the analysis, we also removed responses 'I do not know' where respondents were asked about their current employer AI / ML maturity. \n\n- **[Kaggle Analytics 2019 Survey](https://www.kaggle.com/c/kaggle-survey-2019):** We selected the Kaggle dataset from the year 2019 to compare a few metrics from pre-pandemic year to post-pandemic year. Most of the questions from this data were similar to the 2021 dataset so it was easier to compare. \n\n- **[Data Science Jobs, Revenues by Sectors, Countries](https://www.kaggle.com/shivamb/glassdoor-jobs-data):** This data contains data science/machine learning / AI-related jobs counts from Glassdoor and Linkedin. The Glassdoor data contains information about the companies such as - Revenues, Sectors, and Linkedin Data contains the count of jobs by country. We developed a scraper to obtain Glassdoor jobs data, while we manually obtained the counts of job counts by country from Linkedin. \n\n- **[Tech Fundings Since 2020](https://www.kaggle.com/shivamb/tech-company-fundings-2020-onwards):** We used the data of fundings in the technology sector by countries, by sectors, and time. The data is from 2020 and was obtained from GrowthList. The data also contains a specific subset of artificial intelligence fundings.  \n\n- **[Competitive Data Science by Countries](https://www.kaggle.com/hdsk38/comp-top-1000-data):** This is the dataset of top 1000 users in the four types of Kaggle rankings (i.e. Competition, Datasets, Notebooks, and Discussions) as of October 2021. The dataset was prepared by [tropicbird](https://www.kaggle.com/hdsk38) obtaining the kaggle rankings and identifying the locations of the users. \n\n- **[Global Innovation Index 2021](https://www.wipo.int/edocs/pubdocs/en/wipo_pub_gii_2021.pdf):** The Global innovation index (GII) is computed by World Intellectual Property Organization. The GII's overall formula for measuring an economy's innovative capacity and output provides clarity for decision-makers in government, business, and elsewhere as they look forward to creating policies that enable their people to invent and create more efficiently.\n\n\n1. How does AI Adoption looks like in 2021 ?"
"●    Israel with 21% has the highest proportion of models in production for more than 2 years. \n\nThe data shows that about 24% have well-established machine learning methods. Some characteristics of people from this group are:  \n- more in technology and engineering focussed job roles (36% Data Scientists, 12% ML Engineers) than the managerial jobs (4% Managers)\n- more coding experiences (5-10 Years: 23%, 3-5 Years: 20%)\n- larger data science teams (20+ team size: 55%)\n\n**Israel** (with 21% of organizations having models in production for 2 years and more), is no stranger to innovation and is widely acknowledged as a startup nation. Studies suggest that Israel has exceptional levels of time and effort on building a ripe environment in which new tech companies can flourish. Israel Government has played an important role, they even stated in 2019, ""We are now making a national plan to make Israel, within five years, one of the top five leading countries in the world in artificial intelligence technology,"", It seems that they are right on the track in 2021 and going ahead. \n\n●    45% Organizations have not started exploring data science, machine learning or are in the very early stages of exploring\n\n45% is a big number given that it is 2021. Among this group, 38% are from middle-east countries: 'Saudi Arabia', and 'Iraq'. Saudi Arabia Government have started making major efforts in improving their Data Science adoption, they recently launched the development of [data science consortium in next decade](https://www.c4isrnet.com/artificial-intelligence/2021/01/15/saudi-arabia-makes-artificial-intelligence-a-cornerstone-of-its-2030-vision/). So we should expect to see this number improve in the coming years. 25% from this group are from Belarus which is a surprise addition in this group, as most of the other European countries are relatively mature in this field. In general, this 45% has: \n\n- respondents with either **no formal education past high school (about 14%)** or they choose not to answer this question (10%)\n- 1 in every 5 individuals (20%) never written code\n\nThe lower adoption of AI and data science can be due to multiple factors. An AI startup called DLabs.AI from Europe compiled the biggest challenges cited during AI / ML adoption. These included: \n\n1. Skillsets, Data, Use-Cases\n2. Understand the Need for AI \n3. Failing to Explain how AI solutions work \n4. Lack of Collaboration between AI teams \n5. Regulations, Management Issues, Complexity\n\n●    35% from Retail, Online Services, and Insurance sectors have well established data science \n\n- These three sectors especially 'Insurance' have shown a significant improvement in AI adoption over the last couple of years. \n\n2. The Industry Outlook of Data Science Adoption"
"\n\n●    2 out of 5 respondents from Governments or Public Services shared that they have not even started any machine learning or data science \n\n\n\nA key reason for the lower AI adoption among governments is the established set of practices and processes. In governments, there can be less encouragement for employees to take risks and innovate. In private sectors, employers tend to put a strong focus on experimentation, innovation, and growth. These numbers clearly show a big scope of improvements in the AI and data science vertical. Since all the governments have access to huge amounts of data, they have a very high potential of using data science in a meaningful way. Among this group: \n\n- Approx 50% of respondents had less than or equal to 3 years of coding experience  \n- 40% shared that there is no data science team, and the other 40% shared max 1 or 2 people are handling the data science workloads\n- About 85% has less than 2 years of machine learning experience; among then 27% had never used machine learning \n\n\nGartner has identified that the most common use case of data science and AI among Governments is the chatbots to assist customer communications.   \n\n\n\n\n\nBut are these so-called productized/deployed chatbots Mature? Are they good enough to serve the common public? **Maybe not!** \n\nA funny incident occurred last month (October 2021) in Singapore, where a chatbot named Jamie of Singapore's Ministry of Health responded with the highly misaligned reply to a Covid-19 Question. A person asked about covid-19 precautions but the generated response was about safe sex practices. It became a comedy meme on social media and the government was criticized, Ultimately they had to shut down Jamie. [Source - Channel News Asia - Singapore MOH ChatBot Funny Response](https://www.channelnewsasia.com/singapore/moh-ask-jamie-covid-19-query-social-media-2222571). \n\nFollowing example shows that Governments not only need a strong focus on data science but also need strong governance and thought leadership. \n\n\n●    Insurance Industry: 1/3rd participants suggested that their current ML landscape are advanced (more models in the production)\n\n\nAn interesting find in the analysis shows the better adoption of data science in the insurance industry. One possible intution is that this group contains more actuaries, statisticians, and data scientists. Typical productionable use cases in InsurTech includes - claims forecasting and prediction, underwriting risk assessment, improved customer service and experience, customer acquisition, retention, and churn prediction. Also interesting to note that 64% respondents from Insurance sector have higher education and 44% are in the Data Science roles. \n\n\n\n●    Online Services / Internet Based Business have significantly higher proportion of more AI / ML adoption\n\n\n- More than **25%** respondents mentioned that they or their teams have spent more than **USD100,000** on ML / Cloud\n- Close to **50%** from this group have larger Data Science / AI teams with more than 20 Individuals part of it. \n- **One-Third** are from larger Multi-National Corporations (with 10K+ employees)\n- Usage of **Transformer Networks (BERT, GPT-3, etc)** is the highest for Online Services / Internet Based Business: About **15.73%**\n\n3. AI Adoption Index - A Unified Index using Kaggle Survey Data\n\nSo far we have explored only the question about the extent of machine learning incorporated by the organizations (Q23). In the Kaggle Survey Data 2021, there is a bunch of other useful and valuable information that reflects the state of data science, machine learning, and artificial intelligence adoption in different groups. There are questions regarding the skills of the people, the technology they use, the job roles they have, and their educational qualifications, etc. All these aspects are the essential elements that define how mature is the adoption of AI in a country/sector. A standard hypothesis regarding a country having more AI data science adoption and maturity can be due to a combination of:  \n- multiple job openings in a country related to data science\n- many highly qualified individuals in a country (with masters, PHDs)\n- several people/organizations using cloud architectures, machine learning management platforms, machine learning tools\n- more fundings, investments in the technology sector \n- more competitive data science professionals (example - kagglers in top 1000)\n- more organizations with models in production, the larger data science team size, and spend considerate amounts on data related workflows\n- strong focus on innovation, research, and development  \n\nUsing the Kaggle Analytics survey data of 2021, we tried to map the responses of individuals into different categories to derive a standard adoption index. \n\n| | Component | Question | What it means | \n| --- | ---- | ---- | ---- | \n| 1 | Strategy | Q5: Job Roles, Q22: Team Size, Q26: Money Spent | Organizational Strategies - hiring more diverse job roles ie. a mix of engineers, analysts, scientists, managers, etc.,spending more in building data infrastructures and data science teams  |\n| 2 | Data | Q32, Q33 Data Tools Usage | Organizations using more data tools, databases, data lakes are already in better data / ai adoption stages  |\n| 3 | Technology | Q38, Q23: Current ML Adoption, Tools Used | How advanced is current technology stack  |\n| 4 | People | Q4, Q5: Roles, Qualification | Are there enough qualified people, enough people with relevant job titles  |\n| 5 | Governance | Q31, Q37: Platforms Enabling Governance | Use of Saas platforms to enable Governance - Explainability, Regulations, Controlled Access  |\n| 6 | Automation | Q36, Q37: AutoML Platforms, AutoFE  | Organizations focussing on productivity and automating mundane tasks   |"
" Source : DataIku - Key Elements of Enterprise AI  \n\n These components align with several other interesting metrics and indexes developed by organizations. Using the kaggle data, we developed the index in the following manner: \n    \n\nFirst, aggregate the kaggle data by different groups (country/sector), Perform ordinal encoding of questions such as current levels of ML maturity, data science team sizes, current ML spendings. Then measure the counts and mean of tools/technologies used in a group, for example - ml tools used, data tools used, auto ml tools used, technologies used. Also measure the mean and standard deviation of groups such as job roles, education backgrounds, coding experiences, and ML experiences. Finally, the measures are combined and normalized to get a unified index, called the AI adoption index. This index is a mix of people, tools, skills, qualifications, maturity, data, and governance. \n\n \n\n4. Industry Rankings using AI Adoption Index\n\nLet's identify the industries which rank higher in the derived AI adoption index. And also how it is related to educational qualifications within the industry. "
"\n\n●    Manufacturing Sector shows a huge scope of AI adoption going forward while Insurance Sector Leads \n\n\n\nReports suggest that the global market for manufacturing is expected to grow from USD 86.7B in 2020 to USD 117.7B by 2025. These numbers are significant and data science can play a tremendous role in this. However, despite the big market size, the current adoption levels of data science are relatively low. A key reason for the slow adoption is the complexity of edge deployments which allows processing and inferring the data locally on the equipment or machines. Deployments at the edge are not an easy/straightforward task. It requires models to be capable of low latency and high-frequency scoring and can process large chunks of data in near-real-time. In this sector the respondents were: \n\n- Majority (18%) are from Japan, which is a hub for manufacturing giants \n- Majority shared that their day to day job role includes: Analyzing of Understanding data to influence product or business decisions\n- 12% have been the adopters of Google Cloud AutoML \n    \nBroadcasting/Communication sector ranked 4th among the 16 scored industries in this dataset. With an increased focus on hyper-personalization and an increasing number of data-centric use-cases like ""advanced search"", ""content monetization and moderation"", ""auto caption, and sub-title generation"" becoming common, the industry is growing well in AI adoption. Most numbers of the respondents from the broadcasting sector are from India (14%) where the AI companies struggle to serve a billion-plus population. 34% of respondents shared that at least 20 people are responsible for data science workloads in their teams.   \n\nMedical/Pharma sector and Military/Defense sector have the highest percentage of qualified professionals (PHDs, Doctorates, Master's). Military sector had 53% with master's degree while medical had 47%. Both of these sectors especially are doing much better than public services or governments. \n\nThe general insights of the AI adoption index align with the machine learning incorporation by organizations (as asked in Q23). All the computers, technology or internet related sectors leads while Governments, Non-Profits lags. The data shows that these sectors have the least spendings on data science workloads. The insurance sector shows the highest levels of AI adoption, they also have the highest usage of machine learning platforms such as - Sagemaker from Amazon, or DataBricks. This particular analysis is interesting since the market size and the average revenues of the Manufacturing Sector are very high, still, AI adoption remains low. In the next section, we focussed on the analysis of sector-wise revenues. \n\n5. Industries (with Data Science Jobs): Revenues and Adoption \n\n**External Dataset Used:** [Data Science Jobs by Sectors, Countries (2021)](https://www.kaggle.com/shivamb/glassdoor-jobs-data)\n\nWe analyzed an important metric - the average revenue of a sector especially those which are adopting data science or have data science departments. To obtain this data and information, we procured the data about data science-related job roles from Glassdoor and Linkedin along with meta information. The meta-information includes the average revenue of the company, sector, country, and company size. The initial data was very messy and contained text in different languages, we cleaned it manually, used text processing, and translated it all to English. After the preprocessing, we got about 10000 job listings with proper metadata.  "
"\n\n●    Shipping, Energy, Manufacturing and Retail sectors are ranked much higher in average revenues but AI adoption is relatively low\n\n\n\nAccounting/Finance or Military/Defence sectors have higher percentage of companies with more than USD 10B+ USD, but their current AI / data science maturity levels are lower than some of the other sectors such as Insurance. The bigger size of the bubbles shows that there are multiple data science job opportunities in the organizations with higher revenue. The analysis shows the industries/sectors which are leading, such as - Online related, Insurance, Computers and the ones which are lagging such as - Governments, NonProfits, Manufacturing. Hospitality/Entertainment/Sports have also higher revenue but lower percentage of people having machine learning experiences. About 48% shared their current ML experience is less than a year. \n\n6. A Geographical Perspective of AI Adoption \n\nRespondents were asked about their country/region in the Kaggle Survey. There were respondents from 66 different countries, including all parts of the world - Americas, Asia, Europe, Africa, the Middle East, and Australias. \n\nLet's take a look at how different ""Countries"" stand at different levels for machine learning incorporation. The following visualization lists down the top 20 countries with the most kaggle survey respondents. The y-axis is measured directly using the question (Q23) asked - Do your employers incorporate machine learning methods. The size and the color represent the percentage of respondents from that country who mentioned how much ML is incorporated in their organizations. "
"\n\n●    Asia is lagging, Europe is leading, Africa,Middle East are growing\n\n\n\n\n- The most number of countries from Asia which includes China, Bangladesh, India, Indonesia, Pakistan, South Korea, and Taiwan have comparatively lower levels of machine learning incorporation. On an average, close to 30% of respondents from these countries suggested that they are just exploring machine learning methods, the other 25% suggested they have not even started any machine learning in their organizations. In total, about 60% suggested that they are just venturing into the machine learning space. Europe on the other hand, shows several countries having well-established methods in the organizations. \n\n- African and Middle East countries are showing up more in recent years. Though about 45% from Nigera said they have not yet started any machine learning, The growth of AI and data-related startups in countries like Nigeria have increased and they are improving in the overall machine learning. Due to the latest developments (such as - technology adoption, government policies relaxation, improvements in education) in the continent, Many companies are looking at Africa as the next place for expansion and investment. A [report](https://ircai.org/the-growth-of-artificial-intelligence-in-africa-on-diversity-and-representation/), by Internation Research Center of AI (IRCAI), suggested that use AI and related technologies as an opportunity to create and reinforce diversity is the main focus area for companies operating in the African region. This also includes a focus on the skills development of diverse people and to make concerted efforts at leveling the playing field for women and other minorities in the industry.\n\nIn recent years, Government efforts to improve the AI landscape have improved across the globe and it is encouraging to see the countries realizing the true potential of AI and data science. However, The coronavirus pandemic affected all sectors of the economy in 2020, and the AI sector is no different. \n\n7. Pandemic Effect\n\nAs the organizations worldwide manage ongoing pandemic effects and hold a slowly evolving post-pandemic future, a key question became important: Just how much did COVID impact the AI industry? \n\nSeveral AI startups emerged during this period that assisted the impacted industries, including accelerating the Covid-19 Drug Discovery, facilitating the supply chains, and optimizing the logistics during lockdowns. Many organizations had to reprioritize their hiring and operational strategies. The virtual formats of meetings, conferences, and meetups lead to significant spikes in attendance too. There were a lot of layoffs too, companies had to cut down their budgets. The following chart shows the change in percentage change of machine learning incorporation by country before the pandemic and after."
"Many countries saw an overall improvement machine learning adoption and incorporation in the organizations. The positive increase is encouraging to see even during the pandemic. Israel, Norway, UK, Australia, and Poland have the highest percentage of change since 2021. Though some countries observed a decline, this includes - Turkey, Greece, Romania, Ireland, and the Netherlands. Varying levels of AI adoption in different regions can be linked to several factors such as:\n\n 1. Investments \n 2. Jobs\n 3. Research\n 4. Skills\n\n- Investments: AI / Tech Company Fundings  \n- Jobs: AI Related Hiring / Jobs \n- Research: Global Innovation Index   \n- Competitive Data Science: More skilled data scientists \n\nThere could be more factors such as - Education or Government Regulations. In this notebook, we have only explored the above listed factors by using different external datasets and relate them with the AI adoption index using the kaggle survey data. We will use the same methodology to derive the AI adoption index as described in section 3. \n\n8. Investments/Tech Fundings and AI Adoption \n\n**External Dataset:** [Tech Fundings Since 2020](https://www.kaggle.com/shivamb/tech-company-fundings-2020-onwards)\n\nThis external dataset contains funding information in the technology sector by country, by sector, by date. Original source of the data is GrowthList and the cleaned data is posted on Kaggle. The data contains funding information since January 2020 and clearly shows that more dollars are flowing into the AI sector. "
"Interesting to see that few countries with a low AI adoption index have had more technology fundings in recent years. \n\nThis includes countries such as - Indonesia where the data science adoption remains at the exploration stage. However, there a number of tech companies which are making big impacts, example - GoJek, a logistic compnay who recently bagged the funding of USD 1.2B to scale up their technology, AI, and data science operations. Another popular rising startup is the payment platform provider Xendit, which rased USD 150M to incoportate ML and AI driven systems in thier core processes. Other countries include - Belgium, Mexico, and Denmark with more recent fundings in the technology sector. \n\nNigeria is an important regional player in Africa, making about half of West Africa’s population with 202M people and one of the largest youth populations in the world. It has recently become a center of attraction for technology and AI investors. This year SoftBank Vision Fund 2 led a USD 400M funding round for **OPay**, valuing the Nigerian mobile-payments platform at USD 2B and marking the investment vehicle’s first bet in Africa. The other investments in Nigeria included - **FairMoney (USD 42M)** which offers digital banking and instant loans providing collateral-free personal loans. \n\n\n9. Jobs in AI vs Adoption \n\nWhich countries are hiring in the data-related / AI-related roles?\n\n**External Data Used:** [Linkedin Data Science Jobs in 2021 by Country](https://www.kaggle.com/shivamb/glassdoor-jobs-data)\n\nThe AI industry witnessed strong hiring growth during the pandemic. While health care, finance, and other service industries have been strong early adopters of AI, manufacturing, retail, and other sectors are expected to grow in hiring and AI-skill penetration as well. Let's see what data is talking about. \n"
"### 10. Global Innovation Index\n\nThe Global Innovation Index (GII) is developed by World Intellectual Properrty Organization and takes the pulse of the most recent global innovation trends. It ranks the innovation ecosystem performance of economies around the globe each year while highlighting innovation strengths and weaknesses and particular gaps in innovation metrics. The different metrics that the GII offers can be used to monitor performance and benchmark developments against economies within the same region or income group classification. The Global Innovation Index 2021 was released on in September 2021. Switzerland, Sweden, the U.S., and the U.K. continued to lead the innovation ranking, and have all ranked in the top 5 in the past three years. Using this interesting index, we wanted to check if it correlates with our AI adoption index and the relationship between the two. \n\n**External Dataset Used:** **[Global Innovation Index 2021](https://www.wipo.int/edocs/pubdocs/en/wipo_pub_gii_2021.pdf)**"
"\n\n●    Switzerland - Most Innovative Country, Highest AI Adoption, Most models in Production !! \n\n\n\nIt is interesting to note that the two indexes show a significant positive correlation, and also the winners of both indexes are the same - Switzerland. The swiss nation is pushed to focus on research and development and has invested in the establishment and funding of a high number of world-renowned universities, which in turn attract multinationals seeking highly qualified employees. In terms of the AI adoption index, Switzerland has:  \n\n- Very high usage (75%) of Cloud Platforms (GCP, AWS)\n- Very large number of Master's Degree holders (46%)\n- Very large number of PHDs (26%)\n- Large number of individuals with 10+ years of coding experience (44%)\n- The highest percentage of Organizations with Models in production (49%) \n\nOther European countries had similar insights - Norway, Austria, France, Netherlands, Germany, along with UK and USA. \n\n- United Arab Emirates shows good progress and advancements in terms of both innovation and data science first. More than 50% mentioned they have about 0 - 2 years of the machine learning experience. Mostly mid-sized companies with employee size <250 have been prominent (about 46%). Nepal and Ethiopia are appealing as the innovation index is low but the AI adoption level is better. This is because of the startup culture, startup investments, and people building their data science skills through multiple channels. In these countries, more than 50% of respondents are from non-multinational companies. \n\n\n\n11. Competitive Data Science (Top 1000 Kagglers)\n\nWe looked at countries by the number of kagglers in the top 1000 competition ranks. The idea was to understand the presence of skilled people active in competitive data science (by taking kaggle rankings as the proxy). We understand that identifying the numbers by population can give a relative percentage, but we restricted the analysis to absolute counts, to understand which countries lead and is there any correlation between the two. "
"Countries like Japan and Russia, despite lower populations than USA and China, have a lot of competitive data scientists or kagglers in the top 1000 in the competition category. It is in this environment that AI Dynamics is finding increased traction in these countries. The government has also played an important role in the improvement of AI adoption over the years. For example, Russia has adopted a new law (the ""AI Law"") establishing a special legal framework for the development and adoption of AI technologies in Moscow for the period from 1 July 2020 to 1 July 2025 (the ""AI Framework""). Enforcing this type of Law straight by the Government shows that many country leaders realize the potential of AI adoption and have to make more efforts to remain competitive. \n\nIt is interesting to note that all the key European countries where AI adoption levels are higher have fewer than 10 kagglers only in the top 1000. Yes! the Country population does matter, but again our aim here is to compare the regions in an absolute sense. This group of countries includes - Norway, Switzerland, Austria, Netherlands. \n\n\n\nConclusion\n\nOur intention for choosing the theme of AI adoption was to understand the current maturity levels of the data science field across industries, sectors, and regions. The chosen theme and the richness of datasets motivated us to create a unified measure (index) that quantifies the state of AI/data science maturity/adoption. This type of index is analogous to the growth stages in plants, where a plant matures through different stages in its lifetime. This can also be measured by the 'hotness' of something, which means something popular has more hotness and something not popular has less hotness. These reasons inspired us to create a HotSpot chart that measures ""hotness"" by color and shows different stages of plants to reflect the current stage. \n\nIn summary, Adopters of AI continue to have confidence in their capabilities to drive value and advantage, they are realizing the competitive advantage and expect AI-powered transformation to happen for both their organization and industry, ultimately for their country. Those following the adaption approach need to seriously consider shifting to adopting side. By following the right practices and aligning them with current trends, current and future AI adopters can place themselves not just to survive but to flourish in the emerging era of pervasive AI.\n\nFrom the overall analysis and our research during this analysis, one fact became evident; Beyond the pandemic, the adoption of data science is anticipated to grow both horizontally and vertically. It is likely to move from fast adopters to less tech-focused industries, functions, and geographies in 2022 and onwards. To keep up to date in this period of change and transformations, organizations and leaders need to think of the right strategies to incorporate a data-first culture. While organizations are aware of the importance of adopting this culture, they often fail to approach it from a strategic standpoint. It is important to understand that doing data science to be cool will be disappointing. One needs to have a proper adoption plan along with a purpose.\n\nTo overcome this, organizations definitely need ***K.A.G.G.L.E.*** \n\nK: Knowledgable and Skilled Workforce (Skills)   \nA: Adoption of a data-first, data-driven culture (Data)      \nG: Growth-Oriented technology (Strategy)     \nG: Governance and Compliance (Governance)   \nL: Leadership that values a change (Strategy)       \nE: Experienced and Educated Professionals (People)\n\n\nReferences\n\n- [1] [SaferBrand Plant Gorwth Stages](https://www.saferbrand.com/articles/plant-growth-stages)\n- [2] [Stanford Human Centered Intelligence](https://hai.stanford.edu/news/how-has-covid-affected-ai-economy)\n- [3] [ElementAI - AI Maturity Framework](https://s3.amazonaws.com/element-ai-website-bucket/AI-Maturity-Framework_White-Paper_EN.pdf)\n- [4] [Oxford Insights AI Readiness Report 2020](https://static1.squarespace.com/static/58b2e92c1e5b6c828058484e/t/5f7747f29ca3c20ecb598f7c/1601653137399/AI+Readiness+Report.pdf)\n- [5] [Sopra Banking Software - Which Approach is best for your business?](https://www.soprabanking.com/insights/adopt-vs-adapt-which-approach-is-best-for-your-business/)\n- [6] [World Global Intellectual Property Organization - Global Innvoation Index 2021](https://www.wipo.int/global_innovation_index/en/2021/)\n- [7] [Dataiku - 5 Elements of Enterprise AI, Insurance Industry](https://www.dataiku.com/stories/how-ai-empowers-the-insurance-industry/)\n- [8] [HBR - AI Adoption in last 18 Months](https://hbr.org/2021/09/ai-adoption-skyrocketed-over-the-last-18-months)\n- [9] [Deloitte AI Adoption Insights](https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/state-of-ai-and-intelligent-automation-in-business-survey.html)\n- [10] [Tech Fundings in Pandemic](https://theconversation.com/tax-pandemic-profiteering-by-tech-companies-to-help-fund-public-education-155705)\n"
Check the class balance:
Separate the input variable names by excluding the target:
# Import Data
"**In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.**"
# To learn about more complex regression:\n\n1. Regression Neural Network: https://www.kaggle.com/milan400/regression-neural-network\n\n![image.png](attachment:image.png)
"**k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition and observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. It is popular for cluster analysis in data mining. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, Better Euclidean solutions can be found using k-medians and k-medoids. **"
# 2. PCA
"**PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.**"
# 3. Apriori Algorithm
"### Detailed and Full Solution (Step by Step)\n\nHello kagglers ..\n\nThis notebook designed to be as detailed as possible solution for the Houses pricing problem, I tried to make it typical, clear, tidy and **beginner-friendly**.\n\nIf you find this notebook useful press the **UPVOTE** button, This helps me a lot ^-^.  \nI hope you find it helpful.\n\n\n\n\n\n\n\n\n"
\n        \n            1 ) Importing the data:\n        \n
"### Regular differentiation  \n\nSometimes it may be necessary to difference the data a second time to obtain a stationary time series, which is referred to as second order differencing."
The p-value indicates that series is stationary as the computed p-value is lower than the significance level alpha = 0.05. 
### Autocorrelation  \n\nAutocorrelation is the correlation of a time series with the same time series lagged. It summarizes the strength of a relationship with an observation in a time series with observations at prior time steps.\n\nWe create autocorrelation factor (ACF) and partial autocorrelation factor (PACF) plots to identify patterns in the above data which is stationary on both mean and variance. The idea is to identify presence of AR and MA components in the residuals. 
There is a positive correlation with the first 10 lags that is perhaps significant for the first 2-3 lags.\n\nA good starting point for the AR parameter of the model may be 3.\n\nLets try out autocorrelation on the differences...
There are not many spikes in the plots outside the insignificant zone (shaded) so there may not be enough information available in the residuals to be extracted by AR and MA models. \n\nThere may be a seasonal component available in the residuals at the lags of quarters (3 months) represented by spikes at these intervals. But probably not significant.
\n\n\nContents\n\n1 Understanding the data at hand\n\n2 Exploratory Data Analysis\n    \n3 Feature Engineering\n    \n4 Modelling\n          \n\n\n
 \n# 1. Understanding the data at hand
From above statistics it is clear that Women were given more preference than Men while evacuation  
The above graph makes it clear that most of the people were aged between 20-50
It is clear from vizualisation that most of the survivors were children and women 
Most of the people who died were from Passenger Class 3 irrespective of Gender
The above stats show us survival of each class and its clear the ones in better class had a better chance of survival\n## Power of money
Most of the embarkments were from class : S\n\nLeast embarkments were from class : Q
"### %matplotlib notebook\n\nThe  `%matplotlib inline`  function is used to render the static matplotlib plots within the Jupyter notebook. Try replacing the `inline`  part with  `notebook` to get zoom-able & resize-able plots, easily. Make sure the function is called before importing the matplotlib library."
-   **%matplotlib notebook** vs **%matplotlib inline**\n\n![](https://cdn-images-1.medium.com/max/800/1*IAtw6rydG7o58yy2EyzCRA.png)
"# Basic Fashion MNIST Classification from TensorFlow Docs\n## A look at neural network: basic classification using tf.keras\n\nThis notebook trains a neural network model to classify images of clothing, like sneakers and shirts. It's okay if you don't understand all the details, this is a fast-paced overview of a complete TensorFlow program with the details explained as we go.\n\nThis notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow."
"## Import the Fashion MNIST dataset\n\nThis guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\n\n  \n    \n  \n  \n    Figure 1. Fashion-MNIST samples (by Zalando, MIT License). \n  \n\n\nFashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset—often used as the ""Hello, World"" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we'll use here.\n\nThis guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code. \n\nWe will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow, just import and load the data:"
"## Preprocess the data\n\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
"We scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we divide the values by 255. It's important that the *training set* and the *testing set* are preprocessed in the same way:"
Display the first 25 images from the *training set* and display the class name below each image. Verify that the data is in the correct format and we're ready to build and train the network.
"## Build the model\n\nBuilding the neural network requires configuring the layers of the model, then compiling the model."
"## What Kagglers think of MOOC?\nKaggle ML & DS survey asked a very interesting question (#39): \n> _How do you perceive the quality of online learning platforms [MOOCs] as compared to the quality of the education provided by traditional brick and mortar institutions?_\n\nTo answer this, a respondent did not have to have ever used a learning platform - only having an opinon was required, though an option of ""No opinion; I do not know"" was also available. In the end, a little over 60% of survey participants shared their attitude on the matter. The distribution of answers tells a different story accross education and country of residence segments.\n\n_Note_: Before diving in, I preprocess the complete dataset in order to simplify analysis and make data visualization look better: convert range values to be ordinal, shorten some long category names, rename columns, and convert strings to numeric where appropriate."
"### Kagglers generally favour online learning...\nOverall, Kagglers seem to have a favourable view of MOOCs such as those found on platforms like Courseara, Udemy, or DataCamp. From close to 16,000 responses to question 39, I've excluded 1,564 that do not have an opinion on the matter. Close to 60% of remaining respondendents believe MOOCs are better than traditional eduction, while 16% think they are worse. The sample is of course biased. After all we are asking people who actively engage with an online community - Kaggle."
"### ... but higher education means higher expectations\nThe sentiment around MOOCs is not uniform accross various levels of higher eduction. Respondents who have attained Bacherlor's degrees are much more enthusiastic (66% rank better, 12% rank worse) about MOOCs than those who put in more years of study to attain a Doctoral degree (48% rank better, 23% rank worse). This makes sense if we consider the advanced classes and rigorous research supervision that is seldom, if ever, found in online learning platforms. MOOC has other shortcommings as well, such as lack of student engagement in class discussion, low completion rates, and lack of feedback [3]. As it stands, online learning can be seen as augmenting education, not replacing it. The milage varies, by country though."
"### USA disillusioned with MOOC, while India is in love\nLooking at the top three countries, based on number of responses, I noticed something surprising. One would imagine USA, where millionaires who dropped out of college are hailed as role models and where the cost of higher education (and student debt) is evergrowing, would see online courses very favourably. However, American respondents ranked MOOCs worst vs traditional education when compared to other countries. \n\nRespondents from India and China, on the other hand, seem to embrace online learning with open arms. Almost 80% of Indian respondents, for example, see online learning as better than brick and mortal institutions. Since the MOOC content is largely the same between countries, it stands to reason that the difference in opinion is due to quality of traditional education or access to that education. There is evidence to support this, and the MIT Technology Review [4] put it best, I think:\n> In a country [India] of rigid teaching styles and scarce university slots, students and professors are exploring what online learning can be."
"# Which MOOC platforms are winning?\n\nHere it is useful to define two groups of respondents: one that uses a single MOOC platform (n=5,992) and one that engage with multiple platforms (n=9,679). \n\n### Single-platform learners often choose Coursera as their sole study\nAmong single-platform users, Coursera ranks number one, followed by Kaggle's very own learning resource. Koodos to Kaggle for holding their own in a fairly competitive MOOC environment. It's even more impressive given that Kaggle Learn is an ancillary to the core service offering of the data science competition platform. Of course, the sample in the survey is heavily biased toward Kaggle users. Udemy and DataCamp are not far behind Kaggle Learn when it comes to single-platform learners. The takeaway is these 4 MOOCs have enough appeal for Kagglers to stick with them exclusively and to not go looking for greener pastures... at least for some time. \n\n> _Note_: the ""Other"" category in question 36 responses will be explored separately in the appendix of this Kernel, if you are interested to know what other learning platforms your peers use."
"### ... and so do multi-platform learners\nAmong respondents who use multiple MOOC platforms, Coursera holds the highest mindshare. Coursera is where 3,748 Kagglers spend most of their online learning time. DataCamp and Udemy share second place, with roughly equal number of respondents choosing them. Udacity in fourth place, edX in fifth, while Kaggle Learn dropped all the way to #6 spot. To me this suggests that once a learner discovers other MOOC platforms, Kaggle Learn can rarely compete. Still, an impressive performace for a completely free resource. "
"Does this mean Coursera is three times better than its closest rivals? Not exactly, but we can quantify the comparison better by pitting the MOOCs against one another, one pair at a time. Looking at respondents who use Coursera along with another platform (e.g. DataCamp) and using question 37 (which platform do you use the most?) as a proxy for users preference among platforms, I can see what percentage of people chose Coursera over each of the other platforms in top 5. For example, 72% of respondents who have used both Coursera and Udacity (and other platforms potentially), spent more time engaging with the former. Across its 4 closest competitors, Coursera does not dicisevely dominate against either, with DataCamp doing quite well in terms of winning Data Scientists' mindshare."
"Second most popular MOOC platform, DataCamp, is on much shakier ground compared to its competition. Less than 60% of respondents choose it over edX and Udacity, and slightly over 60% when compared to Udemy. Kaggle's very own learning platform, Kaggle Learn, is equally preferred to DataCamp, which is not something that's apparent from the simple count plot above."
"## What's next for MOOCs?\nMOOCs have been around long enough to become established enterprises. In 2018, Forbes estimates that Coursera's revenue alone is close to $140 millionn USD[5]. With the company rumoured to go public, we will see an increased drive for growth from Coursera and its competitors [6]. This growth will be fueled by evergrowing demand in the emerging markets such as India and China, where access to higher education is more limited, as the demand for data science professionals steadily grows. While MOOCs are fighting for Kaggler's mindshare, we should ponder why we do online learning: is it to get an extra notch on our professional belt or is it for the sake of self-development? Answer to that question will help you decide if engaging with any of these learning platforms makes sense for you.\n\nFor myself, I go to MOOCs for one of two reasons: either out of interest in the subject area or to do a crash course in a technology for the purpose of passing a standardized certification exam. While I'm learning the subject I don't care so much about measurable outcome and while studying for a certification exam, I know the certificate (external to MOOC) is backed by a recognized institution. Online learning works for me in these scenarios, but I'm sure you can find many more uses for it."
"\n## Appendix: Other Learning Platforms\nAside from the learning platforms that were offered as multiple choice question in the survey, the respondents had an option to select ""Other"" and provide their own answer. Looking at these, I realize that I haven't heard about most of them. Of course, the brand names of Linkedin, YouTube, and Codecademy I'm familiar with, but stepik, pluralsight, nptel - this is the first I've heard of these. Even as I tried to Google the resources there were some curious results. For exapmle, with [edureka](www.edureka.co) the second search result on Google was ""Do you mean Udemy?"". It was a clever ad by the more famous MOOC platform. When I went to see what [cognitive class ai](cognitiveclass.ai) is all about, there is an obvious banner ad at the top of the landing page directing me to take a Coursera IBM certification. Finally, [mlcourse.ai](mlcourse.ai) is just an offshoot of [ods.ai](ods.ai), a Russia-based Data Science community.  What's clear is the ""Other"" MOOC platforms are mostly smaller, nieche players that attract only a small fraction of Kagglers to their sites. In the next year's survey I would like Kaggle to include some of the heavyweights in the multiple choice, namely Linkedin and Codecademy, while TheSchool.AI and DataQuest can be left for ""Other"" category to be filled out by those who use them.\n\n_Random fact_: more people spell Code**a**cademy (with an ""a"") rather than the actual name Codecademy. I wonder why they chose to drop the ""a"" in their brand. "
"## References\n[1] [_Udacity U-Turns on Money Back Guarantee_](https://www.insidehighered.com/news/2018/03/16/udacity-ends-pledge-students-get-hired-or-get-their-money-back). Insider Higher Ed. March 16, 2018.\n[2]  [_Will MOOCs be Flukes?_](https://www.newyorker.com/science/maria-konnikova/moocs-failure-solutions). The New Yorker. November 7, 2014.\n[3] [_The Future of Massively Open Online Courses_](https://www.forbes.com/sites/quora/2017/03/23/the-future-of-massively-open-online-courses-moocs/#4f7289046b83). Forbes. March 23, 2017.\n[4] [_India Loves MOOCs_](https://www.technologyreview.com/s/539131/india-loves-moocs/). MIT Technology Review. July 26, 2015. \n[5] [_This Company Could Be Your Next Teacher: Coursera Plots A Massive Future For Online Education_](https://www.forbes.com/sites/susanadams/2018/10/16/this-company-could-be-your-next-teacher-coursera-plots-a-massive-future-for-online-education/#2e348b5e2a39). Forbes. October 16, 2018.\n[6] [_Beware of the Great MOOC Bait-and-Switch_](https://www.forbes.com/sites/dereknewton/2018/11/19/beware-of-the-great-mooc-bait-and-switch/#7f3bce8b12f2). Forbes. November 19, 2018."
Now lets take a look at how the housing price is distributed
"With this information we can see that the prices are skewed right and some outliers lies above ~500,000. We will eventually want to get rid of the them to get a normal distribution of the independent variable (`SalePrice`) for machine learning."
Now lets plot them all:
"Features such as `1stFlrSF`, `TotalBsmtSF`, `LotFrontage`, `GrLiveArea`... seems to share a similar distribution to the one we have with `SalePrice`. Lets see if we can find new clues later."
"Perfect, we now have a list of strongly correlated values but this list is incomplete as we know that correlation is affected by outliers. So we could proceed as follow:\n\n- Plot the numerical features and see which ones have very few or explainable outliers\n- Remove the outliers from these features and see which one can have a good correlation without their outliers\n    \nBtw, correlation by itself does not always explain the relationship between data so ploting them could even lead us to new insights and in the same manner, check that our correlated values have a linear relationship to the `SalePrice`. \n\nFor example, relationships such as curvilinear relationship cannot be guessed just by looking at the correlation value so lets take the features we excluded from our correlation table and plot them to see if they show some kind of pattern."
"We can clearly identify some relationships. Most of them seems to have a linear relationship with the `SalePrice` and if we look closely at the data we can see that a lot of data points are located on `x = 0` which may indicate the absence of such feature in the house.\n\nTake `OpenPorchSF`, I doubt that all houses have a porch (mine doesn't for instance but I don't lose hope that one day... yeah one day...)."
Let's explore these outliers\n
Edit: Look for fliers in other columns
More filtering of data to try
"This chart does not look linear, or at least the line is not matching the data across the entire x axis. Looks like a drop off for High GrLivArea, seems home buyers are not willing to pay a corresponding amount extra for the large living area, looking for a ""volume discount"" maybe...\nLets look at this closer, first fit a line, next try a polynomial fit to compare"
Need to look at the y_log relationship since that is what we will be predicting in the model (convert back later)
Now we look at a polynomial fit
plot the poly fit data
Now lets use a log y scale
# **Species Distribution - Whale vs Dolphin**
# **Most Frequently Occuring Individual IDs**
# **Dolphin Subspecies Distribution**
# **Whale Subspecies Distribution**
# **Species distribution by Individual IDs**
# **Visualizing Whales**
# **Visualizing Dolphins**
"\n# **Map individual_id to Images using W&B Tables**\n\n[Source](https://docs.wandb.ai/guides/data-vis/tables)\n\nW&B Tables are used to log and visualize data and model predictions. Interactively explore your data:\n\n📌 Compare changes precisely across models, epochs, or individual examples\n\n📌 Understand higher-level patterns in your data\n\n📌 Capture and communicate your insights with visual samples\n"
"# **TPU Intialization and Distribution Strategy **\n\n\n![](https://drive.google.com/uc?id=1q6AUi9XZRRWBjov49PSl3thB9idGsUKV)\n\nTensor Processing Units (TPUs) are Google's custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads.It's easy to run replicated models on Cloud TPU using High-level Tensorflow APIs .\n\n### **Performance**\n\nTPU can achieve a high computational throughput on massive multiplications and additions for neural networks , at blazingly fast speeds with much less power consumption and smaller footprint.\n\n📌 **TPU Initialization:** TPUs  are usually on Cloud TPU workers and hence have to be connected to remote clusters and then initialized .\n\n📌 **Distribution strategies :** A distribution strategy is an abstraction that can be used to drive models on CPU, GPUs or TPUs."
# 0. Importing libraries\n\n\n
# 1. Data exploration and pre-processing\n\n\n
"As you can see, there is a great motivation to calculate the PDF difference between target = 0 and target = 1 distributions. They are clearly different, so it would make sense to say that, if pdf(target = 1) - pdf(target = 0) > 0, then there is a high probability of the client making a transfer.\n\n#### **2.2. CREATING OUR PDF FUNCTION**\n\nHere is the strategy used in this kernel:\n- Calculate the PDF for each feature;\n- Aggregate each feature values within bins;\n- Use the PDF's difference between target = 0 and target = 1 for each bin as a new feature;\n\nNotice that we could create this feature differently, by, for example, getting the target probability.\n\nBelow we will see the difference between the previously seen PDF, where the graph is very smooth, and the plot of the PDF for each bin we created."
"### **3. UNDERSTANDING OVERFITTING**\n\nThe difference of smoothness on the graph above is caused by the IQR_multiplier and the bin_bandwidth_multiplier parameters.\n\n**IQR_multiplier**\nIQR stands for Inter-Quartile Range and is used to define the number of bins for our distribution. This parameter is proportional to the bin size. A bigger IQR_multiplier will yield a bigger bin size and, therefore, less bins per distribution.\n\n**bin_bandwidth_multiplier**\nThe bandwidth value is used to smooth the graph\n\nLet's see how the PDFs behaves as we play with those parameters"
"When you describe your data with a high number of bins and a low smoothing parameter, you get really ""noisy"" PDF, because your pool of candidates for each bin is too small and specific. This is the opposite of what we want for a robust solution.\n\nWhen we use this model to predict the target on the test dataset, it will overfit, not because the datasets are way too different, but because you didn't give the model any chance to adapt for small changes.\n\nTo prove that to you, let's check the test dataset distribution\n\n#### **3.1. THE TEST DATASET**\n\nHere we are going to plot the target = 1 and target = 0 for the train and test dataset in the same graph.\n\nTo do that, we will get the submission with score 0.901 shared [here](https://www.kaggle.com/darbin/clustering-blender-of-0-901-solutions) and say that the 20098 highest probabilities (same number of target = 1 in the train_df) are equivalent to target = 1 in the test_df. Everything else will be set as target = 0."
"First, let's plot the smoothed PDFs to see if there is any significant difference"
"There is a small, but still significant difference in the distribution of target = 1. Now, let's see how big is this difference when we don't smooth the PDF."
"As you can see, every point representing a bin has a different PDF for train_df and test_df. Expecting that their distributions will match is equivalent of what is happening in the image below:\n\nYou can find [here](https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42) the article about over/underfitting from where this image was taken.\n![overfitting](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle---Why-your-model-is-overfitting/master/1_SBUK2QEfCP-zvJmKm14wGQ.png) \n\nI hope that these graphs can make it clear why your model overfits when you are too specific about a single variable, instead of a group of variables.\n\nA good question here would be: What happens if I use this feature with smoothed values? Well, the smoothier it is, the lesser the impact on your score, and here is why."
"\n\n4.1 INVESTIGATE THE FREQUENCY OF VARIOUS ORGAN SEGMENTATION EXAMPLES\n\n---\n\nOBSERVATIONS\n\nWe can observe the following distribution:\n* **kidney**\n    * **99** images\n* **prostate**\n    * **93** images\n* **large intestine**\n    * **58** images\n* **spleen**\n    * **53** images\n* **lung**\n    * **48** images\n    \nAll of the organ types are represented fairly well within our data, with **kidney** and **prostate** being about twice as common as the other three organ types: **large intestine**, **spleen**, and **lung**"
"\n\n4.2 INVESTIGATE THE IMAGE SIZES\n\n---\n\nIt's observable that not all images have the same size... however, given that, there is not that much variation between image sizes:\n\nOBSERVATIONS\n* Globally, we can see that most of the images are dominated by a single size $(3000 \times 3000)$\n* All the other sizes (19) only have 1 or 2 occurences each.\n* All image sizes are square\n\nNOTE: THE HOSTS TELL US THAT ALL HPA IMAGES SHOULD BE 3000x3000... HOWEVER, WE CAN SEE THAT ISN'T THE CASE!\n\nOTHER INFORMATION\n\n* Expect roughly 550 images in the hidden test set. \n* All HPA images are 3000 x 3000 pixels with a tissue area within the image around 2500 x 2500 pixels. \n* The Hubmap images range in size from 4500x4500 down to 160x160 pixels.\n\n"
\n\n4.3 INVESTIGATE AGE\n\n---\n\nOBSERVATIONS\n* The age distribution is skewed to be 50+\n* The distribution of examples with **large intestine** segmented is much higher among the oldest demographic\n* The general distribution of organ segmentation types across age groups appears relatively stratified (with the previously noted exception)\n\n
\n\n4.4 INVESTIGATE SEX\n\n---\n\nOBSERVATIONS\n* Women do not have a prostate and as such there are no prostate examples where gender is Female.\n* Kidney's seem skewed towards the Male gender\n* All other organ's appear evenly distributed\n\n
"# Titanic Data Science Solutions\n\n\n### This notebook is a extension to the notebook [titanic data science solutions](https://www.kaggle.com/startupsci/titanic-data-science-solutions). \n\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\n\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\n\n## Workflow stages\n\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.\n\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\n\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\n\n\n## Question and problem definition\n\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\n\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\n\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\n\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n![](https://img.memecdn.com/titanic_fb_1023579.jpg)\n\n## Workflow goals\n\nThe data science solutions workflow solves for seven major goals.\n\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\n\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\n\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\n\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\n\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals."
## Acquire data\n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.
## Analyze by visualizing data\n\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.\n\n### Correlating numerical features\n\nLet us start by understanding correlations between numerical features and our solution goal (Survived).\n\nA histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\n\nNote that x-axis in historgram visualizations represents the count of samples or passengers.\n\n**Observations.**\n\n- Infants (Age <=4) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n- Most passengers are in 15-35 age range.\n\n**Decisions.**\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n- We should consider Age (our assumption classifying #2) in our model training.\n- Complete the Age feature for null values (completing #1).\n- We should band age groups (creating #3).
"### Correlating numerical and ordinal features\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\n**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training."
"**This kernel has been a work of more than 10 days If you find my kernel useful and my efforts appreciable, Please Upvote it , it motivates me to write more Quality content**"
# Configuring TPU's\n\nFor this version of Notebook we will be using TPU's as we have to built a BERT Model
Plot to see distribution of 7 different classes of cell type
Its seems from the above plot that in this dataset cell type Melanecytic nevi has very large number of instances in comparison to other cell types
"Plotting of Technical Validation field (ground truth) which is dx_type to see the distribution of its 4 categories which are listed below :\n**1. Histopathology(Histo):**  Histopathologic diagnoses of excised lesions have been\nperformed by specialized dermatopathologists. \n**2. Confocal:** Reflectance confocal microscopy is an in-vivo imaging technique with a resolution at near-cellular level , and some facial benign with a grey-world assumption of all training-set images in Lab-color space before\nand after  manual histogram changes.\n**3. Follow-up:** If nevi monitored by digital dermatoscopy did not show any changes during 3 follow-up visits or 1.5 years biologists  accepted this as evidence of biologic benignity. Only nevi, but no other benign diagnoses were labeled with this type of ground-truth because dermatologists usually do not monitor dermatofibromas, seborrheic keratoses, or vascular lesions. \n**4. Consensus:** For typical benign cases without histopathology or followup biologists  provide an expert-consensus rating of authors PT and HK. They applied the consensus label only if both authors independently gave the same unequivocal benign diagnosis. Lesions with this type of groundtruth were usually photographed for educational reasons and did not need\nfurther follow-up or biopsy for confirmation.\n"
Plotting the distribution of localization field 
"It seems back , lower extremity,trunk and upper extremity are heavily compromised regions of skin cancer "
"Now, check the distribution of Age"
It seems that there are larger instances of patients having age from 30 to 60
Lets see the distribution of males and females
Now lets visualize agewise distribution of skin cancer types
"It seems that skin cancer types 0,1, 3 and 5 which are Melanocytic nevi,dermatofibroma,Basal cell carcinoma and Vascular lesions are not much prevalant below the age of 20 years "
"## Loading a bunch of stuff\nImports are from my Jupyter notebooks on my PC, in those notebooks I import 'em all so that later I don't have to bother with importing things."
## Loading datasets
 Data Visualization \n\n Scatterplot before and after StandardScaler Standardization between citric acid and total sulfur dioxide 
 Scatterplot before and after MinMaxScaler Standardization between citric acid and total sulfur dioxide 
 Scatterplot before and after RobustScaler Standardization between citric acid and total sulfur dioxide 
 Distplot before and after StandardScaler Standardization between citric acid and total sulfur dioxide 
 Distplot before and after MinMaxdScaler Standardization between citric acid and total sulfur dioxide 
 Distplot before and after RobustScaler Standardization between citric acid and total sulfur dioxide 
 Model Building 
## Importing Library's
## Some utilities functions 
# Target Feature\n- Let's see the distribution and if we can identify what is the nature of this feature
"Cool;  \nWe can see that our target is a binary feature and as it is 0 or 1 we can't know what is about.\nAnother interesting thing to note is that isn't so imbalanced:\n- Category 0 with 79.4% \n- Category 1 with 30.6\n\nNow, as we have much of them, let's explore the patterns of other binary features"
# Correlation Between Signal and Open Channels\nLet's look closely at random intervals of signal and open channels to observe how they relate. We notice that they are highly correlated and move up and down together. Therefore we can probabily predict open channels from the one feature signal. The only complication is the synthetic drift that was added. So we will remove it.
# Test Data\nLet's display the test data signal
"## Reflection\nFrom this plot we can locate the 5 models in action. And we can recognize the added drift. Batch 1 appears to be 5 subsamples where A, B, C, D, E were created by models 1s, 3, 5, 1s, 1f respectively. Model 1s is the model with maximum 1 channel open with low prob. Model 1f is the model with maximum 1 channel open with high prob. And models 3, 5, 10 are models with maximum 3, 5, 10 channels respectively. We observe slant drift in subsamples A, B, E, G, H, I. We observe parabola draft in batch 3. "
"## Training Data Drift\nWe observe drift whereever the following plot is not a horizontal line. We see drift in batches 2, 7, 8, 9, 10."
"## Test Data Drift\nWe observe drift in test subsamples A, B, E, G, H, I and test batch 3.\n"
# Remove Test Data Drift
"## Refactor Release 2017-Jan-29\n\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\n\n### User comments\n\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\n\n### Porting issues\n\n- Specify plot dimensions, bring legend into plot.\n\n\n### Best practices\n\n- Performing feature correlation analysis early in the project.\n- Using multiple plots instead of overlays for readability."
"### Correlating categorical features\n\nNow we can correlate categorical features with our solution goal.\n\n**Observations.**\n\n- Female passengers had much better survival rate than males. Confirms classifying (#1).\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions.**\n\n- Add Sex feature to model training.\n- Complete and add Embarked feature to model training."
"### Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\n**Observations.**\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n- Consider banding Fare feature."
"## Wrangle data\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\n\n### Correcting by dropping features\n\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent."
\n# 1. Importing Libraries and Packages\nWe will use these packages to help us manipulate the data and visualize the features/labels as well as measure how well our model performed. Numpy and Pandas are helpful for manipulating the dataframe and its columns and cells. We will use matplotlib along with Seaborn to visualize our data.
"\n# 2. Loading and Viewing Data Set\nWith Pandas, we can load both the training and testing set that we wil later use to train and test our model. Before we begin, we should take a look at our data table to see the values that we'll be working with. We can use the head and describe function to look at some sample data and statistics. We can also look at its keys and column names."
"* Looks like the distribution of ages is slightly skewed right. Because of this, we can fill in the null values with the median for the most accuracy."
> Note that the numbers printed above are the proportion of male/female survivors of all the surviviors ONLY. The graph shows the propotion of male/females out of ALL the passengers including those that didn't survive.
Here is one final cumulative graph of a pair plot that shows the relations between all of the different features
"\n# 5. Feature Engineering\nBecause values in the Sex and Embarked columns are categorical values, we have to represent these strings as numerical values in order to perform our classification with our model. We can also do this process through **One-Hot-Encoding**."
\n## 2. Import Libraries 📚 
\n## 3. Read Dataset 📝 
## Gender Split
"The graph clearly shows that there are a lot more male respondents as compared to female. It seems that Ladies were either busy with their coding, **or ladies don't code**...:p. Just Kidding."
## Respondents By Country
"**USA and India**, constitute maximum respondents, about 1/3 of the total. Similarly Chile has the lowest number of respondents. Is this graph sufficient enough to say that majority of Kaggle Users are from India and USA. I don't think so, as the total users on Kaggle are more than 1 million while the number of respondents are only 16k."
Look at that humungous Salary!! Thats **even larger than GDP of many countries**. Another example of bogus response. The minimum salary maybe a case of a student. The median salary shows that Data Scientist enjoy good salary benefits.
### Compensation by Country
"The left graph shows the Top 15 high median salary paying countries. It is good to see that these countries provide salary more than the median salary of the complete dataset. Similarly,the right graph shows median salary of the Top 15 Countries by respondents. The most shocking graph is for **India**. India has the 2nd highest respondents, but still it has the lowest median salary in the graph. Individuals in USA have a salary almost 10% more than their counterparts in India. What may be the reason?? Are IT professionals in India really underpaid?? We will check that later."
### Salary By Gender
The salary for males look to be high as compared to others.
## Age
The respondents are young people with majority of them being in the age bracket if 25-35.
## Profession & Major
"Data Science and Machine Learning is used in almost every industry. This is evident from the left graph,as people from different areas of interest like Physics, Biology, etc are taking it up for better understanding of the data. The right side graph shows the Current Job of the respondents. A major portion of the respondents are Dats Scientists. But as it is survey data, we know that there may be many ambigious responses. Later on we will check are these respondents real datas-scientists or self proclaimed data-scientists."
First Let's check the impact of feature **Sex** on **Survived**
We can say that Female passangers have higher probability of survival than Male passangers
Ratio of Survived and Not Survived passangers for S and Q Embarked are similar but Passengers from C embarked have higer chances of survival.
Passengers from **Pclass 3** have lesser chances of Survival while passengers from **Pclass 1** have higher chances of survival
Average Fare for passangers who survived is higher than not survived.
## Visualising the Time Series data\n
There has been a steady increase in Maruti's Stock prices except a slimp around 2019. We shall use Pandas to investigate it further in the coming sections.\n
"# Visualizing the volume weighted average price (VWAP)\n\nWhen working with time-series data, a lot can be revealed through visualizing it. \n\n\n## Visualizing using markers\nIt is possible to add markers in the plot to help emphasize the specific observations or specific events in the time series."
## Visualising using KDEs\n\nSummarizing the data with Density plots to see where the mass of the data is located
## Visualising using Lineplots
### Basic EDA
### Brief Analysis of the data
### Analysis Of Diabetic Cases
### PairPlots:\n\nLets us see the distribution of the features in the dataset
"### Observations:\n\n1)The diagonal shows the distribution of the the dataset with the kernel density plots.\n\n2)The scatter-plots shows the relation between each and every attribute or features taken pairwise.\nLooking at the scatter-plots, we can say that no two attributes are able to clearly seperate the two outcome-class instances."
### K-Nearest Neighbours
### In a Nutshell
**1. Age and Sex:**
"You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\n\nFor men the probability of survival is very low between the age of 5 and 18, but that isn't true for women. Another thing to note is that infants also have a little bit higher probability of survival.\n\nSince there seem to be **certain ages, which have increased odds of survival** and because I want every feature to be roughly on the same scale, I will create age groups later on."
"**3. Embarked, Pclass  and Sex:**"
"Embarked seems to be correlated with survival, depending on the gender. \n\nWomen on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S. \n\nPclass also seems to be correlated with survival. We will generate another plot of it below."
**4. Pclass:**
"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below."
"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive."
"_few observation from boxplots,As we can see,SalePrice for fullbath=3 is higher than 0,1, or 2. SalePrice for OverallQal=10 which is very excellent is higher than others._"
"#### _Inference: SalePrice is not normally distributed, it is positively or right skewed_"
"### For visualization,before using visual library (matplotlib, seaborn, ..)we need to convert SparkDataframe to PandasDataFrame "
"## Checking null values in Pyspark\n\n* isnan() is a function of the pysparq.sql.function package, we have to set which column we want to use as an argument of the function. \n* isNull()"" belongs to pyspark.sql.Column package, to check the null status of a column\n\nTo check null in Pyspark, we use both function above"
"Firstly, I will explore through 3 different columns:\n- Time\n- Amount\n- Class"
"We have a clearly imbalanced data.\nIt's very common when treating of frauds... \n\nFirst I will do some explore through the Time and Amount. \nSecond I will explore the V's Features, that are PCA's "
### Looking a scatter plot of the Time_min distribuition by Amount
### Looking a scatter plot of the Time_hour distribuition by Amount
I will use boxplot to search differents distribuitions: \n- We are searching for features that diverges from normal distribuition
"We can see a interesting different distribuition in some of our features like V4, V9, V16, V17 and a lot more.  \nNow let's take a look on time distribuition"
"Data scientist respondents from United States and India are nearly equal as seen below. Both of them have been predominate over the survey. So i have analyzed other countries separately. Because most of the data scientist taken part in the survey from India and US, I have looked closer the two countries."
"# Age, Gender, Education and Job Title Distribution"
"The age group distribution of all respondents and data scientists are similar except for 18-21... I think most of 18-21 are students. Apparently, because of the popularity of data science, young ones are trying yo enter this field."
"Although young people are the majority, all age groups seem to be interested in data science. It's never too late to begin."
In the chart females and males show a similar distribution
"It seems that the younger generations in the India are the peak unlike the United States. It's closely related to more students participation from India. (See ""Titles of Respondents (India - USA Comparison)"")"
It can be deduced from the chart that master's and doctoral degrees in data science gain importance.
"It seems, US is better than India in terms of education degree. That's because the Indian Kagglers is younger than the American ones. (see above ""Age Group (India - U.S. Comparison)"")"
The students have participated in the survey as much as data scientist.\nI think it's so interesting that more Software Engineer participate in the survey than Data Engineer and Statistician.
The difference between the US and India is the interchange of students and data scientists. The rest have similar distribution.
### 2D Representation: Sound Waves
"### Fourier Transform\n\n* Function that gets a signal in the time domain as input, and outputs its decomposition into frequencies\n* Transform both the y-axis (frequency) to log scale, and the “color” axis (amplitude) to Decibels, which is approx. the log scale of amplitudes."
"### The Spectrogram\n\n* What is a spectrogram? A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams ([wiki](https://en.wikipedia.org/wiki/Spectrogram)).\n* Here we convert the frequency axis to a logarithmic one."
"### Mel Spectrogram\n\n* The Mel Scale, mathematically speaking, is the result of some non-linear transformation of the frequency scale. The Mel Spectrogram is a normal Spectrogram, but with a Mel Scale on the y axis."
### Harmonics and Perceptrual\n\n* Harmonics are characteristichs that human years can't distinguish (represents the sound color)\n* Perceptrual understanding shock wave represents the sound rhythm and emotion
### Tempo BMP (beats per minute)\n\nDynamic programming beat tracker.
"### Spectral Rolloff\n* is a measure of the shape of the signal. It represents the frequency below which a specified percentage of the total spectral energy, e.g. 85%, lies"
### Mel-Frequency Cepstral Coefficients:\n\n* The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice.
Data needs to be scaled:
### Chroma Frequencies\n\n* Chroma features are an interesting and powerful representation for music audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave.
"## EDA\n\nEDA is going to be performed on the `features_30_sec.csv`. This file contains the mean and variance for each audio file fo the features analysed above. \n\nSo, the table has a final of 1000 rows (10 genrex x 100 audio files) and 60 features (dimensionalities)."
> # **Importing Libraries**
## **EDA**
> # **Analysis of Correlation**
> # **Label Encoding**
> # **Confusion Matrix**
> # **ROC AUC**
## Import the Dataset
## Distribution of Amount
"**Highlights**\n\nMost the transaction amount falls between 0 and about 3000 and we have some outliers for really big amount transactions and it may actually make sense to drop those outliers in our analysis if they are just a few points that are very extreme.\n\nMost daily transactions are not extremely expensive, but it’s likely where most fraudulent transactions are occurring as well."
### Distribution of Amount for Fradulent & Genuine transactions
"**Highlights**\n\nThis graph shows that most of the fraud transaction amount is less than 500 dollars. This also shows that the fraud transaction is very high for an amount near to 0, let's find that amount."
### Load Libraries
"## GPU use \nSince this is a deep learning model, The use of GPU will accelerate the training. \nThe first models are not so demanding so you can still use CPU training (but it will be slower)."
"## Plot The distribution \nPlot the distribution before and after the Normalization. \nAs you can see, we kept the distribution of the data, but we change its scales."
# Create Sequances \nIn this part we allign the data into input features and labels with techniches whic adapt for Time series processing \nThis is out Time Series \n\n![TS2.JPG](attachment:TS2.JPG)\n\nOr in a more schematic ilustriation \n\n![TS1.JPG](attachment:TS1.JPG)
"# DATA SCIENTIST\n**In this tutorial, I only explain you what you need to be a data scientist neither more nor less.**\n\nData scientist need to have these skills:\n\n1. Basic Tools: Like python, R or SQL. You do not need to know everything. What you only need is to learn how to use **python**\n1. Basic Statistics: Like mean, median or standart deviation. If you know basic statistics, you can use **python** easily. \n1. Data Munging: Working with messy and difficult data. Like a inconsistent date and string formatting. As you guess, **python** helps us.\n1. Data Visualization: Title is actually explanatory. We will visualize the data with **python** like matplot and seaborn libraries.\n1. Machine Learning: You do not need to understand math behind the machine learning technique. You only need is understanding basics of machine learning and learning how to implement it while using **python**.\n\n### As a summary we will learn python to be data scientist !!!\n\n**Content:**\n1. [Introduction to Python:](#1)\n    1. [Matplotlib](#2)\n    1. [Dictionaries ](#3)\n    1. [Pandas](#4)\n    1. [Logic, control flow and filtering](#5)\n    1. [Loop data structures](#6)\n1. [Python Data Science Toolbox:](#7)\n    1. [User defined function](#8)\n    1. [Scope](#9)\n    1. [Nested function](#10)\n    1. [Default and flexible arguments](#11)\n    1. [Lambda function](#12)\n    1. [Anonymous function](#13)\n    1. [Iterators](#14)\n    1. [List comprehension](#15)\n1. [Cleaning Data](#16)\n    1. [Diagnose data for cleaning](#17)\n    1. [Exploratory data analysis](#18)\n    1. [Visual exploratory data analysis](#19)\n    1. [Tidy data](#20)\n    1. [Pivoting data](#21)\n    1. [Concatenating data](#22)\n    1. [Data types](#23)\n    1. [Missing data and testing with assert](#24)\n1. [Pandas Foundation](#25)\n    1. [Review of pandas](#26)\n    1. [Building data frames from scratch](#27)\n    1. [Visual exploratory data analysis](#28)\n    1. [Statistical explatory data analysis](#29)\n    1. [Indexing pandas time series](#30)\n    1. [Resampling pandas time series](#31)\n1. [Manipulating Data Frames with Pandas](#32)\n    1. [Indexing data frames](#33)\n    1. [Slicing data frames](#34)\n    1. [Filtering data frames](#35)\n    1. [Transforming data frames](#36)\n    1. [Index objects and labeled data](#37)\n    1. [Hierarchical indexing](#38)\n    1. [Pivoting data frames](#39)\n    1. [Stacking and unstacking data frames](#40)\n    1. [Melting data frames](#41)\n    1. [Categoricals and groupby](#42)\n1. Data Visualization\n    1. Seaborn: https://www.kaggle.com/kanncaa1/seaborn-for-beginners\n    1. Bokeh 1: https://www.kaggle.com/kanncaa1/interactive-bokeh-tutorial-part-1\n    1. Rare Visualization: https://www.kaggle.com/kanncaa1/rare-visualization-tools\n    1. Plotly: https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners\n1. Machine Learning\n    1. https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners/\n1. Deep Learning\n    1. https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n1. Time Series Prediction\n    1. https://www.kaggle.com/kanncaa1/time-series-prediction-tutorial-with-eda\n1. Statistic\n    1. https://www.kaggle.com/kanncaa1/basic-statistic-tutorial-for-beginners\n1. Deep Learning with Pytorch\n    1. Artificial Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n    1. Convolutional Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n    1. Recurrent Neural Network: https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch"
"**WARNING - UYARI**\n* If you run the code above, if it outputs like in the picture, you need to put the "".csv"" path in pd.read_csv () (as in the picture).\n* Yukarıdaki kod bloğunu run edince sonuç ne veriyorsa read_csv içerisine onu yazmanız gerekli.\n* Mesela, eğer yukarı bulunan kodu çalıştırdığınızda, resimdeki gibi bir output veriyorsa pd.read_csv() içerisine resimdeki "".csv"" yolunu koymanız gerekli (resimde olduğu gibi). Yukarıda kod bloğunu run edince ne çıkıyorsa onu yazmanız lazım mesela aşağıdaki gibi.\n\n* read_csv içerisine yukarıda çıkan .csv dosyaları yazılmalı."
"Looking at below count plot, looks like very less number of females attended the black friday sale. \nBut it could also mean less number of females paid for the products and may be their spouse paid for them. "
"Now, on plotting a count plot for age, seems like the majority of the population in the ages group 26-35 attended the sale."
"Further, I could also check among the age groups, which gender was a majority by adding a hue.\nAnd as seen below, more males spent in the sale than females."
May be we could check further - how many of these males were actually married?\nFor this lets create a column that represents gender+married status and then use it as hue.
"As we see above, there are no bars for the married in the 0-17 range which makes sense. And then if we look at the 46 and above groups, females are very less. But on the other hand, married males paying in range 46-55 are also comparatively more than married females. So it could also imply that though ladies do shop a lot, their spouses are possibly paying for it and hence data reflects that men shopped more.\nIf we had more categorical data defining what kind of products were purchased by men, we could dig in this statement further. However, since in this dataset we don't know if there is a category that implies feminine products/clothes we cannot further explore this case."
Even below plots don't provide any hint of whether some products are particularly being purchased by either females or married males.
Let us check back rest of the columns again to see what next we could explore.
# 2. Data Understanding\n\n## 2.1 Import Libraries\nFirst of some preparation. We need to import python libraries containing the necessary functionality we will need. \n\n*Simply run the cell below by selecting it and pressing the play button.*
## 2.2 Setup helper Functions\nThere is no need to understand this code. Just run it to simplify the code later in the tutorial.\n\n*Simply run the cell below by selecting it and pressing the play button.*
"## 2.3 Load data\nNow that our packages are loaded, let's read in and take a peek at the data.\n\n*Select the cell below and run it by pressing the play button.*"
Here is the code to create the Partial Dependence Plot using the scikit-learn library.
"The y axis is interpreted as **change in the prediction** from what it would be predicted at the baseline or leftmost value.\n\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning ""Man of The Match.""  But extra goals beyond that appear to have little impact on predictions.\n\nHere is another example plot:"
This graph seems too simple to represent reality. But that's because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model's structure.\n\nYou can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model.
"This model thinks you are more likely to win *Man of the Match* if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\n\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model.  Though this dataset is small enough that we would be careful in how we interpret any model.\n\n# 2D Partial Dependence Plots\nIf you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify this.  \n\nWe will again use the Decision Tree model for this graph.  It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself."
"This graph shows predictions for any combination of Goals Scored and Distance covered. \n\nFor example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km.  If they score 0 goals, distance covered doesn't matter. Can you see this by tracing through the decision tree with 0 goals?\n\nBut distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too?"
"# Introduction\nThe sinking of Titanic is one of the most notorious shipwrecks in the history. In 1912, during her voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.\n\n\nContent: \n\n1. [Load and Check Data](#1)\n1. [Variable Description](#2)\n    * [Univariate Variable Analysis](#3)\n        * [Categorical Variable](#4)\n        * [Numerical Variable](#5)\n1. [Basic Data Analysis](#6)\n1. [Outlier Detection](#7)\n1. [Missing Value](#8)\n    * [Find Missing Value](#9)\n    * [Fill Missing Value](#10)\n1. [Visualization](#11)\n    * [Correlation Between Sibsp -- Parch -- Age -- Fare -- Survived](#12)\n    * [SibSp -- Survived](#13)\n    * [Parch -- Survived](#14)\n    * [Pclass -- Survived](#15)\n    * [Age -- Survived](#16)\n    * [Pclass -- Survived -- Age](#17)\n    * [Embarked -- Sex -- Pclass -- Survived](#18)\n    * [Embarked -- Sex -- Fare -- Survived](#19)\n    * [Fill Missing: Age Feature](#20)\n1. [Feature Engineering](#21)\n    * [Name -- Title](#22)\n    * [Family Size](#23)\n    * [Embarked](#24)\n    * [Ticket](#25)\n    * [Pclass](#26)\n    * [Sex](#27)\n    * [Drop Passenger ID and Cabin](#28)\n1. [Modeling](#29)\n    * [Train - Test Split](#30)\n    * [Simple Logistic Regression](#31)\n    * [Hyperparameter Tuning -- Grid Search -- Cross Validation](#32) \n    * [Ensemble Modeling](#33)\n    * [Prediction and Submission](#34)"
\n# Load and Check Data
"### 1a. The Large Synoptic Survey Telescope\n\n\nPLAsTiCC is intended to simulate sources that vary with time in the night-sky as seen during the first three years of operation of the Large Synoptic Survey Telescope (LSST). \n\nThe LSST (illustrated below) is a telescope with an 8.4 meter primary mirror diameter being built high up in Atacama desert of Chile, on a mountain called Cerro Pachon.\n"
"#### _Figure 1: The LSST Telescope, Credit: LSST/NSF/AURA/Todd Mason Productions Inc_"
"Data Exploration\nLet's explore the data we have as this will give us a hint on the algorithm we will use if we have to choose. Exploring data is also very important because it will tell you which accuracy metric you are going to use, if the data is balanced which means all the classes have fair contribution in the dataset regarding its numbers then we can easily use accuracy, But if the data is skewed then we won't be able to use accurace as it's results will be misleading and we may use F-beta score instead."
From the previous results we can see that the dataset consists of 60000 training example each is an image of dimention 28 * 28. We can see that the number of occurances of each class is almost balanced and based on that it is safe to use accuracy as our metric later.
"In this notebook, we use a dataset we've shared on Kaggle Datasets: [Articles Sharing and Reading from CI&T Deskdrop](https://www.kaggle.com/gspmoreira/articles-sharing-reading-from-cit-deskdrop).  \nWe will demonstrate how to implement **Collaborative Filtering**, **Content-Based Filtering** and **Hybrid methods** in Python, for the task of providing personalized recommendations to the users."
# Loading data: CI&T Deskdrop dataset
"A W&B Table (wandb.Table) is a two dimensional grid of data where each column has a single type of data—think of this as a more powerful DataFrame. Tables support primitive and numeric types, as well as nested lists, dictionaries, and rich media types. Log a Table to W&B, then query, compare, and analyze results in the UI.\n\nTables are great for storing, understanding, and sharing any form of data critical to your ML workflow—from datasets to model predictions and everything in between.\n\n[Source ](https://docs.wandb.ai/guides/data-vis)"
Lets explore first five rows of train dataset
#  Machine Learning \n#  Multilayer Perceptron from Scratch 
"# 2. Artificial Neural Networks\n\nArtificial Neural Networks are mathematical models inspired by the human brain, specifically the ability to learn, process, and perform tasks. The Artificial Neural Networks are powerful tools that assist in solving complex problems linked mainly in the area of combinatorial optimization and machine learning. In this context, artificial neural networks have the most varied applications possible, as such models can adapt to the situations presented, ensuring a gradual increase in performance without any human interference. We can say that the Artificial Neural Networks are potent methods can give computers a new possibility, that is, a machine does not get stuck to preprogrammed rules and opens up various options to learn from its own mistakes. \n"
"## 3.1. Some Python Libraries \n\nIn the first place, Let's define some libraries to help us in the manipulation the data set, such as `numpy`, `matplotlib`, `seaborn` and `scikit-learn`. In this tutorial, I am implementing a Multilayer Perceptron without any framework like Keras or similar ones. The goal here is to be as simple as possible! So to help you with this task, we implementing the neural network without using ready-made libraries. You can use numpy to work with array operations! There is no problem it! "
## 3.2. An analysis about the Iris Flower Dataset\n
## 4.1. Plot our training Samples
## 4.2. Plot our test Samples
"\n\n# 5. Multilayer Perceptron\n\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve performance) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, ""they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images"".\n\nThey have found most use in applications difficult to express in a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to axons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it.\n\n More information here: [Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)\n\n\n\nThe Multilayer Perceptron Networks are characterized by the presence of many intermediate layers (hidden) in your structure, located between input layer and output layer. With this, such networks have the advantage of being able to classify more than two different classes and It also solve non-linearly separable problems.\n\n      \n      \n           \n      \n  \n"
# 6. Implementation the Multilayer Perceptron in Python\n
"## Finding the best parameters \n\nFor find the best parameters, it was necessary to realize various tests using different values to the parameters. The graphs below denote all tests made to select the best configuration for the multilayer perceptron. These tests were important in selecting the best settings and ensuring the best accuracy. The graph was drawn manually, but you can change the settings and note the results obtained. The tests involve different activation functions and the number of neurons for each layer."
"![](https://i.imgur.com/LMDjyOy.jpg)\n\n# 1 | INTRODUCTION\n\n\nNOTEBOOK AIM\n\n- The aim of this notebook is to provide a brief overview on what type of geospatial library tools we can use to visualise & analyse map geospatial data\n- As the title suggests, the notebook is aimed at exploring Australian based maps & visualisation data, subsequent data sources for map geometry & visualisation data is specified\n- Various visualisation approaches are discussed & some examples are also shown of how we can apply the visualisation method\n\n\nGEOSPATIAL ANALYSIS TOOLS\n\nThere are a number of useful geospatial tools we can use to visualise & analyse data, **Plotly** & **GeoPandas** pretty much contains everything we need:\n\n> - **Choropleth Maps** - Region based visualisation | Requires Boundary Locations (unique boundary identifier)\n> - **Hexbin Maps** - Hexagonal region based point counting | Requires Point Locations (long,lat)\n> - **Cluster Maps** - Standard Scatter Plot | Requires Point Locations (long,lat)\n> - **Density Heatmaps** - Model based approximation for scatter data | Requires Point Locations (long,lat)\n> - We will also look at an interpolation method commonly used for geospatial analysis called **Kriging**\n"
"###  2.2 | Unemployment rate in Queensland \n\nPROBLEM AIM\n\n- We are interested in the __unemployment statistics__ of a specific demographic (Both Male & Female) in the state of Queensland on a __Local Government Area__ level.\n- The user wants the ability to explore the exact unemployment value for their own purposes (if possible) and understand any overall differences between male and female unemployment.\n\nSOME OBSTACLES\n\n- Unfortunately Australian internal boundary maps are not integrated into __Plotly__ or __Geopandas__ \n- We'll be requried to look for this __boundary segment__ data & combine it with our visualisation data we wish to display. \n- Both our downloaded __boundary data__ & __visualisation data__ should simply have an index corresponding to the unique boundary identifier & fortunately, pandas is our friend.\n- Its not really relevant what it is, it could be names (sometimes you'd have to clean the name column a little), or more often it is a __unique code__.\n\nWHAT WE WANT TO SHOW\n\n\nWe probably should know what type of geospatial boundaries we are interested in:\n\n> - We might be interested in __State Bondaries__ (eg. Victoria...) (**[State Boundaries AUG2020](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-ee6c0f18-3f4b-4275-932e-0b6e434e316f/details?q=)**)\n> - We might be interested in __Local Government Boundaries__ (eg. Boroondara ... ) (**[Local Government Areas NOV2020](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-6b4e69ed-6f7f-4422-854d-1013ac716bbe/details?q=)**)\n> - We might be interested in __Suburb Boundaries__ (eg. Carlton ... ) (**[All State Suburbs (November 2020)](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-2d59ddfc-1c0f-41a3-8de8-06fa5d11e72f/?q=)**)\n> - Others might include Electoral Boundaries, or you might even have your own.\n\nDATA SOURCES\n\n- **[Data.gov](https://www.data.gov.au)** , has a very wide range of Geographical datasets; including **[Geoscape Administrative Boundaries](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/details?q=)**, in this example we are interested in __LGA__ boundaries, which we can find in the above link or a direct link [here](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-6b4e69ed-6f7f-4422-854d-1013ac716bbe/details?q=) ( Just note that the data is already split by state )\n- **[Australian Bureau of Statistics](https://www.abs.gov.au)**, which contains two .shp files (for Local Government & State Electoral Divisions), found specifically [here](https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003June%202020?OpenDocument)\n- In this example, we will use __Local Government Areas ASGS Ed 2020 Digital Boundaries in ESRI Shapefile Format__ from __ABS.gov__ & select our specific state (Queensland)\n\nBOUNDARY FORMAT\n\n\n- I think it's much more straightforward to use .shp & simply convert them with geopandas, purely because GeoPandas is very straightforward to use and visualise the geopandas dataframe, like normal pandas dataframes.\n- json files on the other hand, may need a little bit of tweaking to get the read format correct, especially if you are using .read_json from the Pandas library in order to understand what your data contains.\n- The above mentioned sources all contain .shp formats, which is an indicator that it is quite popular, neverthelesss __json__ variants are also available, mainly __Data.gov__\n- Let's load & read the .shp file, noting that all the other files that come with the .shp are needed. The current file contains __LGA__ of all states (as it might be obvious by the STE_NAME16, so we need to select only a subset of the data."
"INTERACTIVE CHOROPLETH MAPS\n\n\n- Queensland contains a variety of boundary segment sizes, this makes it difficult to visualise smaller region values if the map is static & zoomed out.\n- Interactive maps allow us to explore the region and outline any notable key difference.\n- Let's plot the __Female Demographic in the Plotly Go__ plot & __Male Demographic in the Plotly Express__ plot.\n\n**INTERACTIVE** **PLOTLY GO w/ MAPBOX ACCESS TOKEN**\n\n- We can use Plotly Go with or/without a MapBox Access token.\n- MapBox tokens are useful to __outline key area names__ on top of the choropeth map plot, we can do this by modifying the layout.\n- Let's plot the female demographic on the plot with the __mapbox layout__."
"**INTERACTIVE** **  PLOTLY EXPRESS W/O MAPBOX ACCESS TOKEN**\n\n- We can use plotly express without having to use a MapBox Access Token (if you don't need the names to go on top), this may make the map let useful from an EDA point of view, nevertheless we can always refer to the hover data to get the names.\n- Let's plot the male demographic using the plotly express version. \n- Plotly express versions can also be used with __mapbox__ as shown later using the fig.update_layout() option."
"- In this way, we can create choropeth maps in separate code cells and extract data we need from these interactive maps. \n- We could also use a button based load option, which Plotly also offers. An example will be shown below in Section 2.4.\n\n**STATIC** **CHOROPLETH MAP**\n\n- Static plots are more than sufficient as long as they can get the specific point across which was intended.\n- Perhaps a little less insightful comapared to __interactive plots__ when it comes to trying to __show resuts for two very different sized bundaries__. \n- We may have a very big difference in boundary sizes, so regions like Woorabinda, will be hard to spot on a static map that tries to shows the entire state.\n- The aim of the static plot below is to demonstrate that the __level of unemloyment is sightly higher for males than for females__ in the Queensland for the __age group 20-24__, in all statistical quadrant data, which overall we can see, even on static plots. We can also use statistics data in the form of tables to complement our choropleth maps."
"# Choosing the right role\n\nIf you are a student or just recently started your career and are still unsure about the technology skills to target or if you are an experienced data scientist looking for ways to improve your chances on the job market, the first step is to identify all possibilities and explore the differences between them. The roles I personally consider most relevant are:\n\n* **Data Scientist**\n* **Data Analyst**\n* **Software Engineer**\n* **Research Scientist**\n* **Data Engineer**\n* **Statistician**\n\n#### Mapping the community\n\nI cleaned up the data by imputing missing values, transforming to dummy variables and normalizing numerical values. I then used [LDA](https://sebastianraschka.com/Articles/2014_python_lda.html) to perform supervised [dimensionality reduction](https://idyll.pub/post/dimensionality-reduction-293e465c2a3443e8941b016d/) and map the resulting feature vectors to a 2 dimensional space while maximing the separation of the clusters.  The feature set I used for LDA is based on questions which I felt are the best predictors of a person's daily activity at work. \n\nThe interpretation of the resulting principal components and naming of the axes is highly subjective because it is the weighted combination of multiple features and can be too complex to describe in simple words. Based on the resulting positions of the various groups I like to think of **PC1(horizontal)** as a **Business (left) vs.  Code (right)** axis and of **PC2 (vertical)** as an **Engineering (bottom) vs. Research (top)** axis.\n\n### Where are these role situated in relation to each-other?\n\nAs you probably noticed on the first image, the clusters are clearly overlapping, the definitions of these roles are extremely fuzzy. For example the **Data Analyst** and **Business Analyst** roles share most of the same characteristics with slight differences in the application of the conclusions resulting from the analysis.  To avoid cluttering / having too many categories, in some of the charts I merged some of the less-represented groups:\n* (Student, Not employed, Consultant, Other, Developer Advocate) → drop\n* (Project Manager, Chief Officer) → Manager\n* (Research Assistant, Principal Investigator) → Research Scientist\n* (Data Journalist, Business Analyst) → Business Analyst\n* (Database Engineer) → Data Engineer\n\nDespite the overlaps, the positions of the cluster centroids reveal some interesting patterns. The size of the bubbles represent the size of the group.\n"
"#### Observations:\n* The marketing & business roles form a cluster group in the center left area of the chart, not far from the closely related **Management** clusters, which use the reports and analyses provided by the **Data Analysts** to making data driven business decisions.\n* The academical roles, namely **Principal Investigator**, **Research Investigator** and **Research Scientist** form a tight cluster group in the top center part of the chart. They are the most similar to the **Data Scientist** role from a technical point of view.\n* The **Student** cluster lies close to the other academic roles, with an observable offset towards the more practical, engineering-related clusters.\n* **Database Engineer** lies further than expected from the **Data Engineer** cluster. The reason might be that these roles consist of a lot of querying, report generation and data maintenance which is more closely related to what the Business cluster group does. \n* **Student**, and **Chief Officer** and **Consultant** are the real Jack-of-All-Trades roles in the **Data Science** community. I imagine these **Chief Officers** as former **Data Scientist** now leading a **AI** or **Big Data**-driven startups."
"#### Observations\n* **Statisticians** are leaders in advanced statistics by a far margin.\n* **Research Scientists** are leaders in usage of other types of Software Packages, these are probably the domain-specific software they use for bioinformatics, physics, math."
### Black-box models?\nNah! You just haven't studied enough!
"Observations:\n* Notice that the order of the roles is exactly the same for both answers. Being able to deeply understanding **Machine Learning** models is an essential skill for **Data Scientists**.\n* The **Data Scientists** on the left side of the 0 axis are the self-doubters. They picked the **Data Scientist** title when they were asked the first time, but began to question their true identity when facing the question for the second time: ""Really? You? Look at yourself! You consider yourself to be a data scientist?""\n\nWe can estimate each role's similarity to the **Data Scientist** role using our LDA feature-space by calculating the [Euclidean distance](https://tekmarathon.com/2015/11/15/different-similaritydistance-measures-in-machine-learning/) between the cluster centroids.\n\nNow that we have a general idea of what each role entails and have seen the way they seen themselves, let's plot against each other the ""Self Assessed"", perceived similarity and the actual similarity between each role and the **Data Scientist** role.\n\nThe size of the bubble represents the size of the group, the X axis represents the distance of the group to the **Data Scientist** role measured by self-assessment (Q26 - Are you a Data Scientist?), the Y axis represents the distance of the group to the **Data Scientist** role."
#### Observations:\n \n * **Statisticians** seem to consider themselves **Data Scientists** despite the differences highlighted in the charts above.\n * **Data Engineers** on the other hand seem to be unaware of how similar their job is to one of a **Data Scientist**.
"# 3. Importing libraries and exploring Data\n## 3a.Importing Libraries\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate"
## 3b.Extracting dataset
"## 4b. ....... Outliers\nOutliers are unusual values in your dataset, and they can distort statistical analyses and violate their assumptions. Unfortunately, all analysts will confront outliers and be forced to make decisions about what to do with them. Given the problems they can cause, you might think that it’s best to remove them from your data. But, that’s not always the case. Removing outliers is legitimate only for specific reasons.Outliers can be very informative about the subject-area and data collection process. It’s essential to understand how outliers occur and whether they might happen again as a normal part of the process or study area. Unfortunately, resisting the temptation to remove outliers inappropriately can be difficult. Outliers increase the variability in your data, which decreases statistical power. Consequently, excluding outliers can cause your results to become statistically significant. In our case, **let's first visualize our data and decide on what to do with the outliers**"
"As you see, we have very less number of outliers in our features. Especially we have majority of the outliers in **hsc percentage** Let's clear em up!"
# 5.Data Visualizations\n## 5a. Count of categorical features- Count plot
**Inference**\n* We have **more male candidates** than female\n* We have candidates who did **commerce** as their hsc course and as well as undergrad\n* **Science background** candidates are the second highest in both the cases\n* Candidates from **Marketing and Finance** dual specialization are high \n* Most of our candidates from our dataset **don't have any work experience**\n* Most of our candidates from our dataset **got placed** in a company
now check outliers and noisy data. For this step we use scatter plot and box plot
"\n    📌 In the box plot below, you can select the columns you want, from the right side of the chart to display.😊\n"
"An outlier is an observation that is unlike the other observations and we see some of these in above boxplot for some comlumns but outliers are innocent until proven guilty. With that being said, they should not be removed unless there is a good reason for that. According to pairplot, noisy data does not appear to exist. Therefore, we do not delete any data.\n\nNow everything is ok. There is only one small point that needs to be fixed. As it was said in section 1, the Income column expresses the annual income, while the CCAvg column expresses the Avg. spending on credit cards per month, so to standardize the units of the columns, we convert the annual income to monthly."
"An Insightful Story About Successful Crowdfunding Projects \n\n\n***Crowdfunding*** is the practice of funding a project or a venture by raising monetary contributions from many people across the globe. There are a number of organisations such as DonorsChoose.org, Patreon, Kickstarter which hosts the crowdfunding projects on their platforms. Kickstarter has hosted more than 250,000 projects on their website with more than $4 Billion collective amount raised.  \n\n\n![](https://people.safecreative.org/viewimage/viewmagazine?path=kickstarter-patreon.jpg&defaultPath=)\n\n\nWhile it is true that crowdfunding is one of the most popular methods to to raise funds however the reality is that **not every project is able to completely reach the goal**. Infact, on KickStarter, only about 35 percent of the total projects have raised successful fundings in the past. This fact raises an important question - **which projects are able to successfully achieve their goal?**. In other words, can project owners somehow know what are the key project characteristics that increases the chances of success. \n\nIn many studies, Researchers and analysts have used the descriptive analysis methods on the crowdfunding data to obtain insights related to project success. While many others have also applied predictive modelling to obtain the probability of project success. However, these approaches have the fundamental problems: \n\n- Descriptive analysis - only gives surface level insights    \n- Predictive analysis - models act as the blackboxes    \n\n### About this Kernel  \nIn this kernel, I have shared a hybrid analysis approach that uses the concepts of both types of analysis enriched with the concepts of **machine learning explainability** which can be used to answer the key questions related to the success (or failure) of any crowdfunding project. The framework uses the interpretations derived from a trained machine learning model. Unlike descriptive analysis to find key insights, the focus in this approach is to make use of model behaviours and characteristics such as : Relative Feature Importances, Partial Dependencies, Permutation Importances, SHAP values. I have explained the intuition behind every approach in layman terms. Following are the contents of the kernel: \n\n## Contents \n\n1. Business Use-Case and Problem Statement    \n2. Hypothesis Generation    \n3. Dataset Preparation     \n4. Modelling the Project Success       \n5. Model Interpretation : Insights Generation    \n    5.1 Which are the most important features (relatively) of a project? ( **Relative Feature Importance** )     \n    5.2 Which features have the biggest impact on the project success? ( **Permutation Importance** )     \n    5.3 How does changes in those features affact the project success? ( **Partial Dependencies** )     \n    5.4 Digging deeper into the decisions made by the model ( **SHAP values** )    \n6. Final Conclusions       \n  \n\n1. Understanding the Business Use Case \nThe essential business use-cases in the crowdfunding scenario can be considered from two different perspectives - from the project owner's perspective and the companies perspective. \n\n1. From the **project owner's perspective**, it is highly beneficial to be aware about the key characteristics of a project that greatly influence the success of any project. For instance, it will be interesting to pre-emptively know about following questions:   \n\n     - What is an ideal and optimal range of the funding goal for my project ?  \n     - On which day of the week, I should post the project on Kickstarter ?  \n     - How many keywords should I use in my project title ?  \n     - What should be the total length of my project description ?     \n\n\n2. From the **perspective of companies** which hosts the crowdfunding projects such as DonorsChoose.org, Patreon, and Kickstarter, they receive hundreds of thousands of project proposals every year. A large amount of manual effort is required to screen the project before it is approved to be hosted on the platform. This creates the challenges related to scalability, consistency of project vetting across volunteers, and identification of projects which require special assistance. \n\nIt is due to these two perspectives, there is a need to dig deeper and find more intutive insights related to the projects success. Using these insights, more people can get their projects funded more quickly, and with less cost to the hosting companies. This also allows the hosting companies to optimize the processes and channel even more funding directly to projects.   \n\n\n2. Hypothesis Generation   \nHypothesis Generation is very powerful technique which can help an analyst to structure a very insightful and a relevant solution of a business problem. It is a process of building an intuitive approach of the business problem without even thinking about the available data. Whenever I start with any new business problem, I try to make a comprehensive list of all the factors which can be used to obtain the final output. For example, which features should affect my predictions. Or, which values of those features will give me the best possible result. In case of crowdfunding, the question can be - which features are very important to decide if a project will be successful or not.  \n\nSo, to generate the hypothesis for the use-case, we will write down a list of factors (without even looking at the available data) that can possibly be important to model the project success.   \n\n1. **Total amount to be raised** - More amount may decrease the chances that the project will be successful.  \n2. **Total duration of the project** - It is possible that projects which are active for very short or very long time periods are not successful.  \n3. **Theme of the project** - People may consider donating to a project which has a good cause or a good theme.  \n4. **Writing style of the project description** - If the message is not very clear, the project may not get complete funding.  \n5. **Length of the project description** - Very long piecies of text may not perform good as compared to shorter crisp texts.  \n6. **Project launch time** - A project launched on weekdays as compared to weekends or holidays may not get complete funding amount.  \n\nSo this is an incomplete list of possible factors we can think at this stage that may influence the project success. Now, using machine learning interpretability, not only we can try to understand which features are actually important but also what are the feature values which these features can take. \n\n\n3. Dataset Preparation  \nOur business use-case is identified, problem statement is formulated, and we have defined a hypothesis. We can now start the analysis, modelling, and interpretting in order to find out the key insights. First, we load the available dataset. \n\n### 3.1 Load Dataset "
"### 3.2 Dataset Preprocessing  \n\nIn this dataset, we can see that a number of features are about the active stage of the project. This means that a project was launched on a particular date and a partial amount is already raised. The goal of our problem statement is a little bit different, we want to focus on the stage in which the project is not launched yet and identify if it will successful or not. Additinaly, find the most important features (and the feature values) that influence this output. So we perform some pre-processing in this step which includes the following: \n\n- Get rid of unwanted columns (active stage columns)  \n- Feature Engineering (driven from our hypothesis generation)    \n- Remove Duplicates  \n- Handle Missing Values  \n- Encode the Categorical Features  "
"Now, we have a model which predicts the probability of a given project to be successful or not. In the next section we will interpret the model and its predictions. In other words, we will try to prove or disprove our hypothesis. \n\n\n5. Insights from Predictive Modelling  \n\n- 5.1 Which are the most important features (relatively) of a project? ( **Relative Feature Importance** )       \n- 5.2 Which features have the biggest impact on the project success? ( **Permutation Importance** )      \n- 5.3 How does changes in those features affact the project success? ( **Partial Dependencies** )       \n- 5.4 Digging deeper into the decisions made by the model ( **SHAP values** )     \n\n\n5.1 Which are the most important features (relatively) of a project? (Relative Feature Importance)   \n\nIn tree based models such as random forest, a number of decision tress are trained. During the tree building process, it can be computed how much each feature decreases the weighted impurity (or increases the information gain) in a tree. In random forest, the impurity decrease from each feature is averaged and the features are ranked according to this measure. This is called relative feature importance. The more an attribute is used to make key decisions with decision trees, the higher its relative importance. This indicates that the particular feature is one of the important features required to make accurate predictions. \n"
"**Inferences** \n> - From the graph, it is clear that the features which are important to predict the project success are: project goal, length of the project name, launched week, duration, and number of syllables present in the name. While the least important features are are mostly related to the project categories    \n> - **What does this mean for the project owner?** For someone who is willing to raise funds, they should consider evaluating the ideal project goal and duration. A high or a medium-high project goal may almost lead to the case of failure. Additionally, number of characters used in the project title will also affact if the project will be succeeded or failed.   \n> - **What does this mean for the company?** The company can identify the projects with high importance based on their meta - features such as length of the project.    \n\nBy applying this approach, we primariy obtained the factors to look at a high level, But still we need to answer, what are the optimal values of these features. This will be answered when we apply other techniques in the next sections. Before moving on to those techniques, I wanted to explore a little more about relative feature importance using a graph theory perspective.  \n\n A Graph Theory Perspective : Relative Feature Importances \n\nThe idea of relative feature importance is very simple (more the times a feature appear in the decision tree splits, it is important) but many a times people forget an underlying important concept that these importances are ""relative"". This means that in comparison to other features what is the importance of a particular feature. \n\nBut a question here is - Even if it is relative, what if some features from a set of features are removed, do we still obtain the same feature importances ? This problem can infact be formulated as graph problem. Consider a graph based structure, in which every feature (of the dataset) is a node, and the edges are defined between two features (nodes) if the two features appers in the *top 10 important features of the individual decision tree*. \n\n**But, What is the benefit?** Well, Some of the problems when viewed as network or graph problems can help to identify the solutions quickly and easily. For instance, using the graph properties one can identify which is the most important node of the network. A node having higher degree centrality (connections) indicates that the node is highly connected to the network. Which means that if a node is removed from the network, a large majority of the network will be disrupted. \n\nThis idea can be applied to relative feature importances, a feature which is highly important, which appears in most of the decision tree's top 10 feature can be clearly identified from the feature importance network graph. If this feature is removed from the dataset then the predictions will be affacted. Let's plot this network. "
"> From the above plot, we can identify that the maximum degree centrality nodes are - duration, syllable count, goal etc. From the network perspective, it means that if we remove these nodes from the network (high degree centrality nodes), this will lead to network disruption. This is similar to what we obtained in the relative feature importance plot. The key takeaway from this graph is that one can not afford to ignore these features while optimizing any crowdfunding project. \n\n\n5.2 Which features have the biggest impact on the project success? (Permutation Importance)     \n\nIn the last section, we mainly identified which the features at a very high level which are relatively important to the model outcome. In this section, we will go a little deeper and understand which features has the biggest impact on the model predictions (in absolute sense). One of the ways to identify such behaviour is to use permutation importance. \n\nThe idea of permutation importance is very straightforward. After training a model, the model outcomes are obtained. The most important features for the model are the ones if the values of those feature are randomly shuffled then they lead to biggest drops in the model outcome accuracies. Let's look at the permutation importance of features of our model."
"**Inferences** \n> - This is an interesting plot, We can observe that the features shown in top and in green are the most important as if their values are randomized then the outcome performance suffers.   \n> - We can observe that the top features are are the features which we mostly saw in the relative importance section, but using this graph we can quantify the amount of importance associated with them. And also obtain the ones which are least important, for example - launched week, if it was weekend or not etc.  \n\nWith this method, we obtained the importance of a feature in a more absolute sense rathar than a relative sense. Let's assume that our feature space forms a majority of the universe. Now, it will be interesting to plot both permutation and relative feature importances and make some key observations. "
"**Inferences**  \n> - Very Interesting Insights can be obtained from the above plot, There are some features which showed up higher in the relative feature importance, but when we look at their permuatation importance we see that they are not important. (Though, permutation importance results cannot be reproduced exactly because of randomness, but when I first plotted this plot I observed that launched week and month had high feature importance but lower permutation importance.)  \n> - From this plot, we can again observe that our hypothesis is almost true, the project goal, duration, number of characters, number of words all are the most important features that one should look at while creating a new project page. \n\n### Pressence of which keywords makes the biggest impact in the predictions?  \n\nUsing permutation importance, we can also evaluate which keywords makes the biggest impact in the model prediction. Let's train another model which also uses keywords used in the project name and observe the permutation importance."
"**Inferences**\n> - From the first plot, we can observe that there are a certain keywords which when used in the project name are likely to increase the probability success of a project. Example -  ""project"", ""film"", and ""community"". While on the other hand, keywords like ""game"", ""love"", ""fashion"" are likely to garner less attraction. This implies that crowdfunding projects related to games or entertainment such as love or fashion may not be very successful as compared to the ones related to art, design etc. \n\n**Note -** It is possible to get different results when run again, thus it is recommended to use this approach on a much bigger dataset. But ofcourse, its not a very big problem, atleast it gives an understanding about the words to focus on. \n\n\n5.3 How does changes in features lead to changes in model outcome? (**Partial Dependencies**)     \n\nSo far we have only talked about which features are most or least important from a pool of many features. For example, we observed that Project Goal, Project Duration, Number of Characters used etc are some of the important features related to project success. In this section, we will look at what are the specific values or ranges of features which leads to project success or failure. Specifically, we will observe that how making changes such as increasing or decreasing the values affect the model outcomes. These effects can be obtained by plotting the partial dependency plots of different features. \n\n### Project Name - Features "
"> We observe that the projects having fewer number of words (<= 3) in the name does not show any improvement in model success. However, if one start increasing the number of words in the project name, the corresponding model improvement also increases linearly. For all the projects having more than 10 words in the name, the model becomes saturate and shows similar predictions. Hence, the ideal word limit is somewhere around 7 - 10.  \n"
> Change in Syllables does not show significant differences in model improvements. \n\nLet's also plot the interaction between number of words and characters used. 
"> From the above plot, it can be observed that about 40 - 65 characters and 10 - 14 words are the good numbers for the project name. \n\n### Project Launched Day and Duration "
"# How to understand feature importance of categorical features reported by LightGBM?\nLightGBM allows one to specify directly categorical features and handles those internally in a smart way, that might out-perform OHE. Originally, *I was puzzled about feature importance reported for such categorical features*. After  iterating in comments, and learning more about feature importance reported, it seems that:\n\n- **the default implementation is not very useful**, as there are several types of importance and importance values do not behave according to intuitive expectation. See [this blog post](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27) for clear motivation and introduction into SHAP;\n- it is beneficial to use [SHAP package in python](https://github.com/slundberg/shap) to produce stable feature-importance evaluation.\n\nIt all started with abnormally high importance reported for `ORGANIZATION_TYPE` in [an earlier version of my modified fork](https://www.kaggle.com/mlisovyi/modular-good-fun-with-ligthgbm?scriptVersionId=3888846) of  [olivier's](https://www.kaggle.com/ogrellier) very popular [Good_fun_with_LigthGBM kernel](https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm). After some investigation I realized that the problem was due to missing OHE of categorical features (beacuse categorical feature were stores as `categories` instead of `objects`). I fixed that, `ORGANIZATION_TYPE` got OHE-transformed and disappeared from tops of  important features. \n\nThen I started to looking into how to use internal handling of categorical features in LightGBM. It turns out that the **sklearn API of LightGBM actually has those enabled by default**, in a sense that by default it tries to guess which features are categorical, if you provided a `pd.DataFrame` as input (because it has `feature_name='auto', categorical_feature='auto'` as the defaults in the `lgb.LGBMModel.fit()` method). And it makes that guess assuming that all features of type `category` have to be treated with the internal categorical treatment (i.e. following [this procedure from the docs](https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst#categorical-feature-support)). It turns out that in such case LightGBM reports unexpectedly high importance  in some cases.\n\nBelow is a minimalistic example to reproduce this behaviour and an illustrastion of SHAP usage."
## Read in the basic 'application' data
### Plot feature importance
"Upsss. `ORGANIZATION_TYPE` pops up as the most *important*. But do not celerbate- if you train the same model on the same data with OHE for categorical features you will get the same ROC AUC (and a similar importance for the `EXT_SOURCE_x` features as on this plot), i.e. most likely just importance of `ORGANIZATION_TYPE` is reported wrong, unless i misunderstand something. Any feedback will be helpful for me to make the next step in LightGBM usage."
\n Import Libraries 
\n Import Dataset \n
"Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1."
There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)
Number of characters in tweets
The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.
**Number of words in a tweet**
**Average word length in a tweet**
**Frequencies**\nNow we want to count the frequency of each word in our corpus.
\nloading all the required libraries.....
\nColor paleete for this notebook...
Lets Extract Labels from Image Names...
# **Distribution of Meta Features vs Target **
# **Frequency Distribution of Meta Features**
# **Clustering**
# **t-SNE**
# **K - Nearest Neighbours**
# **PCA**
"# **Visualize Dataset Interactively using W&B Tables**\n\nIt only requires 5 lines of extra code to get the power of W&B Tables. \n\n1. You first need to initialize a W&B run using `wandb.init` API. This step is common for any W&B Logging.\n2. Create a `wandb.Table` object. Imagine this to be an empty Pandas Dataframe. \n3. Iterate through each row of the `train.csv` file and `add_data` to the `wandb.Table` object. Imagine this to be appending new rows to your Dataframe. \n4. Log the W&B Tables using `wandb.log` API. You will use this API to log almost anything to W&B.\n5. In a Juypter like interactive session, you need to call `wandb.finish` to close the initialized W&B run. \n\nSource : Content copied from Ayush notebook\n"
### Import modules
### Reading in input file
### Distributions of attributes
"__Notes for Data Cleaning & Preprocessing:__ \nUni-modal, skewed distributions could potentially be log transformed: \n> LotFrontage, LotArea, 1stFlrSF, GrLivArea, OpenPorchSF\n\nAfter-note: This will be a future addition."
The kernels below helped me in writing this kernel. Thanks!\n\nAndrew Lukyanenko: https://www.kaggle.com/artgor/eda-and-models\n\nLeonardo Ferreira: https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\nKonstantin Yakovlev: https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again\n\nKonstantin Yakovlev: https://www.kaggle.com/kyakovlev/ieee-simple-lgbm
The functions used for visualization are below.
First let's check the sample submission.
## Title grouped
"It's interesting... Children's and ladys first, huh?"
# Step 1: Imports #\n\nWe begin by importing several Python packages.
# Step 2: Distribution Strategy #\n\nA TPU has eight different *cores* and each of these cores acts as its own accelerator. (A TPU is sort of like having eight GPUs in one machine.) We tell TensorFlow how to make use of all these cores at once through a **distribution strategy**. Run the following cell to create the distribution strategy that we'll later apply to our model.
## Checking Categorical Missing Values\n\nNumerical missing values were filled in. Let's check again what the remaining missing values are.
" Observation:\n    \n* PoolQC, MiscFeature, Alley, Fence, and FireplaceQu features have too many missing values.\n* Garage-related features have 157 to 159 missing values. It is unknown whether the houses lacked garages or were intentionally omitted.\n* Basement-related features also have 79 to 82 missing values.\nIt seems that we need to focus more on the process of filling in the missing values ​​of the corresponding Garage and Basement features."
"----------------------------------------------------------\n# Checking Target\n\nThe problem is a regression problem. Therefore, we analyze the distribution of the target and check whether there is necessary preprocessing based on this. If it is a classification problem\nWe need to check target imbalance."
" Observation:\n\nThe skewness was about 1.88. Also, since the metric is RMSLE, we will perform log scaling."
# 1. Articles database  \n\nThis databasecontains information about the assortiment of H&M shops. It's important not to confuse it with the number of transactions for each article what is given in a different database.
"Unique indentifier of an article:\n* ```article_id``` (int64) - an unique 9-digit identifier of the article, 105 542 unique values (as the length of the database)\n\n5 product related columns:\n* ```product_code``` (int64) - 6-digit product code (the first 6 digits of ```article_id```, 47 224 unique values\n* ```prod_name``` (object) - name of a product, 45 875 unique values\n* ```product_type_no``` (int64) - product type number, 131 unique values\n* ```product_type_name``` (object) - name of a product type, equivalent of ```product_type_no```\n* ```product_group_name``` (object) - name of a product group, in total 19 groups\n\n2 columns related to the pattern:\n* ```graphical_appearance_no``` (int64) - code of a pattern, 30 unique values\n* ```graphical_appearance_name``` (object) - name of a pattern, 30 unique values\n\n2 columns related to the color:\n* ```colour_group_code``` (int64) - code of a color, 50 unique values\n* ```colour_group_name``` (object) - name of a color, 50 unique values\n\n4 columns related to perceived colour (general tone):\n* ```perceived_colour_value_id``` - perceived color id, 8 unique values\n* ```perceived_colour_value_name``` - perceived color name, 8 unique values\n* ```perceived_colour_master_id``` - perceived master color id, 20 unique values\n* ```perceived_colour_master_name``` - perceived master color name, 20 unique values\n\n2 columns related to the department:\n* ```department_no``` - department number, 299 unique values\n* ```department_name``` - department name, 299 unique values\n\n4 columns related to the index, which is actually a top-level category:\n* ```index_code``` - index code, 10 unique values\n* ```index_name``` - index name, 10 unique values\n* ```index_group_no``` - index group code, 5 unique values\n* ```index_group_name``` - index group code, 5 unique values\n\n2 columns related to the section:\n* ```section_no``` - section number, 56 unique values\n* ```section_name``` - section name, 56 unique values\n\n2 columns related to the garment group:\n* ```garment_group_n``` - section number, 56 unique values\n* ```garment_group_name``` - section name, 56 unique values\n\n1 column with a detailed description of the article:\n* ```detail_desc``` - 43 404 unique values"
"Only one column - ```detail desc``` - has missing values but this is a very small fraction of the dataset - about 0.4%.\n\nLet's visualise some of the articles. To do so I will create a helper function below. As mentioned in the competition description not all articles have an image. Therefore, this function will show selected amount of images from a given folder. It will also write an article name. What I've found is that a leading zero in the name of the folder does not correspond to the first digit of the ```article_id``` and it has to be stripped when looking for a product in the articles database. E.g.: *0108775015.jpg* is ```article_id``` *108775015*."
In a hidden cell below there's a helper function for plotting sorted horizontal barplots.
  \n    Importing Python Libraries 📕 📗 📘 📙\n    \n
  \n    Sample Images\n    \n
  \n    The above sample Images after applying preprocessing Technique which is  equalize histogram\n    \n
You are more likly to survive if you are travels with 1 to 3 people and if you have 0 or more than three you have a less chance.
Shows the number of females and males who has number of siblings or spouse that is Parch.
## Data Acquisition
**some useful functions**
**loading the data**
"### Seaborn Distplots \n**Distribution of Age as function of Pclass, Sex and Survived**"
"Best chances to survive for male passengers was in Pclass 1 or being below 5 years old.  \nLowest survival rate for female passengers was in Pclass 3 and being older than 40.  \nMost passengers were male, in Pclass 3 and between 15-35 years old."
"Default mode for seaborn barplots is to plot the mean value for the category.  \nAlso, the standard deviation is indicated.  \nSo, if we choose Survived as y-value, we get a plot of the survival rate as function   \nof the categories present in the feature chosen as x-value."
"As we know from the first Titanic kernel, survival rate decreses with Pclass.  \nThe hue parameter lets us see the difference in survival rate for male and female. "
"### Passengers dataset\n\nEvery field of knowledge has **the** dataset that is used for teaching purposes: machine learning has Iris and CIFAR, differential equations - Canadian lynx data, and statistics has the airline passengers dataset between 1949 and 1960, first compiled by Box and Jenkins (you will be hearing those two names again in the next module) in 1976. We will use this dataset to demonstrate in practice what kind of information can be obtained using seasonal decomposition.\n\n"
Let's start with a basic additive decomposition:
"Trend and seasonality are behaving more or less in line with expectations, but the behavior of the residuals is clearly not consistent over time (average level of oscillations in the middle of the sample is very different than on either end). While there are many possible reasons, one quick explanation is the additive vs multiplicative relationship between the series components - which is something we can examine quickly:"
"Not much of a qualitative change in trend and seasonality components, but the residuals looks much more stable around a constant level - such phenomenon does not of course imply stationarity by itself, but at least a clear signal in the opposite direction is not there anymore. "
"# Bank Marketing DataSet - Intelligent Targeting:\n***\n## Marketing Introduction:\n*The process by which companies create value for customers and build strong customer relationships in order to capture value from customers in return.*\n\n**Kotler and Armstrong (2010).**\n***\n\n**Marketing campaigns** are characterized by  focusing on the customer needs and their overall satisfaction. Nevertheless, there are different variables that determine whether a marketing campaign will be successful or not. There are certain variables that we need to take into consideration when making a marketing campaign. \n\n## The 4 Ps:\n1) Segment of the Population: To which segment of the population is the marketing campaign going to address and why? This aspect of the marketing campaign is extremely important since it will tell to which part of the population should most likely receive the message of the marketing campaign. \n2) Distribution channel to reach the customer's place: Implementing the most effective strategy in order to get the most out of this marketing campaign. What segment of the population should we address? Which instrument should we use to get our message out? (Ex: Telephones, Radio, TV, Social Media Etc.)\n3)  Price: What is the best price to offer to potential clients? (In the case of the bank's marketing campaign this is not necessary since the main interest for the bank is for potential clients to open depost accounts in order to make the operative activities of the bank to keep on running.)\n4)  Promotional Strategy: This is the way the strategy  is going to be implemented and how are potential clients going to be address. This should be the last part of the marketing campaign analysis since there has to be an indepth analysis of previous campaigns (If possible) in order to learn from previous mistakes and to determine how to make the marketing campaign much more effective."
# What is a Term Deposit? \nA **Term deposit** is a deposit that a bank or a financial institurion offers with a fixed rate (often better than just opening deposit account) in which your money will be returned back at a specific maturity time. For more information with regards to Term Deposits please click on this link from Investopedia:  https://www.investopedia.com/terms/t/termdeposit.asp
"# Outline: \n***\nA. **Attribute Descriptions**\nI. *[Bank client data](#bank_client_data)\nII. *[Related with the last contact of the current campaign](#last_contact)\nIII. [Other attributes](#other_attributes) \n\nB. **Structuring the data:** \nI. *[Overall Analysis of the Data](#overall_analysis)\nII. *[Data Structuring and Conversions](#data_structuring) \n\nC. **Exploratory Data Analysis (EDA)**\nI. *[Accepted vs Rejected Term Deposits](#accepted_rejected) \nII. *[Distribution Plots](#distribution_plots) \n\nD. **Different Aspects of the Analysis: **\nI. *[Months of Marketing Activty](#months_activity) \nII. *[Seasonalities](#seasonality) \nIII. *[Number of Calls to the potential client](#number_calls) \nIV. *[Age of the Potential Clients](#age_clients) \nV. [Types of Occupations that leads to more term deposits suscriptions](#occupations) \n\nE. **Correlations that impacted the decision of Potential Clients.**\nI. *[Analysis of our Correlation Matrix](#analysis_correlation) \nII. *[Balance Categories vs Housing Loans](#balance_housing)\nIII. [Negative Relationship between H.Loans and Term Deposits](#negative_relationship) \n\nF. ** Classification Model **\nI. [Introduction](#classification_model) \nII. [Stratified Sampling](#stratified)\nIII. [Classification Models](#models)\nIV. [Confusion Matrix](#confusion)\nV. [Precision and Recall Curve](#precision_recall)\nVI. [Feature Importances Decision Tree C.](#decision) \n\nG. ** Next Campaign Strategy**\nI. [Actions the Bank should Consider](#bank_actions)\n\n# A. Attributes Description: \n\nInput variables:\n# Ai. bank client data:\n\n1 - **age:** (numeric)\n2 - **job:** type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n3 - **marital:** marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n4 - **education:** (categorical: primary, secondary, tertiary and unknown)\n5 - **default:** has credit in default? (categorical: 'no','yes','unknown')\n6 - **housing:** has housing loan? (categorical: 'no','yes','unknown')\n7 - **loan:** has personal loan? (categorical: 'no','yes','unknown')\n8 - **balance:** Balance of the individual.\n# Aii. Related with the last contact of the current campaign:\n\n8 - **contact:** contact communication type (categorical: 'cellular','telephone') \n9 - **month:** last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n10 - **day:** last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n11 - **duration:** last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n# Aiii. other attributes:\n\n12 - **campaign:** number of contacts performed during this campaign and for this client (numeric, includes last contact)\n13 - **pdays:** number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n14 - **previous:** number of contacts performed before this campaign and for this client (numeric)\n15 - **poutcome:** outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\nOutput variable (desired target):\n21 - **y** - has the client subscribed a term deposit? (binary: 'yes','no')"
"\n Exploring the Basics \n\n## Summary:\n***\n\n Mean Age is aproximately 41 years old. (Minimum: 18 years old and Maximum: 95 years old.)\n The mean balance is 1,528. However, the Standard Deviation (std) is a high number so we can understand through this that the balance is heavily distributed across the dataset.\nAs the data information said it will be better to drop the duration column since duration is highly correlated in whether a potential client will buy a term deposit. Also, duration is obtained after the call is made to the potential client so if the target client has never received calls this feature is not that useful. The reason why duration is highly correlated with opening a term deposit  is because the more the bank talks to a target client the higher the probability the target client will open a term deposit since a higher duration means a higher interest (commitment) from the potential client. \n\n\n**Note: There are not that much insights we can gain from the descriptive dataset since most of our descriptive data is located not in the ""numeric"" columns but in the ""categorical columns"".**\n"
"# 1. Introduction\n\n\n    1.1 Objectives\n\n\nI'm very excited to participate in kaggle's first **unsupervised clustering** TPS competition. The goal is to **predict** the cluster each sample belongs to. However, we are not even given the number of clusters there should be beforehand. \n\nI will try to answer the following questions:\n* *How **many clusters** should we use?*\n* *What is the **competition metric** and where does it come from?*\n* *What is the **best model** for the data*?\n* *How do we **ensemble** predictions together?*\n\n\n    1.2 Libraries\n"
"# 2. Data\n\n\n    2.1 Load data\n\n\n* There are **29** features, all of them masked.\n* There are **almost 100,000** data points."
"# 3. EDA\n\n\n    3.1 Discrete features\n\n\n* There are **7** discrete features: *f_07* to *f_13*.\n* Values are **non-negative**. \n* Distributions are all similar, perhaps **Poisson**."
"\n    3.2 Continuous features\n\n\n* There are **22** continuous features: *f_00* to *f_06* and *f_14* to *f_28*\n* Distributions are all **Normal**, usually with mean 0 and standard deviation 1.\n* Values typically lie between -5 and +5."
"\n    3.3 Hypothesis testing\n\n\n**Shapiro-Wilk Test** (Copied from [Francisco Javier Gallego & Torch me](https://www.kaggle.com/code/javigallego/outliers-eda-clustering-tutorial))\n\nThis test is used to test whether a dataset is distributed **normally** or not. The null hypothesis is that a sample $$x_1\hspace{0.1cm},\hspace{0.1cm}\cdots\hspace{0.1cm},\hspace{0.1cm}x_n$$ comes from a normally distributed population. It was published in 1965 by Samuel Shapiro and Martin Wilk and **is considered to be one of the most powerful tests for normality testing.** The test statistic is \n\n$$W = \frac{(\sum_{i=1}^{n}a_{i}x_i)^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$$\n\nwhere\n\n* $x_i$ is the number from the i-th data point (where the sample is ordered from smallest to largest).\n* $\bar{x}$ is the sample mean. \n* Variables $a_i$ are calculated via\n\n$$(a_1, ... , a_n) = \frac{m^T V^{-1}}{(m^T V^{-1}V^{-1}m)^{1/2}} \hspace{2cm}m = (m_1 , ... , m_n)$$\n\nwhere $m_1 , ... , m_n$ are the mean values of the ordered statistic, of independent and identically distributed random variables, sampled from normal distributions and $V$ denotes the covariance matrix of that order statistic. **The null hypothesis is rejected if W is too small. The value of W can range from 0 to 1.**"
"\n    3.4 Q-Q plots\n\n\nQ-Q plots, aka **Quantile-Quantile** plots, are used to **visually compare** how similar two distributions are to each other. They consist of plotting the quantiles (i.e. regular intervals) of the **observed** distribution against the quantiles of the **theoretical** distribution. The closer the Q-Q plots are to forming a **straight line**, the more confident you can be that the observed and theoretical distributions are the **same**. \n\n**Normal Q-Q plots**"
"Even though the features *f_22* to *f_28* failed the Shapiro-Wilk test, they still appear to be quite close to being normally distributed. This behaviour could be because these features are made up of a **mixture** of normal distributions. There is not an easy way to verify this however.\n\n\n\n\n**Poisson Q-Q plots**"
"We can see more clearly that these features are not distributed according to independent Poisson distributions. However, some of them are quite close. It could be also that these features are made up of a **mixture** of Poisson distributions. Unfortunately, there isn't an easy way to verify this.\n\n\n\n    3.5 Correlations\n\n\n* Features *f_00* to *f_06* and *f_14* to *f_21* are **independent** of all other features.\n* Discrete features (*f_07* to *f_13*) and features *f_22* to *f_28* are **weakly dependent** of each other."
"# 4. Elbow method\n\n\n    4.1 How it works\n\n\n\nThe **elbow method** is a practical way to determine the number of clusters in a dataset. It works by plotting the **inertia** (or sometimes distortion) against the **number of clusters**, where **inertia** is defined to be the sum of squared distances of samples to their closest cluster center, i.e. a measure of the models bias.  The '**elbow**' (point of sudden flattening) of the curve is then chosen to be the optimal number of clusters in the dataset.\n\n\n\n\n\nThe idea is that we want **low inertia** (because that means we have a good model), but not too low otherwise this will lead to **overfitting** (since if k=number of samples then every point is a cluster and the inertia is 0). The elbow usually represents the point of **diminishing returns** and therefore is a good **heuristic** for the optimal number of clusters.\n\n\n    4.2 Applying it \n\n\nSee my [discussion post](https://www.kaggle.com/competitions/tabular-playground-series-jul-2022/discussion/335079) where I used 50 clusters and the whole dataset. To save time here, we will just use 30 clusters and 10% of the data."
"It is **hard** to tell exactly what the optimal value for the number of clusters should be since the curve is quite smooth. We will go with **k=7** for now, but it might be worth experimenting with different values of k as well. \n\n# 5. Competition metric\n\nIt is worth spending some time trying to understand the competition metric. This is called the **Adjusted Rand Index (ARI)**. But to do this, we first need to look at the **Rand Index (RI)**.\n\n\n    5.1 Rand Index\n\n\nThe Rand Index (named after **William Rand** from 1971) is a measure of **similarity** between the predicted clusters and the ground truth clusters. It looks at whether **pairs** of data points are in the same or different clusters. Let's work through an **example** to see how it works.\n\n$$\Large RI = \frac{a+b}{{n \choose 2}}$$\n\n\n\n\n\nFirst note that there are $n=5$ data points (denoted by greek letters, alpha to epsilon). The prediction is made up of **3 clusters**, whereas the ground truth is made up of **2 clusters**.\n\nThe combinatorial **formula** for the total **number of pairs** of data points is given by ${n \choose 2} = \frac{n(n-1)}{2}$. So for $n=5$, there are 10 total pairs. These are:\n\n$\{\alpha, \beta\}, \{\alpha, \gamma\}, \{\alpha, \delta\}, \{\alpha, \epsilon\}, \{\beta, \gamma\}, \{\beta, \delta\}, \{\beta, \epsilon\}, \{\gamma, \delta\}, \{\gamma, \epsilon\}, \{\delta, \epsilon\}$.\n\n\n\nTo work out the Rand Index, we need to calculate **two quantities**:\n* $a$ = # pairs in the **same** cluster in the prediction and the **same** cluster in the ground truth.\n* $b$ = # pairs in **different** clusters in the prediction and **different** clusters in the ground truth.\n\nThis can be a **bit confusing** but for example, the points ${\color{orange} \alpha}, {\color{orange} \beta}$ are in the same cluster in the prediction (orange) and in the same cluster in the ground truth (orange), so the pair $\{{\color{orange} \alpha}, {\color{orange} \beta}\}$ counts towards $a$. On the other hand, the points ${\color{orange} \alpha}, {\color{green} \delta}$ are in different clusters in both the prediction and ground truth (orange, green) so the pair $\{{\color{orange} \alpha}, {\color{green} \delta}\}$ counts towards $b$. \n\n\n\nIf we continue like this (**check this yourself**), you will find that the pairs $\{{\color{orange} \alpha}, {\color{orange} \beta}\}, \{{\color{green} \delta}, {\color{green} \epsilon}\}$ are in the **same** cluster for both prediction and ground truth so $a=2$ and the pairs $\{{\color{orange} \alpha}, {\color{green} \delta}\}, \{{\color{orange} \alpha}, {\color{green} \epsilon}\}, \{{\color{orange} \beta}, {\color{green} \delta}\}, \{{\color{orange} \beta}, {\color{green} \epsilon}\}, \{{\color{red} \gamma}, {\color{green} \delta}\}, \{{\color{red} \gamma}, {\color{green} \epsilon}\}$ are in **different** clusters for both prediction and ground truth so $b=6$.\n\nGreat, so putting the numbers in we find that $RI=\frac{2+6}{10}=0.8$.\n\n\n    5.2 Properties of RI\n\n\n* RI lies **between 0 and 1**. The closer to 1 the better.\n* If the prediction is **perfect**, i.e. equal to the ground truth, then **RI = 1**.\n* We say a pair of points is in '**agreement**' if they count towards a or b (above), and in '**disagreement**' otherwise. If we pick two points at **random**, RI gives the **probability** that this pair of points is in agreement. (i.e. the predicted 'state' of the pair is 'correct')\n* RI is equivalent to **accuracy** when viewed from a **binary classification** problem over the **pairs** of data points. In particular, each pair is either in agreement (1) or in disagreement (0), in which case $a=\text{True Positives} \, (TP)$ and $b=\text{True Negatives} \, (TN)$ so the Rand Index becomes:\n\n$$RI = \frac{TP + TN}{TP + FP + FN + TN} = \, \text{accuracy of pairs}$$\n* The only main **downside** to RI is that the **expected value** of RI, $\mathbb{E}(RI)$, isn't the same for different clustering problems. That means, some problems are **easier** to get a good RI score than others so we can't really **compare** RI between different problems. This is where the adjusted RI comes in. \n\n\n    5.3 Adjusted Rand Index\n\n\nThe **Adjusted Rand Index** (ARI) (introduced by **Hubert** and **Arabie** in 1985) is a 'corrected-for-chance' version of the Rand Index. It substracts RI by the expected value of RI for the specific clusterting problem. It then scales this number so that it has a maximum value of 1. \n\n$$\Large ARI = \frac{RI - \mathbb{E}(RI)}{max(RI)-\mathbb{E}(RI)}$$\n\n**Properties of ARI:**\n* It has a **maximum value of 1** but **no minimum** value (it can be negative). \n* If the prediction is **perfect**, i.e. equal to the ground truth, then **ARI = 1**.\n* A score of **0**, means the prediction is as good as picking all the clusters at **random**. \n* It is **comparable** between different clustering problems as its expected value is **constant** (0).\n\n\n\nWe know how to work out the RI and also that $max(RI)=1$, but working out the expectation $\mathbb{E}(RI)$ is much **trickier**. Hubert derived the following (rather complicated) **formula** for the entire ARI. See the **appendix** if you are interested to see the derivation.\n\n$$\n\large ARI = \frac{ \left. \sum_{ij} \binom{n_{ij}}{2} - \left[\sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2}\right] \right/ \binom{n}{2} }{ \left. \frac{1}{2} \left[\sum_i \binom{a_i}{2} + \sum_j \binom{b_j}{2}\right] - \left[\sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2}\right] \right/ \binom{n}{2} }\n$$\n\n**Note:** This is equivalent to the ARI formula above. \n\n\n    5.4 ARI example\n\n\nFirst, we start by **formalising** the clustering problem. We denote our dataset with $n$ objects by $S = \{o_1, o_2, \ldots, o_n \}$ (Each element is just a data point). Let's represent the ground truth and predicted clustering by two **partitions**: $X=\{X_1, \ldots, X_r\}$ and $Y=\{Y_1, \ldots, Y_s\}$, respectively.\n\nContinuing from our previous example, $X=\{X_1, X_2\}$ with $X_1=\{{\color{orange} \alpha},{\color{orange} \beta},{\color{orange} \gamma}\}$, $X_2=\{{\color{green} \delta},{\color{green} \epsilon}\}$ and $Y=\{Y_1, Y_2, Y_3\}$ with $Y_1=\{{\color{orange} \alpha},{\color{orange} \beta}\}$, $Y_2=\{{\color{red} \gamma}\}$, $Y_3=\{{\color{green} \delta},{\color{green} \epsilon}\}$.\n\n\n\nThen we draw a **contingency** table. Each entry, $n_{i,j}$, denotes how many data points there are in common between the ground truth cluster $X_i$ and the predicted cluster $Y_j$. Mathematically, the formula is $n_{i,j} = |X_i \cap Y_j |$, i.e. the **intersection**.\n\n$$ \n\begin{array}{c|cccc|c}\n{{} \atop X}\!\diagdown\!^Y &\nY_1&\nY_2&\n\cdots&\nY_s&\n\text{sums}\n\\\n\hline\nX_1&\nn_{11}&\nn_{12}&\n\cdots&\nn_{1s}&\na_1\n\\\nX_2&\nn_{21}&\nn_{22}&\n\cdots&\nn_{2s}&\na_2\n\\\n\vdots&\n\vdots&\n\vdots&\n\ddots&\n\vdots&\n\vdots\n\\\nX_r&\nn_{r1}&\nn_{r2}&\n\cdots&\nn_{rs}&\na_r\n\\\n\hline\n\text{sums}&\nb_1&\nb_2&\n\cdots&\nb_s&\n\end{array}\n$$\n\nFor example, to work out $n_{11}$, we look at clusters $X_1=\{{\color{orange} \alpha},{\color{orange} \beta},{\color{orange} \gamma}\}$ and $Y_1=\{{\color{orange} \alpha},{\color{orange} \beta}\}$ and find the points which appear in both of them. In this case, there are 2:  $X_1 \cap Y_1 = \{{\color{orange} \alpha},{\color{orange} \beta}\}$ so $n_{11}=2$. If we continue like this (**check this yourself**) we get:\n\n$$ \n\begin{array}{c|ccc|c}\n{{} \atop X}\!\diagdown\!^Y &\nY_1&\nY_2&\nY_3&\n\text{sums}\n\\\n\hline\nX_1&\n2&\n1&\n0&\n3\n\\\nX_2&\n0&\n0&\n2&\n2\n\\\n\hline\n\text{sums}&\n2&\n1&\n2&\n\end{array}\n$$\n\n\n\nWe are almost there. To work out ARI, we need to calculate these 3 quantities: $\sum_{i,j} {n_{ij} \choose 2}$, $\sum_{i} {a_{i} \choose 2}$, $\sum_{j} {b_{j} \choose 2}$.\n\nFirst recall the formula ${n \choose 2} = \frac{n(n-1)}{2}$, which we will be using a lot. E.g. ${5 \choose 2} = 10$\n\n1. $\sum_{i,j} {n_{ij} \choose 2} = {2 \choose 2} + {1 \choose 2} + {0 \choose 2} + {0 \choose 2} + {0 \choose 2} + {2 \choose 2} = 1 + 0 + 0 + 0 + 0 + 1 = 2$.\n\n2. $\sum_{i} {a_{i} \choose 2} = {3 \choose 2} + {2 \choose 2} = 3 + 1 = 4$.\n\n3.  $\sum_{j} {b_{j} \choose 2} = {2 \choose 2} + {1 \choose 2} + {2 \choose 2} = 1 + 0 + 1 = 2$.\n\nSo if we plug everything into the formula, we get $ARI = \frac{2 - (4 \times 2) / 10}{(4 + 2)/2 - (4 \times 2)/10} = 0.55$. \n\nThe ARI score (0.55) is quite a bit smaller than the RI score (0.80) we got earlier. This is somewhat expected though with such a small clustering problem; since the number of points $n$ is small, the problem is relatively easy so the ARI makes a **large adjustment**.\n\n# 6. Modelling\n\n\n    6.1 Scaling\n\n\nIt is always important to scale the data for clustering problems so that it is easier to compare the distance between data points. \n\n\n\n\n\nThere are several ways to do this, e.g.\n\n* *StandardScaler*: scales each column independently to have mean 0 and standard deviation 1, by subtracting by the column **mean** and dividing by the column **standard deviation**.\n* *RobustScaler*: does the same as above but uses statistics that are **robust to outliers**, i.e. it subtracts by the **median** and divides by the **interquartile range**. \n* *PowerTransformer*: makes columns more gaussian like by **stabilising variance** and **minising skew**. "
GMM performs much better than k-Means (around ARI=0.49 on public leaderboard). \n\nWe can plot the position of the **center** of each cluster to visualise how well each feature is able to **separate** the different clusters.
From this plot we can see that the features *f_00* to *f_06* and *f_14* to *f_21* **don't separate** the clusters at all! This means we might as well **drop** these features as they are not helping us in any way.
"\n### 1.2. Scope of analysis\n\nAmong the many questions that clutter the mind, the one that occupies the top spot is this:. \nWho does this survey data respresent? In this section, we try to address this very question by looking at the sample data (2020 survey data), the auxiliary documents (viz. the `kaggle_survey_2020_methodology.pdf` and the`kaggle_survey_2020_answer_choices.pdf`), the meta-kaggle datasets. We argue that given the survey methodology ([see detailed below](#method)), most of the survey respondents are active Kaggle users and therefore represent the actively involved data-enthusiasts and data-science-experts (especially the ones who are involved enough to fill up a long multiple choice/ multiple selection survey).\n\nAbout the meta-kaggle data: Along with many other details, the meta-kaggle datasets contain the list of Kaggle 'Users' with their registration date (on Kaggle), the 'User Achievements', and the list of 'Submissions' made by the users (with data of submission). As expected the 'Users' data shows exponetial growth in Kaggle userbase over time - since inception (2010), every year, the number of new registrations on the platform has multiplied; starting with a humble 4558 users, today kaggle userbase is reaching a humongous ~6 million! Out of these ~6M users, more than a third registered in 2020 itself. "
"However compared to these huge numbers, the number of users who participated in the survey in 2020 seems too tiny at ~20k, and indicates that only a fraction of these ~6M registered users might be  actively involved in the community. To confirm the same, we chart out the number of tiered users. Turns out that as suspected, **less than 2% of all registered users, i.e. 102419 users on Kaggle are tiered** (*There are 4 performance tiers - contributor, expert, master, and grandmaster*).\n\n\n---\n\n\n\n\nThis year the survey-invitation was sent out to the community via e-mails (anyone who\nopted-in to the Kaggle Email List was invited). The survey was also promoted on the Kaggle website and on the Kaggle Twitter channel. So the target audience included both \n* professionals and non-professionals (students/ unemployed/ ones who never spent any money on cloud)\n* males/ females/ LGBTQA+\n* 18 years and older\n* people with or without any formal education\n* people residing in 171 countries\n\nThe 20,036 people who responded, can accordingly be grouped under any of these broad categories or a combintion thereof.\n\n[Source: `kaggle_survey_2020_methodology.pdf`]\n\nNOTE: Though a respondent did not need to be registered Kaggle user to fill up the Kaggle survey, because the survey was promoted on the Kaggle website and the Kaggle Twitter channel and the invitations to participate in the survey were sent to anyone who\nopted-in to the Kaggle Email List, it seems highly likely that most survey respondents are registered users because otherwise they would have been unlikely to see any of the survey promotions.\n"
"Similarly, the number of users who made at least one submission in that year, are consistently low. Less than 90k users have made any submission on Kaggle in 2020. So turns out that every year, the number of users on Kaggle who made at least one submission in the year is at most about 5 times the number of people who filled up the Kaggle survey in that year.\nTakeaway: Based on the number of respondents, the sample seems to be large enough  to be representative of the active Kaggle users' community (and therfore the data science community at large).\n\n\n---\n\nActive users: For the purpose of this analyis we have labeled a user as active in any particular year if s/he made at least 1 submission on Kaggle that year."
"#### Response rate - question-wise and participant-wise:\n\nNow that the total number of survey participants has been put in perspective, couple of questions regarding the 2020 survey demand our attention next: \n\n1. How many of the survey-questions did each survey-respondent answer?\n2. How many of the survey-respondents did answer any particular survey-question?\n\nRegarding the first question, we find that in 2020, approximately, \n* 15k participants (75% of the respondents) answered half the questions. \n* 10k participants (half of the respondents) answered 30 or more questions.\n* a thousand of the respondents i.e. ~5% of all respondents responded to 5 questions or less.\n\n---\n\nNote: Thoughout this analysis, 'NA' responses are considered as missing responses.\n"
"Before addressing the second question (i.e. what percentage of respondents answered a particular question), it is important to note that -\n* 7 of the questions were follow-up questions and therefore shown to a few respondents only (selected based on their response to the related mandatory questions). These follow up are: Q18, Q19, Q27A, Q28A, Q30, Q32, and Q34.\n* 8 questions (Q26-Q29, Q31, and Q33-35) all had two versions (A, and B) - version A was for the professionals and version B was for the non-professionals (as discussed earlier in [[1](#s1)])\nSo throughout this analysis, we combine the responses to the two versions of each of these questions wherever needed.\n\nSo given all these details, it is but natural that the questions with worst response rates are the follow-up questions (which were asked to a very few respondents to begin with and therefore only a tiny fraction of the 2020 survey respondents answered these questions).\n\nIt is also worth noting that:\n* The demographic-questions, like age, gender, country of residence, etc. have the highest response rate i.e. most respondents answered these questions properly (with non-NA replies).\n* Most respondents (more than 4-out-of-5) also answered all the questions relating to their current designation and programming experience (languages, notebooks, IDEs, etc.)\n* Apart from the follow-up questions (which naturally have low response rate), the questions relating to professional details and practices, and paid services (e.g. team size, company size, compensation, Auto ML/ cloud products and services,etc.) have relatively poor response rate. \n\nNote: The question-wise response rates clearly hint at a high percentage of non-professionals (students, unemployed, and people who have never spent any money on cloud/ML products/services) among the respondents. 💭\n"
"Time taken to complete the survey:\n\nBefore moving on further, it might be worth noting that while the survey seems to be longish, the median user took about 10 mins only to complete the survey, and even the median professional user, who by design was faced with more questions (relating to their profession and data-science related spending behavior and experiences), completed the survey in only 12.5 minutes. ~85% repondents completed the 2020 survey in less than half an hour.\n\n\n---\n\nNote that, 95% of all respondents completed the 2020 survey in less than 4 hours. So we have removed the respondents who took more than 4 hrs to complete the survey while creating the following chart. "
"\n### 2.1. Gender distribution\nAccoring to [United Nations](https://population.un.org/wpp/Download/Standard/Population) estimates, the world housed 49.58% females, as of 1 July 2020. Compared to that, only 21% of the survey participants in 2020 identified themselved as female/ LGBTQA+! So as far as gender-equality is concerned, we as a community, have a lot to do.  On the brighter side however, this **21% participation rate in 2020** implies a significant **improvement (by 3%)** over the previous year (2019), when the female/ LGBTQA+ survey participation rate was 18%!"
"On dissecting the data further, we find that the improvement in the gender-distribution data comes purely from the female segment (and not LGBTQA+) of participants."
The improvement in gender-distribution in the community in 2020 becomes even more pronounced when we look at all 4 years of survey data (adjusted for the total number of participants in each of those years). We find that the **female participation rate in 2020 has seen a sharp 3% improvement this year** and after remaining stable for past three years (2017-2019). \n\nTakeaway: Female Kagglers are on the rise. 📈
[Go back to the top](#qa)
"\n### 2.2. Country-wise expanse\nGiven the enormous amount of data being generated every day (source: [visualcapitalist](https://www.visualcapitalist.com/wp-content/uploads/2019/04/data-generated-each-day-full.html)), it is but obvious that the world is in need for a large number of sufficiently trained people capable of handling and utilizing it. The natural question then is how well-distributed is our community? Do we have enough people distributed around the world addressing the world's data needs or are there specific data-expert-hubs, that could cater to the data science needs of the rest of the world.  This year (2020), the survey was sent out to Kagglers in 171 countries, and to protect the identities of the survey-respondents, countries with less than 50 respondents were grouped together under the common country bucket - 'Other'. As a result, in 2020, we had *54 countries*, each with 50 or more survey-participants and another group of countries, under the name 'Other'.\nCharting world population estimates , as of 1 July 2020, from [United Nations](https://population.un.org/wpp/Download/Standard/Population) and the country-wise survey partication on world maps placed side-by-side, it becomes clear that -\n* At ~30%, India has the highest head-count contribution to the 2020 survey. This is much greater than India's contribution to the world population, which currently stands at ~17.5%\n* While less than 5% of the world's population resides in the U.S.A, and yet more than 11% of the 2020 survey participants reside in the U.S.A.\n* The world's largest country by population, China (>20%), has a suspiciously low participation rate in the 2020 survey (<2.5%). Given that China is one of the leading nations when it comes to data science according to different media sources including [Analytics Insight](https://www.analyticsinsight.net/countries-which-hold-the-greatest-opportunities-for-data-scientists), this low rate of participation from China seems to point at a cultural difference between different countries regarding survey participation. \n\n*Note that for the world population chart, we have used only the data pertaining to people in the age bracket of 15 years or older, since our survey data includes people of age (i.e. 18 years or older only). We could not use 18 years as the minimum age for the UN world-popuation data, because the in raw data from UN, the people in the age bucket 15-19 years were grouped together.*"
"Looking at the historical survey data (2017-20), we find that consistently :\n* India has replaced U.S.A. to become the home to the largest number of Kaggle survey participants. (Almost a third of the 2020 survey participants live in India).\n* With more than 1-in-10 participants residing in the U.S.A., U.S.A. is currently home to the second largest number of participants.\n* ~40% of the survey participants live in just two countries, India and U.S.A.\n* ~Half of the survey participants come from just 5 countries every year.\n* Russia with ~3% of the survey participants has also consistently featured in the list of top 5 countries where the participants lived in 2017-20. "
"Looking at the female/LGBTQA+ participants only, we find that \n* India and U.S.A. again feature at the top and in that order.\n* U.K. consistently comes third with ~3% of the female/LGBTQA+ participants.\n* Russia did not feature among the top 5 countries after 2018.\n* ~Half of the female/LGBTQA+ reside in just the top 2 countries, viz. India and U.S.A.\n* Interestingly, Turkey with 95 female/LGBTQA+ respondants has stormed its way into the list of top 5 countries for female/LGBTQA+ users in the 2020."
A closer look reveals that among the 10 countries with largest number of female/LGBTQA+ participants in 2020:\n* Indonesia(30%) and Turkey (27%) have the highest share of female/LGBTQA+ participants.\n* Brazil(13%) and Russia(15%) have the lowest share of female/LGBTQA+ participants.\n* Only 5 out of these top 10 countries have average or above-average share of female/LGBTQA+ respondents ([21% of all respondents are female/LGBTQA+](#q3)).\n\nTakeaway: The jump in the proportion of female respondents is largely driven by the rise in female participation from India.
"**India vs. U.S.A: A comparative analysis of the paticipation trends**\n\n* As already noted previously, there is a conistently growing number of Indians among the survey-participants (29% in 2020). \n* The same holds true for the cohort of female/LGBTQA+ survey-participants; Indians account for 32% of the female/LGBTQA+ survey participants.\n* Meanwhile, it is a bit surprising to see that even based on the absolute numbers of survey-participants, Americans are a shrinking community as far as the Kaggle surveys are concerned. \n* Another positive takeaway from India: the share of female/LGBTQA+ respondents in India saw a sharp jump this year (thus taking the number of female/LGBTQA+ among every 100 Indian survey-participants to 23, straight up from 17 just a year ago!)\n* The share of female/LGBTQA+ respondents in the U.S.A. meanwhile remained stable at ~25%\n\n\n---\n\nThe receding number of respondents from U.S.A. adds to our inhibition that there might be a fewer number of repeat survey-respondents i.e. people who submit the survey once, might be less enthusiastic about filling it up again. This is just a hunch and at the moment unverifiable using the available data. \nRelevant suggestions:💡\n- Going forward, it might be worthwhile to ask the respondents to select whether or not they filled the Kaggle survey in any of the previous years.\n- A prefilled survey (with data from the user's public Kaggle profile) could also be provided to the users to encourage more users to respond to the annual survey."
"* Our data is sign language digits dataset.\n* Shape of our data is (2062,64,64). \n    * 2062 is number of images.\n    * There are 64x64 pixels.\n* Lets load the data. We will use all images that are from zero to nine.\n* There are totally 2062 images.\n* I will plot one sample from each digits."
"* As a training set we will use all images.\n* As a test set we will choose ten images and use them.\n* reshape(-1) :  It simply means that it is an unknown dimension and we want numpy to figure it out\n* Difference between ""//"" and ""/"": for example 4097/2 = 2048.5 (division) and 4097//2 = 2048\n "
"## Principle Componenet Analysis (PCA)\n* I think it is very cool and good understanding way of PCA. \n* Fundemental dimension reduction technique\n* It is real life example. I hope it makes more sence for you.\n* Now lets try one more thing. \n* As you remember training time is almost 45 second. I think it is too much time. The reason of this time is number of sample(2062) and number of feature(4096). \n* There is two way to decrease time spend.\n    1. Decrease number of sample that I do no recommend.\n    1. Decrease number of features. As you see from images all signs are in the middle of the frames. Therefore, around signs are same for all signs. \n    * Think that all of the pixels are feature. Features which are out of the red frame is useless but pixels in red frame take part in  training and prediction steps.  Therefore, we need to make dimension reduction. \n    * PCA: Principle componenet analysis\n        * One of the most popular dimension reduction technique.\n        * PCA  uses high variances. It means that it likes diversity. For example compare two images above. oUt of red frame there is no diversity (high variance). On the other hand, in red frames there is diversity.\n            * first step is decorrelation:\n                * rotates data samples to be aligned with axes\n                * shifts data SAmples so they have mean zero\n                * no information lost\n                * fit() : learn how to shift samples\n                * transform(): apply the learned transformation. It can also be applies test data( We do not use here but it is  good to know it.)\n                * Resulting PCA features are not linearly correlated\n                * Principle components: directions of variance\n            * Second step: intrinsic dimension: number of feature needed to approximate the data essential idea behind dimension reduction\n                * PCA identifies intrinsic dimension when samples have any number of features\n                * intrinsic dimension = number of PCA feature with significant variance\n* Lets apply PCA and visualize what PCA says to us."
"* In sign language digits, most important things are fingers\n* After PCA, as you can see fingers are emphasized.\n* Try other images and play with n_components"
"# CNN Model Intro with Fashion MNIST Implementation\n### Brief Introduction\nThis notebook will cover the following two major topics :\n\n#### Understand the basic concepts of CNN model\n#### Implement CNN model in realtime using Fashion MNIST dataset\n\n## Understand the basic concepts of CNN model :\n\nMankind is an awesome natural machine and is capable of looking at multiple images every second and process them without realizing how the processing is done. But same is not with machines. \n\nThe first step in image processing is to understand, how to represent an image so that the machine can read it?\n\nEvery image is an cumulative arrangement of dots (a pixel) arranged in a special order. If you change the order or color of a pixel, the image would change as well. \n\n![](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-07-at-9-15-21-pm.png)\nThree basic components to define a basic convolutional neural network.\n\n### The Convolutional Layer\n### The Pooling layer\n### The Output layer\n\nLet’s see each of them in detail\n\n### The Convolutional Layer :\n\nIn this layer if we have an image of size 6*6. We define a weight matrix which extracts certain features from the images*\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28010254/conv1.png)\nWe have initialized the weight as a 3*3 matrix. This weight shall now run across the image such that all the pixels are covered at least once, to give a convolved output. The value 429 above, is obtained by the adding the values obtained by element wise multiplication of the weight matrix and the highlighted 3*3 part of the input image.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28011851/conv.gif)\nThe 6*6 image is now converted into a 4*4 image.  Think of weight matrix like a paint brush painting a wall. The brush first paints the wall horizontally and then comes down and paints the next row horizontally. Pixel values are used again when the weight matrix moves along the image. This basically enables parameter sharing in a convolutional neural network.\n\nLet’s see how this looks like in a real image.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28132834/convimages.png)\n\n* The weight matrix behaves like a filter in an image, extracting particular information from the original image matrix. \n* A weight combination might be extracting edges, while another one might a particular color, while another one might just blur the unwanted noise.\n* The weights are learnt such that the loss function is minimized and extract features from the original image which help the network in correct prediction.\n* When we use multiple convolutional layers, the initial layer extract more generic features,and as network gets deeper the features get complex.\n\nLet us understand some concepts here before we go further deep\n\n#### What is Stride?\n\nAs shown above above, the filter or the weight matrix we moved across the entire image moving one pixel at a time.If this is a hyperparameter to move weight matrix 1 pixel at a time across image it is called as stride of 1. Let us see for stride of 2 how it looks.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28090227/stride1.gif)\n\nAs you can see the size of image keeps on reducing as we increase the stride value. \n\nPadding the input image with zeros across it solves this problem for us. We can also add more than one layer of zeros around the image in case of higher stride values.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28093553/zero-padding.png)\nWe can see how the initial shape of the image is retained after we padded the image with a zero. This is known as same padding since the output image has the same size as the input. \n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28094927/padding.gif)\nThis is known as same padding (which means that we considered only the valid pixels of the input image). The middle 4*4 pixels would be the same. Here we have retained more information from the borders and have also preserved the size of the image.\n\n#### Having Multiple filters & the Activation Map\n\n* The depth dimension of the weight would be same as the depth dimension of the input image.\n* The weight extends to the entire depth of the input image. \n* Convolution with a single weight matrix would result into a convolved output with a single depth dimension. In case of multiple filters all have same dimensions applied together.\n* The output from the each filter is stacked together forming the depth dimension of the convolved image. \n\nSuppose we have an input image of size 32*32*3. And we apply 10 filters of size 5*5*3 with valid padding. The output would have the dimensions as 28*28*10.\n\nYou can visualize it as –\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28113904/activation-map.png)\nThis activation map is the output of the convolution layer.\n\n### The Pooling Layer\n\nIf images are big in size, we would need to reduce the no.of trainable parameters.For this we need to use pooling layers between convolution layers. Pooling is used for reducing the spatial size of the image and is implemented independently on each depth dimension resulting in no change in image depth. Max pooling is the most popular form of pooling layer.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28022816/maxpool.png)\nHere we have taken stride as 2, while pooling size also as 2. The max operation is applied to each depth dimension of the convolved output. As you can see, the 4*4 convolved output has become 2*2 after the max pooling operation.\n\nLet’s see how max pooling looks on a real image.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28133544/pooling.png)\nIn the above image we have taken a convoluted image and applied max pooling on it which resulted in still retaining the image information that is a car but if we closely observe the dimensions of the image is reduced to half which basically means we can reduce the parameters to a great number.\n\nThere are other forms of pooling like average pooling, L2 norm pooling.\n\n#### Output dimensions\n\nIt is tricky at times to understand the input and output dimensions at the end of each convolution layer. For this we will use three hyperparameters that would control the size of output volume.\n\n1. No of Filter: The depth of the output volume will be equal to the number of filter applied.The depth of the activation map will be equal to the number of filters.\n\n2. Stride – When we have a stride of one we move across and down a single pixel. With higher stride values, we move large number of pixels at a time and hence produce smaller output volumes.\n\n3. Zero padding – This helps us to preserve the size of the input image. If a single zero padding is added, a single stride filter movement would retain the size of the original image.\n\nWe can apply a simple formula to calculate the output dimensions.\n\nThe spatial size of the output image can be calculated as( [W-F+2P]/S)+1. \nwhere, W is the input volume size, \n       F is the size of the filter, \n       P is the number of padding applied \n       S is the number of strides. \n       \nLet us take an example of an input image of size 64*64*3, we apply 10 filters of size 3*3*3, with single stride and no zero padding.\n\nHere W=64, F=3, P=0 and S=1. The output depth will be equal to the number of filters applied i.e. 10.\n\nThe size of the output volume will be ([64-3+0]/1)+1 = 62. Therefore the output volume will be 62*62*10.\n\n### The Output layer\n* With no of layers of convolution and padding, we need the output in the form of a class.\n* To generate the final output we need to apply a fully connected layer to generate an output equal to the number of classes we need.\n* Convolution layers generate 3D activation maps while we just need the output as whether or not an image belongs to a particular class. \n* The Output layer has a loss function like categorical cross-entropy, to compute the error in prediction. Once the forward pass is complete the backpropagation begins to update the weight and biases for error and loss reduction.\n\n### Summary:\n* Pass an input image to the first convolutional layer. The convoluted output is obtained as an activation map. The filters applied in the convolution layer extract relevant features from the input image to pass further.\n* Each filter shall give a different feature to aid the correct class prediction. In case we need to retain the size of the image, we use same padding(zero padding), otherwise valid padding is used since it helps to reduce the number of features.\n* Pooling layers are then added to further reduce the number of parameters\n* Several convolution and pooling layers are added before the prediction is made. Convolutional layer help in extracting features. As we go deeper in the network more specific features are extracted as compared to a shallow network where the features extracted are more generic.\n* The output layer in a CNN as mentioned previously is a fully connected layer, where the input from the other layers is flattened and sent so as the transform the output into the number of classes as desired by the network.\n* The output is then generated through the output layer and is compared to the output layer for error generation. A loss function is defined in the fully connected output layer to compute the mean square loss. The gradient of error is then calculated.\n* The error is then backpropagated to update the filter(weights) and bias values.\n* One training cycle is completed in a single forward and backward pass.\n\n### Implement CNN model in realtime using Fashion MNIST dataset\n\n![](https://pyimagesearch.com/wp-content/uploads/2019/02/fashion_mnist_dataset_sample.png)\nFashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\nThe original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try.  ""If it doesn't work on MNIST, it won't work at all"", they said. ""Well, if it does work on MNIST, it may still fail on others.""\n\nZalando seeks to replace the original MNIST dataset\n\n### Data Description\n\n* Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\n* Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. \n* The training and test data sets have 785 columns. \n* The first column consists of the class labels (see above), and represents the article of clothing. \n* The rest of the columns contain the pixel-values of the associated image.\n\nTo locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\nFor example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below. \n\n### Get the Data\n\nYou can use direct links to download the dataset.\n\n| Name  | Content | Examples | Size | Link | MD5 Checksum|\n| --- | --- |--- | --- |--- |--- |\n| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|\n\nAlternatively, you can clone this GitHub repository; the dataset appears under `data/fashion`. This repo also contains some scripts for benchmark and visualization.\n   \n```bash\ngit clone git@github.com:zalandoresearch/fashion-mnist.git\n```\n\n#### Labels\nEach training and test example is assigned to one of the following labels:\n\n* 0 T-shirt/top \n* 1 Trouser\n* 2 Pullover\n* 3 Dress\n* 4 Coat\n* 5 Sandal\n* 6 Shirt\n* 7 Sneaker\n* 8 Bag\n* 9 Ankle boot \n\n#### TL;DR\n\n* Each row is a separate image\n* Column 1 is the class label.\n* Remaining columns are pixel numbers (784 total).\n* Each value is the darkness of the pixel (1 to 255)\n\n### Acknowledgements\n\n* Original dataset was downloaded from https://github.com/zalandoresearch/fashion-mnist\n\n* Dataset was converted to CSV with this script: https://pjreddie.com/projects/mnist-in-csv/"
**Create dataframes for train and test datasets**
Now let us visualise the some samples after the resize of the data which needs to be ready for train the network .\n
Labels\nEach training and test example is assigned to one of the following labels as shown below:\n\n* 0 T-shirt/top\n* 1 Trouser\n* 2 Pullover\n* 3 Dress\n* 4 Coat\n* 5 Sandal\n* 6 Shirt\n* 7 Sneaker\n* 8 Bag\n* 9 Ankle boot\n\nI think the best way is to visualise the above 10 types of classes to get a feel of what these items look like :) .So let us visualise\n
As you can observe above the shape of shoe from the sample image\n\n### Create the Convolutional Neural Networks (CNN)\n\n#### Define model\n\n#### Compile model\n\n#### Train model\n\nFirst of all let us define the shape of the image before we define the model. Defined the shape of the image as 3d with rows and columns and 1 for the 3d visualisation\n
Let us plot the Training Accuracy vs Loss to get a better understanding of the model training.
- #### Evaluate /Score the model
### 1.3 - Exploring
"SibSp and Parch don't seem to have a clear relationship with the target, so put them together can be a good idea.\nFor Ticket and Cabin a good strategie can be count the number of caracteres."
## Data Interaction\n* Scatter plot
## Data Visualization\n* Box and density plots
## Data Visualization\n* Grouping of One hot encoded attributes
## Data Cleaning\n* Remove unnecessary columns
## Feature selection\n* RFE
#Feature Selection\n* SelectPercentile
#Feature Selection\nRanking summary
#Feature Selection\nRank features based on median
# 1. Importing the necessary libraries
#  2. Reading the datasets
## Exploring the 'keyword' column\nThe keyword column denotes a keyword from the tweet.Let's look at the top 20 keywords in the training data
Let's see how often the word 'disaster' come in the dataset and whether this help us in determining whether a tweet belongs to a disaster category or not.
"## Exploring the 'location' column\nEven though the column `location` has a number of missing values, let's see the top 20 locations present in the dataset. Since some of the locations are repeated, this will require some bit of cleaning."
"> #  4. Text Data Preprocessing\n\n## 1. Data Cleaning\n\nBefore we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Some of the  basic text pre-processing techniques includes:\n\n* Make text all **lower case** or **uppercase** so that the algorithm does not treat the same words in different cases as different\n* **Removing Noise** i.e everything that isn’t in a standard number or letter i.e Punctuation, Numerical values,  common non-sensical text (/n)\n* **Tokenization**: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n* **Stopword Removal**: Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words\n\n### More data cleaning steps after tokenization:\n\n* **Stemming**: Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”.\n* **Lemmatization**: A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.\n* Parts of speech tagging\n* Create bi-grams or tri-grams\nAnd more...\n\nHowever, it is not necessary that you would need to use all these steps. The usage depends on your problem at hand. Sometimes removal of stop words helps while at other times, this might not help.Here is a nice table taken from the blog titled : [All you need to know about Text Preprocessing for Machine Learning & NLP](https://kavita-ganesan.com/text-preprocessing-tutorial/#.Xi2BhhczZTY) that summarizes how much preprocessing you should be performing on your text data:\n\n![](https://kavita-ganesan.com/wp-content/uploads/2019/02/Screen-Shot-2019-02-23-at-1.36.52-PM-590x270.png)"
Just for fun let's create a wordcloud of the clean text to see the most dominating words in the tweets.
"## 2. Tokenization\n\nTokenization is a process that splits an input sequence into so-called tokens where the tokens can be a word, sentence, paragraph etc. Base upon the type of tokens we want, tokenization can be of various types, for instance"
Let's start by loading all necessary libraries:
"### Part 1. Toy dataset ""Will They? Won't They?"""
"![image.png](attachment:image.png)\n\n# Jane Street Market Prediction: A simple EDA\n\n> ""*Machine learning (ML) at Jane Street begins, unsurprisingly, with data. We collect and store around 2.3TB of market data every day. Hidden in those petabytes of data are the relationships and statistical regularities which inform the models inside our strategies. But it’s not just awesome models. ML work in a production environment like Jane Street’s involves many interconnected pieces.*"" -- [Jane Street Tech Blog ""*Real world machine learning*""](https://blog.janestreet.com/real-world-machine-learning-part-1/).\n\nThis notebook is a simple exploratory data analysis (EDA) of the files provided for the kaggle [Jane Street Market Prediction](https://www.kaggle.com/c/jane-street-market-prediction) competition. Here we shall...\n\n> ""**Explore the data:** *It’s hard to know what techniques to throw at a problem before we understand what the data looks like, and indeed figure out what data to use. Spending the time to visualize and understand the structure of the problem helps pick the right modeling tools for the job. Plus, pretty plots are catnip to traders and researchers!*""\n\n## Contents\n* [The train.csv file is big](#train_csv)\n* [resp](#resp)\n* [weight](#weight)\n* [Cumulative return](#return)\n* [Time](#time)\n* [The features](#features)\n* [The `features.csv` file](#features_file)\n* [Action](#action)\n* [The first day (""day 0"")](#day_0)\n* [Are there any missing values?](#missing_values)\n* [Is there any missing data: Days 2 and 294](#missing_data)\n* [DABL plots (targets: action and resp)](#DABL)\n* [Permutation Importance using the Random Forest](#permutation)\n* [Is there any correlation between day 100 and day 200?](#Pearson)\n* [The test data](#test_data)\n* [Evaluation](#evaluation)"
\n## The train.csv file is big\n\nThe train.csv is large: 5.77G. Let us see just how many rows it has:
We now have loaded `train.csv` in less than 17 seconds.\n\n\n## resp\n\nThere are a total of 500 days of data in `train.csv` (*i.e.* two years of trading data). Let us take a look at the cumulative values of `resp` over time
"as well as four [time horizons](https://www.investopedia.com/terms/t/timehorizon.asp)\n> ""*The longer the Time Horizon, the more aggressive, or riskier portfolio, an investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt.*"""
"We can see that `resp` (in blue) most closely follows time horizon 4 (`resp_4` is the uppermost curve, in purple). \n\nIn the notebook [""*Jane Street: time horizons and volatilities*""](https://www.kaggle.com/pcarta/jane-street-time-horizons-and-volatilities) written by [pcarta](pcarta), if I understand correctly, by using [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) it is calculated that if the time horizon $(T_j$) for `resp_1` (*i.e.* $T_1$) is 1, then \n* $T_j($ `resp_2` $) ~\approx 1.4 ~T_1$\n* $T_j($ `resp_3` $) ~\approx 3.9 ~T_1$ \n* $T_j($ `resp_4` $) ~\approx 11.1 ~T_1$\n\nwhere $T_1$ could correspond to 5 trading days.\n\nLet us now plot a histogram of all of the `resp` values (here only shown for values between -0.05 and 0.05)"
This distribution has very long tails
"Finally, let us fit a [Cauchy distribution](https://en.wikipedia.org/wiki/Cauchy_distribution) to this data"
"Note that a Cauchy distribution can be generated from the ratio of two independent normally distributed random variables with mean zero. The paper by [David E. Harris ""*The Distribution of Returns*""](https://www.scirp.org/pdf/JMF_2017083015172459.pdf) goes into detail regarding the use of a Cauchy distribution to model returns.\n\n\n## weight\n\n> *Each trade has an associated `weight` and `resp`, which together represents a return on the trade.\nTrades with `weight = 0` were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.*"
Let us take a look at a histogram of the non-zero weights
"There appear to be two peaks, one situated at `weight` $\approx$ 0.17, and a lower, broader peak at `weight` $\approx$ 0.34. Could this be indicative of two underlying distributions that we see here, superimposed on each other? Maybe one distribution of weights correspond to selling, and the other to buying?\n\nWe can plot the logarithm of the weights (*Credit*: [""*Target Engineering; CV; ⚡ Multi-Target*""](https://www.kaggle.com/marketneutral/target-engineering-cv-multi-target) by [marketneutral](https://www.kaggle.com/marketneutral))"
and we can now try to fit a pair of Gaussian functions to this distribution
"with limited success; the narrower left hand peak seems to be some other distribution. (Just in case, the $\mu$ of the small Gaussian is located at -1.32, and the large Gaussian at 0.4).\n\n## Cumulative return\n\nLet us take a look at the cumulative daily return over time, which is given by `weight` multiplied by the value of `resp`"
## Importing all necessary libraries\n
### Load the data
" 2.1 Box Plots\n\nBecause the foremost goal of modeling is to understand variation in the attribute, the first step should be to understand the distribution of the attribute. For a continuous attribute such as the Free Sulfur Dioxide attribute, it is important to understand if the attribute has a symmetric distribution, if the distribution has a decreasing frequency of larger observations (i.e., the distribution is skewed), if the distribution appears to be made up of two or more individual distributions (i.e., the distribution has multiple peaks or modes), or if there appears to be unusually low or high observations (i.e outliers).\n\n**Box Plot** help us visualize distribution of single attribute which further help us understanding dataset. \n\n![](https://miro.medium.com/max/1400/1*2c21SkzJMf3frPXPAR_gZA.png)\n\nMoving towards technical definition of Box Plots it is a method for graphically demonstrating the locality, spread and skewness groups of numerical data through their quartiles. In addition to the box on a box plot, there can be lines (which are called whiskers) extending from the box indicating variability outside the upper and lower quartiles, thus, the plot is also termed as the box-and-whisker plot and the box-and-whisker diagram. Outliers that differ significantly from the rest of the dataset may be plotted as individual points beyond the whiskers on the box-plot. \n\n\n**Median**\nThe median (middle quartile) marks the mid-point of the data and is shown by the line that divides the box into two parts. Half the scores are greater than or equal to this value and half are less.\n\n**Inter-quartile range(IQR)**\nThe middle “box” represents the middle 50% of scores for the group. The range of scores from lower to upper quartile is referred to as the inter-quartile range. The middle 50% of scores fall within the inter-quartile range.\n\n**Upper quartile**\nSeventy-five percent of the scores fall below the upper quartile.\n\n**Lower quartile**\nTwenty-five percent of scores fall below the lower quartile.\n\n**Whiskers**\nThe upper and lower whiskers represent scores outside the middle 50%. Whiskers often (but not always) stretch over a wider range of scores than the middle quartile groups."
If we try to interpret above box plot it has lots of outliers all values greater thar 42 are considered as outlier. In terms of skewness if we try to analyze it is right skewed or positive skewed means density of data is more near the origin.
Above Box plot again has lots of outliers as lots of values are greater than (Q3 + 1.5IQR) and some vales are less than (Q1 - 1.5IQR).
Lets look at the **drawbacks of box plot**
"A **drawback of the box plot** is that it is not effective at identifying distributions that\nhave multiple peaks or modes. As an example, consider the distribution of citric acid. Part (a) of the figure above is a histogram of the\ndata. Like box plots, histograms are simple to create, and these figures offer the ability to see additional distributional characteristics. In the citric acid distribution, there are more than one peak can be seen The box plot (b) is unable to capture this important nuance. To achieve a compact visualization of the distribution that retains histogram-like characteristics, Hintze and Nelson (1998) developed the violin plot (c)."
" 2.2 Violin Plots\n\nViolin plot is created by generating a density or distribution of the data and its mirror image. In the above figure we can see the violin plot, where we can now see the many distinct peaks in citric acid distribution. The lower quartile, median, and upper quartile can be added to a violin plot to also consider this information in the overall assessment of the distribution."
"We can read above violin plot like we read the box plots, the difference is just in violin plot we can also get density of attribute which help us understand more about the attribute. The\n\nFor more deeper understanding you can refer this blog post [Violin plots explained](https://towardsdatascience.com/violin-plots-explained-fb1d115e023d)"
" 2.3 Histograms\n\nA histogram is a graphical representation that organizes a group of data points into user-specified ranges. Similar in appearance to a bar graph, the histogram condenses a data series into an easily interpreted visual by taking many data points and grouping them into logical ranges or bins.\n\n\nHistograms are commonly used in statistics to demonstrate how many of a certain type of variable occurs within a specific range. For example, a census focused on the demography of a country may use a histogram to show how many people are between the ages of 0 - 10, 11 - 20, 21 - 30, 31 - 40, 41 - 50, etc. This histogram would look similar to the example below."
"We can consider how to define the y-axis. The most basic label is to use the frequency of occurrences observed in the data, but one could also use percentage of total or density instead."
" 3.1 1:1 Transformations\n\nThere are a variety of modifications that can be made to an individual attribute that might improve its utility in a model. The first type of transformations to a single attribute discussed here are those that change the scale of the data. A good example is the transformation described in plot below. In that case, the attribute residual sugar had very skewed distributions and it was shown that using the inverse of the log values improves distribution."
" 3.1.1 Box-Cox and Yeo-Johnson Transformation\n\n\nA Box-Cox transformation (Box and Cox, 1964) was used to estimate this transformation. The Box-Cox procedure, originally intended as a transformation of a model’s outcome, uses maximum likelihood estimation to estimate a transformation\nparameter $\lambda$ in the equation\n\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRkHjrl213TAXMJVBHK3yVaRLxxi-_dUqPJxg&usqp=CAU)\n\nIn this procedure, $\lambda$ is estimated from the data. Because the parameter of interest is in the exponent, this type of transformation is called a **power transformation**. Some values of $\lambda$ map to common transformations, such as $\lambda$ = 1 (no transformation), $\lambda$ = 0 (log), $\lambda$ = 0.5 (square root), and $\lambda$ = −1 (inverse). As you can see, the Box-Cox transformation is quite flexible in its ability to address many different data distributions. \n\n**It is important to note that the Box-Cox procedure can only be applied to data that is strictly positive.**\n\nTo address this problem, **Yeo and Johnson (2000)** devised an analogous procedure that can be used on any numeric data.\n\nAlso, note that both transformations are unsupervised since, in this application, the outcome is not used in the computations. While the transformation might improve the attribute distribution, it has no guarantee of improving the model. However, there are a variety of parametric models that utilize polynomial calculations on the attribute data, such as most linear models, neural networks, and support vector machines. In these situations, a skewed attribute distribution can have a harmful effect on these models since the tails of the distribution can dominate the underlying calculations.\n"
" 3.1.2 Logit Transformation\n\nAnother important transformation to an individual variable is for a variable that has values bounded between zero and one, such as proportions. The problem with modeling this type of outcome is that model predictions might may not be guaranteed to be within the same boundaries. For data between zero and one, the logit transformation could be used. If p is the variable, the logit transformations is\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT82WhzCNQVRXt1NE-rnzE_6sqsQ7yZxK-tXg&usqp=CAU)\n\nThis transformation changes the scale from values between zero and one to values between negative and positive infinity. On the extremes, when the data are absolute zero or one, a small constant can be added or subtracted to avoid division by zero. Once model predictions are created, the inverse logit transformation can be used to place the values back on their original scale. An alternative to the logit transformation is the arcsine transformation. This is primarily used on the square root of the proportions (e.g., y = arcsine(sqrt(p))."
" 3.1.3 Centring\n\n\nAnother common technique for modifying the scale of a predictor is to standardize its value in order to have specific properties. Centering a predictor is a common technique. The predictor’s training set average is subtracted from the predictor’s individual values. When this is applied separately to each variable, the collection of variables would have a common mean value (i.e., zero). Similarly, scaling is the process of dividing a variable by the corresponding training set’s standard deviation. This ensures that that variables have a standard deviation of one. \n\n![](https://cdn-images-1.medium.com/max/1600/0*aR0ivCUZJjM9DFDw.png)\n\nAlternatively, range scaling uses the training set minimum and maximum values to translate the data to be within an arbitrary range (usually zero and one). Again, it is emphasized that the statistics required for the transformation (e.g., the mean) are estimated from the training set and are applied to all data sets (e.g., the test set or new samples). These transformations are mostly innocuous and are typically needed when the model requires the predictors to be in common units. For example, when the distance or dot products between predictors are used (such as K-nearest neighbors or support vector machines) a standardization procedure is essential."
 3.2 1:Many Transformations\n\n1:Many transformations can be made on a single numeric predictor to expand it to many predictors. These one-to-many transformations of the data can be used to improve model performance. Will discuss following 1:Many transformation methods:-\n\n- Nonlinear Features via Basis Expansions and Splines\n- Discretize the attributes
\n## **1. Library and data loading** ##
\n## **2. Data cleaning** ##
\n## **4. Covariance Matrix. Variability comparison between categories of variables** 
\n## **5. Some charts to see data relationship** 
Distribiution and density by Age
Separate by treatment
How many people has been treated?
Draw a nested barplot to show probabilities for class and sex
Barplot to show probabilities for family history
Barplot to show probabilities for care options
Barplot to show probabilities for benefits
Barplot to show probabilities for work interfere
"## Matrix Formulation\n\nIn general we can write above vector as $$ \mathbf{ x_{ij}} = \left( \begin{smallmatrix} \mathbf{x_{i1}} & \mathbf{x_{i2}} &.&.&.& \mathbf{x_{in}} \end{smallmatrix} \right)$$\n\nNow we combine all aviable individual vector into single input matrix of size $(m,n)$ and denoted it by $\mathbf{X}$ input matrix, which consist of all training exaples,\n$$\mathbf{X} = \left( \begin{smallmatrix} x_{11} & x_{12} &.&.&.&.& x_{1n}\\\n                                x_{21} & x_{22} &.&.&.&.& x_{2n}\\\n                                x_{31} & x_{32} &.&.&.&.& x_{3n}\\\n                                .&.&.&. &.&.&.& \\\n                                .&.&.&. &.&.&.& \\\n                                x_{m1} & x_{m2} &.&.&.&.&. x_{mn}\\\n                                \end{smallmatrix} \right)_{(m,n)}$$\n\nWe represent parameter of function and dependent variable in vactor form as  \n$$\theta = \left (\begin{matrix} \theta_0 \\ \theta_1 \\ .\\.\\ \theta_j\\.\\.\\ \theta_n \end {matrix}\right)_{(n+1,1)} \n\mathbf{ y } = \left (\begin{matrix} y_1\\ y_2\\. \\. \\ y_i \\. \\. \\ y_m \end{matrix} \right)_{(m,1)}$$\n\nSo we represent hypothesis function in vectorize form $$\mathbf{ h_\theta{(x)} = X\theta}$$.\n\n"
In above plot we fit regression line into the variables.
###  Check for missing value
There is no missing value in the data sex
### Plots
Thier no correlation among valiables.
"If we look at the left plot the charges varies from 1120 to 63500, the plot is right skewed. In right plot we will apply natural log, then plot approximately tends to normal. for further analysis we will apply log on target variable charges. "
"From left plot the insurance charge for male and female is approximatley in same range,it is average around 5000 bucks. In right plot the insurance charge for smokers is much wide range compare to non smokers, the average charges for non smoker is approximately 5000 bucks. For smoker the minimum insurance charge is itself 5000 bucks."
\n\n3.2 CREATE TF.DATA.DATASET\n\n---\n\n**INPUT**\n* Raw Image (256x256x3)\n\n**OUTPUT/TARGET**\n* Segmented Image (256x256x[3|1])\n\n---\n
\n\n\n\n\n\n\n    4  MODEL TRAINING    ⤒\n\n\n---
"##  Classification Metrices \n\n- Dataset: Pima Indians onset of diabetes dataset.\n- Evaluation Algorithm: Logistic Regression, SGDClassifier, RandomForestClassifier."
"* Quick Note : SkLearn's ""predict_log_proba"" gives the logarithm of the probabilities, this is often handier as probabilities can become very, very small."
"###  ROC Curve \n\nROC can be broken down into sensitivity and specificity. Choosing the best model is sort of a balance between predicting 1's accurately or 0's accurately. In other words sensitivity and specificity.\n\n- True Positive Rate (Sensitivity/ Recall) : True Positive Rate is defined as TP/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.\n\n- False Positive Rate (Specificity) : False Positive Rate is defined as FP / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.\n\nTrue Positive Rate and False Positive Rate both have values in the range [0, 1]. TPR and FPR both are computed at threshold values such as (0.00, 0.02, 0.04, …., 1.00) and a graph is drawn."
"#### Interpreting ROC Plot:\n\nInterpreting the ROC plot is very different from a regular line plot. Because, though there is an X and a Y-axis, we don't read it as: for an X value of 0.25, the Y value is .9.\n\nInstead, what we have here is a line that traces the probability cutoff from 1 at the bottom-left to 0 in the top right.\n\nThis is a way of analyzing how the sensitivity and specificity perform for the full range of probability cutoffs, that is from 0 to 1.\n\nIdeally, if we have a perfect model, all the events will have a probability score of 1 and all non-events will have a score of 0. For such a model, the area under the ROC will be a perfect 1.\n\nSo, if we trace the curve from bottom left, the value of probability cutoff decreases from 1 towards 0. If we have a good model, more of the real events should be predicted as events, resulting in high sensitivity and low FPR. In that case, the curve will rise steeply covering a large area before reaching the top-right.\n\nTherefore, the larger the area under the ROC curve, the better is the model.\n\nThe ROC curve is the only metric that measures how well the model does for different values of prediction probability cutoffs.\n\n"
"Data Scientists & Analysts: What's the difference?\n\nIntroduction \n\nThe WEF 2020 Future of Jobs Report lists Data Analysts and Data Scientists as the highest emerging job roles of the decade. The report, like many others, groups the professions of data scientists and analysts together, regarding them to be one and the same. This is part of the common trend that people fail to understand how the two professions differ, often them lumping both these data professions together for all intents and purposes.\n\nA simliar situation is how the terms Artificial Intelligence(AI), deep learning, and machine learning are used interchangeably in the news and spurious marketing campaigns. A similar situation is encountered when trying to understand the difference between data scientists and data analysts. "
"Even a simple search for “data scientist vs data analyst” shows no shortage of articles which delve into this common misunderstanding. Clearly this is a problem that a lot of people seem to have...\n\n    \n\n    \n    \nScouring through these 50 odd search results you’ll find multiple cases where the definitions of the two terms overlap, offering little to no clarity at times. In worse cases the two are even used as synonyms for one another. Clearly even turning to Google to give us a cut and dry answer is problematic!\n\nOne of the major reasons for this is that over the past decade terms like data analysis, have **attained the status of buzzwords**. With more businesses and online content creators getting involved in the hype around data science and analytics, these buzzwords are thrown around with little care for correct terminology.\n    \n Source - Google Trends data for data science, machine learning and data analysis. "
"And indeed, searches for data science related terms have taken off over the past decade. Some of the popular related search terms include.\n\n Source - Google Trends data for data science and data analysis - ""Related queries"" section \n\n\n \n    Data Scientistrelated queries     Data Analystrelated queries  \n  1  data scientist salary   1  big data analytics  \n  2  data science jobs   2  data analytics certificate  \n  3  machine learning   3  coursera  \n  4  data science online   4  data analytics meaning  \n  5  master data science   5  ms data analytics  \n  6  data science courses   6  how to become data analyst  \n  7  data analyst   7  data analytics career  \n  8  big data   8  data analysis excel 2013  \n  9  data science salary   9  mba in data analytics  \n  10  r data science   10  udacity  \n\n"
"With degrees and online courses on both topics being incredibly popular at the moment, not having a clear understanding of what each profession entails causes further issues down the line. Students, especially those planning on pursuing them for graduate studies, should understand **what skills each field will teach them** and the **job opportunities** that will be opened up to them as a result of their chosen path.\n\nWhen looking at Google trend data, we see how interest in these fields of learning has increased dramatically over the years.\n\n Source - Google Trends data for data science, machine learning and data analysis courses. "
"The Basics\n\nData Analysts\n\nSo what do data analysts do? Well, in simple terms they examine large datasets, generate insights and present their findings to help organizations make better decisions. It is their job to discover, interpret and communicate meaningful patterns and trends in the data to a non-technical audience. \n\nHowever the job isn’t as simple as playing around with perfect data to answer business questions. To get to this stage analysts work closely with teams to manage collection and storage of data, cleaning the obtained data, at times defining processes to automate these tasks.\n\n Source: Kaggle ML & DS Survey 2021 - Q24. Select any activities that make up an important part of your role at work. "
"Data Scientists\nData science has had various definitions thrown around over the years, but in essence it describes a field which uses a combination of mathematics, statistics and machine learning to clean, process and interpret data to extract insights from it. They design and construct new processes for data modelling and production using prototypes, algorithms, predictive models and custom analysis.\n\n Source: Kaggle ML & DS Survey 2021 - Q24. Select any activities that make up an important part of your role at work. "
"I understand that reading the above might leave many feeling like they still can’t see a clear distinction between the two professions - and the truth is that the lines are a bit blurred when it comes to data science and data analysis. Both the roles perform varying degrees of data collection, cleaning, and analysis to gain actionable insights for data-driven decision making - **leaving room for a lot of overlap**.\n\n Source: Kaggle ML & DS Survey 2021 - Q24. Select any activities that make up an important part of your role at work. "
"The main takeaway we can gain from the survey respondents is that data scientists have a greater focus on machine learning related tasks, whereas analysts have their main role as deriving insights from data and dabble in the machine learning aspects of the job."
"It is quite clear that across almost all nations, analysts are paid less than data scientists. It might be possible that a reason for this is that data scientists possess skills in machine learning - a field that is greatly sought after in the job market. We have also seen earlier how lower percentages of data analysts reported that they performed any machine learning related roles. \n\nWe look at what data analysts are paid depending on whether or not they possess each of the previous roles.\n\n Source: Kaggle ML & DS Survey 2021 - Q25. What is your current yearly compensation?  note: Only data analysts in United States of America considered. \n\n Salary differences in Analysts based on roles \n \n  Role   with role   without   difference      \n  Analyse data for business decisions  21844  4610  17234  \n  Build and manage data infrastructure  20114  16433  3681  \n  Build ML prototypes  29996  14396  15600  \n  Build ML services for workflows  24589  16922  7667  \n  Improve existing ML models  33490  15774  17716  \n  Research in ML  16406  18056  -1650  \n  None of these roles  7277  18559  -11282  \n  Other  12665  18114  -5449  \n\n\n\nWith data scientists earn on average 50,005 USD in similar conditions, we see how none of these roles by themselves help data analysts reach this level of compensation.   \nData analysts that perform machine learning related tasks, namely building machine learning prototypes and improving ML models seem to be paid the highest amounts on average. In both cases, simply having these skills almost doubled their average earnings.\n\n\n Source: Kaggle ML & DS Survey 2021 - Q25. What is your current yearly compensation?  note: Only data analysts in United States of America considered. \n\n Salary differences in Data Scientists based on roles \n \n      Role   with role   without   difference  \n      Build ML prototypes   170374   109055   61319  \n      Improve existing ML models   167579   132222  35356  \n\n\n\nEven for data scientists these two roles seem to make a world of difference in terms of their earnings.\n\n\nDo data analysts need to know how to code?\n\nThis is a question that a lot of individuals entering the field ask, especially those switching careers from non-technical backgrounds. Fortunately for them, a simple google search will tell us that when it comes to data analysis, advanced coding skills aren't always necessary. Basic coding ability to wrangle data and having an understanding of analytics tools like Tableau, Power BI, etc. is often more than sufficient for most data analysis roles.\n\nHowever when it comes to data science, the public opinion seems to agree that having a good command of coding is essential to make it in this field Whether its basic data preparation, analysis, modeling or writing production code, data scientists are bound to have to write code for most of their daily tasks. I will admit that it is rather odd generalisation that *one field doesn't require much coding while in the other its essential*.\n\n Source: Kaggle ML & DS Survey 2021 - Q6. For how many years have you been writing code and/or programming? "
"From the survey data we see that data scientists in general have more experience writing code. That said, the difference between the two isn't as drastic as the many articles would lead you to believe. For example, more data analysts have experience with SQL than data scientists.\n\nWith the majority of data professionals having less than three years of coding experience, we also see how they prefer Python and SQL when starting out on their learning journey.\n\nThe tasks that a data professional performs on a day to day basis may not always require that they have to write code. Looking at coding experience based on the role performed reveals the following insights.\n\n Source: Kaggle ML & DS Survey 2021\n    - Q6. For how many years have you been writing code and/or programming? \n    - Q24. Select any activities that make up an important part of your role at work.\n"
"A few key takeways from the above are:\n* Roles prominent in data analysis ('analysing data for business decisions' and 'build data infrastructure') show higher percentages of individuals with coding experience of 3 years or less.\n* Machine learning related roles have greater percentages at the higher experience ranges.\n* Data Analysts/Scientists that perform none of the mentioned roles also have lower coding experience in comparison.\n\nThis all falls in line with how the data analysis fields deal with coding, to many it is simply a means to an end. They need to be able to be able to surf through vast datasets at blazing speeds, analyse and recognise patterns and trends. This quick generation of insights may often result in sloppy or ad-hoc coding practices, which is rarely expected to be pushed to the production code.\n\nDo data analysts require Machine Learning?\nIf we were to google this question, we see a similar situation as we did in the previous section - numerous articles tell us that data analysts are not expected to have hands-on machine learning experience or build statistical models, which fall under the responsibilities of data scientists.\n\nSource: Kaggle ML & DS Survey 2021 - Q15. For how many years have you used machine learning methods? "
"In general, machine learning tasks are often out of the scope of a data analyst’s work, resulting in them having far less hands-on experience with machine learning models as compared to their data science counterparts. However when we zoom out and look at machine learning experience in all data fields, it is surprising how low analysts rank overall.\n\nSource: Kaggle ML & DS Survey 2021 - Q15. For how many years have you used machine learning methods? "
"How early in their coding journey did they start learning ML?\nFollowing the same line of thought, it might be interesting to look at how machine learning fits into the coding journey of professionals in different fields. This gives us an idea of those who might have had prior experience with coding before delving into machine learning and those whose start in ML was hand-in-hand with their learning to code.\n\nIn the following chart we see how:\n* A lot of data scientists started learning machine learning roughly around the same time that they started to learn to code(represented by the cells on the diagonal of the heatmap, i.e. their ML experience lines up with their coding experience).\n* Data analysts fall in the category of fields whose coding skills far exceed the machine learning abilities, especially in the later stages of their career.\n\nSource: Kaggle ML & DS Survey 2021\n    - Q6. For how many years have you been writing code and/or programming?  \n    - Q15. For how many years have you used machine learning methods?  \n"
"Additionally, you can use the following piece to see how this plays out for the other data-related professions"
Education in Data roles\nFrom previous sections we see how data scientists require a solid understanding of both coding practises as well as a solid grasp on machine learning concepts. It might also make sense that they pursue higher education in order to get a better understanding of the theoretical aspects of the field.\n\n\nSource: Kaggle ML & DS Survey 2021 - Q4. What is the highest level of formal education that you have attained or plan to attain within the next 2 years? 
"With more than 60% of candidates having a masters degree or higher level of education, it becomes apparent that having at least a master's degree is a common route as a data scientist. Data analysts show a higher percentage of individuals that entered the field with a bachelor’s degree at the expense of far fewer pursuing doctorate-level programs.\n\nA [deep-dive into requirements on job postings for Facebook/Meta](https://www.reproducible-hq.com/notebook/land-that-data-job-at-meta.html) highlights another point of difference between data analysts and scientists - for data scientists having a PHD gives is not only uncommon, but is often preferred for data scientist positions.\n\nSource: Job posting data collected from Facebook Careers "
\n\n4.0 FUNCTIONS FROM OTHER KAGGLERS!\n\n---\n\n\n\nI want to use the incredible and useful functions built by other Kagglers. Resources are listed below with proper attribution and code is in the cell below.\n\n\n\nAnimation Function(s)\n    Content Description: Visualization of the coordinate data given to us with stabilization to remove jitter (in recent versions)\n    Notebook Link: Animated Data Visualization\n    Author (Profile Link): danielpeshkov\n
\n\n4.1 HELPER FUNCTIONS\n\n---\n\n\n\nDon't worry about these for now. I've hidden them in the notebook viewer to not add complexity. I will explain any functions that are important in-line later.
\n\n5.2 EXAMINE THE `PARTICIPANT_ID` COLUMN\n\n---\n\n\n    Number Participants: 21\n    Average Number of Rows Per Participant: 4498.91\n    Standard Deviation in Counts Per Participant: 490.77\n    Minimum Number of Examples For One Participant: 3338\n    Maximum Number of Examples For One Participant: 4968\n\n\nIt's also worth pointing out that the folders in the train_landmark_files directory are named based on the participant_id for whom the respective isolated sign event parquet files are for.
\n\n5.3 EXAMINE THE `SEQUENCE_ID` COLUMN\n\n---\n\nThere's not much here. This is a unique value assigned to every isolated sequence/event. One sequence corresponds to a single isolated sign that we have to detect and label.\n* Every value is unique for every row\n
\n\n5.4 EXAMINE THE `SIGN` COLUMN\n\n---\n\nThis is the label for each respective event/sequence.\n    \n\n    Number Of Unique Signs: 250\n    Average Number of Rows Per Sign: 377.908\n    Standard Deviation in Counts Per Sign: 19.356537293638034\n    Minimum Number of Examples For One Sign: 299\n    Maximum Number of Examples For One Sign: 415\n\n\nIt's a pretty balanced dataset!
\n\n5.5 INCLUDING SEQUENCE METADATA IN TRAIN DATAFRAME\n\n---\n\nWe are going to identify certain pieces of relevant metadata that we want to scrape from the parquet files and include in our main dataframe\n\n\n\nWe will retrieve the following for each sequence\n\n    start_frame\n    end_frame\n    total_frames\n    face_count\n    pose_count\n    left_hand_count\n    right_hand_count\n    x_min\n    x_max\n    y_min\n    y_max\n    z_min\n    z_max\n\n\n\n\nWhat can we observe about the sequences with this new metadata:\n\n    There are always the same keypoints present\n    For each part of the body ('type') we have the following keypoint counts:\n        Right Hand --> 21 Keypoints\n        Left Hand  --> 21 Keypoints\n        Pose --> 33 Keypoints\n        Face --> 468 Keypoints\n    \n    Sequences can start almost anywhere from frame 0 to frame 484 but the mean is ~30\n    Sequences can end almost anywhere from frame 1 to frame 499 but the mean is ~67\n    Sequences can be different lengths (and are inclusive of their bounds) from a length of 2 to a length of 500. Sequences have a mean length of ~37.5\n
"In order to make some analysis, we need to set our environment up. To do this, I firstly imported some modules and read the data. The below output is the head of the data but if you want to see more details, you might try removing ***#*** signs in front of the ***df.describe()*** and ***df.info()***. "
#  k-Nearest Neighbors (k-NN)\n#### [Return Contents](#0)\n
### FARE & SURVIVAL
### 'SibSp' & 'Parch' PLOTS W.R.T 'SURVIVED' 
- FILL THE MISSING VALUES IN THE DATA 
This is likely a very valuable feature for our model. If we know how many people are in a match we can normalize other features and get stronger predictions on individual players.
There are a few matches with fewer than 75 players that are not displayed here. As you can see most of the matches are nearly packed a have nearly 100 players. It is nevertheless interesting to take these features into our analysis.
Let's look at the point of visualization
As you see the sale price value is right skewed. We need to make this normal distributed.
we can see the most corelated parameters in numerical values above plotting. And we can pick these as features for our macine learning model.
### Import libraries
## Sales data \n\n\n### Sample sales data
"These are sales data from randomly selected stores in the dataset. As expected, the sales data is very erratic, owing to the fact that so many factors affect the sales on a given day. On certain days, the sales quantity is zero, which indicates that a certain product may not be available on that day (as noted by Rob in his kernel)."
### Sample sales snippets
"In the above plots, I simply zoom in to sample snippets in the sales data. As stated earlier, we can clearly see that the sales data is very erratic and volatile. Sometimes, the sales are zero for a few days in a row, and at other times, it remains at its peak value for a few days. Therefore, we need some sort of ""denoising"" techniques to find the underlying trends in the sales data and make forecasts."
The below diagram illustrates these graphs side-by-side. Red graphs represent original sales and green graphs represent denoised sales.
"### Average smoothing\n\nAverage smooting is a relatively simple way to denoise time series data. In this method, we take a ""window"" with a fixed size (like 10). We first place the window at the beginning of the time series (first ten elements) and calculate the mean of that section. We now move the window across the time series in the forward direction by a particular ""stride"", calculate the mean of the new window and repeat the process, until we reach the end of the time series. All the mean values we calculated are then concatenated into a new time series, which forms the denoised sales data."
"## Stores and states \n\nNow, I will look at the sales data across different stores and states in order to gain some useful insights."
"# TensorFlow deep NN\n#### A high-level tutorial into Deep Learning using MNIST data and TensorFlow library.\nby [@kakauandme](https://twitter.com/KaKaUandME) and [@thekoshkina](https://twitter.com/thekoshkina)\n\nAccuracy: 0.99\n\n**Prerequisites:** fundamental coding skills, a bit of linear algebra, especially matrix operations and perhaps understanding how images are stored in computer memory. To start with machine learning, we suggest [coursera course](https://www.coursera.org/learn/machine-learning) by Andrew Ng.\n\n\nNote: \n\n*Feel free to fork and adjust* CONSTANTS *to tweak network behaviour and explore how it changes algorithm performance and accuracy. Besides **TensorFlow graph** section can also be modified for learning purposes.*\n\n*It is highly recommended printing every variable that isn’t 100% clear for you. Also, [tensorboard](https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html) can be used on a local environment for visualisation and debugging.*\n## Libraries and settings"
"## Data preparation\nTo start, we read provided data. The *train.csv* file contains 42000 rows and 785 columns. Each row represents an image of a handwritten digit and a label with the value of this digit."
"To output one of the images, we reshape this long string of pixels into a 2-dimensional array, which is basically a grayscale image."
"The corresponding labels are numbers between 0 and 9, describing which digit a given image is of."
## Let's start a mini-analysis of the data by constructing a general distribution of features by their number.
"Conclusions from the presented graphs:\n* Variable study Gender is balanced, applied to the same number of men and women to obtain a relevant result.\n* most of the subjects correspond to the age of 11–25 years, this is the age when we can confidently talk about sustainable adaptation to learning.\n* values ​​are contributed by the level of education indicates that most of the subjects have only a school education. These data correlate with a certain schedule, where most of the subjects are in the age range from 7 to 20 years.\n*existing education services are provided by non-private institutions. This is due to the development of the education market.\n* it should also be noted that most of the commercials are sold on the phone through the 4G network, which shows that people get education in any place convenient for them.\n* It should be noted that some of the online courses are not assimilated by students."
## Let's study the distribution of the number of students depending on the level of their adaptation.
"Conclusions from the obtained distributions:\n* Men are easier to adapt to new knowledge, the level of poor adaptation between men and women is approximately the same.\n* the best adaptation is shown at the age of 21-25, and 11-15, the worst - after 26 years and in the interval from 16 to 20 years. Worse adaptation to new knowledge can be explained by social and physiological factors.\n* It should also be noted that the best digestibility of the material is observed in the middle class. We can talk about this phenomenon for a very long time :)\n* high adaptation to the material is also noted among urban residents, this is due to social and economic factors.\n* The level of adaptation to new knowledge also depends on the quality of the Internet.\n"
# Let's check target distrubution\n\n# Check for Class Imbalance
# Check Text Content
# What about test data?
## I think there is not a big difference in location between train data and test data
"### * From above two plots it is seen that 'Global_intensity' and 'Global_active_power' correlated. But 'Voltage', 'Global_active_power' are less correlated. This is important observation for machine learning purpose. "
# Correlations among features
## Imports
## Functions
"ohh,as expected ! There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)"
### Number of characters in tweets
### Number of words in a tweet
###  Average word length in a tweet
"Now,we will analyze tweets with class 1."
"In both of them,""the"" dominates which is followed by ""a"" in class 0 and ""in"" in class 1."
First let's check tweets indicating real disaster.
"Now,we will move on to class 0."
### Common words ?
"## If you like the content of this notebook, please consider upvoting it.\n\nNot only it will show to visitors that this notebook have valuable information, but it will also encourage me to produce more quality notebooks. :)"
"# Domain Specific Language (DSL)\n\nWe will build a domain specific language specialized on processing list of images. To allow easy chaining of keyword from this language together, each *function* provided by this language will be take one or more images and transform it to none, one or more. The final result of our program will then be a list of images.\n\nThe DSL is so constituted by a collection of functions of type `np.array -> [np.array]` and `[np.array] -> [np.array]`.\n\nThe first kind of function take an image, and produce a list of images (for example, the image split by different colors). The second type of function take a list of images and produce a new list (for exemple, intersect).\n[](http://)"
"Welcome!\n\nThis will be an extensive visualization, analysis, and prediction notebook. \n\nHere are some of my other notebooks:\n\n**Gold price prediction using Prophet**\n\nhttps://www.kaggle.com/joshuaswords/eda-gold-price-prediction-prophet\n\n\n**Computer Vision with FastAI - Pneumonia prediction**\n\nhttps://www.kaggle.com/joshuaswords/computer-vision-pneumonia-prediction-fastai\n\n\n**Stroke Prediction with SMOTE and LIME explainer**\n\nhttps://www.kaggle.com/joshuaswords/predicting-a-stroke-95-acc-with-lime-explainer\n\n\n**2021 World Happiness Index EDA**\n\nhttps://www.kaggle.com/joshuaswords/awesome-eda-2021-happiness-population\n\n\nLet's get to it...\n\n\n\n# Context\n\n**Problem Statement**: Predict the probability of a candidate looking for a new job.\n\nEssentially, I'll be attempting to predict **who is job-seeking and who is not.** \n\n**Note** - The main focus of this notebook is on **data visualization** so the predictive models can almost certainly be improved upon with some basic tweaks. \n\nRemember though, in industry it is difficult to deploy models that take hours to retrain daily. A good model is often better - and more practical - than a perfect one. Although this should be assessed on a case-by-case basis of course.\n\n"
"# Data Visualization \n\n**This will be an EXPLORATORY visualization, as opposed to EXPLANATORY.**\n\n\nI will also make use of **GridSpec** as I want to practice this technique. \n\n\nLet's see if we can understand why people might look for a new job...\n\n\nThe colour palette I will use is below:"
"# How many job-seekers are there?\n\nFirst of all, I want to see how many job-seekers there are in our training set. \n\nDo we have a balanced dateset? Or Imbalanced? The answer to these question may influence our models later on."
"We have an imbalanced dataset - that is,cmany **more non job-seekers than job-seekers**.\n\nThis is a problem that we can address later. For now, let's continue **exploring the data**"
Here I'll show an example of how you can use GridSpec.\n\nGridSpec enables you to plot multiple plots and is highly customisable - a skill worth picking up!\n\nThis is really valuable because you can convey a lot of information in a small amount of space. \n\nI'll compare the Train & Test sets here...
"# The Train & Tests sets are similar - that's good news\n\nIf the training set has wildy different characteristics to our test set then we really are in for a difficult time.\n\nWe'd need to ask if the training population can really help us predict the target. \n\nIn this case though, we're fine. \n\n# Now let's focus on the Training set and explore the data..."
"You'll note that I often incoroprate text in to my visuals. I'll often an annotations to the plots themselves, for example at the 'mean', or at peaks in the data etc. \n\nIn this case, I've included an explanation of what we're seeing and what it might mean. This helps your audience to understand your data, but it also helps them to get thinking in a way that is in line with the story you are trying to craft."
# Let's now explore other factors like company size & employee experience...\n\nDo more experienced employees seek new challenges?\n\nDo employees at larger companies feel less valued?\n\nDo employees at smaller companies crave new opportunities?\n\nBoth seem plausible. These are the questions that a good EDA can answer!\n
"It appears the as employee experience increases, they tend to work for larger companies.\n\nWhy might this be? Perhaps larger companies pay better, or perhaps more job security.\n\n# Employee Experience & Company Size\n\nIs there a noticeable difference between job-seekers and non-job-seekers?"
"# Education\n\nWe've explored some interesting feautures of our data, including employee experience and company size. \n\nLet's throw education in to the mix. \n\nDo job-seekers have a higher education level? Are they less educated? Or is there no difference at all?"
"These results are interesting. It does appear that job-seekers are less educated - I would suggest that this is becuase they are younger and still on their education journey, and are also seeking new challenges.\n\nWhat do you think?"
  \nImport Libraries\n
  \nImport Dataset 📃\n
* In Train data we have *42000* instances and *785* features.\n* In Test data we have *28000* instances and *784* features.\n\nNow we see that we have 785 features in train data which means that there are 784 pixel valuess in the image and one column is about label that which digit it is.\n
### **Checking the Values in Image Form.**
### **Checking the Images One by One:**
\n\n\nQuick Navigation\n\n* [Overview](#1)\n* [Annotations](#2)\n    \n    \n    \n* [ETT - Abnormal](#4)\n* [ETT - Borderline](#5)\n* [ETT - Normal](#6)\n* [NGT - Abnormal](#7)\n* [NGT - Borderline](#8)\n* [NGT - Incompletely Imaged](#9)\n* [NGT - Normal](#10)\n* [CVC - Abnormal](#11)\n* [CVC - Borderline](#12)\n* [CVC - Normal](#13)\n* [Swan Ganz Catheter Present](#14)\n    \n\n* [Venn Diagrams](#50)\n    \n    \n* [Submission](#100)
\nOverview
# 1. Importing necessary modules
# 2. Importing Dataframes
# 3. Target Value Distribution
*  We can observe that about 43% of tweets in the dataframe is about real disaster.
## 4.1 Number of tweets according to location(top 20)
"*  Most of the tweets are from USA,London,Canada."
## 4.2 Number of tweets according to location per class (0 or1)
* The graph says it all!!
# **Categorical Features Exploration**
`Year_Factor:` is the year in which the weather and energy usage factors were observed . \n\n📌 There are more number of observations for Year6 and Year5 in comparison with other years
`facility_type:` is the building usage type .\n\n📌 There are more number of observations for 'Multifamily Uncategorized'
# ** Numerical Features VS Target **
### 🎯floor_area vs site_eui
### 🎯Year_Factor vs site_eui
# **Feature Correlation**
# **Log Plots to W&B environment**
"## Plotting 4D probabilistic atlas maps...\n\nProbabilistic atlasing is a research strategy whose goal is to generate anatomical templates that retain quantitative information on inter-subject variations in brain architecture (Mazziotta et al., 1995). A digital probabilistic atlas of the human brain, incorporating precise statistical information on positional variability of important functional and anatomic interfaces, may rectify many current atlasing problems, since it specifically stores information on the population variability.\n\nFor further reading you may visit.[click here](http://users.loni.usc.edu/~thompson/prob_atlas.html)"
"## Plotting a statistical map...\n\nStatistical parametric mapping or SPM is a statistical technique for examining differences in brain activity recorded during functional neuroimaging experiments.The measurement technique depends on the imaging technology (e.g., fMRI and PET). The scanner produces a 'map' of the area that is represented as voxels. Each voxel represents the activity of a specific volume in three-dimensional space. The exact size of a voxel varies depending on the technology. fMRI voxels typically represent a volume of 27 mm3 (a cube with 3mm length sides).\n\nParametric statistical models are assumed at each voxel, using the general linear model to describe the data variability in terms of experimental and confounding effects, with residual variability. Hypotheses expressed in terms of the model parameters are assessed at each voxel with univariate statistics.\n\nAnalyses may examine differences over time (i.e. correlations between a task variable and brain activity in a certain area) using linear convolution models of how the measured signal is caused by underlying changes in neural activity.\n\nBecause many statistical tests are conducted, adjustments have to be made to control for type I errors (false positives) potentially caused by the comparison of levels of activity over many voxels. A type I error would result in falsely assessing background brain activity as related to the task. Adjustments are made based on the number of resels in the image and the theory of continuous random fields in order to set a new criterion for statistical significance that adjusts for the problem of multiple comparisons."
# Get our environment set up\n\nThe first thing we'll need to do is load in the libraries we'll be using. 
"# Scaling vs. Normalization: What's the difference?\n\nOne of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that:\n- in **scaling**, you're changing the *range* of your data, while \n- in **normalization**, you're changing the *shape of the distribution* of your data. \n\nLet's talk a little more in-depth about each of these options. \n\n# Scaling\n\nThis means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1.  You want to scale data when you're using methods based on measures of how far apart data points are, like [support vector machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) or [k-nearest neighbors (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). With these algorithms, a change of ""1"" in any numeric feature is given the same importance. \n\nFor example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices, methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n\nBy scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in [**the following exercise**](https://www.kaggle.com/kernels/fork/10824404)!)"
"Notice that the *shape* of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n\n# Normalization\n\nScaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n\n> **[Normal distribution:](https://en.wikipedia.org/wiki/Normal_distribution)** Also known as the ""bell curve"", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n\nIn general, you'll normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with ""Gaussian"" in the name probably assumes normality.)\n\nThe method we're using to normalize here is called the [Box-Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Let's take a quick peek at what normalizing some data looks like:"
"Notice that the *shape* of our data has changed. Before normalizing it was almost L-shaped. But after normalizing it looks more like the outline of a bell (hence ""bell curve""). \n\n# Your turn\n\nIt's time to [**apply what you just learned**](https://www.kaggle.com/kernels/fork/10824404) a dataset of Kickstarter projects."
"![](https://www.wsp.com/-/media/Hubs/Global/Congestion-Management/bnr-congestion.jpg)\n\n# Goal\n\nForecast twelve-hours of traffic flow in a U.S. metropolis. The time series in this dataset are labelled with both location coordinates and a direction of travel -- a combination of features that will test your skill at spatio-temporal forecasting within a highly dynamic traffic network.\n\n# Metric\n\nSubmissions are evaluated on the **mean absolute error** between predicted and actual congestion values for each time period in the test set. The congestion target has integer values from 0 to 100.\n\n# Data\n\n**train.csv** - the training set, comprising measurements of traffic congestion across 65 roadways from April through September of 1991.\n* row_id - a unique identifier for this instance\n* time - the 20-minute period in which each measurement was taken\n* x - the east-west midpoint coordinate of the roadway\n* y - the north-south midpoint coordinate of the roadway\n* direction - the direction of travel of the roadway. EB indicates ""eastbound"" travel, for example, while SW indicates a ""southwest"" direction of travel.\n* congestion - congestion levels for the roadway during each hour; the target. The congestion measurements have been normalized to the range 0 to 100.\n\n**test.csv** - the test set; you will make hourly predictions for roadways identified by a coordinate location and a direction of travel on the day of 1991-06-30."
# Load data
# Target: congestion level
The abnormal columns are originated by the constant values filling the missing observations during the morning of each day. See also the daily animations in the [notebook](https://www.kaggle.com/sytuannguyen/tps-mar-2022-time-space-animation) for more details.
# Morning vs Afternoon
"If we add 5 congestion units to the moning data, its distribution fits quite well with that of the afternoon"
# Histograms for each roadway
"# Correlation between 12 locations\n\nSome strong correlations of congestion can be observed between the locations. For example, the location x=1, y=2 and the one with x=1 and y=3 have a high correlation coefficient of 0.79."
# Correlation between 12 locations at two consecutive instants
"# Daily correlation between the locations (x,y)=(1,2) and (1,3)\n\nThese two locations seem to be highly connected during the rush hours (8am and 5pm)."
# Correlation between different directions at each location
# Average congestion per month
# Average congestion per month for each roadway
Notebooks with SVC:\n* [Fast SVC using scikit-learn-intelex for Digit Recognizer](https://www.kaggle.com/kppetrov/fast-svc-using-scikit-learn-intelex-for-mnist)\n* [Fast SVC using scikit-learn-intelex for NLP with Disaster Tweets](https://www.kaggle.com/kppetrov/fast-svc-using-scikit-learn-intelex-for-nlp)\n* [Fast SVC using scikit-learn-intelex for What's cooking](https://www.kaggle.com/kppetrov/using-scikit-learn-intelex-for-what-s-cooking)\n* [Fast SVC using scikit-learn-intelex for TPS - December 2021](https://www.kaggle.com/alexeykolobyanin/tps-dec-svc-with-sklearnex-20x-speedup)
\n## NuSVR\nGo to Algorithms
Notebooks with NuSVR:\n* [Fast NuSVR with scikit-learn-intelex for House prices](https://www.kaggle.com/alexeykolobyanin/house-prices-nusvr-sklearn-intelex-4x-speedup)\n* [Fast NuSVR with scikit-learn-intelex for TPS - August 2021](https://www.kaggle.com/alexeykolobyanin/tps-aug-nusvr-with-intel-extension-for-sklearn )
\n## RandomForest\nGo to Algorithms
Notebooks with RandomForest:\n* [Fast RF with scikit-learn-intelex for TPS - August 2021](https://www.kaggle.com/andreyrus/tps-apr-rf-with-intel-extension-for-scikit-learn)\n* [Fast RF with scikit-learn-intelex for TPS - July 2021](https://www.kaggle.com/alexeykolobyanin/tps-jul-rf-with-intel-extension-for-scikit-learn)
\n## KNN\nGo to Algorithms
Notebooks with KNN:\n* [Fast KNN using scikit-learn-intelex for Digit Recognizer](https://www.kaggle.com/kppetrov/fast-knn-using-scikit-learn-intelex-for-mnist)\n* [Fast KNN using scikit-learn-intelex on synthetic classification data](https://www.kaggle.com/kppetrov/accelerate-sklearn-algorithms-using-sklearnex)
\n## SVM\nGo to Algorithms
Notebooks with SVM:\n* [Fast SVM with scikit-learn-intelex for TPS - May 2021](https://www.kaggle.com/napetrov/svm-tps-may-2021-with-scikit-learn-intelex)\n* [Fast SVM with scikit-learn-intelex for TPS - April 2021](https://www.kaggle.com/napetrov/tps04-svm-with-intel-extension-for-scikit-learn)
\n## Stacking\nGo to Algorithms
Notebooks with Stacking:\n* [Fast Stacking with scikit-learn-intelex for TPS - June 2021](https://www.kaggle.com/masdevas/fast-ml-stack-with-scikit-learn-intelex)\n* [Fast Stacking with scikit-learn-intelex for TPS - July 2021](https://www.kaggle.com/alexeykolobyanin/tps-jul-stacking-with-scikit-learn-intelex)
\n## Logistic Regression\nGo to Algorithms
Notebooks with Logistic Regression:\n* [Fast Logistic Regression with scikit-learn-intelex for TPS - June 2021](https://www.kaggle.com/kppetrov/tps-jun-fast-logreg-with-scikit-learn-intelex#%F0%9F%93%9C-Conclusions)\n* [Fast Logistic Regression with scikit-learn-intelex for TPS - November 2021](https://www.kaggle.com/alexeykolobyanin/tps-nov-log-regression-with-sklearnex-17x-speedup)
\n## AutoML\nGo to Algorithms
Notebooks with AutoML:\n* [Fast AutoGluon using scikit-learn-intelex for TPS - June 2021](https://www.kaggle.com/alex97andreev/tps-jun-autogluon-with-sklearnex)\n* [Fast PyCaret using scikit-learn-intelex for TPS - January 2022](https://www.kaggle.com/lordozvlad/tps-jan-fast-pycaret-with-scikit-learn-intelex)\n* [Fast AutoGluon using scikit-learn-intelex for Titanic](https://www.kaggle.com/lordozvlad/titanic-automl-with-intel-extension-for-sklearn)
\n## Feature Importance\nGo to Algorithms
Notebooks with Feature Importance:\n* [Fast Feature Importance using scikit-learn-intelex for TPS - November 2021](https://www.kaggle.com/lordozvlad/fast-feature-importance-using-scikit-learn-intelex)\n* [Fast Feature Importance using scikit-learn-intelex for TPS - December 2021](https://www.kaggle.com/lordozvlad/fast-feature-importance-using-scikit-learn-intelex)
"# Installation\n\nIntel(R) Extension for Scikit-learn is available at the [Python Package Index](https://pypi.org/project/scikit-learn-intelex/), on Anaconda Cloud in [Conda-Forge channel](https://anaconda.org/conda-forge/scikit-learn-intelex) and in [Intel channel](https://anaconda.org/intel/scikit-learn-intelex). Intel(R) Extension for Scikit-learn is also available as a part of [Intel® oneAPI AI Analytics Toolkit (AI Kit)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html)."
" Observation:\n* latitue and longitude are important features.\n* rating is of relatively low importance.\n    \nI still don't know if this information is helpful for learning. If it is helpful, I still don't know how to use it for learning."
------------------------------------\n# Understanding Train Dataset\n\nTrain data are audio files. Let's hear it for ourselves and see the waveform to see what strategy can solve this problem.
-------------------------------\n## Normoc\n\n![](https://live.staticflickr.com/7070/6873951614_2dd80c1d7c_b.jpg)\n\nRef: https://cdn.download.ams.birds.cornell.edu
\n    Libraries\n
\n    Colors\n
\n    Visualizations\n
Let's check out the distribution of the features.
"**Hardenss of water**: The simple definition of water hardness is the amount of dissolved calcium and magnesium in the water. Hard water is high in dissolved minerals, largely calcium and magnesium. You may have felt the effects of hard water, literally, the last time you washed your hands. Depending on the hardness of your water, after using soap to wash you may have felt like there was a film of residue left on your hands. In hard water, soap reacts with the calcium (which is relatively high in hard water) to form ""soap scum"". When using hard water, more soap or detergent is needed to get things clean, be it your hands, hair, or your laundry."
"**pH level:** The  pH  of  water  is  a  measure  of  the  acid–base  equilibrium  and,  in  most  natural  waters,   is   controlled   by   the   carbon   dioxide–bicarbonate–carbonate   equilibrium   system. An increased carbon dioxide concentration will therefore lower pH, whereas a decrease will cause it to rise. Temperature will also affect the equilibria and the pH. In pure  water,  a  decrease  in  pH  of  about  0.45  occurs  as  the  temperature  is  raised  by  25  °C.  In  water  with  a  buffering  capacity  imparted  by  bicarbonate,  carbonate  and  hydroxyl  ions,  this  temperature  effect  is  modified  (APHA,  1989).  The  pH  of  most  drinking-water lies within the range 6.5–8.5. Natural waters can be of lower pH, as a result of, for example, acid rain or higher pH in limestone areas."
"**TDS**: TDS means concentration of dissolved particles or solids in water. TDS comprises of inorganic salts such as calcium, magnesium, chlorides, sulfates, bicarbonates, etc, along with many more inorganic compounds that easily dissolve in water. "
**Chloramines**: Chloramines (also known as secondary disinfection) are disinfectants used to treat drinking water and they:\n\n* Are most commonly formed when ammonia is added to chlorine to treat drinking water.\n* Provide longer-lasting disinfection as the water moves through pipes to consumers.\n\nChloramines have been used by water utilities since the 1930s.
"**Sulfate**: Sulfate (SO4) can be found in almost all natural water. The origin of most sulfate compounds is the oxidation of sulfite ores, the presence of shales, or the industrial wastes.\nSulfate is one of the major dissolved components of rain. High concentrations of sulfate in the water we drink can have a laxative effect when combined with calcium and magnesium, the two most common constituents of hardness."
"**Conductivity**: Conductivity is a measure of the ability of water to pass an electrical current. Because dissolved salts and other inorganic chemicals conduct electrical current, conductivity increases as salinity increases. Organic compounds like oil do not conduct electrical current very well and therefore have a low conductivity when in water. Conductivity is also affected by temperature: the warmer the water, the higher the conductivity."
"**Organic Carbon**: Organic contaminants (natural organic substances, insecticides, herbicides, and other agricultural chemicals) enter waterways in rainfall runoff. Domestic and industrial wastewaters also contribute organic contaminants in various amounts. As a result of accidental spills or leaks, industrial organic wastes may enter streams. Some of the contaminants may not be completely removed by treatment processes; therefore, they could become a problem for drinking water sources. It is important to know the organic content in a waterway."
# Gathering a Basic Insight of our Data:\n\n\n\n\n\n## Summary:\n\n The distribution of  house prices  is right skewed.\n There is a drop in the number of houses sold during the year of 2010. \n
"## Right-Skewed Distribution Summary:\nIn a right skew or positive skew the mean is most of the times to the right of the median. There is a higher frequency of occurence to the left of the distribution plot leading to more exceptions (outliers to the right). Nevertheless, there is a way to transform this histogram into a normal distributions by using log transformations which will be discussed further below."
" Economic Activity: \n\n\nWe will visualize how the housing market in **Ames, IOWA** performed during the years 2006 - 2010 and how bad it was hit by the economic recession during the years of 2007-2008.  \n\n## Level of Supply and Demand (Summary):\n\nJune and July were the montnths in which most houses were sold. \n The  median house price  was at its peak in 2007 (167k) and it was at its lowest point during the year of 2010 (155k) a difference of 12k. This might be a consequence of the economic recession. \n Less houses were sold and built during the year of 2010 compared to the other years. \n\n\n"
"# Problem Definition\nFor this tutorial, we will build a model to predict the depth to groundwater of an aquifer located in Petrignano, Italy. The question we want to answer is\n> What is the future depth to groundwater of a well belonging to the aquifier in Petrigrano over the next quarter?\n\n> The wells field of the alluvial plain between Ospedalicchio di Bastia Umbra and Petrignano is fed by three underground aquifers separated by low permeability septa. The aquifer can be considered a water table groundwater and is also fed by the Chiascio river. The groundwater levels are influenced by the following parameters: rainfall, depth to groundwater, temperatures and drainage volumes, level of the Chiascio river.\n\n> Indeed, both rainfall and temperature affect features like level, flow, depth to groundwater and hydrometry some time after it fell down. \n\n# Data Collection \nIn a typical workflow for time series, this would be the time for data collection. In this example, we will skip the data collection step and use data from the [Acea Smart Water Analytics challenge](https://www.kaggle.com/c/acea-water-prediction/). Therefore, this section will be a dataset overview. \n\nAlthough the dataset contains multiple waterbodies, we will only be looking at the Aquifer_Petrignano.csv file.\n\nTime series data usually comes in **tabular** format (e.g. csv files)."
"Since we are working with time series, the most essential features are the time related feature. In this example, we have the column `Date` which  uniquely identifies a day. Ideally, the data is already in chronological order and the time stamps are equidistant in time series. This is already the case in our data: The time interval is one day and the data is already in chronological order. Therefore, we do not have to do this additional data preparation step. \n\n\nThis column is provided in string format. Let's convert it to the `datetime64[ns]` data type."
Features:\n* **Rainfall** indicates the quantity of rain falling (mm)\n* **Temperature** indicates the temperature (°C) \n* **Volume** indicates the volume of water taken from the drinking water treatment plant (m$^3$)\n* **Hydrometry** indicates the groundwater level (m)\n\nTarget:\n* **Depth to Groundwater** indicates the groundwater level (m from the ground floor)\n
"# Data Preprocessing\n\n## Chronological Order and Equidistant Timestamps\nThe data should be in **chronological order** and the **timestamps should be equidistant** in time series. The chronological order can be achieved by sorting the dataframe by the timestamps. Equidisant timestamps indicates constant time intervals. To check this, the difference between each timestamp can be taken. If this is not the case, you can decide on a constant time interval and resample the data (see [Resampling](#Resampling)).\n\nThis is already the case in our data: The time interval is one day and the data is already in chronological order. Therefore, we do not have to do this additional data preparation step. "
"## Handling Missing Values\n\nWe can see that `Depth_to_Groundwater` has missing values.\n\nFurthermore, plotting the time series reveals that there seem to be some **implausible zero values** for `Drainage_Volume`, and `River_Hydrometry`. We will have to clean them by replacing them by `nan` values and filling them afterwards."
Now we have to think about what to do with these missing values. 
"* **Option 1: Fill NaN with Outlier or Zero**\n\n    In this specific example filling the missing value with an outlier value such as -999 is not a good idea. However, many notebooks in this challenge have been using -999. \n    \n* **Option 2: Fill NaN with Mean Value**\n\n    Also in this example, we can see that filling NaNs with the mean value is also not sufficient.\n\n* **Option 3: Fill NaN with Last Value with `.ffill()`**\n\n    Filling NaNs with the last value is already a little bit better in this case.\n\n* **Option 4: Fill NaN with Linearly Interpolated Value with `.interpolate()`**\n\n    Filling NaNs with the interpolated values is the best option in this small examlple but it requires knowledge of the neighouring values.\n"
"## Resampling\n\nResampling can provide additional information on the data. There are two types of resampling:\n* **Upsampling** is when the frequency of samples is increased (e.g. days to hours)\n* **Downsampling** is when the frequency of samples is decreased (e.g. days to weeks)\n\nIn this example, we will do some downsampling with the `.resample()` function."
"In this example, resampling would not be necessary. On the other hand, there is no necessity to look at the daily data. Considering weekly data seems to be sufficient as well. Therefore, we will **downsample the data to a weekly basis**."
"## Stationarity\n\nSome time-series models, such as such as [ARIMA](#ARIMA), assume that the underlying data is stationary. \nStationarity describes that the time-series has\n* constant mean and mean is not time-dependent \n* constant variance and variance is not time-dependent \n* constant covariance and covariance is not time-dependent \n\n> If a time series has a specific (stationary) behavior over a given time interval, then it can be assumed that the time series will behave the same at a later time.\n\nTime series **with trend and/or seasonality are not stationary**. Trend indicates that the mean is not constant over time and seasonality indicates that the variance is not constant over time."
"The check for stationarity can be done via three different approaches:\n1. **visually**: plot time series and check for trends or seasonality\n2. **basic statistics**: split time series and compare the mean and variance of each partition\n3. **statistical test**: Augmented Dickey Fuller test\n\nLet's do the **visual check** first. We can see that all features except `Temperature` have non-constant mean and non-constant variance. Therefore, **none of these seem to be stationary**. However, `Temperature` shows strong seasonality (hot in summer, cold in winter) and therefore it is not stationary either."
"Next, we will **check the underlying statistics**. For this we will **split the time series into two sections** and check the mean and the variance. You could do more partitions if you wanted.\n\nWith this method, `Temperature` and `River_Hydrometry` show **somewhat similar (constant) mean and variance** and could be seen as stationary. However, with this method, we are not able to see the seasonality in the `Temperature` feature."
"Let's evaluate the histograms. Since we are looking at the mean and variance, we are expecting that the data conforms to a Gaussian distribution (bell shaped distribution) in case of stationarity."
"**Augmented Dickey-Fuller (ADF) test**  is a type of statistical test called a unit root test.  Unit roots are a cause for non-stationarity.\n\n* **Null Hypothesis (H0)**: Time series has a unit root. (Time series is **not stationary**).\n\n* **Alternate Hypothesis (H1)**: Time series has no unit root (Time series is **stationary**).\n\nIf the **null hypothesis can be rejected**, we can conclude that the **time series is stationary**.\n\nThere are two ways to rejects the null hypothesis:\n\nOn the one hand, the null hypothesis can be rejected if the p-value is below a set significance level. The defaults significance level is 5%\n\n* **p-value > significance level (default: 0.05)**: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n* **p-value <= significance level (default: 0.05)**: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n    \nOn the other hand, the null hypothesis can be rejects if the test statistic is less than the critical value.\n* **ADF statistic > critical value**: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n* **ADF statistic < critical value**: Reject the null hypothesis (H0), the data does not have a unit root and is stationary."
Load Packages
Import the Data
### Check for Class Imbalance
Classification augment
-----------------------------------------\n# Data visualization
We can see that the negative/positive ratio is not entirely 50/50 as there are 130k negatives and 90k negatives. The ratio is closer to 60/40 meaning that there are 1.5 times more negative images than positives.\n\n### Plot some images with and without cancer tissue for comparison
"**To see the effects of our augmentation, we can plot one image multiple times.**"
"### Compute image statistics\n**Do not use augmentation here!**\n\nCalculating statistics will give channel averages of [0.702447, 0.546243, 0.696453],\nand std's of [0.238893, 0.282094, 0.216251].\n\nWhile we are calculating statistics, we can check if there are images that have a very low maximum pixel intensity (almost totally black) or very high minimum pixel intensity (almost totally white). These kind of images could be caused by bad exposure or cropping to an empty area. In case of an empty area, the image would not be an outlier but equally valid negative sample.\n\nWe find that there is at least one very dark and 6 very bright images."
### Plot some of the very bright or very dark images
"All the dark and bright images are labeled negative. I think the bright ones are just cropped from a non-stained part or they don't have any tissue (plain glass?) so the labels are correct. The samples don't have tumor tissue present. I am not so sure about the dark image, is it an outlier crop from badly exposed area or just some very large cell part filling the whole image. Anyway, removing only a small amount of outliers from this size data set has little or no effect on the prediction performance.\n\n-----------------------------------------"
"Given this situation, one main question that comes to people's mind is that where can I learn about DS and ML. An answer to this question will be very helpful for people to get started in this field. So in this notebook, let us explore the different options where people learn about DS / ML skills. \n\n**Most of the plots are interactive. So please feel free to hover over the plots, zoom in / out, rotate them as needed**\n\nFirstly, I would like to thank Kaggle for conducting this DS / ML survey again this year and making the data available for people like us to use."
"## Learning Category:\n\nIn this intial phase of the analysis, let us take up question 35 in the survey. The question is:\n\n*What percentage of your machine learning/ data science training falls under each category?*\n\nThe choices given for this question are\n 1. Self-taught \n 2. Online courses like Coursera, Udemy, edX etc\n 3. Work\n 4. University\n 5. Kaggle Competiitons\n 6. Other - Free text field\n \nWe need to give a percentage value for each of these learning categories and the total should sum up to 100%.\n\nOverall there are 23,858 responses in this survey and let us check the number of respondents for each of these training categories (percentage of the category is greater than 0)."
"**Observations:**\n * There are some missing values for this question and after dropping them we have 15,745 responses in total.\n * 'Self taught is the category with most number of respondents having percentage of learning greater than 0.\n * With the recent explosion of MOOC courses, 'Online courses' come in second \n * Learning as part of work is third and traditional way of learning - 'University' is fourth\n * Though Kaggle competitions take the fifth spot, the number of respondents is not much lesser than third and forth place.\n \n \n ### Percentage Contribution of Learning Categories:\n \n Now let us see, how much percentage each of the learning categories contribute to the learning process."
"**Observations:**\n \n * Looking at the median of each of the learning categories, it seems there is no one category that completely dominated the learning process of ML / DS\n * Self-taught seems to have higher percentage of share in the learning process compared to others. \n * Only less than half of the respondents have the percentage share of 'University' as greater than 0 \n \n \n ### Distribution of DS / ML Learning Category at different Countries:\n \n Now let us have a look at how these learning categories are distributed across the top countries. We will take the respondents from top 10 countries and do the analysis."
"**Observations:**\n * Compared to other top countries, the percentage contribution of universities in USA is much higher. I think this might be because there are multiple universities in US offering courses in DS / ML when compared with other countries.\n * In countries like India, Russia & Japan, the role of universities in Learning DS / ML is much lesser compared to other categories. \n * Also if we look at **Asian countries** in the list (India, China, Russia and Japan), **median percentage of Kaggle contribution** for learning DS / ML is greater than 0 while it is zero for other countries. \n * Contribution from learning at Work is more in Russia, France and Japan\n * In Brazil, the contribution of MOOC courses seem to be more than other learning categories\n * I personally think these plots directly represent the comfortable ways to acquire knowledge at these corresponding regions.\n \n \n ### Distribution of Learning Category By Profession:\n \n In this section, let us look at the how the learning categories vary based on the profession."
"**Observations:**\n \n * As expected, University plays a major role in imparting DS / ML knowledge among students and 'Work' has the least contribution\n * In case of Data Scientist, most of the respondents have mentioned that 'Work' plays a major role in learning the concepts. Self-learning also plays an equally important role.\n * For 'Software Enginner' who are learning DS / ML, 'Self-taught' and 'Online courses' are the ways to acquire knowledge compared to other means.\n * Also respondents with title 'Software Engineer' mentioned that Kaggle competitions share a higher percentage in learning DS / ML compared to other professions (looking at the third quartile)\n \n \n ### Distribution of Learning Category by Degree Attained:\n \n Now let us see how the learning categories vary based on the highest degree attained."
**Observations:**\n * Respondents having a Masters or Doctoral degree have a higher contribution for learning DS / ML from university compared to other two sections (looking at the third quartile of university)\n * Respondents with Bachelors degree have higher contribution of learning from self taught courses and online courses (looking at the median of different learning categories of Bachelors degree)\n * Learning from Kaggle competitions seem to have a fairly stable contribution across all sections.\n \n### Distribution of Learning Category by Gender
"**Observations:**\n\n* Looking at the third quartile of all the categories, 'Self-taught' has a higher contribution for Male while 'University' has the higher contribution for Female. \n\n### Distrbution of Learning Categories by Age:\n \n In this section, let us see how the percentage contribution each of the learning categories change based on age."
"**Observations:**\n * 'Self-taught' category contributes more for respondents aged more than 35 compared to respondents aged less than 35\n * Looking at the distribution of online courses by age, we can see that respondents aged more than 60 seem to have a lower median score compared to other age groups\n * Median contribution of 'Work' as learning category is high for middle aged people compared to younger and older ones\n * Median contribution of 'University' as learning category is high for people less than 30 years of age\n * Contribution of Kaggle as a learning category is fairly consistent across age groups with a slght higher third quartile for people aged less than 21. Looks like Kaggle is quite popular with the younger bunch ;)\n \n\n### Other ML / DS Learning Category - FreeForm Text\n \n There is also a free form text column that contains the responses for the DS / ML learning category apart from the choices given. Let us look at them."
"**Observations:**\n\n* Apart from the given choices, bootcamps and books seem to be the next popular choices of learning DS / ML.\n \n \n ## Online Course Platforms\n\nAs we could see from the previous analysis, online platforms play a major role in imparting DS / ML education. So now let us focus on the different online plarforms. There are multiple online platforms available from which we can learn DS. Some of them can be seen below."
### Number of Respondents for each online platform\n\nFirst let us look at the number of respondents for each online platform.
"We also have a free form text column for this question, to add any other sources apart from the one mentioned. Let us look at them now."
\n### Most Used Online Platform to Learn DS\n\nNext let us look at the online platforms where the people had spent most of their time.
"**Observations:** \n * Coursera tops the list with about 39% of the respondents mentioning that they spent most of their time there.\n * DataCamp and Udemy are neck to neck with each other with about 12% share\n * Udacity and edX are about 8.5% and 8% respectively\n * Though Kaggle Learn is relatively new, it is preferred by about 7% of respondents\n \n \n  ### Most used Online Platform by Country:\n  \n  The world map plots are interactive. Please rotate them to have a better view of the countries you would like to see."
 Most of the Airlines has Economic Class as common
" Does price vary with Airlines?\n\n\n    \n\n    As we can see Vistara has Maximum Price range\n    Vistara and Air_India Airlines Have Maximum Price when compared to Others\n    SpiceJet , AirAsia , GO_First and Indigo has some what equal prices  \n    \n\n"
\n     How Does the Ticket Price vary between Economy and Business Class?\n   \n \n    Ticket Price is Maximum for Bussiness Class When compared to Economy Class\n\n    
 How Does the Ticket Price vary with the number of stops of a Flight?\n\n \nFlights having one stop has maximum ticket price\n
"\n    \n How the Ticket Price change based on the Departure Time and Arrival Time?\n\n    \n1. Departure Time Vs Ticket Price\n\n   \n    Ticket Price is More for the Flights when the Departure Time is at Night\n   Ticket Price is almost equal for flights Having Departure time at Early_morning , Morning and Evening\n   Ticket Price is Low for the Flights Having Departure Time at Late_night\n   \n    \n \n \n2. Arrival Time Vs Ticket Price\n\n   \n    Ticket Price is More for the Flights when the Arrival Time is at Evening\n    Ticket Price is almost equal for flights Having Arrival time is at Morning and Night\n   Ticket Price is Low for the Flights Having Arrival Time at Late_night as same as Departure Time\n    \n "
"\n How the price changes with change in Source city and Destination city?\n    \n1. Source City Vs Ticket Price\n\n    Ticket Price is More for the Flights whose Source City is Kolkata\n   Ticket Price is almost equal for flights Having Source Cities as Mumbai and chennai , Hyderabad and Bangalore\n     Ticket Price is Low for the Flights Having Source City as Delhi\n    \n \n2. Destination City Vs Ticket Price\n\n    Ticket Price is More for the Flights whose Destination City is kolkata and Chennai\n    Ticket Price is almost equal for flights Having Destination Cities as Mumbai and Bangalore\n   Ticket Price is Low for the Flights Having Destination City as Delhi\n"
"\n     How Price Varies with the Flight Duration Based on Class?\n\n With increase in Duration, the Ticket Price is also Increases In both the Economy and Business classes\n\n"
\n     How does the price affected on the days left for Departure?\n    \n As we can see when compared to others when there are two days remaining for departure then the Ticket Price is very High for all airlines\n    \n
"**Created by *Peter Nagy* February 2017**  \n[github][1] \n[Linkedin](https://www.linkedin.com/in/peternagyjob/) \n[My other kernel on LSTM](https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras) \n\n**Sentiment Analysis:**\nthe process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n\n  [1]: https://github.com/nagypeterjob"
"I decided to only do sentiment analysis on this dataset, therfore I dropped the unnecessary colunns, keeping only *sentiment* and *text*."
"As a next step I separated the Positive and Negative tweets of the training set in order to easily visualize their contained words.  After that I cleaned the text from hashtags, mentions  and links. Now they were ready for a WordCloud visualization which shows only the most emphatic words of the Positive and Negative tweets."
"Interesting to notice the following words and expressions in the positive word set:\n **truth**, **strong**, **legitimate**,  **together**, **love**, **job**\n\nIn my interpretation, people tend to believe that their ideal candidate is truthful, legitimate, above good and bad.\n\n\n----------\n\n\nAt the same time, negative tweets contains words like:\n**influence**, **news**, **elevator music**, **disappointing**, **softball**, **makeup**, **cherry picking**, **trying**\n\nIn my understanding people missed the decisively acting and considered the scolded candidates too soft and cherry picking."
Hereby I plotted the most frequently distributed words. The most words are centered around debate nights.
Using the nltk NaiveBayes Classifier I classified the extracted tweet word features.
![image.png](attachment:image.png)
 Objective  \n\nGoal of this kernel is following:\n- Basic Exploratory Data Analysis.\n- Feature Analysis
\n   Prerequisites 
\n   Load and Check Data  
"Hello everyone!! Hope everything is fine and you are enjoying things on Kaggle as usual. The rage for competing on Kaggle should never end. \nMachine Learning and Deep Learning have a huge scope in healthcare but applying them in healthcare isn't that simple. The stake is very high. It's more than just a `classification` problem. But if applied very carefully, it can benefit the world in enormous ways. **And as a Machine learning engineer, it's our responsibility to help people as much as we can in all possible ways.**\n\nPneumonia is a very common disease. It can be either: 1) Bacterial pneumonia  2) Viral Pneumonia  3) Mycoplasma pneumonia   and 4) Fungal pneumonia.\nThis dataset consists pneumonia samples belonging to the first two classes.  The dataset consists of only very few samples and that too unbalanced. The aim of this kernel is to develop a robust deep learning model from scratch on this limited amount of data. We all know that deep learning models are data hungry but if you know how things work, you can build good models even with a limited amount of data. "
Reproducibility is a great concern when doing deep learning. There was a good discussion on `KaggleNoobs` slack regarding this. We will set a numer of things in order to make sure that the results are almost reproducible(if not fully). 
### How many samples for each class are there in the dataset?
As you can see the data is highly imbalanced. We have almost with thrice pneumonia cases here as compared to the normal cases. This situation is very normal when it comes to medical data. The data will always be imbalanced. either there will be too many normal cases or there will be too many cases with the disease. \n\nLet's look at how a normal case is different from that of a pneumonia case. We will look at somes samples from our training data itself.
"If you look carefully, then there are some cases where you won't be able to differentiate between a normal case and a pneumonia case with the naked eye. There is one case in the above plot, at least for me ,which is too much confusing. If we can build a robust classifier, it would be a great assist to the doctor too."
The correlation matrix and the distance matrix look like this
"This is an interesting chessboard. We can see that there are highly (anti-)correlated features. In particular, three clusters stand out: `ft_17` to `ft_40`, the two overlapping squares in `ft_73` to `ft_93` and `ft_85` to `ft_117`, and the *chessboard* between `ft_121` and `ft_129`. A very important **caveat** at this point: we are examining correlations for the first thirty days of trading. Correlations are not static entities, on the contrary, they tend to change and evolve depending on the time period and the market regime. Analyzing the stability of these clusters over time is an interesting point that could be analyzed. Stable clusters could be thought as intrinsic properties of the dataset rather than temporary events."
\n\n# Table of Contents\n\n* [0. Introduction](#0)   \n* [1. Templates](#1)\n* [2. Basic Plots](#2)\n    * [2.1 Histogram: basic](#2.1)\n    * [2.2 Histogram: ordered](#2.2)\n    * [2.3 Histogram: oredered and highlighted](#2.3)\n    * [2.4 Bar plots/horizontal histogram: (+ annotation)](#2.4)\n    * [2.5 Scatter/line plots](#2.5)\n    * [2.6 Pie Charts](#2.6)\n* [3. Distribution/kde plot](#3)\n* [4. Correlation Heatmaps](#4)\n    * [4.1 Correlation Heatmaps:full matrix ](#4.1)\n    * [4.2 Correlation Heatmaps:lower-triangular matrix ](#4.2)\n    * [4.3 Correlation Heatmaps:annotated matrix ](#4.3)\n* [5. Annotations](#5)\n* [6. Subplots](#6)\n    * [6.1 Mixed plots/subplots](#6.1)\n    * [6.2 Facet grids/pairplots](#6.2)\n    * [6.3 Daigonal Facet grids (custom made)](#6.3)\n* [7. Radar Charts](#7)\n* [8. 3D Plots](#8)\n* [9. Choropleth Map (Animation)](#9)\n* [10. Bonus: Getting creative with annotated heatmaps](#10)\n* [11. Reference](#11)
 Import libraries and load datasets 
"\n1. Templates\n\nAs good a plotting library as plotly is, its default template is not pleasing to the eye and we often need to customize in order to have an informative as well as better looking plots and charts. Plotly offers the following background/plot-area templates to choose from. \n\n >- `plotly` \n >- `ggplot2`     \n >- `seaborn`\n >- `plotly_dark`\n >- `simple_white`\n     \n**Note**: You can also customize your own backgroud/paper color if you wish to. Let's demonstrate using the titanic dataset.\n"
Back to top\n\n\n2. Basic Plots\n\n\n2.1 Histogram: basic\n
\n2.2 Histogram: ordered\n\n> Ordered bar/histogram according to y-value (percentage of students in each race/ethnicity group).
\n2.3 Histogram: ordered and highlighted\n\n> Highlight *important* message you want to communicate. We can do so by assigning different colors to specific categories (max and min in this example)
\n2.4 Bar plots/Horizontal histogram
\n\n2.5 Scatter/line plots\n\n>- Highlight only the head story while keeping others in the background\n>- Add secondary y-axis\n
"\n2.6 Pie Charts\n\nGood practice while using Pie charts:\n>- Avoid using/displaying `too many categories` in a pie-chart. If you have to display more that say 10 categories, may be its time to consider other chart types.\n>- Hide a `long-list of legends`. When it is too long it's a distruction. \n>- Use `pulled-sector` only to highlight something interesting, not because it looks cool.\n>- Use `sunburst` charts to your advantage. They are quite useful in communicating hierarchical data.  \n>- If you use `donut` chart, use the center to display useful information. However, try to avoid very thin donut pies."
"\n4. Correlation Heatmaps\n\nCorrelation heatmap is a very good way of summerizing how features of a dataset are related to one another or with the target variable. It gives a helicopter-view of all the features in a compact and beautifull correlation matrix. Often times seaborn offers an excellent plotting functon and it is a go-to option even when people try to make a plolty-only EDA. The main reason is that plotly's default heatmap is not pleasing to the eye. However, when cutomized properly plotly can also be beautifully pleasing with the added interactivness as well.\n\nBellow are few examples. \n\n\n4.1 Correlation Heatmap: full matrix"
\n4.2 Correlation Heatmap: annotated full matrix
# Model setup
We are going to use the dataset I have cleaned up of the original data. More info in a kaggle [thread](https://www.kaggle.com/competitions/amex-default-prediction/discussion/328514).
We can see that three `D` (as for **D**elinquency) features stand out. We will investigate this further. But first let's see how accurate our very simple model is in predicting the time rank.
It seems the model is able to predict the rank correctly 40-50% of all instances. Quite good. But this rejects the hypothesis that we have a `time_since_customer` feature in the dataset - the accuracy should be 100% if the hypothesis was true.\n\nLet's dig in a bit deeper!
### segment with temporal `D_59`:
### segment without temporal `D_59`:
"As expected, our model accuracy is 2-3x better for the segment showing temporal `D_59` behavior."
"# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\n\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\n\n[Stacking Starter][1] : by Faron \n\n\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\n\n\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808"
"# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features."
**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us.
"# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!"
\n# Import libraries
 ⬆️Back to Table of Contents ⬆️
# **Feature Scaling**
## Lets look at overall survival stats[^](#3_1)
* Sad Story! Only 38% have survived. That is roughly 340 out of 891. 
### Feature: Sex[^](#3_2_1)
"* While survival rate for female is around 75%, same for men is about 20%.\n* It looks like they have given priority to female passengers in the rescue.\n* **Looks like Sex is a good predictor on the survival.**"
"---\n### Feature: Pclass[^](#3_2_2)\n**Meaning :** Ticket class : 1 = 1st, 2 = 2nd, 3 = 3rd"
"* For Pclass 1 %survived is around 63%, for Pclass2 is around 48% and for Pclass2 is around 25%.\n* **So its clear that higher classes had higher priority while rescue.**\n* **Looks like Pclass is also an important feature.**"
---\n### Feature: Age[^](#3_2_3)\n**Meaning :** Age in years
* Survival rate for passenegers below Age 14(i.e children) looks to be good than others.\n* So Age seems an important feature too.\n* Rememer we had 177 null values in the Age feature. How are we gonna fill them?.
"---\n### Feature: Embarked[^](#3_2_4)\n**Meaning :** Port of Embarkation. C = Cherbourg, Q = Queenstown, S = Southampton"
* Majority of passengers borded from Southampton\n* Survival counts looks better at C. Why?. Could there be an influence from sex and pclass features we already studied?. Let's find out 
"Oh. \n\n(1595 in real data, 20 if you're in the Kaggle sample dataset)\n\nWell, that's also going to be a challenge for the convnet to figure out, but we're going to try! Also, there are outside datasources for more lung scans. For example, you can grab data from the LUNA2016 challenge: https://luna16.grand-challenge.org/data/ for another 888 scans.\n\nDo note that, if you do wish to compete, you can only use free datasets that are available to anyone who bothers to look.\n\nI'll have us stick to just the base dataset, again mainly so anyone can poke around this code in the kernel environment.\n\nNow, let's see what an actual slice looks like. If you do not have matplotlib, do *pip install matplotlib*\n\n\n\nWant to learn more about Matplotlib? Check out the [Data Visualization with Python and Matplotlib tutorial][1].\n\n\n  [1]: https://pythonprogramming.net/matplotlib-intro-tutorial/"
"Now, I am not a doctor, but I'm going to claim a mini-victory and say that's our first CT scan slice.\n\nWe have about 200 slices though, I'd feel more comfortable if I saw a few more. Let's look at the first 12, and resize them with opencv. If you do not have opencv, do a *pip install cv2*\n\nWant to learn more about what you can do with Open CV? Check out the [Image analysis and manipulation with OpenCV and Python tutorial][1].\n\nYou will also need numpy here. You probably already have numpy if you installed pandas, but, just in case, numpy is *pip install numpy*\n\n\n  [1]: https://pythonprogramming.net/loading-images-python-opencv-tutorial/"
# Section 2: Processing and viewing our Data #\n\n
"Alright, so we're resizing our images from 512x512 to 150x150. 150 is still going to wind up likely being waaaaaaay to big. That's fine, we can play with that constant more later, we just want to know how to do it.\n\nOkay, so now what? I think we need to address the whole non-uniformity of depth next. To be honest, I don't know of any super smooth way of doing this, but that's fine. I can at least think of A way, and that's all we need.\n\nMy thought is that, what we have is really a big list of slices. What we need is to be able to just take any list of images, whether it's got 200 scans, 150 scans, or 300 scans, and set it to be some fixed number.\nLet's say we want to have 20 scans instead. How can we do this?\nWell, first, we need something that will take our current list of scans, and chunk it into a list of lists of scans.\n\nI couldn't think of anything off the top of my head for this, so I Googled ""how to chunk a list into a list of lists."" This is how real programming is happens.\n\nAs per Ned Batchelder via Link: [http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks][1], we've got ourselves a nice chunker generator. Awesome!\n\nThanks Ned!\n\nOkay, once we've got these chunks of these scans, what are we going to do? Well, we can just average them together. My theory is that a scan is a few millimeters of actual tissue at most. Thus, we can hopefully just average this slice together, and maybe we're now working with a centimeter or so. If there's a growth there, it should still show up on scan.\n\nThis is just a theory, it has to be tested.\n\nAs we continue through this, however, you're hopefully going to see just how many theories we come up with, and how many variables we can tweak and change to possibly get better results.\n\n\n  [1]: http://%20http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks"
"Okay, the Python gods are really not happy with me for that hacky solution. If any of you would like to improve this chunking/averaging code, feel free. Really, any of this code...if you have improvements, share them! This is going to stay pretty messy. But hey, we did it! We figured out a way to make sure our 3 dimensional data can be at any resolution we want or need. Awesome!\n\nThat's actually a decently large hurdle. Are we totally done? ...maybe not. One major issue is these colors and ranges of data. It's unclear to me whether or not a model would appreciate that. Even if we do a grayscale colormap in the imshow, you'll see that some scans are just darker overall than others. This might be problematic and we might need to actually normalize this dataset.\n\nI expect that, with a large enough dataset, this wouldn't be an actual issue, but, with this size of data, it might be of huge importance.\n\nIn effort to not turn this notebook into an actual book, however, we're going to move forward! We can now see our new data by doing:"
"# Section 3: Preprocessing our Data #\n\n\n\nOkay, so we know what we've got, and what we need to do with it.\n\nWe have a few options at this point, we could take the code that we have already and do the processing ""online."" By this, I mean, while training the network, we can actually just loop over our patients, resize the data, then feed it through our neural network. We actually don't have to have all of the data prepared before we go through the network.\n\nIf you can preprocess all of the data into one file, and that one file doesn't exceed your available memory, then training should likely be faster, so you can more easily tweak your neural network and not be processing your data the same way over and over.\n\nIn many more realistic examples in the world, however, your dataset will be so large, that you wouldn't be able to read it all into memory at once anyway, but you could still maintain one big database or something.\n\nBottom line: There are tons of options here. Our dataset is only 1500 (even less if you are following in the Kaggle kernel) patients, and will be, for example, 20 slices of 150x150 image data if we went off the numbers we have now, but this will need to be even smaller for a typical computer most likely. \n\nRegardless, this much data wont be an issue to keep in memory or do whatever the heck we want.\n\nIf at all possible, I prefer to separate out steps in any big process like this, so I am going to go ahead and pre-process the data, so our neural network code is much simpler. Also, there's no good reason to maintain a network in GPU memory while we're wasting time processing the data which can be easily done on a CPU.\n\nNow, I will just make a slight modification to all of the code up to this point, and add some new final lines to preprocess this data and save the array of arrays to a file:"
"# Section 4: 3D Convolutional Neural Network #\n\n##Moment-o-truth##\n\n\n\nOkay, we've got preprocessed, normalized, data. Now we're ready to feed it through our 3D convnet and...see what happens!\n\nNow, I am not about to stuff a neural networks tutorial into this one. If you're already familiar with neural networks and TensorFlow, great! If not, as you might guess, I have a tutorial...or tutorials... for you!\n\nTo install the CPU version of TensorFlow, just do *pip install tensorflow*\n\nTo install the GPU version of TensorFlow, you need to get alllll the dependencies and such.\n\n**Installation tutorials:**\n\n[Installing the GPU version of TensorFlow in Ubuntu][1]\n\n[Installing the GPU version of TensorFlow on a Windows machine][2]\n\n**Using TensorFlow and concept tutorials:**\n\n[Introduction to deep learning with neural networks][3]\n\n[Introduction to TensorFlow][4] \n\n[Intro to Convolutional Neural Networks][5]\n\n[Convolutional Neural Network in TensorFlow tutorial][6]\n\nNow, the data we have is actually 3D data, not 2D data that's covered in most convnet tutorials, including mine above. So what changes? EVERYTHING! OMG IT'S THE END OF THE WORLD AS WE KNOW IT!!\n\nIt's not really all too bad. Your convolutional window/padding/strides need to change. Do note that, now, to have a bigger window, your processing penalty increases significantly as we increase in size, obviously much more than with 2D windows.\n\nOkay, let's begin.\n\n\n  [1]: https://pythonprogramming.net/how-to-cuda-gpu-tensorflow-deep-learning-tutorial/\n  [2]: https://www.youtube.com/watch?v=r7-WPbx8VuY\n  [3]: https://pythonprogramming.net/neural-networks-machine-learning-tutorial\n  [4]: https://pythonprogramming.net/tensorflow-introduction-machine-learning-tutorial/\n  [5]: https://pythonprogramming.net/convolutional-neural-network-cnn-machine-learning-tutorial/\n  [6]: https://pythonprogramming.net/cnn-tensorflow-convolutional-nerual-network-machine-learning-tutorial/"
"\n# Appendix\n\n\n## Dice example\n\nlet's have a look at an example to calculate the probability. I deliberately chose a basic and simple dice example with discrete values so it is easier to understand connecting to our available/intuitive knowledge about chances when throwing dice. Bayesian logic is not always very intuitive, so I do want to recommend playing around with some examples to familiarize yourself with Bayesian logic.  \n\nIn this dice example, a person will throw secretly either 1 or 2 six-sided dice, mostly 2 dice (60%) but also 1 dice (40%). You will just get the resulting number and you have to guess how many dice were thrown.  \n(a) If the person tells you the result is 1 you probably figured out it had to be with 1 six-sided dice, kind of hard to throw 1 with 2 dice.  \n(b) If the person tells you the result is 4 what would be more likely 1 or 2 six-sided dice? A naive approach would be to guess 2 dice because mostly 2 dice are thrown, or isn't it? Bayes' Theorem can help you to figure it out.  \n\nSo we are asking:\n\n(a) $$p(Dice=2 \mid  Observation=1)=\frac{p(Observation=1 \mid Dice=2) * p(Dice=2)}{p(Observation=1)}$$ \nwhich is 0 because $p(Observation=1 \mid Dice=2)$ is 0. So also Bayes's theorem confirms it is kind of hard to throw a 1 with 2 dice. Good start.\n\n(b) $$p(Dice=2 \mid  Observation=4)=\frac{p(Observation=4 \mid Dice=2) * p(Dice=2)}{p(Observation=4)}$$ \n  \nwhich is equal to (apply sum rule, see equation [2a] next paragraph):\n$$=\frac{p(Observation=4 \mid Dice=2) * p(Dice=2)}{p(Observation=4 \mid Dice=1) * p(Dice=1)+p(Observation=4 \mid Dice=2) * p(Dice=2)}$$\n  \nSince this is a dice example and we remember the dice probabilities from high school the probability can be evaluated directly:\n$$=\frac{\frac{3}{36} * \frac{6}{10}}{\frac{1}{6} * \frac{4}{10}+\frac{3}{36} * \frac{6}{10}}=\frac{0,05}{0,1166}\approx 43\% $$\n  \nBoth (a) and (b) are point estimates for easy understanding. Bayesian logic also works with complete probability distributions (discrete and continuous). Did generate the discrete probability distributions below for reference. Please play around with the example below to better familiarize yourself with Bayes' Theorem."
"\n# Conventions and Symbols\n\nUnfortunately have observed various types of symbols over various documents, not making it easier to read or understand. Below the symbols used in this notebook  \n\nSymbol | Description | Alternative |\n--- | --- | --- |\nx  | external hidden environment states the brain tries to infer. Shorthand notation for the vector $\vec{x}_{hypotheses}\in\mathbb{R}^n$ with n number of states. | |\ny | sensory observations, the data from the available sensory set (effect of the environment on the ¨system¨). Shorthand notation for the vector $\vec{y}_{observation} \in  \mathbb{R}^{q}$ with q number of sensors.  | also sometimes noted ¨s¨ as of sensory states |\nu | actions, control signal, The action that can be performed on the environment. (effect of the ¨system¨ on the environment) Shorthand notation for the vector $\vec{u} \in  \mathbb{R}^{l}$ with l number of controls.  | also noted as ¨a¨ or ¨$\alpha $¨ |\n$\mathcal{F}(y,\zeta)$ or $\mathcal{F}(y_{observation},\zeta)$ | The Free Energy | simplifies under lapace transformation to $\mathcal{F}(y,\mu)$\n$q(x; \zeta )$ or $q(x_{hypotheses}; \zeta )$    | recognition probability density with sufficient statistics $\zeta$, the brain to approximate posterior  $p(x_{hypotheses}\mid  y_{observation})$ probability density. Expectation of the states x given observations y  | Ensemble density; simplifies under lapace transformation to $q(x; \mu )$ |\n$p(x,y)$ or $p(x_{hypotheses},y_{observation})$ | Generative probability density: the brain encoding a probabilistic model of the environment/world in which it is immersed. |  |\n$\zeta$ | sufficient statistics (e.g. for a Gaussian distribution: mean 𝜇, variance $𝜎^2$) for the recognition probability density | $\mu$ (because under laplace approximation sufficient statistics simplifies to mean $\mu$, see next notebook), in few papers noted as $\lambda$ (but in other papers $\lambda$ sometimes also used for precision of random noise) |\n\n\n"
 Cheatsheet - Matplotlib Charts 
 Please Upvote my kernel and keep it in your favourite section if you think it is helpful.
\n\n\nTable of Content\n    \n- Introduction\n- Libraries\n- Data\n- Scatter Plot\n- Line Plot\n- Multiline Plot\n- Histogram\n- Bar Plot\n- Horizontal Bar Plot\n- Error Bar Plot\n- Stacked Bar Plot\n- Box Plot\n- Area Plot\n- Stacked Area Plot\n- Density Plot\n- Hexbin Plot\n- Lollipop Plot
\nIntroduction
\nLibraries
\nLoad Data
![](https://datavizcatalogue.com/methods/images/anatomy/scatterplot.png)\n Source: [DataVizCatalogue](https://datavizcatalogue.com/methods/images/anatomy/scatterplot.png) 
\nLine Plot
![](https://datavizcatalogue.com/methods/images/anatomy/line_graph.png)\nSource: [DataVizCatalogue](https://datavizcatalogue.com/methods/images/anatomy/line_graph.png)
"\nHistogram\n\nAn histogram is an accurate graphical representation of the distribution of numerical data. It takes as input one numerical variable only. The variable is cut into several bins, and the\nnumber of observation per bin is represented by the height of the bar."
![](https://datavizcatalogue.com/methods/images/anatomy/histogram.png)\nSource: [DataVizCatalogue](https://datavizcatalogue.com/methods/images/anatomy/histogram.png)
"\nBar Plot\n\nA barplot (or barchart) is one of the most common type of plot. It shows the relationship between a numerical variable and a categorical variable. For example, you can display the height of several individuals using bar chart."
\nHorizontal Bar Plot
\nError Bar Plot
\nBox Plot\n\nA Box and Whisker Plot (or Box Plot) is a convenient way of visually displaying the data distribution through their quartiles.
\nSimple Box Plot
\nMultiple Box Plot
"# Introduction: Manual Feature Engineering\n\nIf you are new to this competition, I highly suggest checking out [this notebook](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/) to get started.\n\nIn this notebook, we will explore making features by hand for the Home Credit Default Risk competition. In an earlier notebook, we used only the `application` data in order to build a model. The best model we made from this data achieved a score on the leaderboard around 0.74. In order to better this score, we will have to include more information from the other dataframes. Here, we will look at using information from the `bureau` and `bureau_balance` data. The definitions of these data files are:\n\n* bureau: information about client's previous loans with other financial institutions reported to Home Credit. Each previous loan has its own row.\n* bureau_balance: monthly information about the previous loans. Each month has its own row.\n\nManual feature engineering can be a tedious process (which is why we use automated feature engineering with featuretools!) and often relies on domain expertise. Since I have limited domain knowledge of loans and what makes a person likely to default, I will instead concentrate of getting as much info as possible into the final training dataframe. The idea is that the model will then pick up on which features are important rather than us having to decide that. Basically, our approach is to make as many features as possible and then give them all to the model to use! Later, we can perform feature reduction using the feature importances from the model or other techniques such as PCA. \n\nThe process of manual feature engineering will involve plenty of Pandas code, a little patience, and a lot of great practice manipulation data. Even though automated feature engineering tools are starting to be made available, feature engineering will still have to be done using plenty of data wrangling for a little while longer. "
"## Example: Counts of a client's previous loans\n\nTo illustrate the general process of manual feature engineering, we will first simply get the count of a client's previous loans at other financial institutions. This requires a number of Pandas operations we will make heavy use of throughout the notebook:\n\n* `groupby`: group a dataframe by a column. In this case we will group by the unique client, the `SK_ID_CURR` column\n* `agg`: perform a calculation on the grouped data such as taking the mean of columns. We can either call the function directly (`grouped_df.mean()`) or use the `agg` function together with a list of transforms (`grouped_df.agg([mean, max, min, sum])`)\n* `merge`: match the aggregated statistics to the appropriate client. We need to merge the original training data with the calculated stats on the `SK_ID_CURR` column which will insert `NaN` in any cell for which the client does not have the corresponding statistic\n\nWe also use the (`rename`) function quite a bit specifying the columns to be renamed as a dictionary. This is useful in order to keep track of the new variables we create.\n\nThis might seem like a lot, which is why we'll eventually write a function to do this process for us. Let's take a look at implementing this by hand first. "
"### Kernel Density Estimate Plots\n\nThe kernel density estimate plot shows the distribution of a single variable (think of it as a smoothed histogram). To see the different in distributions dependent on the value of a categorical variable, we can color the distributions differently according to the category. For example, we can show the kernel density estimate of the `previous_loan_count` colored by whether the `TARGET` = 1 or 0. The resulting KDE will show any significant differences in the distribution of the variable between people who did not repay their loan (`TARGET == 1`) and the people who did (`TARGET == 0`). This can serve as an indicator of whether a variable will be 'relevant' to a machine learning model. \n\nWe will put this plotting functionality in a function to re-use for any variable. "
We can test this function using the `EXT_SOURCE_3` variable which we [found to be one of the most important variables ](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) according to a Random Forest and Gradient Boosting Machine. 
"## Imports\n\nWe are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. "
"## Read in Data \n\nFirst, we can list all the available data files. There are a total of 9 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 6 other files containing additional information about each loan. "
"> many reference & image from [matplotlib cheatsheet](https://github.com/rougier/matplotlib-cheatsheet)\n\n***This is a notebook which organizes various tips and contents of matplotlib which we browse every day.***\n\nI am a developer who loves visualization.\n\nSo far I've built a kernel to share the tips I've gained from doing a lot of visualizations.\n\n**matplotlib** is the most basic visualization tool, and even if you use it well, sometimes you don't need to use the rest of the visualization tools.\n\n### Table of Contents \n\n\n0. **Setting**\n    - dpi\n    - figsize\n    - title\n1. **Alignments**\n    - subplots, tight_layout\n    - subplot2grid\n    - add_axes\n    - add_gridspec\n2. **Colormap**\n    - diverging\n    - qualitative\n    - sequential\n    - scientific\n3. **Text & Annotate & Patch**\n    - parameter\n    - text example\n    - patches example\n4. **Details & Example** \n    - font weight, color, size, etc\n    - Horizontal and Vertical (barplot)\n    - Border(edge) color and thickness\n    - Main Color & Sub Color\n    - Transparency\n    - Span\n5. **MEME**\n    - xkcd style"
"## 0. Setting \n\nSet the resolution through the **dpi** (Dots per Inch) setting of the figure.\nmatplotlib has a low default resolution itself, so setting this up is a bit more professional.\n\n`plt.rcParams['figure.dpi'] = 200` or `dpi=200`\n\nAnd for every plot set **figsize**.\nThe graph gives a very different feeling depending on the ratio. (I think it's best to try this heuristic multiple times.)\n\nThere are many places in matplotlib where you can write **titles** based on objects such as `plt.title ()`, `ax.set_title ()`, `fig.suptitle()`. If you add a title that takes into account font size and font family, fontweight, position, etc., it will be more readable."
"## 1. Alignments\n\n> The first nine graph plots (3 by 3) are a combination of matplotlib layout and design.\n\n- `subplots`\n- `subplot2grid`\n- `add_axes`\n- `gridspec`, `add_subplot`\n- `inset_axes` \n- `make_axes_locatable`\n\nTwo or more graphs are much more visually and semantically better than just one.\n\nThe easiest way to do this is to place the rectangles of the same shape.\n\nUsually you can start with the initial size with subplots."
"The first of the `plt.subplot()` parameters specifies the number of rows and the second the number of columns.\nThe graph looks a bit frustrating. In this case, you can use `plt.tight_layout()` to solve the frustration."
"But should it be the same size depending on the subplot?\nFor example, bar graphs and pie charts are often very different in ratio.\n\nIn this case, the layout should be different.\n\nIn this case, you can easily use the grid system using `plt.subplot2grid`.\nIf you're a ***front-end developer***, it may be easier to understand."
"Alternatively, you can use `plt.add_axes()` to create an ax where you want."
"Another way is to use gridspec. This allows you to use `add_subplot` together, similar to subplots to grid.\n\nThis approach allows you to take advantage of the concept of `list` to use a developer-friendly grid."
"Here you can change the color of ax or plt itself, such as facecolor, to make it look more dashboard-like."
"## 3. Text & Annotate & Patch\n\nMany people often end up with just a picture in the graph, but the detail of the graph comes from the description. Just putting text on a specific part can change the feel of the graph.\n\n`ax.text` and `ax.annotate` are almost similar, but each has a different purpose.\n\n- In `ax.text`, The `first two numbers` represent the ratio coordinates in the graph. \n- In `ax.annotate`, `xy` represent the coordinates in the graph. \n\n- `va`, `ha` is a parameter that determines whether the current coordinate is the center of the text or the left / right of the text.\n- `color` stands for color, and you can enter a custom color or rgb value directly.\n- `bbox` sets an element for the box that wraps the text.\n    - Internal color (`facecolor`) and edge color(`edgecolor`) can be set separately.\n    - You can adjust the space by setting `pad`ding like in html.\n    - You can use the `boxstyle` to adjust the end of the rectangle."
"Using a patch with text is more effective.\n\nExcept for path patches, they are provided by default, so you can use them well.\n\n**Arrow is especially effective.**"
# Loading packages 
# Exploring at the meta data 
"Ok, that's good! We only have one age value per patient or none at all. "
### Insights\n\n* Most of the patients are older than 40 years. \n* It seems that we have two peaks around the age of 50 and close to 70. \n* There is a drop of patients counts after the age of 70. \n* For patients with cancer it's more likely that they are older and above the age of 50. 
## Image features 
### Insights\n\n* The laterality is quite balanced. \n* In the data description we can find that there are usually two views per breast. The most common views are CC and MLO and given these 2 views for the right and left breast we end up with 4 images that most of the patients show.  \n* Only a very few images show implants. \n* Most of the images show medium dense images of category B and C. Nonetheless there are also cases that are very dense (D) or less dense (A). Given the information that it could be more difficult to identify cancer in dense tissues this could be an interesting feature when thinking about validation strategies. 
##  Importing Dependencies\n   We shall start by importing all the neccessary libraries. I will explain the exact use of each library later in this notebook.
"#  Dataset Preprocessing\nIn this notebook, I am using **Sentiment-140** from [Kaggle](https://www.kaggle.com/kazanova/sentiment140). It contains a labels data of 1.6 Million Tweets and I find it a good amount of data to train our model."
Here are decoding the labels. We map **0 -> Negative and 1 -> Positive** as directed by the datset desciption. Now that we decoded we shall now analyse the dataset by its distribution. Because it's important that we have almost small amount of examples for given classes.
It's a very good dataset without any skewness. Thank Goodness.\n\nNow let us explore the data we having here... 
**Aaww.. It is clean and tidy now. Now let's see some word cloud visualizations of it.**\n\n### Positive Words
### Negative Words
## Train and Test Split
"As we can see from our graphs and the MSE values above, a random forest of 10 trees achieves a better result than a single decision tree and is comparable to bagging with 10 trees. The main difference between random forests and bagging is that, in a random forest, the best feature for a split is selected from a random subset of the available features while, in bagging, all features are considered for the next best split.\n\nWe can also look at the advantages of random forests and bagging in classification problems:"
"The figures above show that the decision boundary of the decision tree is quite jagged and has a lot of acute angles that suggest overfitting and a weak ability to generalize. We would have trouble making reliable predictions on new test data. In contrast, the bagging algorithm has a rather smooth boundary and has no obvious signs of overfitting.\n\nNow, let's investigate some parameters which can help us increase the model accuracy.\n\n## 3. Parameters\n\nThe [scikit-learn library](http://scikit-learn.org/stable/) implements random forests by providing two estimators: `RandomForestClassifier` and `RandomForestRegressor`.\n\nThe full list of random forest parameters for regression is shown below:"
"As you can see, when a certain number of trees is reached, our accuracy on the test set is very close to the asymptote. You can decide by yourself which value would be the optimal number of trees for your problem.\n\nThe figures also show that we achieved 100% accuracy on the training set, which tells us that we overfit. In order to avoid overfitting, we need to add regularization parameters to our model. \n\nWe will start with the maximum depth of trees `max_depth` and fix the number of trees at 100:"
Parameter `max_depth` copes well with the regularization of our model and it does not overfit as badly as before. The model accuracy has increased slightly.\n\nAnother important parameter worth tuning is `min_samples_leaf`. It also contributes to regularization.
"In this case, we do not see an improvement in accuracy on the validation set, but we significantly reduce the overfitting down to 2% while keeping the accuracy at about 92%.\n\nLet's consider the parameter `max_features`. For classification, the value $\large \sqrt{d}$ (the total number of features) is typically used as the default choice. Let's check whether it would be optimal to use 4 features in our case:"
"In our case, the optimal number of features is equal to 10. This is the value at which the best result is achieved.\n\nWe have seen how the learning curves change with different values of the basic parameters. Now, let's use `GridSearch` to find the optimal parameters for our example:"
"If we plot the original data, we can see that one of the classes is linearly separable, but the other two are not."
Let's try to use a Linear SVC to predict the the labels of our test data.
"Below are the imports needed to run the code.  The code has been written and run in Python 3.6 and 3.7 Anaconda environments.  Many of these libraries request a citation when used in an academic paper.  Note the use of the Scikit-Learn (Pedregosa et al. (2011), XGBoost (Chen & Guestrin, 2016) and LightGBM (Ke, et al., 2017) libraries for machine learning and support.  Numpy is utilized to provide many numerical functions for feature creation (van der Walt, Colbert & Varoquaux, 2011). Pandas is very helpful for its ability to support data manipulation and feature creation (McKinney, 2010).  SciPy is utilized to provide signal processing functions, especially filtering and for Pearson's correlation metrics (Jones E., et al, 2001).  The Jupyter environment in which this project is presented is a descendant of the IPython environment originated by Pérez & Granger (2007)."
Define some constants.\nThe signal constants define how the signal and Fourier transforms will be filtered to produce bandwidth limited features.
"The expected batch length increases with the batch size. It even surpasses the maximum length of 220 at batch_size=32768, and it is significantly smaller than the fixed padding at a reasonable batch size of e. g. 512. When looking at the histogram, you can also see very well that the number of outliers increases when increasing the batch size. Because we are padding to the maximum length, the expected batch size is strongly influenced by outliers.\n\nNote that the difference between the green line and the red line for each batch size does not directly relate to the speedup; there is some small overhead to dynamically padding the sequences."
"When padding to the 95th percentile of batch lengths instead, we can see another interesting pattern. The expected sequence length does not change that much when increasing batch size because it is more robust to outliers. In fact, it very quickly approaches the 95th percentile of lengths in the whole dataset!"
"*'Very well... It seems that your minimum price is larger than zero. Excellent! You don't have one of those personal traits that would destroy my model! Do you have any picture that you can send me? I don't know... like, you in the beach... or maybe a selfie in the gym?'*"
"*'Ah! I see you that you use seaborn makeup when you're going out... That's so elegant! I also see that you:*\n\n* *Deviate from the normal distribution.*\n* *Have appreciable positive skewness.*\n* *Show peakedness.*\n\n*This is getting interesting! 'SalePrice', could you give me your body measures?'*"
### Relationship with numerical variables
"*Hmmm... It seems that 'SalePrice' and 'GrLivArea' are really old friends, with a linear relationship.*\n\n*And what about 'TotalBsmtSF'?*"
"*'TotalBsmtSF' is also a great friend of 'SalePrice' but this seems a much more emotional relationship! Everything is ok and suddenly, in a strong linear (exponential?) reaction, everything changes. Moreover, it's clear that sometimes 'TotalBsmtSF' closes in itself and gives zero credit to 'SalePrice'.*"
### Relationship with categorical features
"*Like all the pretty girls, 'SalePrice' enjoys 'OverallQual'. Note to self: consider whether McDonald's is suitable for the first date.*"
"*Although it's not a strong tendency, I'd say that 'SalePrice' is more prone to spend more money in new stuff than in old relics.*\n\nNote: we don't know if 'SalePrice' is in constant prices. Constant prices try to remove the effect of inflation. If 'SalePrice' is not in constant prices, it should be, so than prices are comparable over the years."
#### Correlation matrix (heatmap style)
"In my opinion, this heatmap is the best way to get a quick overview of our 'plasma soup' and its relationships. (Thank you @seaborn!)\n\nAt first sight, there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, and the second one refers to the 'Garage*X*' variables. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. Heatmaps are great to detect this kind of situations and in problems dominated by feature selection, like ours, they are an essential tool.\n\nAnother thing that got my attention was the 'SalePrice' correlations. We can see our well-known 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hi!', but we can also see many other variables that should be taken into account. That's what we will do next."
#### 'SalePrice' correlation matrix (zoomed heatmap style)
"According to our crystal ball, these are the variables most correlated with 'SalePrice'. My thoughts on this:\n\n* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!\n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').\n* 'FullBath'?? Really? \n* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?\n* Ah... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start feeling that we should do a little bit of time-series analysis to get this right. I'll leave this as a homework for you.\n\nLet's proceed to the scatter plots."
"# PLOTLY ULTIMATE GUIDE FOR BEGINNERS📈📊\n## If you find this notebook useful, support with an upvote👍"
\n## Imports
\n### Basic Scatter PLot
\n### Scatter PLots with Variable-sized circular markers
\n### Continous Color Scatter Plots
Different marker styles available here - https://plotly.com/python/marker-style/
\n#### Single Linear Refression fit
\n#### Multiple Linear Refression fit
\n#### Single Trendline with multiple traces
"Kaggle Survey 2021\n\nIn the fifth year running this survey, Kaggle staffs are once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves them wanting to know more about the many specific communities comprised within the survey. For that reason, they’re inviting the Kaggle community to dive deep into the survey datasets and help them tell the diverse stories of data scientists from around the world.\n\nThe challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A “story” could be defined any number of ways, and that’s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!"
Our Data:
The First Question! What is your age?
"What did we find?\n\nHere, we can clearly see that majority of Kagglers are between 21 to 29 years of age.\nVery few Kagglers are over the age of 50.\nThis is not very unexpected, as many students who like to explore Machine Learning(ML) or Data Science(DS) usually start off with Kaggle. But! we are not sure about this claim as of now. This is just a speculation. Let's explore further to find out, if this is really the case."
"Next Question, What is your gender?"
"What do we see here?\n\nKaggle community mostly comprises of Male ML Enthusiasts. They consists of almost 80% of the people here on Kaggle.\nThere are also 18% Female Kagglers. The ratio is not that bad, but it can be improved upon.\nFinally, about 2% are either Non-Bianary, Do not wish to disclose or Prefer to self explain."
Where is everyone from?
"A lot of Kagglers are from India!\n\nThe higher the saturation, the more the number of people from that country. Therefore, it is clearly visible that most of the Kagglers live in India, USA, Japan and China.\nVery less number of people can be found from- Kazakhstan, Norway, Algeria, Ehiopia and Iraq.\nThis maybe due to the fact that people from that Demography, probably prefer some other profession rather than ML, AI, DataScience or Machine Learning.\nEfforts can be made to introduce students and working professionals of those countries to the world of Data! This will encourage them to explore more datasets and ML problem statements. Who knows, maybe one day, these essential skills might help solve a serious problem in the country or even the whole wide world. Think big!"
Let's take a look at Kaggler's Highest Level of Formal Education...
"Very Qualified Professionals here\n\n![image.png](attachment:289e12ba-674d-40c6-bf5a-9c6d1a830d71.png)\n\nMost of the Kagglers either hold a Master's degree or a Bachelor's degree\nFew people have a Professional Doctorate or No Formal education past High School.\nA decent amount of people have Doctoral Degree.\nWe have diversity in terms of qualifications, however, it is dominated by people holding a Bachelor's, Master's or Doctoral degrees."
"A Quick Peek, at Professional Role/Job Titles of Kagglers"
"I am Studying here...\n\nWell, I'll be honest with you guys. I thought most of the Kagglers were gonna be Data Scientists or ML Enginneers, however, after this analysis, we can see that majority of the Kagglers are Students!. That is DOUBLE! the amount of Data Scientists we have here.\nTherefore, Students, followed by Data Scientists, Software Engineers and Data Analysts are pre-dominantly the main professions here on this platform.\nThere is also an interesting CATCH here! Notice how 'Others' bar is pretty high? This goes to show that, ML, AI and working with Data is clearly attracting people from various different professions onto Kaggle! Now this is good news!\nThis is expected as we are moving into a whole new generation where we have loads of devices, sensors etc that generate BIG! data every second. With more and more availablility of data, it opens the ground to train various machine learning models and opens up a lot of opportunities to seek and explore. This has proved to be  very essential for people working in any domain!"
"Okay, but for how long have they been programming?"
"Let me just paste this one from StackOverflow...\n\nNow this is what I was expecting atleast. If you see the graphs above, you will realise that ""Our Kaggle Community is very young!"" like Marília Prata put it in her words.\nTherefore, there are many Kagglers who's Experience in Programming is just 1-3 years.\nWe see that, as the years of experience increases, the number of Kagglers in our community decreases.\nKaggle is a great place for anyone willing to step into the World of Data. There is, or at-least I would say, that there are good proportion of skilled and novice Kagglers on the platform!"
User Resource Preferences:
"print(""Python is better!"")\n\nThis is not a surprise at all! From our previous analysis, we had concluded that most of the Kagglers are Students. They have barely started to code. Therefore, for beginners, Python is a very friendly coding language to pick up and learn.\nBecause of this, most of the newbies love to use Python. Not only that, Python has a large number of modules that make life easier for any developer, analyst etc!\n\n![meme](https://i.pinimg.com/originals/a4/31/4a/a4314a37a2a0a1ce775d55b4c8b1383d.png)\n\nPython therefore, wins this battle against R on Kaggle by a large margin! Python:80% and R:20%"
"It is widely used everywhere...\n\nThough in a life of a Data Scientist, several programming languages like Java, C++, C etc are used, Python still holds a distinct place in most organizations across the globe\nAlso, as per our previous analysis, we can clearly see that many Kagglers prefer Python over several other programming languages. Hence, we can safely conclude that 7/10 people will always recommend Python over any other programming languages."
"We like it SIMPLE!\n\nJupyter Notebook is very user friendly! We simply type the codes in the cell and press Shift+Enter to execute. Simple! And the result is also displayed just below.\nSimilar case for VS Code and PyCharm. They are very user friendly! Also, there are several plugins available to make the user experience even better! Therefore, Jupyter Notebook, PyCharm and VS Code are favourite among many developers."
"https://colab.research.google.com/\n\nGoogle colab just barely edges out Kaggle Hosted Notebook services. Just like the Kaggle hosted interface, it is simple to use and has no limits on the GPU usage. Maybe that is one of the reason why people prefer this? Or is it because, users can directly get their datasets from the google drive? The fact that we can connect to our local runtime? Who knows.\nWhatever might be the case, it is worth to notice that a decent number of people still prefer to run their codes on their local machine. Probably, in my opinion if Kaggle Kernels can be made more user friendly and provide more features similar to that of Google colab, it might turn out to be a popular choice by next year!"
### 2.2. Data Visualization\nlet's take look at our data in the most raw shape.\nI really recommend scatter plot because we can get the idea of our data without any manipulation
It seems that the data suffer from outliers\n\nLet's see for example pregnency distribution
# 2. Import Packages\n[Table of contents](#0.1)
"Before going further, let's have a quick look on what is **bfloat16 floating-point format**.\n\n> This format is a truncated (16-bit) version of the 32-bit IEEE 754 single-precision floating-point format (binary32) with the intent of accelerating machine learning and near-sensor computing.\n\n> Bfloat16 is used to reduce the storage requirements and increase the calculation speed of machine learning algorithms. \n\n[wiki](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)"
"**Objective** is to build a Deep Learning model which can identify if the person is wearing a mask or not, also detecting if people vilating social distancing norms."
"### Using haar cascade to detect faces\n\nObject Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, ""Rapid Object Detection using a Boosted Cascade of Simple Features"" in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images. We'll be using a Haar Cascade Model trained to detect faces in order to obtain the bounding box coordinates of faces in an image."
Example Plot:
"First, we have to calculate P_trans from the true states:"
Here you can see each probability distribution:
"They are all Gaussian with a common standard deviation, however, there are not enough data points for low states. We could calculate smooth Gaussians for each state (I did this via calculating weighted mean / std) and linear interpolation, but it really doesn't change much. It's simply not a problem when p_signal is bad in places where there are anyway not many points.\n\nLet's visualize P_signal again in matrix form:"
We finally need to put the actual signal into these bins as well:
"## **0. Introduction**\n\nI decided to write this kernel because **Titanic: Machine Learning from Disaster** is one of my favorite competitions on Kaggle. This is a beginner level kernel which focuses on **Exploratory Data Analysis** and **Feature Engineering**. A lot of people start Kaggle with this competition and they get lost in extremely long tutorial kernels. This is a short kernel compared to the other ones. I hope this will be a good guide for starters and inspire them with new feature engineering ideas.\n\n**Titanic: Machine Learning from Disaster** is a great competition to apply domain knowledge for feature engineering, so I made a research and learned a lot about Titanic. There are many secrets to be revealed beneath the Titanic dataset. I tried to find out some of those secret factors that had affected the survival of passengers when the Titanic was sinking. I believe there are other features still waiting to be discovered. \n\nThis kernel has **3** main sections; **Exploratory Data Analysis**, **Feature Engineering** and **Model**, and it can achieve top **2%** (**0.83732**) public leaderboard score with a tuned Random Forest Classifier. It takes 60 seconds to run whole notebook. If you have any idea that might improve this kernel, please be sure to comment, or fork and experiment as you like. If you didn't understand any part, feel free to ask."
"* Training set has **891** rows and test set has **418** rows\n* Training set have **12** features and test set have **11** features\n* One extra feature in training set is `Survived` feature, which is the target variable"
"#### **1.2.4 Cabin**\n`Cabin` feature is little bit tricky and it needs further exploration. The large portion of the `Cabin` feature is missing and the feature itself can't be ignored completely because some the cabins might have higher survival rates. It turns out to be the first letter of the `Cabin` values are the decks in which the cabins are located. Those decks were mainly separated for one passenger class, but some of them were used by multiple passenger classes.\n![alt text](https://vignette.wikia.nocookie.net/titanic/images/f/f9/Titanic_side_plan.png/revision/latest?cb=20180322183733)\n* On the Boat Deck there were **6** rooms labeled as **T, U, W, X, Y, Z** but only the **T** cabin is present in the dataset\n* **A**, **B** and **C** decks were only for 1st class passengers\n* **D** and **E** decks were for all classes\n* **F** and **G** decks were for both 2nd and 3rd class passengers\n* From going **A** to **G**, distance to the staircase increases which might be a factor of survival"
"* **100%** of **A**, **B** and **C** decks are 1st class passengers\n* Deck **D** has **87%** 1st class and **13%** 2nd class passengers\n* Deck **E** has **83%** 1st class, **10%** 2nd class and **7%** 3rd class passengers\n* Deck **F** has **62%** 2nd class and **38%** 3rd class passengers\n* **100%** of **G** deck are 3rd class passengers\n* There is one person on the boat deck in **T** cabin and he is a 1st class passenger. **T** cabin passenger has the closest resemblance to **A** deck passengers so he is grouped with **A** deck\n* Passengers labeled as **M** are the missing values in `Cabin` feature. I don't think it is possible to find those passengers' real `Deck` so I decided to use **M** like a deck"
### **1.3 Target Distribution**\n* **38.38%** (342/891) of training set is **Class 1**\n* **61.62%** (549/891) of training set is **Class 0**
### **1.4 Correlations**\nFeatures are highly correlated with each other and dependent to each other. The highest correlation between features is **0.549500** in training set and **0.577147** in test set (between `Fare` and `Pclass`). The other features are also highly correlated. There are **9** correlations in training set and **6** correlations in test set that are higher than **0.1**.
# What are the strategies to earn discussion medals?
## 1. Introduction\n\nFrom my experience I noticed that silver and gold discussion medals (5 and 10 upvotes respectively) are much harder to get compared to bronze discussion medals (1 upvote). So I began wondering **how forum messages with silver/gold medals are different from messages with bronze medals?**\n\nIn this kernel I will present my analysis and for this purpose will use a number of Python libraries:\n\n- `pandas` - working with DataFrames\n- `BeautifulSoup` and `re` - cleaning messages from HTML tags\n- `spacy` - natural language processing (NLP)\n- `TSNE` and `KMeans` from `sklearn` - dimension reduction and clustering\n- `seaborn` - visualization\n- `Counter` from `collections` - counting words\n- `bokeh` - interactive visualization
"In order to use Python in Bokeh callback functions, it is also required to install the module `pscript`."
"Finally, the messages are visualized on the scatter plot."
It looks like there are at least 3 clusters present here. Let's use `KMeans()` to identify these clusters and add the cluster labels to the DataFrame. I have also added message tokens to the DataFrame to analyze the most frequent words in the clusters.
The scatter plot of the labeled clusters is shown below.
"Since `KMeans` assigns labels in random order, they might be completely different after commiting the kernel. Therefore, I print the most common words of all 3 clusters below and discuss them in no particular order.\n\nIn one of the clusters the top words are ""kernel"", ""thank"", ""great"", ""upvote"", ""share"", etc. One could think of possible phrases from these words such as ""great kernel"", ""thanks for sharing"", etc. These messages probably show appreciation to kernels. \n\nIn another cluster the top words are ""thank"", ""work"", ""nice"", ""great"", ""share"", etc. These are very similar to the words from the previous cluster with an exception of ""kernel"". These messages probably show appreciation in general.\n\nIn another cluster the top words are ""datum"" (lemmatized form of ""data""), ""model"", ""thank"", ""kernel"", ""good"", etc. This is probably a mix of messages that discuss models/kernels and show appreciation. Another possible reason for this mix is that the clusters are not separated very well on the plot and `KMeans` might mistakenly assign a part of one cluster to the other."
"# 1. Introducing the subject \n**Ahoy**! If you are like me, you've always been looking up to the **top Data Scientists**, the **cream of the community**, the very few that, somehow, through some *magic* only they know, manage to understand, teach and perform like none others.\n\n**But how**? This survey is an opportunity to take this curiosity very *close and personal* in the search for the treasure that might reveal us the steps to reach that greatness.\n\n**Buckle up pirates; the treasure hunt is ON**.\n\nCriteria\n\nUnfortunately, there will need to be some *bias* involved.\n\nIn the Kaggle Survey, there is no feature or clue to identify which of the respondents are Masters/Grandmasters - neither the rank nor any performance within the Kaggle Community.\n\nHence, I had to *define* what is a *successful* data scientist. Some assumptions were:\n\n\n\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:""#010307"";\n    text-align:center;\n    vertical-align:top}\n\n\n\n  \n    Criteria\n    Value\n    Strength\n    Flaw\n  \n\n\n  \n    Level of Education\n    Masters / Doctoral\n    Majority of Grandmasters havesome level of upper education\n    Many bright data scientists areself-taught\n  \n  \n    Years of Programming/ML\n    4 - 5+ years\n    Might show increased experience\n    The passing of time doesn't reflecthow much one has learned\n  \n  \n    Pay\n    USD 100,000+\n    Very high pay can signal high skill\n    High pay doesn't guarantee skill;Regional bias also involved\n  \n  \n    Spending\n    USD 100,000+\n    A company investing thousands in MLis investing in bright DS employees\n    How much a company spends on ML doesn'tdefine the skill of the employer\n  \n\n\n\n\n  In the end, I decided to use pay as my delimiter. Firstly, I consider that it has the lowest bias out of all. Secondly, I could lower the regional bias significantly (more of that in the next chapter). I would also believe that highly skilled people usually have very high pay and the exceptions from that rule aren't that many.\n\n\n#### Libraries below ⬇"
"# 2. How many McMeal menus can you buy?\n\n## 2.1 The problem\nOk, so we agreed on using the pay as the indicator. However, this feature has many issues on its own:\n\n\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:""#010307"";\n    text-align:center;\n    vertical-align:top}\n\n\n\n  \n    No.\n    Problem\n  \n\n\n  \n    1.\n    It doesn't take into account the purchasing power (e.g., a person in Ukraine with 90k a year may buy more ""valuables"" in their country than another person in Japan with 150k a year).\n  \n  \n    2.\n    It might be misunderstood: there might be people that completed the salary in their base currency. Nevertheless, this column is expressed in dollars.\n  \n  \n    3.\n    More than 50% of the respondents didn't respond altogether, so only half remain available.\n  \n  \n    4.\n    There are the occasional trolls who might give a lower or a much higher salary than they actually have.\n  \n\n"
"> Hence ... **what do we choose**? We can't just take all people with a salary > 80k - this would be highly bias and inefficient (80k is lots of money in the UK, but not that much in the US). Unfortunately, points *2.*, *3.* and *4.* are systematic issues, so we'll have to go ahead and trust the Kagglers that they completed the *pay* question to the best of their abilities.\n\n## 2.2 The Solution 🍟\n\n**Thanks to a good friend** who gave me this idea, we can all have a snack break now.\n\n\n\nA McMeal may solve our regional bias problem. Instead of using pay, we can look around the world at **how many McMeals can one respondent buy with their salary in their own country**. Afterward, we can use the McMeal(units) as our non-bias indicator.\n\nNow, the only thing that remains is where do we draw the line in the McMeal units?"
"About 2,300 people have at least 10,000 Meals or more (10,000 threshold was chosen looking at Q3). I will be analyzing these people from now on. However, I will also be segmenting them into three categories:\n\n\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:""#010307"";\n    text-align:center;\n    vertical-align:top}\n\n\n\n  \n    Category Name\n    Meaning\n    Number of respondents\n  \n\n\n  \n    High\n    10,000 - 20,000 units: These people can buy more McMeals than more than 75% of our base users.\n    ~1600\n  \n  \n    Very High\n    20,000 - 50,000 units: These people can buy more McMeals than more than 90% of our base users.\n    ~700\n  \n  \n    Crazy High\n    50,000 + units: These people can buy more McMeals than more than 99% of our base users.\n    ~100\n  \n\n"
"## The Pareto Principle\n\nOur McMeal units distribution match the Pareto Principle very well: [roughly 80% of consequences come from 20% of the causes](https://en.wikipedia.org/wiki/Pareto_principle).\n\nHence, we are noticing a **""top 20%"" of the ""top 20%"" of the ""top 20%""** situation, meaning:\n* out of 9,893 total respondents, 2,301 (*25%*) have more than 10,000 units\n* out of the 2,301 respondents, 705 (*~ 20%*) have more than 20,000 units\n* out of the 705 respondents, 100 (*~ 15%*) have more than 50,000 units\n\n> What does this mean? It doesn't matter to which category you look, the distribution is always going to be **skewed to the right**. This is why segmentation is so important here."
"# 3. On our way to finding the treasure\n\nAlright! We established our feature, normalized it, excluded the regional bias, and segmented our target respondents. We can say **our map** is already laid out in front of us (well structured and ready, but still empty); hence we now need to follow the steps to our treasure.\n\n\n  The map is BLANK for the moment, but it will start revealing itself once we begin discovering new Realms.\n\n\n\n*📌 Note: From now on, when I'll mention **pay**, I will refer to the units we got after the pay normalization on the McMeal units :)*\n\n3.1 The Personal Profile\n\nWhere do they reside?\n\nSome pointers we observe here:\n* The top countries do **match the profile** of the majority **of masters/grandmasters** on Kaggle.\n* The top 2 countries for all categories are **the USA** and **India**.\n* **Japan, China, Indonesia, Russia, Canada, and the UK** are the other 6 places where these highly skilled people reside."
"Who are they?\n\nLet's take it step by step:\n* As the percentage of the entire population of the survey is **mostly formed by males**, the ""high end"" people we're studying match accordingly.\n* Looking at age, we can observe that the *average* age increases by category:\n    * High Pay: the average age is around *35* yo\n    * Very High Pay: average age starts moving towards *40 yo and 45 yo* (these bars start to rise)\n    * Crazy High Pay: the 30s drop suddenly, whereas the beginning of *40s* stays the same. Later age (*45, 50s*) are also visible.\n* Hence, there is a clear, **direct correlation between high pay and age** - the older, the wiser, the wealthier.\n\nMy personal opinion is that the 20s and 30s are hectic anyway, and the golden ages are still after 40. Glad that this survey also reflects that. 😁 [@Dieter](https://www.kaggle.com/christofhenkel) might agree with me as well. 👀"
"\n  Takeaway: We now know that the high end of respondents is mostly from the US and India, but also located in Japan, China, Indonesia, UK, Russia, or Canada, and have a Male 30-45 yo profile.\n  However, these ""personal"" aspects don't define an outstanding Data Scientist. Hence, we'll start from now on to look at education, expertise, work ethic, and knowledge, rather than focusing on biological, racial, or other environmental aspects.\n\n\n> Oh, and look! The first portion of the map is clear now!\n\n\n\n> Closer look 🔎\n\n\n3.2 The Education\n\nAs for education, the 3 Pay Categories differentiate through:\n* *High Pay* and *Very High Pay* have very similar distributions. **50% of the respondents** have or plan to complete a **Masters's degree** in the next 2 years.\n* However, *Crazy High Pay* steals ~15 percentage points from the *Master* category and adds to the **Doctoral** category.\n* Hence, the majority of extremely well-paid respondents choose to continue their superior studies to a Doctoral."
"\n  Takeaway: The high-end respondents have mostly a high to very high education; the majority have at least a Masters's complete. The difference between High Pay and Crazy High Pay is in majority's choice to pursue their passion further to a Doctoral.\n  As you may know, indeed, many Masters/Grandmasters discuss (on Twitter, podcasts, etc.) that they have a Doctoral in a field supported by Data Science work.\n\n\n> And the Education Mountains revealed themselves!\n\n\n\n> Closer look 🔎\n\n\n3.3 The Expertise and Work Environment\n\nHere is our chance to take a glimpse into the work environment and skills acquired. How long does it take to get there? How hard do we need to work? What job roles should we pursue?\n\nFor how long have they been practicing?\n\nThe most important points here:\n* These exceptional people have **more coding experience** than ML in terms of time.\n* Looking at the coding expertise, we see more than 50% of them having **10+ years of coding experience**.\n* Looking at the ML expertise, the donut is more evenly split between **1 and 10 years**, with *less than 10% having 10+ years of experience*.\n* What does this mean? It means that exceptional people do have lots of coding experience but NOT ML necessarily, and most of them are on Kaggle because they might have recently (or in the last years) found their passion in data."
"Role and Duties at Work\n\nPointers here:\n* The most frequent jobs for the top respondents are Data Scientist, ML Engineer or Software Engineer (Analyst incorporates 3 jobs - Data Analyst, Business Analyst, and Statistician).\n* Some of the most crucial duties are:\n    * *Software Engineer*: more oriented towards **data infrastructure, exploration, and creating ML models**.\n    * *ML Engineer*: most prominent duties are for **building and exploring new ideas for ML models**.\n    * *Data Scientist*: the **most versatile** out of all, it incorporates almost equally all the duties (however, lower in the research areas). Basically, they need to know everything 👀."
"How large are the company and the team?\n\nThe 2 plots match very well the overall distribution of the respondents ([you can see the overall summary here](https://www.kaggle.com/kaggle-survey-2020)):\n* On average, most respondents (regardless of their pay) are located in **small companies in 20% of the cases**, and more than **50% of the cases in large ones** (1000+ employees).\n* Also, the team size is **half the time bigger than 10 people**; however, **40% of cases are in small teams, of a maximum of 4 people**. This is a direct implication of the companies' size (there are many respondents in tiny companies, hence smaller DS teams and vice versa)."
"The Interest of the Company in Machine Learning\n\nThis question *might have some bias* because some people could have guessed a rough estimate, as they might not know of the ""business side"" of the company. However, because we're talking about high-end Data Scientists, who most certainly are involved in their projects' finances, this bias might be lower.\n\n*Or are they?*\n\nThe following graphs show some fascinating insights:\n* Firstly, respondents around **all groups** agreed that their company is either using  well established ML - or - they've just started implementing ML into the business ([and these numbers are increasing as the years pass, according to the general summary](https://www.kaggle.com/kaggle-survey-2020)).\n* We can see that the respondents with **High Pay** are located in companies that spend much less on ML than the other 2 groups. In the **Very High Pay** and **Crazy High Pay** groups, *more than 50% of the respondents* are employed in companies that *spend tens of thousands of dollars* on their ML equipment and team.\n* Hence, the graph shows that the bigger the individual pay, the more the company invests in ML in its business model."
"\n  Takeaway: We've learned that our high-end respondents are usually Data Scientists or Software/Machine Learning Engineers, with lots of coding experience, but not necessarily ML seniority.\n   They come from large and small companies, but the higher the investment and interest of the company in ML, the higher the individual income received.\n\n\n> And another area of the map has revealed itself: the Work Habitat!\n\n\n\n> Closer look 🔎\n\n\n3.4 The Coding Preferences\n\nNow we know that more than half of our respondents have 10+ years of coding experience. But let's discover what languages they use most, what is their DS setup, how do they deal with cloud and big data, and what advice they have to share.\n\nLanguages Used and Advice to Community\n\nMost coders secretly root for Python, but are all the other languages obsolete?\n* **Python** is by far the most popular language in most areas, both in **usage** and as **a recommendation** for future users.\n* The next 3 most used languages are **SQL, R, and Bash**, all very useful in the Data Science discipline.\n* Hence, all our top respondents (regardless of the Pay Category) use and recommend the most Python, followed by SQL, R, and Bash."
"What's the best setup?\n\nThe graphs below are about the same for all 3 Pay Categories:\n* Most work on their **personal laptop/computer**.\n* When working on the personal computer, the usage is usually oriented towards the classics: **Jupyter, RStudio, and PyCharm**.\n* They also use cloud computing services, like **Google Cloud Datalab/ AI Platform**, but most excessively **Colab** or **Kaggle Notebooks**.\n* To conclude, the top data scientists have *similar behavior in terms of environments and IDE with the average Kaggler*: use the most the personal gear, Colab and Kaggle Notebooks and work on a combination of Jupyter, PyCharm, and RStudio environments."
"Accelerators : Yay or Nay?\n\nFirst, let's understand some concepts:\n\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:""#010307"";\n    text-align:center;\n    vertical-align:top}\n\n\n\n  \n    Accelerator\n    Explanation\n  \n\n\n  \n    GPU (Graphics Processing Unit)\n    Designed to rapidly manipulate and alter memory to accelerate the creation of images. Used greatly in all areas of Data Science.\n  \n  \n    TPU (Tensor Processing Unit)\n    AI accelerator developed by Google specifically for neural network machine learning. Mostly used in Deep Learning problems.\n  \n\n\n\n\nIs GPU better? Technically no - either if you choose CPU, GPU or TPU, they all outperform in some areas and underperform in others. [Learn more about this comparison here.](https://analyticsindiamag.com/tpu-vs-gpu-vs-cpu-which-hardware-should-you-choose-for-deep-learning/#:~:text=TPU%20vs%20GPU%20vs%20CPU%3A%20A%20Cross%2DPlatform%20Comparison&text=TPU%3A%20Tensor%20Processing%20Unit%20is,small%20batches%20and%20nonMatMul%20computations.)\n\nSo, what are the top Data Scientists using to move fast during competitions and achieve the best scores? We know that they mostly use their personal laptop/ workstation, but how do these look like?\n* The visualization below is *representative* of all 3 Pay Categories.\n* Respondents are *split in half* (with very few exceptions): **~44% use no Acceleration**, while the other **44% use GPUs** most often.\n* Most of this **88% percent have never even tried TPUs** before, and if they did, they only used it between 2 to 5 times.\n* TPU users are **~ 7%** out of all respondents; however, half of them have been using TPUs more than 6 times (and around a quarter of them more than 25 times).\n* Hence, the vast majority of our respondents use GPU acceleration; however, there are a few very experienced TPU users that rely mostly on TPU during their work/competitions. Moreover, this graph is also a relief for beginners: you don't necessarily need heavy computing power to be very good or earn well."
# Setup TPU configuration
## 1.2 Check the target
"- Wow, very-well balanced target! Fun with this competition :)."
Designed and run in a Python 3 Anaconda environment on a Windows 10 computer.  
### Code Setup
"These are the needed library imports for problem setup.  Many of these libraries request a citation when used in an academic paper.  Numpy is utilized to provide many numerical functions needed in the EDA (van der Walt, Colbert & Varoquaux, 2011). Pandas is very helpful for its ability to support data manipulation (McKinney, 2010).  SciPy is utilized to provide signal processing functions (Jones E., et al, 2001).  Matplotlib is used for plotting (Hunter, 2007).  The Jupyter environment in which this code is presented and was run is a descendant of the IPython environment originated by Pérez & Granger (2007)."
Define some constants for data location.
"A basic time series plot of the raw data.  Because of the length of the data, the plot samples every 100th data point. These plots are very common on the Kaggle site's kernels section, this one is taken from Preda (2019).  Earthquakes occur when the time-to-failure signal (blue) jumps up from very near zero to a much higher value where that new higher value is the time to what is then the next quake.  There appears to be a short term high amplitude oscillation very shortly before each quake.  But, there also several similar such peaks that occur nearer the region centered in time between quakes.  Signal noise seems to increase as time gets closer to failure, though there is also a drop after the big peak.  A signal's standard deviation may prove to be a helpful predictor.  The region just after the big peak may be especially hard to predict."
"From the above plot we can see that 16 earthquakes occur in the data.  The earthquakes happen when the time-to-failure reaches very nearly zero and then jumps up.  There are only 15 complete time ramps that result in an earthquake and 2 incomplete time ramps.  One challenge of this competition is that there are only these very few earthquakes to work with.  The is a signal spike (high amplitude) just before an earthquake, but there are also signal spikes in other places that may complicate matters.  While the acoustic signal is very large at over 600m rows, the very small number of actual earthquakes available will make machine learning a challenge."
"Having validated that the test samples are 150,000 samples long, it could help to examine the training data in 150,000 sample chunks.  Here are some random samples of the training data and plots of the acoustic signal along with the time-to-failure.  Note that the y-axis scales vary for all plots.  It is very interesting that the high level spike in the signal occurs in row 3, column 1 in a signal only 0.32 seconds before failure.  This spike reaches vales above 2000 and below 4000.  A similar situation occurs in row 4, column 2.  Contrast this with other plots where the time-to-failure is many seconds away and the signal level peaks around 100.  Time-to-failure plots as a ramp because we are slowly approaching the next quake as the signal progresses in time.  Apparently the time-to-failure has limited resolution and so is represented by a stair step rather than a smooth line.  It appears that if signal spikes could be captured in some way it might help the effort to model time-to-failure.  Somewhat shockingly, we are trying to predict an earthquake from only around 0.04 seconds or so worth of data, as can be noted from examining the right hand axes of the plots. "
"Find the indices for where the earthquakes occur, then plotting may be performed in the region around failure."
"Below is a look at the signal just before failure (an earthquake).  It is very interesting that the signal becomes quiet in the 150k sample slice before an earthquake.  Thus, the signal spike observed in the big picture plot above must occur more than one slice (more than 150k samples) before the earthquake.  These do not appear much different than plots 5 or even 8 seconds before the quake that are presented above where the time to failure ramps are shown.  This looks like it will create major problems for accurate prediction.  Earthquakes are deemed to have occurred where the blue line jumps up in value, representing the time to the next quake."
"By expanding the time to failure plots, it appears that the big spike in the signal before failure is remarkably consistent.  The problem is that the 150k sample slices are actually very short compared to the overall time between quakes and thus these big apparently meaningful signal spikes are unlikely to be present in many training or test samples.  If slicing the training data directly into 150k chunks, then only 16 of 4194 training samples (0.38%) would contain a meaningful high-valued spike.  It is possible that the analysis of these spikes in some manner could add predictive capability to any small number of test samples that might contain these features, this possibility has not been explored by this author yet."
"A check of the test data in the time domain is presented below, it is difficult to tell from such short signal bursts if they match the character of the training data.  None of the very high valued signal spikes were caught in a partial look at the test data.  However, these are rare and the existence of the spikes will be taken up again later. "
"Frequency components of the signal could be very interesting to look at.  This is a plot of the Fourier transform magnitude for some of the test signals.  Note that there appears to be little information in the signal above the 20,000th frequency line.  Noise appears to mostly disappear above the 25,000th frequency line.  It is difficult to translate this to a frequency because of the signal gaps noted earlier.  Still, it may be best to concentrate signal analysis on frequencies below those represented by the 20,000th frequency line.  Also, there are peaks in the frequency analysis that may be valuable to collect in some manner.  The DC component was eliminated for plotting purposes because it would otherwise dominate the plot and make the other frequencies hard to see.  Also note that while referred to as an ""FFT"" in the code below, this is actually a Discreet Fourier Transform (DFT) because the signal length of 150k samples is not a number that is a power of two."
 RandomForestClassifier  
"> **Accuracy is determining out of all the classifications, how many did we classify correctly? This can be represented mathematically as:**\n\n"
**Library and Data **
**Model with plots and accuracy**
"# Logistic Regression \n**It’s a classification algorithm, that is used where the response variable is categorical. The idea of Logistic Regression is to find a relationship between features and probability of particular outcome.**   \n* odds= p(x)/(1-p(x)) = probability of event occurrence / probability of not event occurrence \n\n**Example- When we have to predict if a student passes or fails in an exam when the number of hours spent studying is given as a feature, the response variable has two values, pass and fail. \n**\n![multinomial-logistic-regression-with-apache-spark-4-638.jpg](attachment:multinomial-logistic-regression-with-apache-spark-4-638.jpg)"
## **Import Modules** 
* With this code below you can check if the kernel use GPU or not.
\n# Importing Libraries
\n# Basic Exploration
\n# About Dataset
\n# Custom Palette For Visualization
\n# Top Anime Community\n
**Insights:**\n\n* **Death Note** wears the crown for highest community members followed by **Shingeki no Kyojin** and **Sword Art Online**
"Once trained our model, we can then visualize how changing some of its Hyperparameters can affect the overall model accuracy. In this case, I decided to observe how changing the number of estimators and the criterion can affect our Random Forest accuracy."
"We can now evaluate how our model performed using Random Search. In this case, using Random Search leads to a consistent increase in accuracy compared to our base model."
And this is distribution of # of sub-questions.
"## 1. Simple Distribution (Age, Gender, Country)\n\n### About Q1, Q2, Q3\n\n- Q1. What is your age (# years)?\n- Q2. What is your gender? - Selected Choice\n- Q3. In which country do you currently reside?\n\n\nLet's look at a simple distribution first.\n\nWhat kind of distribution does Kagler have? \n\nI have an [AI-related Facebook page](https://web.facebook.com/AI.Lookbook) with about 2000 followers in Korea and compare it lightly with the distribution.\n"
"I can certainly see that the proportion of female is lower than that of male.\n\nThe distribution of my Facebook page is:\n\n![img](https://i.imgur.com/00tja7U.png)\n\nBoth data vary by about 5 to 6 times, depending on gender.\n\nThis distribution corresponds to the entire engineering world, including AI.\n\nI hope to see more female AI researchers.\n\n"
"India and USA are overwhelming compared to other countries.\nIt's almost the opposite of Earth, but it's fun, although it's not special. (It reminds me of Sheldon and Rajesh of the *Big Bang Theory*.)\n\n\n![bigbang](https://media.giphy.com/media/SXJfIASq4Ayxq/giphy.gif)\n> img from https://giphy.com/gifs/the-big-bang-theory-sheldon-cooper-jim-parsons-SXJfIASq4Ayxq\n\nLet's look at the map and graph of the ratio of men and women by country."
Table of Content:\n\n1. Data Handling\n1.1. Getting sense of the data\n1.2. Univariate and Bivariate Data Exploration\n1.3. Treatment of data for missing values\n1.4. Categorical Data Essense\n1.4.1. About Categorical Data\n1.4.2. Treatment Techniques for Categorical Data\n1.4.3. One hot encoding implementation\n1.5. Data Scaling\n2. Model Training\n2.1. What is Regression?\n2.2. Linear Regression\n2.3. Ridge and Lasso Regression\n2.3.1. Lasso Regression Implementation\n2.3.2. Ridge Regression Implementation\n3. Stats Model Interpretation and Backward Elimination Technique\n3.1. Selection Techniques in Multiple Regression\n3.2. How to perform backward elimination?\n3.3. Why Stats Model Library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n
# 1. Data Handling\n
## 1.2. Univariate and Bivariate Data Exploration\n
"### Few observations from the above plot\n1. x,y, and z have a very strong relation with price but surprisingly depth (which comes from x,y, and z) doesn't has a significant relation with price.\n2. Carat has a strong relation with price\n3. Table doesn't have a significant relation with price or any other variable as well ( We can try dropping that when making the model) "
Since the dataset is big enough dropping 20 rows shouldn't cost us much hence the nans have been dropped.
## 1.4. Categorical Data Essense\n
* The figure above shows the decision boundary of a decision tree and k-NN classifiers along with their bagging ensembles applied to the Iris dataset. The decision tree shows axes parallel boundaries while the $k=1$ nearest neighbors fits closely to the data points. The bagging ensembles were trained using $10$ base estimators with $0.8$ subsampling of training data and $0.8$ subsampling of features. The decision tree bagging ensemble achieved higher accuracy in comparison to k-NN bagging ensemble because k-NN are less sensitive to perturbation on training samples and therefore they are called *stable learners*. Combining stable learners is less advantageous since the ensemble will not help improve generalization performance.
* The figure above shows learning curves for the bagging tree ensemble. We can see an average error of $0.3$ on the training data and a U-shaped error curve for the testing data. The smallest gap between training and test errors occurs at around $80\%$ of the training set size.
# Importing all Libraries...
"# Here is THE Big TRICK...\n\nBy default running the `fetch_lfw_people()` function downloads the data into the '**~/scikit_learn_data**' subfolders. This is not a big deal when you run the notebook on your **Local PC**. But, this becomes ***Shooting a STAR*** when you do the same in a **KAGGLE KERNEL**.\n\nIts *hard to locate* where your data gets downloaded when you run the above function. Matter of relief, I have collected the data from Sklearn's dataset folder, and uploaded here. So, you can easily get to know that data is available at location: '**../input**'.\n\nAgain, you get *a hit on your nose*; Because, You can not fetch and process data there as '**../input/lfw_people/**' has '***READ ONLY***' permission.\n\n### Solution:\n\nI ***created*** a folder named '**../LFW/lfw_people**'. And, set it as the **path**. So, the next time my fetching function will access and process data here. Then, I **copied** my complete dataset to this location(* Moving is also a great option*).\n\nNow, It is behaving exactly like its running on your Local PC. Hurray !\n"
#### Visualizing the Eigen Faces (reduced components of faces).
#### Visualising the prediction by plotting with Faces and train-test Prediction pairs.
"## General information\n\nRussian community on Kaggle is quite strong. We started taking part in Kaggle competitions long ago and most competitions have at least several teams in medal zone, currently there are a lot of russian Grandmasters. It is worth noticing, that most of them are a part ods.ai - an open DS community, which has more than 38k users.\n\nIf we look at other countries, we can see that the number of respondents in Russia is in the 5th place in the overall ranking (excluding ""other"").\n\nMore than 600 Russians took part in the survey this year. Why this number is lower than the last year? I think this year the information of the survey was less spread, not sure why."
"Not surprisingly, the average age is increasing over time. One of the reasons is that people grow older over time (obviously :)). Another reason - more elder experts are switching career from other spheres to DS.\n\nIt is interesting to notice that most of women are younger than men. I suppose it means that more women go into DS after university which is great!"
"#  About Kiva and the challenge\n***\n\nKiva is a non-profit organization that allows anyone to lend money to people in need in over 80 countries. When you go to kiva.org, you can choose a theme (Refugees, Shelter, Health ...) or a country and you'll get a list of all the loans you can fund with a description of the borrower, his needs and the time he'll need for repayment. So far, Kiva has funded more than 1 billion dollars to 2 million borrowers and is considered a major actor in the fight against poverty, especially in many African countries.\n\nIn this challenge, the ultimate goal is to obtain as precise informations as possible about the poverty level of each borrower / region because that would help setting investment priorities. Kagglers are invited to use Kiva's data as well as any external public datasets to build their poverty estimation model.  \nAs for Kiva's data, here's what we've got : \n* **kiva_loans** : That's the dataset that contains most of the informations about the loans (id of borrower, amount of loan, time of repayment, reason for borrowing ...)\n* **kiva_mpi_region_locations** : This dataset contains the MPI of many regions (subnational) in the world.\n* **loan_theme_ids** : This dataset has the same unique_id as the kiva_loans (id of loan) and contains information about the theme of the loan.\n* **loan_themes_by_region** : This dataset contains specific informations about geolocation of the loans.\n\nThis notebook will be divided into two parts : \n1. First I will conduct an EDA using mainly the 4 datasets provided by Kiva. \n2. After that, I'll try to use the informations I got from the EDA and external public datasets to build a model for poverty level estimation."
"# 1. Exploratory Data Analysis\n\n*** \nIn this part, the goal is to understand the data that was given to us through plots and statistics, draw multiple conclusions and see how we can use those results to build the features that will be needed for our machine learning model. \n\nLet's first see what this data is about."
"Philippines is the country with most borrowers with approximately 25% of all users being philippinians. Elliott Collins, from the Kiva team, explained that this is due to the fact that a couple of Philippine field partners tend to make smaller short-term loans (popular low-risk loans + fast turnover rate). \n\n\nWe also notice that several african countries are in the list such as *Kenya, Mali, Nigeria, Ghana ...* and no european union country at all !     \nFor me, the most surprising was actually the presence of the US in this list, as it doesn't have the same poverty rate as the other countries but it turns out it's indeed a specific case, **I'll explain that in 1.4**.\n\nLet's now move on to the genders."
"In many loans (16.4% as you can see), the borrower is not actually a single person but a group of people that have a project, here's an [example](https://www.kiva.org/lend/1440912). In the dataset, they're listed as 'female, female, female' or 'male, female' ... I decided to use the label *mixed group* to those borrowers on the pie chart above.\n\nYou can see that most borrowers are female, I didn't expect that and it was actually a great surprise. This means that **women are using Kiva to get funded and work on their projects in countries (most of them are third world countries) where breaking in as a woman is still extremely difficult.**"
"## 1.3 Activities, sectors and funding amounts\n***\n\nNow let's take a peek at what people are needing loans for and what's the amounts they're asking for. Let's start with the sectors. There were 15 unique sectors in the summary we've seen above, let's see how each of them fare."
"**The most dominant sector is Agriculture**, that's not surprising given the list of countries that heavily use Kavi. A fast research for Kenya for example shows that all the top page is about agriculture loans, here's a sample of what you would find:  *buy quality seeds and fertilizers to use in farm*, *buy seeds to start a horticulture farming business so as a single mom*, *Purchase hybrid maize seed and fertilizer* ... Food sector occupies an important part too because many people are looking to buy fish, vegetables and stocks for their businesses to keep running.  \nIt's important to note that *Personal Use* occupy a significant part too, this means there are people who don't use Kavi to get a hand with their work but because they are highly in need.\n\nLet's see the more detailed version and do a countplot for **activities**"
"This plot is only a confirmation of the previous one, activities related to agriculture come in the top : *Farming, Food production, pigs ...*. All in all, we notice that none of the activities belong to the world of 'sophisticated'. Everything is about basic daily needs or small businesses like buying and reselling clothes ...\n\nHow about the money those people need to pursue their goals ?"
"Some outliers are clearly skewing the distribution and the plot doesn't give much information in this form : We need to **truncate the data**, how do we do that ? \n\nWe'll use a basic yet really powerful rule : the **68–95–99.7 rule**. This rule states that for a normal distribution :\n* 68.27% of the values $ \in [\mu - \sigma , \mu + \sigma]$\n* 95.45% of the values $ \in [\mu - 2\sigma , \mu + 2\sigma]$\n* 99.7% of the values $ \in [\mu - 3\sigma , \mu + 3\sigma]$     \nwhere $\mu$ and $\sigma$ are the mean and standard deviation of the normal distribution.\n\nHere it's true that the distribution isn't necessarily normal but for a shape like the one we've got, we'll see that applying the third filter will **improve our results radically**.\n"
"Well, that's clearly a lot better !    \n* Most of the loans are between 100\$ and 600\$ with a first peak at 300\$.\n* The amount is naturally decreasing but we notice that we have a clear second peak at 1000\$. This suggets that there may be a specific class of projects that are more 'sophisticated' and get funded from time to time, interesting."
"Now first thing first, we'll plot the this difference that we called *time_funding*. To avoid any outliers, we'll apply the same rule for normal distribution as before."
"I was really surprised when I got this plot (and happy too), you'll rarely find a histogram where the distribution fits in this smoothly !   \nOn top of that, getting two peaks was the icing on the cake, it makes perfect sense ! **We've seen above that there are two peaks for loans amounts, at 300\$ and 1000\$, we're basically saying that for the first kind of loan you would be waiting 7 days and for the second kind a little more than 30 days !   **\nThis gives us a great intuition about how those loans work going forward.\n\nLet's be more specific and check for both loan amounts and waiting time country-wise :   \nWe'll build two new DataFrames using the groupby function and we'll aggregate using the median : what we'll get is the median loan amount (respectively waiting time) for each country."
###  Importing all the libraries
### Reading the file 
### Plotting boxplot to see the distribution of the data
**Separating features and label**
" Outlier Analysis  \n \n> \n \n  \nWhat is an Outlier?  \n Outlier is an observation that is numerically distant from the rest of the data or in a simple word it is the value which is out of the range.let’s take an example to check what happens to a data set with and data set without outliers.\n\n\n|| | Data without outlier |  | Data with outlier | \n|--||--||--|\n|**Data**| |1,2,3,3,4,5,4 |  |1,2,3,3,4,5,**400** | \n|**Mean**| |3.142 | |**59.714** |  \n|**Median**| |3|  |3|\n|**Standard Deviation**| |1.345185| |**150.057**|\n\n As you can see, data set with outliers has significantly different mean and standard deviation. In the first scenario, we will say that average is 3.14. But with the outlier, average soars to 59.71. This would change the estimate completely.\n\n\n> \n> The above meme makes you better understanding of outlier. \n\n Lets take a real world example. In a company of 50 employees, 45 people having monthly salary of Rs.6,000, 5 senior employees having monthly salary of Rs.100000 each. If you calculate the average monthly salary of employees in the company is Rs.14,500, which will give you the wrong conclusion (majority of employees have lesser than 14.5k salary). But if you take median salary, it is Rs.6000 which is more sense than the average.For this reason median is appropriate measure than mean. Here you can see the effect of outlier.\n    \n   \n Outlier  is a commonly used terminology by analysts and data scientists as it needs close attention else it can result in wildly wrong estimations. Simply speaking, Outlier is an observation that appears far away and diverges from an overall pattern in a sample.\n\nCause for outliers \n\n * Data Entry Errors:- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.  \n * Data Entry Errors:- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.  \n * Measurement Error:- It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty.  \n * Natural Outlier:- When an outlier is not artificial (due to error), it is a natural outlier. Most of real world data belong to this category. \n\nOutlier Detection \n\n Outlier can be of two types: Univariate and Multivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space.  \n\nDifferent outlier detection technique. \n\n 1. Hypothesis Testing  \n 2. Z-score method  \n 3. Robust Z-score \n 4. I.Q.R method  \n 5. Winsorization method(Percentile Capping)  \n 6. DBSCAN Clustering \n 7. Isolation Forest  \n 8. Visualizing the data \n\n1. Hypothesis Testing(grubbs test) \n\n$$\n\begin{array}{l}{\text { Grubbs' test is defined for the hypothesis: }} \\ {\begin{array}{ll}{\text { Ho: }}  {\text { There are no outliers in the data set }} \\ {\mathrm{H}_{\mathrm{1}} :}  {\text { There is exactly one outlier in the data set }}\end{array}}\end{array}\n$$\n$$\n\begin{array}{l}{\text {The Grubbs' test statistic is defined as: }} \\ {\qquad G_{calculated}=\frac{\max \left|X_{i}-\overline{X}\right|}{SD}} \\ {\text { with } \overline{X} \text { and } SD \text { denoting the sample mean and standard deviation, respectively. }} \end{array}\n$$\n$$\nG_{critical}=\frac{(N-1)}{\sqrt{N}} \sqrt{\frac{\left(t_{\alpha /(2 N), N-2}\right)^{2}}{N-2+\left(t_{\alpha /(2 N), N-2}\right)^{2}}}\n$$\n\n\begin{array}{l}{\text { If the calculated value is greater than critical, you can reject the null hypothesis and conclude that one of the values is an outlier }}\end{array}"
"2. Z-score method \n\n Using Z score method,we can find out how many standard deviations value away from the mean.  \n\n\n  Figure in the left shows area under normal curve and how much area that standard deviation covers.  \n * 68% of the data points lie between + or - 1 standard deviation.\n * 95% of the data points lie between + or - 2 standard deviation\n * 99.7% of the data points lie between + or - 3 standard deviation\n\n Z-score formula\n\n\begin{array}{l} {Z score=\frac{ X - Mean}{Standard Deviation}}  \end{array}\n\n If the z score of a data point is more than 3 (because it cover 99.7% of area), it indicates that the data value is quite different from the other values. It is taken as outliers.\n"
"8. Visualizing the data \n\nData visualization is useful for data cleaning, exploring data, detecting outliers and unusual groups, identifying trends and clusters etc. Here the list of data visualization plots to spot the outliers.  \n      \n1. Box and whisker plot (box plot).     \n2. Scatter plot.     \n3. Histogram.  \n4. Distribution Plot.     \n5. QQ plot.  \n"
"What Next?? \n\nAfter detecting the outlier we should remove\treat the outlier because it is a silent killer!! yes.  \n      \n* Outliers badly affect mean and standard deviation of the dataset. These may statistically give erroneous results.     \n* It increases the error variance and reduces the power of statistical tests.     \n* If the outliers are non-randomly distributed, they can decrease normality. \n* Most machine learning algorithms do not work well in the presence of outlier. So it is desirable to detect and remove outliers.     \n* They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions. \n  \nWith all these reasons we must be careful about outlier and treat them before build a statistical/machine learning model. There are some techniques used to deal with outliers. \n  \n1. Deleting observations.     \n2. Transforming values. \n3. Imputation.  \n4. Separately treating \n\nDeleting observations:  \nWe delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers. But deleting the observation is not a good idea when we have small dataset. "
"Transforming values: \n\nTransforming variables can also eliminate outliers. These transformed values reduces the variation caused by extreme values. \n   \n \n1. Scalling     \n2. Log transformation \n3. Cube Root Normalization  \n4. Box-Cox transformation \n\n    \n* These techniques convert values in the dataset to smaller values.     \n* If the data has to many extreme values or skewed, this method helps to make your data normal.     \n* But These technique not always give you the best results.  \n* There is no lose of data from these methods.     \n* In all these method boxcox transformation gives the best result. \n "
# Importing the necessary libraries
# Reading the 3 files from the Titanic Data Set
"# Gotta work a little bit in the Name column, \n# Creating a Title column based on the titles found in Name column, mapping them into numbers and finally removing the Name column"
"# we need to impute age column, basically fill in the blanks, I'm filling the blanks based on their Priority Class means"
"*CAVEAT: Sorry but just note this notebook can be a bit slow to load probably due to the Plotly embeddings displaying a large number of points*\n\n#Introduction\n\nThere already exists a plethora of notebooks discussing the merits of dimensionality reduction methods, in particular the Big 3 of PCA (Principal Component Analysis), LDA ( Linear Discriminant Analysis) and TSNE ( T-Distributed Stochastic Neighbour Embedding). Quite a handful of these have compared one to the other but few have gathered all 3 in one go. Therefore this notebook will aim to provide an introductory exposition on these 3 methods as well as to portray their visualisations interactively and hopefully more intuitively via the Plotly visualisation library. The chapters are structuredas follows:\n\n 1. **Principal Component Analysis ( PCA )**  - Unsupervised, linear method\n\n\n 2. **Linear Discriminant Analysis (LDA)** - Supervised, linear method\n\n\n 3. **t-distributed Stochastic Neighbour Embedding (t-SNE)** - Nonlinear, probabilistic method\n\nLets go."
"**Curse of Dimensionality & Dimensionality Reduction**\n\nThe term ""Curse of Dimensionality"" has been oft been thrown about, especially when PCA, LDA and TSNE is thrown into the mix. This phrase refers to how our perfectly good and reliable Machine Learning methods may suddenly perform badly when we are dealing in a very high-dimensional space. But what exactly do all these 3 acronyms do? They are essentially transformation methods used for dimensionality reduction. Therefore, if we are able to project our data from a higher-dimensional space to a lower one while keeping most of the relevant information, that would make life a lot easier for our learning methods."
"Now having calculated both our Individual Explained Variance and Cumulative Explained Variance values, let's use the Plotly visualisation package to produce an interactive chart to showcase this."
*PLEASE CLICK AND MOVE THE SCATTER PLOTS ABOVE. THEY ARE INTERACTIVE. DOUBLE CLICK TO GET BACK TO THE ORIGINAL VIEW*
"**Visualising the MNIST Digit set on its own**\n\nNow just for the fun and curiosity of it, let's plot the actual MNIST digit set to see what the underlying dataset actually represents, rather than being caught up with just looking at 1 and 0's."
"Phew, they are definitely digits all right. So let's proceed onto the main event."
"###Interactive visualisations of PCA representation\n\nWhen it comes to these dimensionality reduction methods, scatter plots are most commonly implemented because they allow for great and convenient visualisations of clustering ( if any existed ) and this will be exactly what we will be doing as we plot the first 2 principal components as follows:"
"###K-Means Clustering to identify possible classes\n\nImagine just for a moment that we were not provided with the class labels to this digit set because after all PCA is an unsupervised method. Therefore how would we be able to separate out our data points in the new feature space? We can apply a clustering algorithm on our new PCA projection data and hopefully arrive at distinct clusters which would tell us something about the underlying class separation in the data. \n\nTo start off, we set up a KMeans clustering method with Sklearn's *KMeans* call and use the *fit_predict* method to compute cluster centers and predict cluster indices for the first and second PCA projections (to see if we can observe any appreciable clusters)."
"**Takeaway from the Plot**\n\nVisually, the clusters generated by the KMeans algorithm appear to provide a clearer demarcation amongst clusters as compared to naively adding in class labels into our PCA projections. This should come as no surprise as PCA is meant to be an unsupervised method and therefore not optimised for separating different class labels. This particular task however is accomplished by the very next method that we will talk about."
"INVESTIGATING THE DATA and EXPLORATORY DATA ANALSIS\n\nFirst, I install all the libraries that I will use in our application. I install all the libraries in the first part because the algorithms I will use later and the analysis I will make more clearly will be done.Furthurmore, I have investigated the data, presented some visualization and analysed features. Let's write it. I will import necessary Python modules and read the data."
"Now, we are uploading our data set to the data variable using the read_csv function in the pandas library. "
"We will perform analysis on the training data. The relationship between the features found in the training data is observed. In this way, comments about the properties can be made\n"
Age Analysis
"First, let's plot the answers to Q15 and Q23 as a standard bar chart (I may not like bar charts, but they are just so useful). "
"They are clearly not the same, but one problem with them is the bins used. Unfortunately, the responses for Q15 and Q23 have not been made equal - Q23 has more granular bins (`'00-01', '01-02', '02-03', '03-04', '04-05', '05-10', '10-15', '20-99'`) than Q15 (`'00-01', '01-02', '03-05', '05-10', '10-20', '20-99'`). To make them more easily comparable, I aggregate the answers to the Q23 so that they correspond 1:1 with the answers to Q15. To do this, I create a new category, `'03-05'` by summing up the categories `'03-04'`, `'04-05'`, and I add the entries from category `'02-03'` to `'01-02'`. It is an arbitrary choice in case of the category `'02-03'`. "
"I am not interested in comparing the two bar plots however. I would like to know how the two questions interrelate. So let's create a pivot table and plot the answers to these survey questions against each other as a heatmap, with insights from Q23 plotted horizontally and from Q15 - vertically. For the sake of comparison, on the sides I draw again the two bar-plots, representing the aggregated values across each dimension. Therefore, the vertical bar plot on the left is a histogram of answers to the Q15, while the horizontal bar plot on the top represents summary of answers to the Q23, with the aggregated bins. Comparing these two bar plots, we can see that the two are not symmetric - there is more experienced coders than experienced ML-practitioners. "
"Plotting the pivot table, confronting the answers to Q15 and Q23, enables us to see much more structure than on a simple bar plot. We will use this heatmap as our anchor, digging deeper in the data. Let's look at it more in detail."
"First of all, we can see that the majority of the Kagglers are concentrated in the top-left corner of the plot. In fact, if we sum all the entries corresponding to less than 2 years of experience using both code to analyze data and machine learning methods, we will cover 52% of the survey responders, and if we increase those limits to less than 5 years in both cases, we cover 75% of participants. "
"There are other features catching our attention here. Let's look at what happens on the diagonal, and off the diagonal. Basing on this criterium, we can split the population of Kagglers into three big groups: \n- Kagglers who have been coding to analyze data for longer than they have been using machine learning (code-first)\n- Kagglers who have been in machine learning for longer than they have been coding for data (ML-first)\n- Kagglers who have similar experience in both coding and machine learning, therefore the two aspects have been likely highly intercorrelated for them (code and ML interrelated)"
"Because the categories were not identical for both of the questions, and because of the fuzzy nature of the boundaries (*If I started with machine learning in 2017, have I been doing it 1-2 years, or 2-3 years?*), the off-diagonal entries that are very close to the diagonal should be intepreted also with a doze of fuzziness and uncertainty. But it doesn't change the fact that 5 Kagglers said that they have been using machine learning for more than 10 years while having coded for less than a year! (were they using visual or point-and-click tools? or proving theorems on paper? or made a mistake in the answer? or just trolling?)"
"We can also name some simple subgroups here, which will help us better interpret the survey results later on. I will focus on the ""prototypical"" groups, on the extreme sides. "
"The first group are the beginners. They have less than 2 years of experience of both coding and ML methods, and so they likely have started with the topic around 2017 the earliest. As I mentioned before, they make up for around 50% of all survey participants. \n\nThe second group are coders in transition. Those people have a decades-long coding experience for working with data, however they have started working with machine learning only recently. These may be for example software engineers transitioning into data engineers. \n\nThe third group belongs in the lower right corner and these are the machine learning veterans. Those people have been coding since long before the current AI revolution - with 10 or even over 20 years of both ML and coding experience, they may have started to specialize in the topic around 2000s or even late 1990s. These people were doing machine learning before it was cool. They likely know methods that are rare to find in the current data science curriculum, as well as have deep understanding of modern methods. \n\nThe last group to point out is the group in the middle: modern data scientists. They have started to be interested in the topic more less around 2015, so at the beginning of the boom, when ML started to go public. They are the most likely to have some kind of a ""standard"" education in the modern state-of-the-art tools, both for coding and machine learning. They have or are about to have passed the initial learning phase and can provide measurable value. "
"This heatmap in general informs us about the distribution of a specific population of survey takers when it comes to their experience in coding and machine learning. What we drew above corresponds to the total number of survey takers. Instead of using absolute number of counts in each bin, we can normalize it to get percentage of given population falling into each bin. We can then use it as a reference - we can calculate such a code-ML heatmap for a given subgroup of survey responders, and compare the two distributions to better understand the differences between that subgroup and general population. \n\nFor clarity, I leave out the axes descriptions for now, and adjust the color scale to better represent the data. "
"We can now compare this for example with the two heatmaps corresponding to the subpopulation of Kagglers, who defined themselves as ""Students"" and ""Data Scientists"" in Question 5 respectively."
"Comparing the three heatmaps as they are already brings some insights. \n- Among the group of students, 42% has less than a year experience in coding and using machine learning, while among the group of data scientists only 6% of people belong to this bin. \n- We also see that 2% of data scientists belongs to the group of the extreme veterans (>20 years of experience), while in general population only 1% belongs to this bin. \n- Among data scientists, the majority (almost 50% in total) is concentrated in the middle part of the heatmap, and there is a disproportionally bigger proportion of people especially in groups that use coding for 3-10 years and machine learning for 2-5 years. "
"However, going through each survey response like this would be extremely tiring, so I developed another way of visualizing these differences: \n1. I subtract the reference histogram (depending on a question, it will be either overall population of survey takers, or for example the ones that declared themselves as non-students - following the survey schema). \n2. Because I mostly want qualitative insights, I leave out the numbers and encode values in the difference histogram as colors. \n\nLet's look at an illustrative example: "
"By reducing the complexity of the data representation and encoding the matrix numbers as an easily interpretable colored pattern, we can gather insights much quicker and with less cognitive load. Here for example we immediately see that the Kagglers who describe as data scientists have on average much more ML and coding experience than the overall population, and there is relatively less people only starting to code in this group. A remark: white cells signify no big difference between the selected group and general population - and not necessarily that there is no counts in this cell! "
## Data Transformation\n* Skew correction
## Data Interaction\n* Correlation
## Data Visualization\n* Categorical attributes
##Data Preparation\n* One Hot Encoding of categorical data
"## Evaluation, prediction, and analysis\n* Linear Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* Ridge Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* LASSO Linear Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* Elastic Net Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* KNN (non-linear algo)"
"## Evaluation, prediction, and analysis\n* CART (non-linear algo)"
"With an average house price of $180921, it seems like I should relocated to Iowa!"
"Looks like a normal distribution? Not quite! Looking at the kurtosis score, we can see that there is a very nice peak. However, looking at the skewness score, we can see that the sale prices deviate from the normal distribution. Going to have to fix this later! We want our data to be as ""normal"" as possible."
"With 81 features, how could we possibly tell which feature is most related to house prices? Good thing we have a correlation matrix. Let's do it!"
"It's a nice overview, but oh man is that a lot of data to look at. Let's zoom into the top 10 features most related to Sale Price."
"Well, the most correlated feature to Sale Price is... Sale Price?!? Of course. For the other 9, they are as listed. Here is a short description of each. (Thank you, data_description.txt!)\n\n1. OverallQual: Rates the overall material and finish of the house (1 = Very Poor, 10 = Very Excellent)\n2. GrLivArea: Above grade (ground) living area square feet\n3. GarageCars: Size of garage in car capacity\n4. GarageArea: Size of garage in square feet\n5. TotalBsmtSF: Total square feet of basement area\n6. 1stFlrSF: First Floor square feet\n7. FullBath: Full bathrooms above grade\n8. TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n9. YearBuilt: Original construction date\n\nLet's take a look at how each relates to Sale Price and do some pre-cleaning on each feature if necessary."
What! People pay more for better quality? Nothing new here. Let's move on.
It makes sense that people would pay for the more living area. What doesn't make sense is the two datapoints in the bottom-right of the plot. \n\n We need to take care of this! What we will do is remove these outliers manually. 
Nice! We got a 0.02 point increase in the Pearson-R Score.
4-car garages result in less Sale Price? That doesn't make much sense. Let's remove those outliers.
"Is Spending $$$ for MS in Data Science worth it ?\nA detailed comparative analysis of people with and without university degrees for data science\n\n\n2019 was an important year for me, Not only I got engaged this year but also I completed my higher education degree. After working for several years in the industry I decided to take a short break, go back to academics and pursue higher education. No doubt, it was one of the best learning experiences I had but there was also a huge investment of time and money. Many people often contact me regularly asking about my experience and viewpoint about such degrees. They ask questions like - whether is it worth spending huge chunks of money for such degrees? Well, there is no fixed answer for such questions because every individual have a different viewpoint and their opinion might be biased. The best way to answer this question is to make use of data, analyse the cohorts of people (example - data scientists) who are well settled in the industry, measure and compare if there are any significant differences in their roles, position, responsibilities, and annual compensation. This type of analysis can provide many interesting insights and help in looking at the broader view of the scenario. In this notebook, I decided to take a stab at this scenario and have shared my experience along with the key insights and a detailed analysis of Kaggle's annual data science survey data. \n\n\nSource: Upslash  \n\nThere are one set of people who wants to pursue higher education degrees due to their passion and interest. For these people, it makes sense to get enrol in the relevant university courses and pursue their passion. On the other hand, there is another set of people, who wants to obtain these degrees only to get a specific job title, a specific job role, or a position that get them more money. For this set, university degrees are not the only option, there are many alternatives which can also result in the same outcomes. \n\nThe most obvious example is in the field of Data Science and Analytics. In recent years, university degrees such as ""Masters in Data Science"" or ""Masters in Analytics"" are sought as one of the must-haves to enter into this field. It is not astonishing that Data Scientist is one of the fastest-growing job titles across the globe and the demand for skilled data scientists is increasing. This has given the universities an option to attract students and make immense money. Several universities have started dedicated degree courses specialized in data science and analytics. Those want to become a data scientist or to switch from another profession to data science profession are now strongly considering these university degrees as the only pathway. \n\nBut these university courses are not easy to get in and affordable for everyone. These degrees don’t come for free, tuition fees can be exorbitant and can range anything from USD 30,000 to USD 100,000. And that doesn’t include the actual cost of living. Many consider applying for student loans but they add a huge lump sum to the existing mountain of debts. A common myth is that the earning potential for those with postgraduate qualifications is higher but of course, there is no guarantee that one will get a stable job at the end of it. Additionally, Pursuing a university’s higher degree takes anything from one to three years, depending on different factors. This can seem like a long time, especially when the fellow peers are getting started on their careers, while one is still studying. \n\nThe question of interest here is - ** does one need to get that expensive higher education degree**, **do they create a difference from those who do not have university degrees?**. Some resources online also suggest that one can get the depth of knowledge, variety of skills and learn something new. But again, **is it possible to get the same skills, same profile, or even better compensation without such degrees?**.\n\n\n\nKaggle conducted their Annual Data Science survey and it was full of interesting questions. Participants of this survey were asked different questions about their demographics, profiles, companies, what they use etc. I analysed this data intending to dig deeper into the profiles of people who completed the university degrees to learn data science and those who did not. The focus of the story in this notebook is to identify if the working data scientists with official higher education degrees differ significantly from the other group. The analysis and storyline are segmented according to different factors.    \n\n*Note* - For the analysis, I removed the respondents who were ""students"" and ""not employed"". The two groups were selected based on the respondent's choice if they completed the university degrees to learn data science or not.  \n\nContents\n\n1. Sources of Learning Data Science  \n    - Why People Choose Higher Education Degrees   \n    - The Academic Landscape : Masters in Data Science Degrees    \n2. Proportion of Individuals with University Degrees  \n3. Are there a Significant Differences - With and Without University Degrees ?   \n    3.1 Compensation   \n        - Key Characteristics : Data Scientists earning > USD 150K   \n    3.2 Job Roles   \n    3.3 Job Profiles   \n        - Other Tools : Usage and Comparison   \n4. Identifying Key Traits   \n5. Conclusions   \n6. References   \n\n\n1. Sources of Learning Data Science            \n\nData science skills are straightforward to obtain, they need experience and learning. Nowadays there are many online and offline which teaches them in detail. While some prefer online courses such as Coursera or Udacity, some prefer to go to universities for a year or two-year long dedicated courses. Let's look at what are the most popular sources of learning data science among the respondents of the kaggle survey. In this question, one participant could have chosen multiple choices, hence the x-axis represents ""percentage"" of respondents who selected a particular choice. "
"- About 44% of the respondents selected **""Coursera""** as the primary source of learning data science. Coursera is the popular online learning platform which provides both free courses and paid specializations.   \n- Coursera and its founder [Andrew NG](https://en.wikipedia.org/wiki/Andrew_Ng) have made very significant contributions to the data science revolution. Back in 2012, Andrew NG released the very popular Machine Learning course which became the first choice for many to learn data science. In 2017, **[Deeplearning.ai](https://www.deeplearning.ai/)** was launched and it became very popular data science specialization. These courses and many others from well known academic names on the online platform makes Coursera as the primary choice among the data science enthusiasts. \n- The search results for Coursera data science page says that there are **1054 courses about Data Science** as of Dec 2019.  \n\n\n\n- Kaggle Learn was selected by almost one-fourth of the respondents. Kaggle team launched these courses somewhere around early 2017. They are composed of notebook style materials which not only focusses on teaching the concepts but also the programming part as well. \n- Then there are other sources such as Udemy, Udacity, edX, fast.ai etc. These platforms als provide online materials and courses to learn data science.  \n\n- Then there is a group of individuals who prefer to go to a university to pursue higher education degrees. Among the survey participants, about **one-fifth of the participants** had completed university degrees. Accoding to multiple sources ( [Masters-And-More](https://www.master-and-more.eu/en/7-reasons-why-you-should-choose-a-masters-degree/), [Uniplaces](https://blog.uniplaces.com/discover/8-good-reasons-apply-masters-degree/), [CareerAdditct](https://www.careeraddict.com/masters-degree-benefits), [MyBaggage](https://www.mybaggage.com/blog/why-do-a-masters-degree-the-pros-and-cons/) ) different individuals have many different reasons to choose university courses over online courses. The most common are:\n\n    - Personal Goal\n    - Better Salary\n    - Gain More Knowledge\n    - Better Job Roles\n    - Career Change  \n\n\nDesigning My Own Survey : Why People Choose Higher Education Degrees\n\nMy personal reason for engaging in a higher education degree was majorly driven by passion and interest, it was a personal goal of mine to get another degree after the bachelors. But I was curious to know why other individuals decide to pursue a higher education degree. I created my own survey to know the student's choices this question and shared it in several groups associated with National University of Singapore. I managed to get about 120 responses from different people for this survey. Following is the response distribution of the Question.\n\nLink of the Survey: [Survey](https://docs.google.com/forms/d/19ov_EmBVdwc70tPUoylm8gPNz2bK4j21UAirDcQHB_Q) (Note : Names are removed to maintain the privacy of the individuals)      \nLink of the Responses: [Responses](https://docs.google.com/spreadsheets/d/1lRd2yLNJV6svqllrNDUcZHHYc5X20BswxEhGN89B4i8/edit?usp=sharing)   "
"- Most of the people decided to pursue higher education degrees to gain more knowledge. This was the primary reason for about 70% of the individuals who were part of this survey.  Every two out of five people decided to pursue university degrees to get better salaries or to change their professions. Only about one-fourth of individuals had their own personal goal to go for a university degree. \n- If we just look at the number of individuals who selected ""gain more knowledge"", the same but important question arises again: **""Is it worth spending a huge amount of money to gain knowledge that one can get from free sources?""** Arent' the free online sources good enough to gain more knowledge. Or, Can't these sources provide enough skills and knowledge to get that better salary, better job roles, or provide a pathway for a career change.   \n- The interesting fact to note that most of the online courses on Coursera, Udemy etc. are also from the same universities or the same professors. Many of them are free as well. So if ""gaining more knowledge"" is the only goal then it is worth considering these free courses. \n\n\nThe Academic Landscape : Masters in Data Science Degrees\n\nWhatever be the debate but one point is definitely clear, Universities across the globe do benefit a lot from this increased interest in higher education degrees. Many universities have now started specialized masters degree programs in analytics, data science, business analytics etc. The following chart shows some of the popular master's degree programmes from US universities along with their tuition fee and duration. Some of them are provided online, but the same are also provided on campus.  \n\nSource of Data : https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html"
"Time and money are the two biggest investments associated with university degrees. The graph shows that the tuition fee for most of these courses is not cheap and the duration can range from anywhere 1 to 3 years depending upon specialization, location, and university type. \n\nThe university courses are of two types: Generic courses and Specializations. Generic courses are typically very comprehensive, they cover all parts of data science but they are not very deep and detailed. These type of courses are good for those who want to get acquainted with main elements of this field. The specializations, on the other hand, aim to cover every possible detail of one particular area. They are generally very deep. For both types of courses, the investment of money and time are always higher as compared to the alternative free ones.  Additionally, these courses are never meant to teach everything and do the spoon-feeding. They are more like the guided paths, and it is mostly the self-effort along that path which makes the students learn. If guided correctly, even through the non-degree courses (such as the ones on coursera or kaggle learn etc), one may also get the same outcomes.\n\nHead of Data Science from Restaurant Technologies, Inc. [shared](https://www.kdnuggets.com/2014/06/masters-degree-become-data-scientist.html), ""No single Masters Program could cover all the disciplines needed in significant depth for one to be an expert in all these areas. Selecting an area or two or three and having depth and expertise in those is common. Many companies do not have just a ""Data Scientist"" but teams comprised of experts from the different disciplines.""   \n\n\n2. Proportion of Individuals with University Degrees   \n\nLet's look at what per cent of individuals completed their university degrees to become a data scientist across different countries. Respondents were asked about their country in one of the questions. "
"- In the American continent, the USA and Canada are the two most sought places to pursue higher education degrees. About **one-fourth of the respondents** from these countries have completed their university degrees to becoming a data scientist. In the United States, about 27% of the individuals who were part of this survey completed their university degrees. In Europe, there are about one-fifth of respondents who completed their university degrees while in Asia, the percentage is a bit lower only about 15%.  \n- Countries with the highest proportion of data scientists with university degrees are **'Tunisia', 'Austria', 'New Zealand' and 'Greece'** with **over 40% of the individuals** completing university degrees. On the other hand, countries **'Japan', 'Nigeria', 'Belarus', and 'Algeria'** shows a lower number (less than 10%) of individuals completing university degrees.   \n- Among the genders, female respondents have a higher number for completing university degrees than male respondents. **23% of the female respondents** and 20% of the male respondents completed their university degrees.    \n\n\n3. Are there Significant Differences - With or Without University Degrees ? \n\nAccording to Forbes, Most people with data science job titles don’t have these new degrees. I also looked at the profiles of a few data scientist in my Linkedin Network and observed that not all of them have data science degrees. People tend to take different paths - some have degrees in business, economics, maths etc, while some have specialed data science degrees, and some have no university degree. But all of them are working in good organizations with good job roles. In the next section, let's look at the key insights from the Survey Data Analysis. The focus of the analysis is to compare the two groups -  individuals who completed university degree vs those without for becoming a data scientist and identify key differences (if any).\n\nMainly, We will look at three perspectives: Are there a fairly equal percentage of individuals from two groups:\n\n1. With every compensation bracket.   \n2. For each type of job role or activity.   \n3. For each type of activity, they do daily.   \n\n\n3.1 Compensation  "
"The plot shows the percentage of respondents from the United States of America in each compensation bracket. Looking at every bucket, it is clear that there are no significant differences between the two groups. Approximately they differ by a few per cent (less than 5). However, a few sections in this chart are very interesting. \n\n- A common belief about university degrees is that one get higher compensation. The chart shows that there is a large percentage of individuals **without a university degree also earning more than 100K USD**. Even without university degrees, if individuals manage to obtain the right skills and the right direction, one can also grab high compensation. That's where Kaggle Learn or Coursera are the best options. As they provide the pathway to get the appropriate skills required to become a data scientist.\n- There are slight differences when the compensation is less than 125K. For this range, there is a higher percentage of individuals having university degrees. The area is shown in the green section in the chart. Well, this compensation range is the average of most of the companies in USA. This means that university degrees in data science can give an initial boost to the candidates in the compensation.  \n- Very Interesting to note that there are more percentage of respondents without university degrees than those who have who are earning in the range of USD 150K-300K. This area is highlighted in red. This implies that there are definately ways to get higher compensation not necessarily after obtaining a masters in data science degree. The obvious reasons can be the amount of experience, age group, or special talent. It will be interesting to specifically look into this cohort where data scientists earn >150K USD. This is analysed in the next section.\n\n\nKey Characteristics : Data Scientists earning > $150K  \n\nThe following graph shows the key characteristics: age distribution, coding experience (in years) etc. for Data Scientist earning more than 150K USD. "
"- The chart shows that there is a considerable number of individuals in each bracket of the machine learning experience, coding experience, and age. The charts are also does not shows any skewness in a particular bracket. \n- With more number of years in experience for coding and machine learning (greater than 5), We see that relatively a higher percentage of people are there earning more than 150K without university degrees.   \n- More number of individuals who are aged less than 34 years and with a university degree earn greater than 150K. Good to see that even more percentage of young individuals, aged 22-24 also get higher compensation in the USA.  \n- The portion on the right side of the age graph shows that experienced people may not need to get university degrees in data science if they are only looking for better salaries.  \n- These trends are also similar in other countries. According to [many](https://www.mastersportal.com/articles/2608/top-9-countries-with-the-highest-investments-in-university-education.html) links, top countries for higher education university degrees are the USA, Germany, Canadas etc. "
- We see similar insights for these countries as the USA where more percentage of non degree holder respondents are earning higher compensation in many brackets. \n\n\n3.2 Job Roles  \n\nNext we look at the job roles of individuals. Does it make a difference in terms of what kind of activity the data scientists do on the daily basis if they are coming with a university degree as compared to without. 
"The two plots capture two different pieces of information: \n- Plot A shows what percentage of respondents selected a particular activity which people do in their day to day activities. This plot shows which are the most common activities people do in their data science project and the difference between a degree and non-degree holders. \n- Plot B shows, out of all the selected choices by all the respondents, what percentage is a particular responsibility is selected. This plot shows, for a particular group (degree or non-degree holders), which activity has more importance. For example, If a particular 'responsibility' is rarely selected, then the overall percentage will be smaller and If it is selected most of the times, then its overall percentage will also be higher. Meaning that it will be important to the particular group - degree or non-degree holders. \n\nFrom these two plots, we can observe that:\n\n- Plot A shows that a higher percentage of individuals with a university degree are involved in different tasks. The biggest differences are observed for data scientists who do ""Data Analysis or Exploration"", and performing ""Research and developing State of the art models"". Since experimentation is always a big part of any data science project, there exists very less differences in the % of respondents of two groups.     \n- The percentage of non-degree holder respondents is always lesser than the counterpart, however, the differences are not extreme. There is still a significant percentage of people who are involved in similar responsibilities as the university holders. Hence, it will be wrong to say that only university holders do a particular type of activities or have specialized activities.   \n- Plot B shows a higher percentage of people with university degrees selected being involved in **Research work, Data Analysis, and Building/Running Infrastructures**. While a slightly more percentage of people without university degrees selected being involved in Experimentation and Building Machine Learning services.  \n- A fairly equal percentage is observed for people who build Machine Learning service despite their degrees. This particular selection shows that there is no major difference in the kind of work a data scientist will do.   "
"- Based on the number of job responsibilities, a data scientist can be classified into two categories - Generalist and Specialist. Generalist are the individuals involved in all parts of life cycle of a data science project (ie. the points on the extereme right). Specialists are the individuals who are focussed on hardly 1 or 2 job responsibilities, ie. (points in the left). \n- We see that slightly more percentage of generalists are there having university degrees. They are they people who are involved in every step - doing analysis, experimentation, building services, and also research. In general, the plot shows that more percentage of people who completed university degrees are involved in multiple responsibilities. \n\n\n3.3 Job Profiles - What type of tools/techniques are used ? \n\nAnother important point in the set of common beliefs is that the job profile of those who completed their university degrees is very different from those who did not. Mainly the differences are considered in terms of tools, technologies, techniques used on day to day basis. Let's analyse from the kaggle survey data and identify are there really major differences in what type of tools and techniques are used by people with or without university degrees (or self made data scientists as compared to those who obtained university degrees). "
"- In terms of machine learning models and techniques, a higher percentage of university degree holders use deep learning approaches. This include **Bayesian approaches, General adversarial networks, and neural networks**. This is likely because of the recent interest and developments in deep learning, universities have also added courses with a lot of focus on deep learning. There are specialized courses on neural networks, deep learning, and even GANs. However, it is worth mentioning here that the cost of the specialized courses is relatively higher than the other courses due to extra credits. If anyone wants to just learn these skills, one can obtain them from resources available on the internet.     \n- A slightly higher percentage of individuals without university degrees use simple linear models such as logistic regression models. This is because in most of the companies, data science projects always start with simple models and they tend to work very well. \n- In terms of specific machine learning models, the two groups do not show any major differences. This is likely because everyone who starts with data science, they at least try to get a taste of different machine learning libraries and techniques. "
"- Most common it is advisable to know and use about three techniques or models, The percentage of such people is higher for degree holders. Similar is the case with several machine learning algorithms, more percentage of people are exposed to more techniques/algorithms when they have university degrees.  "
- We observe that there is more usage of Google Colab and Google Cloud Notebooks among the university degree holders than the self-made data scientists. This aligns with the more acquaintance with deep learning techniques and models of degree holders that we observed in the last section. Mainly the data scientists without any degrees have used kaggle notebooks. These are of course one of the best resources to learn and practice data science. Also completely free to use.   \n- University holders seem to be more R users as a higher percentage is observed for Rstudio. Self-made scientists prefer Python over R as Jupyter has slightly more percentage of people without degrees.  \n- Again we observe that there are no major gaps in the usage patterns of individuals with university degrees and those who became scientists on their own. Kaggle learn has both R and Python free courses which are great. 
"\nOther Tools : Usage and Comparison\n\nThe following graphs summarize the usage patterns of other tools, techniques, databases, platforms, and frameworks used by individuals of both the groups - self made data scientists and the ones with university degrees.  \n\n1. Usage Patterns of data scientists with University Degrees: "
2. Usage Patterns of different tools and frameworks of self made data scientists: 
"\n4. Identifying Key Traits \n\nLet's look at what are the key characteristics of individuals who prefer to go to university rather taking the other path. In this particular task, we aim to identify what are the most important signals strongly related to university degree holders. To identify these signals, we will treat this task as a predictive modelling problem. The first step is to prepare dataset which includes creating the train and test sets. Next, a simple learning classifier will be trained on the dataset the features are information about the individuals (demographics, tools used, company info etc.) and the target determines if the individual completed the university degree or not. All of the features and the target are binary. After the model is trained with a decent evaluation metric score, the important features of the model are obtained using Permutation Importance. These features provide some sense about their importance with an order or ranking. "
# About the notebook\n
# Let's load the required libraries\n
# Load data set
So there are three types of species \n\nIris-setosa\nIris-versicolor\nIris-virginica
So we have equally distributed species all are of 50
# Corelation between features
# Visualizing species based on Sepal length and width
We can easily differentiate setosa based on Sepal but for versicolor and virginica its difficult because the data is scattred.
#  Visualizing species based on petal length and width
Again based on petal we can easily classify setosa and for versicolor and virginica also we can classify but there is a thin line which should be taken care of
# Values distribution based on petal width
# Values distribution based on petal length
# Values distribution based on sepal length
# Let's get started!
# Reading the data
The 'freq' column represents number of observations for that patient
The number of oberservations for every unique patient in the train csv ranges from 6 to 10 wherein most of them have 9 observations.
We notice the range of age to be between 48-88 where we have more records for patients in the age range 64-74.
More number of male patients than female patients.
A big chunk of data is of patients who are Ex-smokers whereas very few patients who currently smoke.
Import Libraries 📚
"\n\nI will be integrating ```W&B``` for ```visualizations``` and ```logging artifacts```!\n\n[Shopee Project on W&B Dashboard](https://wandb.ai/ruchi798/shopee?workspace=user-ruchi798) 🏋️‍♀️\n\n* To get the API key, an account is to be created on the website first.\n* Next, use secrets to use API Keys more securely🤫"
 \n## Step 3: Visualising the Data\n\n- Here we will identify if some predictors directly have a strong association with the outcome variable `price`
#### Insights:\n- Toyota seems to be the most favoured cars.\n- Mercury seems to be the least favoured cars.
#### Visualizing the distribution of car prices
"- The plots seems to be right skewed, the prices of almost all cars looks like less than 18000.\n"
## Introduction to this project on Time Series Forecasting
"\n  Table of Contents\n  Phases of Kaggle/Competitive Data Science1\n    My Advice on ""How to get Started with Kaggle""2\n  Step 1 : Understanding the Problem Statement and Gathering Basic(Initial Domain Knowledge) Domain Knowledge3\n  Step 2 : Understanding the Evaluation Metric 4\n  Step 3: Exploratory Data Analysis ,Advanced Domain Knowledge Gathering and Choosing a Reliable CV5 \n  Step 4: Setting up a Baseline and Gathering All your ideas in One place6\n  Step 5: Skimming through all the dicussion threads and Notebooks , writing down the Ideas to try7\n    Step 6: Perform Experiments and repeat8"
"# Phases of Kaggle/Competitive Data Science\n\nIn this section I try to answer How to get started with Kaggle as before learning how to tackle a competitive problem you should know where to begin right. In my experience , I feel there are following four stages of Kaggling or Competitve Data science for anyone :\n\n* Noobie\n* Had the Feel /Intermediate \n* Seasoned/Pro\n* Elite\n\nLet me frame a clear definition for each of the above so that you know on which stage you are currently in :\n\n### Noobie\nYou are a Noobie if :\n* you have read about data science and are really excited to start with it , but you are not sure how to...\n* you have just started your data science journey , have done some courses and now you are not sure where to go...\n* you have good knowledge of python but are getting started with data science \n* you are switching from a different background alltogether\n\nThere can be many more scenarios but you get the idea right?\n\n### Had the Feel / Intermediate\nYou are at intermediate level if:\n* you have participated in other hackathons on other smaller platforms like Analytics Vidhya , MachineHack ,Zindi ,etc but are afraid of Kaggle\n* you are good with tabular data competitions , EDA ,etc but are afraid of kaggle\n* you have done beginner level projects and now want to try your hands at real competitions, etc \n\n### Seasoned / Pro\nYou are a Pro if:\n* you have participated in Competitions before on other platforms and have got very good ranks but are new to kaggle\n* you are experienced data scientists working in big organizations but are new to kaggle\n* you have participated in live Kaggle competitions and managed to be in top 15 percent\n* you have written some very good kernels and are able to read and understand the advanced kernels etc\n* you have won several bronze medals in competitions\n* you are not afraid anymore but don't know how to get better in order to reach gold or silver not by chance but by pure work\n\n### Elite\nYou are an Elite if:\n* There is huge respect for you in the community\n* People follow you , admire you , look up to you\n* Have reached a stage of competition master or grandmaster\n\nThis is the stage which is a dream of every kaggler when they start right? These are the people whom we look upto and whom we want to follow , these are the people who have taught us , helped us , to reach all the way to pro level .\nThe people in Elite level are something different , I am talking about the brilliant Grandmasters whom we follow , whose kernels we drool over , whom we ask all the tough questions and they are kind enough to readily answer them.\n\n# My Advice on ""How to get Started with Kaggle""\n\n Now I believe you need to be atleast in intermediate stage to get started with kaggle but that doesnt mean you cant do competitive data science if you are in the Noobie stage . Below are my advices stage wise on "" How To Get Started With Kaggle "" \n\n### Noobie\n\nIf you are in this stage means you are just starting , your pure focus should be to move into the `intermediate stage`, here is what you will need :-\n\n* Start with Python , main goal should be to atleast get to an intermediate level in python. Sources such as hackerearth and others can used.\n* Along with learning Python , Kaggle Learn courses are very good to transition into data science, focus should be on getting better with main python data science libraries like pandas , numpy ,scikit learn\n* Also you should keep practicing EDA and skills gained by kaggle learn courses ,by taking any dataset and exploring it with whatever knowledge you have with your own intuition and documenting main insights you might have found\n* Once all the above is done till a level that you are able to understand and write basic level codes , you can slowly start moving to starter level competitions , your focus should be doing only tabular data competitions . You can start by choosing playground kaggle competitions or go at a slightly smaller platform and practice there . This step is very crucial as you need to apply all the knowledge from kaggle learn courses and other places now to get a good rank in starter level competitions at sites like analytics vidhya and machine hack\n* Please note that delving into the maths and the algorithms themselves is not necessary at this stage and focus should be more on the applied part, if you are someone who likes the top down approach\n* Once you have gained good amount of confidence at exploring and modelling tabular data , try doing a unique personal project on tabular data and apply all the skills which you have acquired till now . The most important part with data science is your creativity and you must always use free will while dealing with data science projects and hence practice is necessary.\n\nVoila ! you are near about ready to transition into intermediate phase and you have already got a data science project to showcase , not bad , isn't it?\n\n### Intermediate\n\nIf you are here then it means you have sufficient applied knowledge in data science and have proven yourself with tabular data already .Now you want to enter the real competitions and want to make your name:\n\n* Live Kaggle competitions are tough and might be out of your league given your current skillset but that doesn't mean you can't do anything. The easiest way to get started with Kaggle competitions is to read kernels and write your own . Writing good kernels is a great way to boost your creativity, take any live kaggle competition , use whatever knowledge you have and publish a kernel (Don't worry no one is going to judge), read other people's kernels to get some ideas if the data is totally new to you , but be sure to put your original ideas as well in your kernels .Always remember the point of this exercise is to be creative , original and most of all learning to get comfortable with live kaggle comeptitions and not medals .\n* Don't worry if your kernels fail on upvotes ,if you learned something new by writing it then it was worth it , read the most upvoted kernels , try and understand why they got the upvotes .Kaggle community really appreciates original ideas and creativity so if you keep doing this you will surely get rewards . [Here](https://www.kaggle.com/general/89512#post516909) is a discussion thread in which Andrew (former notebooks rank 1) answers how to write good kernels.\n* Along with all this , its important that you now gain theoritical knowledge as well , learn one algorithm at a time , try to finish all the classical ones , read about different evaluation metrics(both for classification and regression) , bias-variance trade off , etc. The point is read atleast 10 articles everyday . Visit previous kaggle competitions , read best kernels there and make sure you are covering ground\n* Once you are done writing kernels for 3-4 live kaggle competitions in a way I explained with original ideas you will be familiar of how things work at kaggle and will be confident enough to enter a live kaggle competition . \n* There is a quote which I have formed for me and I thought it would be worth mentining ,"" If you are not learning anything new by writing a kernel or by participating in a competition or by doing a personal project ,then its really not worth it"". \n\n### Pro and Elite\n\nWell it would be completely vague if someone like me gives some advice to people in this category. I will say I am just thankful to these people to keep sharing their knowledge in such a easy way for people to understand . Due to presence of these people kaggle becomes such a great platform to learn and practice data science. I hope I keep learning and one day reach the ELITE stage \n\n\nNow when I was starting kaggle there was no tabular data competition so I had to learn a completely new thing altogether just to participate but luckily for you there is this tabular competition going on and you can get started , Now without further ado , lets get started with the steps for tackling any Live Kaggle Competition"
"**Introduction**\n\nI've always wanted to build an end to end ml solution - starting with model creation and ending with a live web app. Here I've managed to do it. Users are able to submit a picture of a skin lesion and get an instant prediction. This kernel details the process I followed to build the model and then convert it from Keras to Tensorflow.js. The javascript, html and css code for the app is available on github. \n\nWeb App:http://skin.test.woza.work/\nGithub: https://github.com/vbookshelf/Skin-Lesion-Analyzer\n\nThis model classifies skin lesions into seven classes. It is a fine tuned MobileNet CNN. All training was done in this kernel. The main challenges were the unbalanced dataset and the small amount of data.  I used data augmentation to reduce the class imbalance and in so doing get categorical accuracy scores that were not heavily skewed by a single majority class.\n\nMobileNet’s small size and speed makes it ideal for web deployment. It’s also a joy to train.\n\nTensorflow.js is a new library that allows machine learning models to run in the browser - without having to download or install any additional software. Because the model is running locally, any data that a user submits never leaves his or her pc or mobile phone. I imagine that privacy is especially important when it comes to medical data.\n\n\n\n**What is the objective?**\n\nI found it very helpful to define a clear objective right at the start. This helps guide the model selection process. For example, if a model has an accuracy of 60% it would usually be seen as a bad model. However, if it also has a top 3 accuracy of 90% and the objective requires that it output 3 predictions then it may actually be quite a good model. \n\n*This is the objective that I defined for this task:*\n\n> Create an online tool that can tell doctors and lab technologists the three highest probability diagnoses for a given skin lesion. This will help them quickly identify high priority patients and speed up their workflow. The app should produce a result in less than 3 seconds. To ensure privacy the images must be pre-processed and analysed locally and never be uploaded to an external server.\n"
"**LABELS**\n\nExcerpts from the paper:\n> The HAM10000 Dataset: A Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions\nhttps://arxiv.org/abs/1803.10417\n\n\n\n **nv**\n Melanocytic nevi are benign neoplasms of melanocytes and appear in a myriad of variants, which all are included in our series. The variants may differ significantly from a dermatoscopic point of view.\n *[6705 images]*\n \n **mel**\n Melanoma is a malignant neoplasm derived from melanocytes that may appear in different variants. If excised in an early stage it can be cured by simple surgical excision. Melanomas can be invasive or non-invasive (in situ). We included all variants of melanoma including melanoma in situ, but did exclude non-pigmented, subungual, ocular or mucosal melanoma.*[1113 images]*\n \n \n**bkl**\n ""Benign keratosis"" is a generic class that includes seborrheic ker- atoses (""senile wart""), solar lentigo - which can be regarded a flat variant of seborrheic keratosis - and lichen-planus like keratoses (LPLK), which corresponds to a seborrheic keratosis or a solar lentigo with inflammation\nand regression [22]. The three subgroups may look different dermatoscop- ically, but we grouped them together because they are similar biologically and often reported under the same generic term histopathologically. From a dermatoscopic view, lichen planus-like keratoses are especially challeng- ing because they can show morphologic features mimicking melanoma [23] and are often biopsied or excised for diagnostic reasons.\n*[1099 images]*\n\n**bcc**\nBasal cell carcinoma is a common variant of epithelial skin cancer that rarely metastasizes but grows destructively if untreated. It appears in different morphologic variants (flat, nodular, pigmented, cystic, etc) [21], which are all included in this set.\n*[514 images]*\n \n**akiec**\nActinic Keratoses (Solar Keratoses) and intraepithelial Carcinoma (Bowen’s disease) are common non-invasive, variants of squamous cell car- cinoma that can be treated locally without surgery. Some authors regard them as precursors of squamous cell carcinomas and not as actual carci- nomas. There is, however, agreement that these lesions may progress to invasive squamous cell carcinoma - which is usually not pigmented. Both neoplasms commonly show surface scaling and commonly are devoid of pigment. Actinic keratoses are more common on the face and Bowen’s disease is more common on other body sites. Because both types are in- duced by UV-light the surrounding skin is usually typified by severe sun damaged except in cases of Bowen’s disease that are caused by human papilloma virus infection and not by UV. Pigmented variants exists for Bowen’s disease [19] and for actinic keratoses [20]. Both are included in this set.*[327 images]*\n\n\n**vasc**\nVascular skin lesions in the dataset range from cherry angiomas to angiokeratomas [25] and pyogenic granulomas [26]. Hemorrhage is also included in this category.\n*[142 images]*\n\n**df**\nDermatofibroma is a benign skin lesion regarded as either a benign proliferation or an inflammatory reaction to minimal trauma. It is brown often showing a central zone of fibrosis dermatoscopically [24].*[115 images]*\n\n\n*[Total images = 10015]*"
### Print few random paintings
## Data Augmentation
### Print a random paintings and it's random augmented version
## Build Model
\n# Logistic Regression with Python\n\nWe'll be trying to predict a classification- survival or deceased.Let's begin our understanding of implementing Logistic Regression in Python for classification.\n\n## Import Libraries\nLet's import some libraries to get started!
## The Data\n
___\n## Data Cleaning\nWe want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation).\nHowever we can be smarter about this and check the average age by passenger class. For example:\n
"We can see the wealthier passengers in the higher classes tend to be older, which makes sense. We'll use these average age values to impute based on Pclass for Age."
Now let's check that heat map again!
Great! Let's go ahead and drop the Cabin column and the row in Embarked that is NaN.
"## Sources:\n### - Notebooks (kernels) of the Prize Competition Winners\n### - Notebooks (kernels) of Kaggle Grandmasters, Masters or Experts\n### - Detailed tutorials of the leading Python libraries\netc."
"## Thanks to:\n\n\n* @agostontorok,\n* @andradaolteanu,\n* @andresionek, \n* @artvolgin, \n* @dwin183287,\n* @haakakak,\n* @ihelon,\n* @kanncaa1, \n* @katemelianova, \n* @masumrumi,\n* @mtodisco10,\n* @mykolazotko,\n* @nareshbhat,\n* @n1sarg,\n* @parulpandey,\n* @pavansanagapati,\n* @pestipeti,\n* @poonaml,\n* @prashant111,\n* @raenish,\n* @robikscube,\n* @shivamb,\n* @siavrez,\n* @spitfire2nd,\n* @subinium,\n* @theshak64,\n* @tkubacka, \n* @toomuchsauce,\n* @tyagit3, \n* @viveknest,\n* @ykhorramz\n\n### for their wonderful and helpful notebooks (kernels)!"
"Semi Supervised Classification using AutoEncoders\n\n## Introduction\n\nBy definition, machine learning can be defined as a complex process of learning the best possible and most relevant patterns, relationships, or associations from a dataset which can be used to predict the outcomes on unseen data. Broadly, their exists three different machine learning processes: \n\n**1. Supervised Learning** is a process of training a machine learning model on a labelled dataset ie. a dataset in which the target variable is known. In this technique, the model aims to find the relationships among the independent and dependent variable. Examples of supervised learning are classification, regression and forecasting. \n\n**2. Unsupervised Learning** is a process of training a machine learning model on a dataset in which target variable is not known. In this technique, the model aims to find the most relevant patterns in the data or the segments of data. Examples of unsupervised learning are clustering, segmentations, dimensionality reduction etc. \n\n**3. Semi-Supervised Learning** is combination of supervised and unsupervised learning processes in which the unlabelled data is used for training a model as well. In this approach, the properties of unspervised learning are used to learn the best possible representation of data and the properties of supervised learning are used to learn the relationships in the representations which are then used to make predictions. \n\nIn this kernel, I have explained how to perform classification task using semi supervised learning approach. This approach makes use of autoencoders to learn the representation of the data then a simple linear classifier is trained to classify the dataset into respective classes.  \n \n\nFraud Detection using Semi Supervised Learning  \n\nI am using the dataset of [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) by ULB machine learning group. Later, I am also applying the same technique on [Titanic](https://www.kaggle.com/c/titanic) dataset. A number of kagglers have shared different approaches such as dataset balancing, anomaly detection, boosting models, deep learning etc but this approach is different. \n\n### Contents \n\n1. Dataset Preparation  \n2. Visualize Fraud Vs Non Fraud Transactions  \n3. AutoEncoders : Latent Representation Extraction  \n4. Obtain the Latent Representations  \n5. Visualize Latent Representations : Fraud vs Non Fraud  \n6. Simple Linear Classifier  \n7. Applying the same technique on Titanic Dataset\n \n## 1. Dataset Preparation\n\nFirst, we will load all the required libraries and load the dataset using pandas dataframe. \n \n\n"
"The dataset consists of 28 anonymized variables, 1 ""amount"" variable, 1 ""time"" variable and 1 target variable - Class. Let's look at the distribution of target. "
## 2. Visualize Fraud and NonFraud Transactions \n\nLet's visualize the nature of fraud and non-fraud transactions using T-SNE. T-SNE (t-Distributed Stochastic Neighbor Embedding) is a dataset decomposition technique which reduced the dimentions of data and produces only top n components with maximum information.  \n\nEvery dot in the following represents a transaction. Non Fraud transactions are represented as Green while Fraud transactions are represented as Red. The two axis are the components extracted by tsne. 
"From the above graph we can observe that there are many non_fraud transactions which are very close to fraud transactions, thus are difficult to accurately classify from a model. \n\n## 3. AutoEncoders to the rescue \n\n\n**What are Autoencoders?** - Autoencoders are a special type of neural network architectures in which the output is same as the input. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input (in other words, model the identity function). These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low-dimensional code that can be used by the decoder to reproduce the original input. \n\n**More about Autoencoders** - If you want to gain more understanding about autoencoders, you can refer to the following kernel : https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases\n\n![](https://i.imgur.com/Rrmaise.png)\n\nWe will create an autoencoder model in which we only show the model non-fraud cases. The model will try to learn the best representation of non-fraud cases. The same model will be used to generate the representations of fraud cases and we expect them to be different from non-fraud ones. \n\nCreate a network with one input layer and one output layer having identical dimentions ie. the shape of non-fraud cases. We will use keras package. "
# Show the Scores\nHere we see the scores and we have to decide about a cut-off for counting an image as ship or not. We can be lazy and pick 0.5 but some more rigorous cross-validation would definitely improve this process.
# Make the RLE data if there is a ship\nHere we make the RLE data for a positive image (assume every pixel is ship)
Let's look at the missing values in the dataset.
"Great, there is no missing information in the data."
"First, we visualize our entire dataset completely, and then we proceed to the analysis of the dependencies we are interested in."
"As we can see, most features have a positive correlation with the target variable.\n\nHypothesis: There is multicollinearity in the dataset."
Let's visualize the top 5 countries according to our numerical features.
Conclusions: mainly developed countries from the European Union are in the lead.
We visualize the relationship of the target feature with the rest of the numerical features.
Hypothesis: Happiness score and Whisker signs are multicollinear.
## Data visualisation\n\nWe  will start by visualising the Close prices for the two assets we have selected.
"The assets have quite different history, but we could check if they correlate in recent times."
## Load all dependencies you need\n from   coffee   import   ***** 
"Let's start seeding everything to make results somewhat reproducible. Anyway, in keras it is quite hard to get 100% reproducible results."
## Some Exploratory Data Analysis With Iris
The above graph shows relationship between the sepal length and width. Now we will check relationship between the petal length and width.
As we can see that the Petal Features are giving a better cluster division compared to the Sepal features. This is an indication that the Petals can help in better and accurate Predictions over the Sepal. We will check that later.
### Now let us see how are the length and width are distributed
### Now let us see how the length and width vary according to the species
The violinplot shows density of the length and width in the species. The thinner part denotes that there is less density whereas the fatter part conveys higher density
"Now, when we train any algorithm, the number of features and their correlation plays an important role. If there are features and many of the features are highly correlated, then training an algorithm with all the featues will reduce the accuracy. Thus features selection should be done carefully. This dataset has less featues but still we will see the correlation."
**Observation--->**\n\nThe Sepal Width and Length are not correlated\nThe Petal Width and Length are highly correlated\n\nWe will use all the features for training the algorithm and check the accuracy.\n\nThen we will use 1 Petal Feature and 1 Sepal Feature to check the accuracy of the algorithm as we are using only 2 features that are not correlated. Thus we can have a variance in the dataset which may help in better accuracy. We will check it later.
"## Historical Data Science Trends on Kaggle \n\nA number of trends have changed over the years in the field of Data Science. Kaggle is the largest and the most popular data science community across the globe. In this kernel, I am using Kaggle Meta Data to explore the Data Science trends over the years. \n\n## 1. Linear Vs Logistic Regression\n\nLets look at the comparison of linear regression and logistic regression discussions on forums, kernels and replies on kaggle. "
"> - From the above graph, we can observe that there were always been **more discussions related to logistic regression** than linear regression. The generel trend is that number of discussions are increasing every month. \n> - One indication is that there are more number of classification problems than regression problems on Kaggle including the most popular **Titanic Survival Prediction competition**. This competition has most number of discussions and is one of the longest running compeition on Kaggle. There is a regression competition as well : House Prices advanced regression, but people more often start it after titanic only.     \n> - The number of logistic regression discussions on forums, kernel comments, and replies boomed to high numbers in October 2017 and March 2018. One of the reason is the the **Toxic Comments Classification Competition""** in which a number of authors shared excellent information related to classification models including logistic regression. \n\n## 2. The dominance of xgboost"
"> - Among both the popular techniques on Kaggle - xgboost and deeplearning, xgboost has remained on top because it is faster and requires **less computational infrastructure** than very complex and deeper neural networks. \n\n\n## 6. What Kagglers are using for Data Visualizations ?"
"> - Plotly has gained so much popularity since 2017 and is one of the most used data visualization library among the kernels. The second best is seaborn which is used extensively as well. Some of the high quality visualization kernels by kaggle grandmasters such as SRK and Anistropic are created with plotly. Personally, I am a big fan of plotly as well. :P\n\n## 7. Important Data Science Techniques "
# Prepare the data analysis  \n\n## Load packages
## Load data
"Let's again imagine we want to do\n- a rolling time series split\n- where we have a gap of 2 days between train and validation sets\n- and we make the maximum size of each train set to be 7 days\n\nHere we specify the number of splits, the maximum number of groups in each train set, and the maximum number of groups in each valdiation set (sklearn has this convention where they call it the ""test"" set; I preserve that in the variable names, but prefer to call it the validation set)."
"# With the Real Competition Data\n\nIn the real competition data, the number of datapoints per day (that is per ""group"") is not constant as it was in the spoofed data. We need to confirm that the time series split respects that there are different counts of samples in the the days.\n\nWe load the data and reduce memory footprint."
"Let's imagine that you want to fit on 15 days of data, leave a gap of 5 days between test and validation, and then validate on 5 days of data."
"![Alt Text](https://media.giphy.com/media/5bGYUuT3VEVLa/giphy.gif)\n\nBoom... there you go. Notice that the sizes of the train, gap, and valid sets respect the different number of samples per day. Now you can split the data on **number of days** in the validation set, **number of days** gap between, and **number of days** in the train set. You can see in the ""group"" bars that the days are different lengths."
## Visualize the CV Results
It seems that the model is not that sensitive to the regularization parameter. I will err on the side of caution and choose a lower one.
4.1 TRAIN METADATA\n\n---\n
4.2 VISUALIZE THE UNIQUE SPECIES PRESENT IN THE DATASET\n\n---\n\n
4.2 VISUALIZE EXAMPLES OF UNIQUE INDIVIDUALS IN THE DATASET\n\n---\n\n
5.0 HELPFUL FUNCTIONS\n\n---\n
5.1 LOAD EMBEDDINGS AND OTHER DATA\n\n---
### 5.3.3 Do 1 backpropagation: Compute the LOSS and OPTIMIZE for our images_example:
"Until now we:\n1. Created a Vanilla FNN\n2. Took 1 image through the network and create prediction\n3. Look at the prediction vs actual and computed the loss\n4. Using the loss we updated the weights and biases\n\nThis is called training. The next chapters will be dedicated to training the network and improving it.\n\n# 6. Training the Neural Network\nOur purpose now that we have the structure in place and the data is to make the Vanilla FNN perform well.\n\n## 6.1 Batches\nWith an artificial neural network, we may want to use more than one image at one time. That way, we can compute the *average* loss across a **mini-batch** of **multiple** images, and take a step to optimize the **average** loss. The average loss across multiple training inputs is going to be less ""noisy"" than the loss for a single input, and is less likely to provide ""bad information"" because of a ""bad"" input.\n\nBatches can have different sizes:\n* one extreme is `batch_size` = 1: meaning that we compute the loss and update after EACH image (so we have 60,000 batches of size 1)\n* a `batch_size` = 60: means that, for 60,000 training images, we'll have 1000 batches of size 60\n* the other extreme is `batch_size` = 60,000: when we input ALL images and do 1 backpropagation (we have 1 batch of size 60,000 images)\n\nThe actual batch size that we choose depends on many things. We want our batch size to be large enough to not be too ""noisy"", but not so large as to make each iteration too expensive to run.\n\n\n\nIn the above example, instead of having 70 noisy losses we'll have just 7 averaged losses."
### 6.1.1 Training the Example Network on a batch instead of image by image:
"## 6.2 Accuracy of the Classifier\nDuring Training, we would usually want to check for the accuracy of the model, to see how good or how bad is performing.\n\n \nNote: During training, it is highly important to set the model into training mode by calling your_model.train(). This enables gradients training, the Dropout() function etc. When you evaluate the model call your_model.eval(). This disables the gradients, Dropout() function etc and sets the model in evaluation mode.\n"
### 6.4.2 Predefined Training Function\n\n
"# 7. Model Evaluation\n\nNow that we have our functions ready, we can start training on the ENTIRE dataset.\n\nBut first, to make the training faster, we will:\n* select 500 training images and 500 testing images\n* `batch_size` will be by default set to 20 images/batch\n* we'll iterate through the data 200 times (`num_epochs`=200)"
"# 8. Overfitting\n\nAs any other Machine Learning Model, Neural Nets can suffer from overfitting. Overfitting is when a neural network model learns about the quirks of the training data, rather than information that is generalizable to the task at hand.\n\n## 8.1 Data Augmentation\nWhy try to collect more data when you can create some on your own? *Data Augmentation* generates more data points from our existing data set by:\n* Flipping each image horizontally or vertically (won't work for digit recognition, but might for other tasks)\n* Shifting each pixel a little to the left or right\n* Rotating the images a little\n* Adding noise to the image\n\n\n\nFor our example we'll rotate the images randomly up to 35 degrees."
"### 8.1.1 Training on Augmented Data:\n\n* `transforms.Normalize()`: means to scale the input features of a neural network, so that all features are scaled similarly. For images is not really necessary, as they all have the same structure, but I threw it here just for reference."
"# 2.2.1 Age\n- In this part of the data we have age groups of participants,\n- By looking at recent years trends and Data Science being labelled as one of the best jobs by some news sources I expect high number of young participants.\n- With the all hype around data and data related jobs I expect increased demand from employers and graduates with interest in  kaggle .\n- Let's see if data confirms my hypothesis..."
"- Please don't burn my plot to the ground because of the ""Pie Chart"", I know many people hate them, but I find them useful for small categories like this.\n- Ok with leaving pie chart hate behind let's check the values we got, it seems my first instinct was true; most of the participants are younger than 35 years old.\n- And we can also see that 18-21 age group almost made it to top (0.1% difference), so the trend for data related jobs still going strong among students.\n- By looking at the other side of the chart we see either really experienced users or people got interest in data science in late stages of their career, believe me it's never too late! It's nice to see couple hundred of them hanging around..."
# 2.2.1.1 Survey Duration vs. Age\n- Maybe it's not that important to check this relationship but I got the curiosity to see if there is a meaningful relation between these two...
"- It looks like younger people tend to finish survey a little bit faster, or maybe experienced people are paying attention to details a little bit more, anyways no need to hurry eh? :) Let's get on the next demographic feature...\n"
"# 2.2.2 Gender\n- While Data Science sector growing fast, the male dominance still stands, that would be worrying if the trend keeps going incoming years. For some of the people it might not sound like a obvious problem but incresed bias in data related works might affect the outcomes of machine learning/AI predictions in near future. Let's take a look how're things stand for this subject on  kaggle ..."
"- Uhmm doesn't look much better than last year if I recall correctly, they stand around same like last year (~78%, ~19%). Maybe looking at some other relations with this data might help us to get better insights.\n\n# 2.2.2.1 Age vs Gender"
"- Another Pie'ish looking chart right? I'm sorry but this one looks little bit cooler, wouldn't you agree :) Anyways;\n- This one gives us a little bit better insight, by looking at age groups of dominant genders we can see that number of younger women are getting into data science/machine learning area a bit more than their counterparts.\n- 18-21 age group takes 5% more in their gender group than males, again 22-24 is 2% more than men followed by 25-29 which again little bit bigger fraction in women than men.\n- So by looking at percentages we would say younger women are getting into data science more and more, it's nice to have diversity among users in future!"
# 2.2.3 Countries\n- Another interesting demographic statistic for me is the country where participant currently residing. Again thanks to  kaggle 's ease to access and global availability we have chance to see how's the data science community develops around the world.\n- For me it's important to see equal growth around the world to build stronger community and wealth.\n- First let's take a look top countries with highest number of participants:
- Maybe plotting these values on World Map would be more visually satisfying and allows us to see wider perspective:
"- Looks like most of the participants are from Asia, led by India which keeps top spot in worldwide too.\n- In Africa; Nigeria is the flag bearer of the number of participants, followed by Egypt.\n- North America is led by USA which also holding global second place.\n- In South America Brazil has the biggest community then comes the Colombia.\n- Europe looks much more balanced than rest of the world if we don't count Austuralia (continent) :)"
# 2.2.3.1 Age vs. Country\n- Here we're going to check age group frequencies of the given countries.\n- It can give us insights about where's the data science community grows with lots of young people getting interested in the field.
"- I see that countries like Viet Nam, India, Indonesia, Bangladesh are having high number of young people are interested in field where bigger portion of their participants are younger than 25. \n- Meanwhile countries from Europe, Arabian Peninsula, North East Asia, North America are getting little older in average age distribution. I wouldn't say surely without related data but my inner instincts are telling me that there could be two/three reasons for this:\n * These countries adopted data science education earlier and already have experienced people.\n * People moving abroad to bigger companies clustered in some countries after getting experienced in their home countries.\n * Or some countries having really big population growth as well as interest in data science is rising among the studends.\n- Hmm which one it could be...\n\n"
The above graph indicates that men are more likely with dementia than women.
The chart shows Nondemented group got much more higher MMSE scores than Demented group.
The chart indicates that Nondemented group has higher brain volume ratio than Demented group. This is assumed to be because the diseases affect the brain to be shrinking its tissue. 
There is a higher concentration of 70-80 years old in the Demented patient group than those in the nondemented patients.\nWe guess patients who suffered from that kind of disease has lower survival rate so that there are a few of 90 years old.
"## Intermediate Result Summary\n1. Men are more likely with demented, an Alzheimer's Disease, than Women.\n2. Demented patients were less educated in terms of years of education.\n3. Nondemented group has higher brain volume than Demented group.\n4. Higher concentration of 70-80 years old in Demented group than those in the nondemented patients."
## Seaborn Pairplot
### Seaborn PairGrid
# Import Libraries 
# Load the Dataset and Add headers
## Ratings
Most of the people has given the rating of 5
"# Part 1: Importing Necessary Libraries and datasets\n***\n\n## 1a. Loading libraries\n\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate. "
## 1b. Loading Datasets\n\n***
"The seasonality patterns can be explored in detail by using boxplots. Seasonality is clearly confirmed for the categories of R03, R06 and N02BE. Some additional conclusions: R03 and N05C has more outliers that the others, indicating that their sales is more difficult to predict."
"Below, boxplots on a weekly scale are shown, for the purpose of exploring the weakly seasonality. Some weekly seasonality is visible."
"Another visualization that can be useful for discovering seasonality patterns is related to rolling window means. Rolling window operations are another important transformation for time series data. Similar to downsampling, rolling windows split the data into time windows and the data in each window is aggregated with a function such as mean(), median(), sum(), etc. However, unlike downsampling, where the time bins do not overlap and the output is at a lower frequency than the input, rolling windows overlap and ""roll"" along at the same frequency as the data, so the transformed time series is at the same frequency as the original time series. That means also that the curve is smoother. Time series data often exhibit some slow, gradual variability in addition to higher frequency variability such as seasonality and noise. An easy way to visualize these trends is with rolling means at larger time scales. Analysis below shows 30-day and 365-day rolling mean and 30-day rolling standard deviation of sales data."
"Image below shows trends for each of the drug categories, represented by the 365-d rolling means for each of those categories."
"# Home Credit Default Risk - Exploration + Baseline Model\n\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders. Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n\nThis is a simple notebook on exploration and baseline model of home credit default risk data \n\n**Contents**   \n[1. Dataset Preparation](#1)    \n[2. Exploration - Applications Train](#2)  \n     [2.1 Snapshot - Application Train](#2.1)    \n     [2.2 Distribution of Target Variable](#2.2)    \n     [2.3 Applicant's Gender Type](#2.3)    \n     [2.4 Family Status of Applicants who takes the loan](#2.4)  \n     [2.5 Does applicants own Real Estate or Car](#2.5)    \n     [2.6 Suite Type and Income Type of Applicants](#2.6)   \n     [2.7 Applicants Contract Type](#2.7)   \n     [2.8 Education Type and Occupation Type](#2.8)   \n     [2.9 Organization Type and Occupation Type](#2.9)   \n     [2.10 Walls Material, Foundation and House Type](#2.10)   \n     [2.11 Amount Credit Distribution](#2.11)    \n     [2.12 Amount Annuity Distribution - Distribution](#2.12)  \n     [2.13 Amount Goods Price - Distribution](#2.13)   \n     [2.14 Amount Region Population Relative](#2.14)    \n     [2.15 Days Birth - Distribution](#2.15)   \n     [2.16 Days Employed - Distribution](#2.16)    \n     [2.17 Distribution of Num Days Registration](#2.17)  \n     [2.18 Applicants Number of Family Members](#2.18)  \n     [2.19 Applicants Number of Children](#2.19)  \n[3. Exploration - Bureau Data](#3)  \n     [3.1 Snapshot - Bureau Data](#3)    \n[4. Exploration - Bureau Balance Data](#4)  \n     [4.1 Snapshot - Bureau Balance Data](#3)     \n[5. Exploration - Credit Card Balance Data](#5)   \n     [5.1 Snapshot - Credit Card Balance Data](#3)   \n[6. Exploration - POS Cash Balance Data](#6)   \n     [6.1 Snapshot - POS Cash Balance Data](#3)   \n[7. Exploration - Previous Application Data](#7)   \n     [7.1 Snapshot - Previous Application Data](#7.1)  \n     [7.2 Contract Status Distribution - Previous Applications](#7.2)  \n     [7.3 Suite Type Distribution - Previous Application](#7.3)    \n     [7.4 Client Type Distribution  - Previous Application](#7.4)    \n     [7.5 Channel Type Distribution - Previous Applications](#7.5)  \n[8. Exploration - Installation Payments](#8)  \n     [8.1 Snapshot of Installation Payments](#3)  \n[9. Baseline Model](#9)  \n     [9.1 Dataset Preparation](#9.1)  \n     [9.2 Handelling Categorical Features](#9.2)     \n     [9.3 Create Flat Dataset](#9.3)     \n     [9.4 Validation Sets Preparation](#9.4)    \n     [9.5 Model Fitting](#9.5)    \n     [9.6 Feature Importance](#9.6)    \n     [9.7 Prediction](#9.7)   \n\n\n\n## 1. Dataset Preparation "
## 2. Exploration of Applications Data \n\n### 2.1 Snapshot of Application Train\n\nApplication data consists of static data for all applications and every row represents one loan.
"> Married people have applied for a larger number of loan applications about 196K, However, people having Civil Marriage has the highest percentage (about 10%) of loan problems and challenges. \n\n### 2.5. Does applicants own Real Estate or Car ?"
"> About 70% of the applicants own Real Estate, while only 34% of applicants own Car who had applied for the loan in the past years. However, a higher percentage of people having payment difficulties was observed with applicants which did not owned Car or which did not owned Real Estate. \n\n### 2.6 Suite Type and Income Type of Applicants "
"> We see that Applicants having Income Types : Maternity Leaves and UnEmployed has the highest percentage (about 40% and 36% approx) of Target = 1 ie. having more payment problems, while Pensioners have the least (about 5.3%). \n\n### 2.7. Applicant's Contract Type"
> Cash loans with about 278K loans contributes to a majorty of total lonas in this dataset. Revolving loans has significantly lesser number equal to about 29K as compared to Cash loans. \n\n### 2.8 Education Type and Housing Type 
### 2.11. Distribution of Amount Credit 
### 2.12 Distribution of Amount AMT_ANNUITY 
### 2.13 Distribution of Amount AMT_GOODS_PRICE 
#### Plot the time series data to detect patterns
"If we want to predict the  temperature for the next few months, we will try to look at the past values and try to gauge and extract the pattern. \nHere we observe a pattern within each year indicating a seasonal effect. Such observations will help us in predicting future values.\n\n**Note: We have used only one variable here , Temp (the temperature of the past 19 years).**\n\nHence this is called as the Univariate Time Series Analysis/Forecasting. "
- Nothing much for the skewness. Quite a normal like distribution for the numerical features.
"- Based on the  matrix, we can observe weak level correlation between the numerical features and the target variable\n- Oldpeak (depression related number) has a positive correlation with the heart disease.\n- Maximum heart rate has negative correlation with the heart disease.\n- interestingly cholesterol has negative correlation with the heart disease.\n"
"# Overall IDE Popularity\nTakeaways:\n* Jupyter is a big favorite, with over 70% of repondents using it.\n* RStudio leads up the rest of the pack. It's safe to say that respondents who code esclusively in R would prefer RStudio as their main IDE.\n* Notepad++ is the top pure text editor, above Sublime.\n* Some reponded with *None*.  Is it safe to assume that these people don't code?"
"# Kaggle vs.  StackOverflow IDE Use\nThe 2018 Stack Overflow survey posed a very similar question we can compare the results with to see how kaggle compares to the stack overflow community regarding IDE use. This data was taken from the [stackoverflow developer survey results](https://insights.stackoverflow.com/survey/2018/).\n\nSome observations about the comparison:\n- Notepad ++ is popular for both communities. 40.97% of Kaggle respondents and 34.2% of Stackoverflow respondents have used it.\n- Jupyter/IPython is much more popular among kagglers, as expected. But the difference is staggering, with 73.29% of kaggle respondents using it and only 7.4% of stackoverflow respondents saying they use it.\n- Visual Studio and Visual Studio Code are the most popular of the stackoverflow developers, but relatively less popular for kagglers."
"# Overlap Between IDE Useage\nSeeing which IDEs are popular is one thing, but more importantly we want to know which software users tend to use together. We can start by making venn-diagrams with the most popular IDES. These are interesting because we can not only see how many people have used the IDE, but also comminality in IDE use."
"# Students Love Jupyter!\n\nNearly half of the Kaggle survey respondents answered that they were ""students"". Since jupyter has a large use within academia and teching, it's interesting to see how the venn diagram differs between students and non-student respondents. While Jupyter is still very popular amonst both groups, the overlap between jupyter and the next top two popular IDEs (RStudio and Notepad++) varies much less for students. For non-students there are some exclusive users of other development environments like RStudio and Notepad++, but also every student that uses these IDEs  has also had some interaction with Jupyter."
"# Notepad++ is the most popular 'text editor' but others are close behind.*\nWe can also gain insights by looking into IDEs that serve the same purpose. Since RStudio is typically used to code in R, and Jupyter is unique in it's interactive cells - what happens if we only look at text editors?\n\nThis is a pretty clean venn diagram. You can see that there are more exclusive Notepad++ users than PyCharm and Sublime Text, but the areas where they overlap are strikingly similar. The 1556 people in the center are possibly just people who enjoy trying out new software.  The question that is being asked states `[which IDE] have you used at work or school in the last 5 years?` This could mean that respondents aren't necessarily stating which IDE they like the most, just the ones they've used.\n\n\* *It's important to note (as pointed out by @sciplic in the comments) Notepad++ is a Windows only text editor. To me this is even understates it's popularity since Mac and Linux users don't have it as an option.*"
"# Job Title and development environments\n\nLet's work off of the hypothesis that the type of work you do will impact the type of tools you use. We compare the IDE use of different job types below. This chart allows us to see what percentage of professional groups use each IDE. \n\nSome interesting insights we can see are:\n1. Jupyter/IPython is popular for pretty much every job title. This is obviously biased because the respondents also use kaggle! Surveying non-kaggler \n2. Statisticians use Jupyter the least (51.9%). Is this because they also tend to be R users (74.68% use RStudio)?\n3. DBA/Database Engineers and Software Engineer are heavy Notepad++ users, but I find it personally suprising that Data Journalists and Developer Advocates also use this IDE.  What drives this similarity among disparate occupations?\n4. Visual Studio and Visual Studio Code are popular amonst Developer Advocates and Software Engineers\n5. Matlab looks to be quite popular within academia. Students, Research Assistants, Research Scientists, and Principal Investigators stand out as Matlab users."
"# Correlations between IDEs and Salary\n\nIt's always fun to plot the data by salary.   Of course, we shoud be weary of drawing any causal conclusions from this analysis. Correlation does note equal causation! Don't change the IDE you use thinking it will make you more money.\n\n*Note: I ignored NA values and those that answered ""I do not wish to disclose my approximate yearly compensation"" to the salary question. This may bias our results and should be considered.\n\nSome takeaways:\n- Each IDE appears to have it's own unique trend when looking at popularity vs. income which I find unexpected and interesting.\n- RStudio use appears to have a somewhat normal distribution around the mid-salary range.\n- PyCharm and VSCode appear to be fairly flat, with little changes in popularity around income.\n- Notepad++ starts to trend downward as salary goes above \$100k. VIM trends upwards."
"## Taking a closer look at the relationship between salary and IDE use\nIn the below chart we can see the raw number of respondents from each salary grouping. \nWith a chart like this insights are usually in the abnormalities. For instance:\n- Matlab is popular among those on the low range of salary. This is consistent with the fact that many students use Matlab. \n- PyCharm and Sublime Text catch up to Notepad++ in popularity in the higher salary range (\$100k+). Could this be because this is the salary range of a serious python developer?\n- We can see the proportinal relationship between Jupyter and unix based text editors varies in salary range. We see a Jupyter to Vim ratio of almost `4:1` in the \$0-\$10k salary, but that drops down to a `2:1` ratio in the \$150-\$200k range. Could it be that those that have more job experience also spend more time in the unix shell?"
# IDE Use By Age\nWe can also take a look at how age and IDE use relate. Suprisingly age did not appear to have any overwhelmingly apparent correlations with IDE use. It does appear that RStudio is less popular proportionally amonst the younger respondents (Ages 18-24).
When we plot the age distribution of each IDE out individually we can see that the 22-24 age group stands out as different depending on IDE. Could this be because 22-24 year olds are still deciding on which IDE they prefer- or could it simply be that school requires them to use different IDEs than the older population?
"# Hosted Notebooks and IDEs\nAs we've already seen, jupyter notebooks are quite popular in the kaggle community. But what about jupyter-like hosted notebooks?  We could make the argument that these are IDEs aswell. Lets look at the popularity of hosted notebooks from the kaggle survey.\n- Over 30% of respondents have no experience with hosted notebooks\n- Not suprisingly Kaggle Kernels are the most popular for kaggle respondents with roughtly 25% of respondents saying they've used them. What may be more suprising is how close the next two popular hosted notebooks are. JupyterHub/binder being used by 20% and Google Colab by 14.8%.\n- After that it's a steep dropoff to the next types of hosted kernels."
Again we can use the venn-diagram to see how users overlap in their use of hosted notebooks. Only 661 respondents have experience with all three of the top 3 most popular hosted notebook platforms.
"# Freeform Responses\nThe survey question we are exploring provided a list of options for respondents to select from. There is an obvious selection bias against any IDE not on that list. We can however explore the freeform responses some users chose to provide. The wordcloud below makes some of the most popular IDEs that were not selection options. Eclipse, emacs, netbeans, xcode, octave all are very popular freeform responses. Kaggle should consider including these as options in the 2019 survey."
# The 4 Types of Kagglers (by IDE use)\n\n\n\n      The Jupyter Lover \n             \n  The Jack of All IDEs \n             \n  The RStudio + Jupyter \n            \n  The Anti-Jupyter \n                 \n\n
# **Exploratory Data Analysis**
# **Target Distribution**\n
# **Feature Distribution**\n\nLets analyze the distribution of first nine features starting from 'f_0' to 'f_8'
# **Correlation Heatmap**\n\nLets analyze the correlation of first nine features starting from 'f_0' to 'f_8'
# **Correlation of Target and Features**\n\nLets analyze the correlation of first nine features starting from 'f_0' to 'f_8' with the target
"# **🎯tf.data**\n\n[Source](https://www.tensorflow.org/guide/data)\n\ntf.data API is used for building efficient input pipelines which can handle large amounts of data and perform complex data transformations . tf.data API has provisions for handling different data formats .\n\n\n\n[Image Source](https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data)\n\nData source is essential for building any input pipeline and tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices can be used to construct a dataset from data in memory .The recommended format for the iput data stored in file is TFRecord which can be created using TFRecordDataset() .The different data source formats supported are numpy arrays , python generators , csv files ,image , TFRecords , csv and text files. \n\n\n\n[Image Source](https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data)\n\nConstruction of tf.data input pipeline consists of three phases namely Extract , Transform and Load . The extraction involves the loading of data from different file format and converting it in to tf.data.Dataset object .\n\n## **🎯tf.data.Dataset**\n\ntf.data.Dataset is an abstraction introduced by tf.data API and consists of sequence of elements where each element has one or more components . For example , in a tabular data pipeline , an element might be a single training example , with a pair of tensor components representing the input features and its label \n\ntf.data.Dataset can be created using two distinct ways\n\nConstructing a dataset using data stored in memory by a data source\n\nConstructing a dataset from one or more tf.data.Dataset objects by a data transformation\n\n\n\n[Image Source](https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data)\n\n"
## División de género
"La gráfica muestra claramente que hay muchos más encuestados masculinos en comparación con las mujeres. Parece que las damas estaban ocupadas con su trabajo, **o las damas no trabajan** ...: p. Es una broma."
## Los encuestados por país
"**USA e India**, constituyen máximos encuestados, alrededor de 1/3 del total. Del mismo modo, Chile tiene el número más bajo de encuestados. ¿Este gráfico es suficiente para decir que la mayoría de los usuarios de Kaggle son de India y EE. UU. No lo creo, ya que los usuarios totales en Kaggle son más de 1 millón, mientras que el número de encuestados es de solo 16k."
¡Mira ese enorme salario! Eso es **incluso más grande que el PIB de muchos países**. Otro ejemplo de respuesta falsa. El salario mínimo tal vez un caso de un estudiante. El salario medio muestra que Data Scientist disfruta de buenos beneficios salariales.
### Compensación por país
"El gráfico de la izquierda muestra los 15 principales países con salarios medios altos. Es bueno ver que estos países proporcionan un salario mejor que el salario promedio del conjunto de datos completo. De manera similar, el gráfico de la derecha muestra el salario promedio de los 15 países principales por encuestados. El gráfico más impactante es para **India**. India tiene los 2dos más altos encuestados, pero todavía tiene el salario medio más bajo en el gráfico. Las personas en los EE. UU. Tienen un salario casi 10 veces más que sus contrapartes en la India. ¿Cuál puede ser la razón? ¿Son los profesionales de TI en la India realmente mal pagados? Lo comprobaremos más tarde."
### Salario por género
El salario para los hombres parece ser alto en comparación con otros.
## Edad
"Los encuestados son jóvenes, la mayoría de ellos en el rango de edad entre 25 y 35 años."
## Profesion & Especialidad
"La ciencia de datos y el aprendizaje automático se utilizan en casi todas las industrias. Esto es evidente en el gráfico de la izquierda, ya que personas de diferentes áreas de interés como Física, Biología, etc. lo están tomando para una mejor comprensión de los datos. El gráfico del lado derecho muestra el trabajo actual de los encuestados. Una gran parte de los encuestados son Dats Scientists. Pero como se trata de datos de encuestas, sabemos que puede haber muchas respuestas ambiguas. Más adelante veremos si estos encuestados son científicos de datos reales o científicos de datos autoproclamados."
#### Distribution of Categorical Features :
- **Outcome** displays a **Normally Distributed** data.
"- **Pregnancies**, **Insulin**, **DiabetesPedigreeFunction** and **Age** have **positively or rightly** skewed data distribution.\n- **BloodPressure** and **Skin Thickness** display a **bidmodal data distribution**. \n- Data distributions of **Glucose** & **BMI** are a bit tricky. This is because they near about highlight a **normal distribution** or **bimodal distribution**. This is because of the small peak present at the value **0**.\n- For this notebook, we will consider these distributions as **bimodal**."
### Target Variable Visualization (Outcome) : 
"- The dataset is **unbalanced** in a **2 : 1** ratio for **Non-Diabetes : Diabetes** cases!\n- Due to this, predictions will be biased towards **Non-Diabetes** cases.\n- Visualizations will also display this bias, thus making it difficult to gain insights."
## Preparing the Titanic dataset ##\n\nFor the Titanic challenge we need to guess wheter the individuals from the *test* dataset had survived or not. But for our current purpose let's also find out what can the data tell us about the shipwreck with the help of a *Classification Tree*. Let's load the data and get an overview.
"Thanks to this overview we can see that our dataset needs some treatment. The class *Survived* is already in binary format so no additional formatting is necessary, but features like *Name*, *Ticket* or *Cabin* need to be adapted for the problem we're trying to solve, and we can also engineer some new features by merging or regrouping existing ones. There's already extended work on this so we're just using one the best approches out there (credit to [Sina][1], [Anisotropic][2] and also [Megan Risdal][3] for the suggestion of the ""Title"" feature).\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n  [2]: https://www.kaggle.com/arthurtok/titanic/introduction-to-ensembling-stacking-in-python\n  [3]: https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic"
"Our dataset is now much cleaner than before, with only numerical values and potentially meaningful features. Let's now explore the relationship between our variables by plotting the Pearson Correlation between all the attributes in our dataset (credit to [Anisotropic][1] for this beautiful plot):\n\n\n  [1]: https://www.kaggle.com/arthurtok/titanic/introduction-to-ensembling-stacking-in-python"
"This heatmap is very useful as an initial observation because you can easily get an idea of the predictive value of each feature. In this case, *Sex* and *Title* show the highest correlations (in absolute terms) with the class (*Survived*): 0.54 and 0.49 respectively. But the absolute correlation between both is also very high (0.86, the highest in our dataset), so they are probably carrying the same information and using the two as inputs for the same model wouldn't be a good idea.  High chances are one of them will be used for the first node in our final decision tree, so let's first explore further these features and compare them."
###  Closing stock price visualizations & maximum price during 5 years   \n
"**Key findings:**\n    \n- **We could find TOP 10 most traded stocks duing period of 2013-2018.**\n- **Out of 10 companies one is bank, 7 of them are tech companies, another two are non-tech lagacy companies namely General electric and Ford motors.**\n- **From closing stock price visualization, we can learn that stocks 'GE' and 'F' are declining and other tech stocks are rising over a five year period time.**\n- **As we can see visualizations are self-explanatory and we can learn all time high stock prices of all the tickers.**"
###  Trade volume of stocks over a period of 2013-2018 \n
"**Observations:**\n\n- **Above visualizations depicts what was the maximum, minimum and average trade volume overa period of 2013-2018.**\n- **As visualizations are self-explanatory in nature we can also learn variance of trade volume for example stock ticker 'INTC' has a higest variance in trade volume compared other tickers.**"
"### What is comparative analysis of stocks in finance and investment industry?\n\n- **An important aspect of the fundamental analysis of stocks is comparing stocks of the same sector. The most basic way to analyse and compare stocks from the same sector is to conduct an analysis of different ratios like Earnings per share (EPS), Price-to-Earnings (P/E Ratio), Return on Equity (ROE), Return on Capital Employed (ROCE), and Debt-to-Equity ratios, and stock-prices of various companies, trade volume of stocks.**\n- **In this project, due to limited data of companies, we can only compare daily mean stock price of companies and can make inferences like stock price comparison and relative stock price fluctuations that I have mentioned below chart.**\n\n![image.png](attachment:d58c76f2-bc7b-4465-9ef2-df7dba41c157.png)"
"**Observations**\n\n- **It is very clear that 'FB' stock was one of the most expensive among all 7 tech stocks**\n- **stock ticker 'AMD' was among the cheapest to buy compared to other stocks**\n- **From above chart we can also conclude that stocks like 'FB' and 'AAPL' were also among the most volatile in nature than other stocks**\n\n### What is volatility?👀📈\n\n> **In finance, volatility (usually denoted by σ) is the degree of variation of a trading price series over time, usually measured by the standard deviation of logarithmic returns.**\n> \n\n*source: wikipedia*\n\n**In layman terms, volatility is nothing but the fluctuation of stock price during given period of time**\n\n![image.png](attachment:3710fd6a-fef5-4c22-b2a3-335d68119b63.png)"
"### Growth of stock prices\n\n- **In finance and investment industry, stock price growth is really important metric one needs to measure to find out the how is stock or investment of an individual is growing**\n- **Below is the formula to find out growth of stock prices**\n\n*source: educba.com*\n\n![image.png](attachment:0712cf5f-a2fd-4aaf-9636-8ee4d9ae0c7b.png)"
**Observations**\n\n- **We can observe that growth of stock 'Facebook' is the highest among all other 10 stocks over a period of 5 years**\n- **It is very much self-explanotary that stocks of 'Ford Motors' and 'General Electric' has given negative return over a years of period.**
"4  TABULAR DATA\n\nRECALL THAT THESE ARE THE TRAIN COLUMNS\n> **`image_id`** - unique image identifier\n**`class_name`** - the name of the class of detected object (or ""No finding"")\n**`class_id`** - the ID of the class of detected object\n**`rad_id`** - the ID of the radiologist that made the observation\n**`x_min`** - minimum X coordinate of the object's bounding box\n**`y_min`** - minimum Y coordinate of the object's bounding box\n**`x_max`** - maximum X coordinate of the object's bounding box\n**`y_max`** - maximum Y coordinate of the object's bounding box"
"4.1  IMAGE_ID COLUMN EXPLORATION\n\n---\n\nThe **`image_id`** column contains a **U**nique **ID**entifier (**UID**) that indicates which patient the respective row (object) relates to.\n\nAs there can be up to three radiologists annotating the same image and potentially multiple objects/bboxes per image, it is possible for a single image UID to occur many times. However, please note that we know from the competition data details that there exists ***only one image for one patient***. This means that if a specific image_id appears 12 times, that there are 4 objects in the image, and each object was annotated by all three radiologists.\n\n*SIDE-NOTE* – Due to the ***one image to one patient*** rule, the column name **`image_id`** could be replaced with **`patient_id`** and it would mean exactly the same thing.\n\n\n\nTOTAL OBJECT ANNOTATIONS PER IMAGE\n\nLet's count the distribution of the amount of annotations per unique **`image_id`** value. Note that we use a log-axis for the count axis to handle the large number of values present at 3 annotations (a single object annotated similarily by 3 radiologists)\n\n---\n\n**From the histogram plotted below we can ascertain the following information:**\n* Images contain at least 3 annotations (1 distinct object annotation by 3 radiologists)\n* Images contain at most 57 annotations (19 distinct object annotations by 3 radiologists)\n* The vast majority of images only have 3 annotations (~11,000 out of 15,000 images)\n* The distribution has a heavy skew (**`value=3.8687`** **`# FROM --> scipy.stats.skew(train_df.image_id.value_counts().values)`**). Remember that a perfectly symetrical distribution would have a skew value of **`0`**."
"UNIQUE OBJECT ANNOTATIONS PER IMAGE\n\nLet's count the distribution of **UNIQUE** object-label annotations per unique **`image_id`** value. This means if a radiologist identifies 8 nodules in an image, we count that as 1 unique object annotation. The goal of this is to determine the distributions of different diseases occuring within the same patient.\n\nNote that we use a log-axis for the count axis to handle the large number of values present at 1 unique abnormality\n\n---\n\n**From the histogram plotted below we can ascertain the following information:**\n* Images contain no more than 10 unique abnormalities (out of a possible 14)\n* The more unique abnormalities present in an image, the rarer it is."
4.2  CLASS_NAME COLUMN EXPLORATION\n\n---\n\nThe **`class_name`** column indicates the label as a string for the respective object/annotation (each row is for one object/annotation). \n\n\nANNOTATIONS PER CLASS\n\nWe know there are 15 different possible **`class_name`**s (including **`No finding`**). To identify the distribution of counts across the labels we will use a bar-chart.
"4.3  CLASS_ID COLUMN EXPLORATION\n\n---\n\nThe **`class_id`** column indicates the label encoded as a number the respective object/annotation (each row is for one object/annotation). Knowing this, we will remove the previous **class_name** column, as we would rather work with a numeric representation. Prior to removal we will generate a map that will allow us to translate the numeric labels back into their respective string represntations."
"4.4  RAD_ID COLUMN EXPLORATION\n\n---\n\nThe **`rad_id`** column indicates the the ID of the radiologist that made the observation. Remember, three radiologists will annotate a given image out of a pool of seventeen possible radiologists, where the radiologist ID is encoded from R1 to R17.\n\n\nANNOTATIONS PER RADIOLOGIST\n\nWe know there are 17 possible radiologists (**`rad_id`**s). To identify the distribution of annotations performed across the radiologists we will use a historgram.\n\n---\n\n**From the histogram plotted below we can ascertain the following information**\n* 3 of the radiologists (R9, R10, & R8 in that order) are responsible for the vast majority of annotations (~40-50% of all annotations)\n* Among the other 14 radiologists there is some variation around the number of annotations made, however, these 14 radiologists all made between 3121 annotations and 812 annotations with the vast majority annotating 1800-2200 objects."
"ANNOTATIONS PER RADIOLOGIST SEPERATED BY CLASS LABEL\n\nWe have already identified that three of the radiologists are responsible for almost 50% of all of the annotations. We would now like to identify if all of the radiologists were able to see and annotate all 15 classes. If so, can we identify any additional skew or problems that might arise?\n\n---\n\n**From the first histogram plotted below we can ascertain the following information**\n* 3 of the radiologists (R9, R10, & R8 in that order) are responsible for the vast majority of annotations (~40-50% of all annotations)\n* Among the other 11 radiologists there is some variation around the number of annotations made, however, these 11 radiologists all made between 3121 annotations and 812 annotations with the vast majority annotating 1800-2200 objects.\n\n---\n\n**From the second histogram plotted below we can ascertain the following information**\n* Among the other 11 radiologists, 7 of them (R1 through R7) have only ever annotated images as **`No finding`**\n* The other 4 radiologists are also heavily skewed towards the **`No finding`** label when compared to the main 3 radiologists (R8 through R10). This seems to actually be closer to the overall distribution, however it might allow us to estimate that radiologists other than R8, R9, and R10, are much more likely to annotate images as **`No finding`**.\n* The downside to this distribution, is that if we include this information in the model than the model will learn that 7 of the radiologists classify images as **`No finding`** 100% of the time!\n\n        Note that this second plot could have been generated by interacting with the first histogram as plotly has this functionality built-in"
"4.5  EXPLORATION OF BBOX COORDINATE COLUMNS\n\n---\n\nThe **`x_min`**, **`y_min`**, **`x_max`**, and **`y_max`** columns indicate the location of the annotated object bounding box, where the top-left corner is represented by the tuple (**`x_min`**, **`y_min`**) and the bottom-right corner is represented by the tuple (**`x_max`**, **`y_max`**).\n\nA value of **`NaN`** coincides with a label 14 (**`No finding`**) and means that there is nothing to annotate (healthy x-ray).\nFor the purpose of examining these columns we will only be examining rows where the objects have been annotated with a bounding box\n(i.e. All rows with a label of **`No finding`** will be discarded)\n\nPLOT HEATMAP REPRESENTING BOUNDING BOXES FOR VARIOUS CLASSES\n\nThere's a lot to digest within these plots. The important thing to focus on will be identifying for each class the approximate range of locations the annotations are found in and the intensity of the locations within the heatmap.\n\n---\n\n**From the heatmaps plotted below we can ascertain the following information**\n* Regarding Aortic Enlargement (CLASS-ID: 0)\n    * Heatmap distribution is slightly oval (vertical) and is very tight and intense, located in the centre of the image (slight drift to the top-right).\n* Regarding Atelectasis (CLASS-ID: 1)\n    * Heatmap distribution is lung shaped and relatively diffuse with a circular focus on the upper-left part of the left lung.\n* Regarding Calcification (CLASS-ID: 2)\n    * Heatmap distribution is lung shaped and relatively diffuse with a oval (vertical) focus on the top-left edge of the right lung.\n* Regarding Cardiomegaly (CLASS-ID: 3)\n    * Heatmap distribution is rectangular and is very tight and intense, located in the bottom-centre (to bottom-centre-right) of the image.\n* Regarding Consolidation (CLASS-ID: 4)\n    * Heatmap distribution is lung shaped and relatively diffuse, the focus of the distribution covers the entire left lung.\n* Regarding ILD (CLASS-ID: 5)\n    * Heatmap distribution is lung shaped and relatively diffuse, the focus leans a little towards the centre of the lungs.\n* Regarding Infiltration (CLASS-ID: 6)\n    * Heatmap distribution is lung shaped and relatively diffuse, the focus of the distribution covers the entire left lung.\n* Regarding Lung Opacity (CLASS-ID: 7)\n    * Heatmap distribution is lung shaped and relatively diffuse, the focus of the distribution covers the entire left lung.\n* Regarding Nodule/Mass (CLASS-ID: 8)\n    * Heatmap distribution is lung shaped and relatively diffuse, the focus leans a little towards the centre of the lungs. (NOTE: The diffusion pattern looks patchy... probably due to smaller bounding boxes)\n* Regarding Other Lesion (CLASS-ID: 9)\n    * Heatmap distribution is incredibly diffuse and covers most of the image, the focus is towards a vertical-strip in the centre of the image.\n* Regarding Pleural Effusion (CLASS-ID: 10)\n    * Heatmap distribution is lung shaped (slightly more rectangular?) and relatively diffuse, the focus is towards the bottom of the lungs and although both lungs are covered, the left lung has a stronger focus.\n* Regarding Pleural Thickening (CLASS-ID: 11)\n    * Heatmap distribution is vaguely lung shaped (patches near top and focus trails down exterior lung edge fading as it goes), the focus is towards the top of the lungs is oval (horizontal).\n* Regarding Pneumothorax (CLASS-ID: 12)\n    * Heatmap distribution is lung shaped (more rectangular), the focus is on the entire left lung however the right lung has some diffuse coverage.\n* Regarding Pulmonary Fibrosis (CLASS-ID: 13)\n    * Heatmap distribution is vaguely lung shaped (patches near top and focus trails down lung fading as it goes), the focus is towards the top of the lung and it is oval.\n"
"INVESTIGATE THE SIZES OF BOUNDING BOXES AND THE IMPACT OF CLASS\n\nAs we wish to examine the average, as well as the upper and lower limits for various class-based bounding box statistics, we will use a box plot to investigate. To make things easier to understand let us consider the following basic buckets.\n\nBounding Box Area - Median\n* Under   0.01 –– Smallest\n* 0.01 to 0.02 –– Small\n* 0.02 to 0.04 –– Medium\n* 0.04 to 0.06 –– Large\n* Above   0.06 –– Largest\n\nBounding Box Area - Quartile Range\n* Under     0.0075 –– Smallest\n* 0.0075 to 0.0125 –– Small\n* 0.0125 to 0.0250 –– Medium\n* 0.0250 to 0.0500 –– Large\n* Above     0.0500 –– Largest\n\n---\n\n**From the boxplot plotted below we can ascertain the following information**\n* Regarding Aortic Enlargement Box Plot (CLASS-ID: 0)\n    * Median Value is Small  –––  Quartile Range is Smallest\n* Regarding Atelectasis Box Plot (CLASS-ID: 1)\n    * Median Value is Medium  –––  Quartile Range is Large\n* Regarding Calcification Box Plot (CLASS-ID: 2)\n    * Median Value is Smallest  –––  Quartile Range is Medium\n* Regarding Cardiomegaly Box Plot (CLASS-ID: 3)\n    * Median Value is Large  –––  Quartile Range is Large\n* Regarding Consolidation Box Plot (CLASS-ID: 4)\n    * Median Value is Medium  –––  Quartile Range is Large\n* Regarding ILD Box Plot (CLASS-ID: 5)\n    * Median Value is Largest  –––  Quartile Range is Largest\n* Regarding Infiltration Box Plot (CLASS-ID: 6)\n    * Median Value is Medium  –––  Quartile Range is Large\n* Regarding Lung Opacity Box Plot (CLASS-ID: 7)\n    * Median Value is Medium  –––  Quartile Range is Large\n* Regarding Nodule/Mass Box Plot (CLASS-ID: 8)\n    * Median Value is Smallest  –––  Quartile Range is Smallest\n* Regarding Other Lesion Box Plot (CLASS-ID: 9)\n    * Median Value is Small  –––  Quartile Range is Large\n* Regarding Pleural Effusion Box Plot (CLASS-ID: 10)\n    * Median Value is Smallest  –––  Quartile Range is Large\n* Regarding Pleural Thickening Box Plot (CLASS-ID: 11)\n    * Median Value is Smallest  –––  Quartile Range is Smallest\n* Regarding Pneumothorax Box Plot (CLASS-ID: 12)\n    * Median Value is Largest  –––  Quartile Range is Largest\n* Regarding Pulmonary Fibrosis Box Plot (CLASS-ID: 13)\n    * Median Value is Small  –––  Quartile Range is Medium\n"
"INVESTIGATE THE ASPECT RATIO OF BOUNDING BOXES AND THE IMPACT OF CLASS\n\nWe want to understand the average shape (wide-narrow, square, etc.) of the bouning-boxes associated with each class, and to do this we will use a bar chart with some pre-drawn lines.\n\n---\n\n**From the bar chart plotted below we can ascertain the following information:**\n* The average size of bounding-boxes by class is usually close to square (usually on the horizontal rectangle size of square).\n* Cardiomegaly has, on average, very thin, rectangular, horizontal boxes (mean width is ~2.9x larger than mean height).\n* Pleural Thickening has, on average, thin, rectangular, horizontal boxes (mean width is ~1.9x larger than mean height).\n* ILD has, on average, somewhat thin, rectangular,  vertical boxes (mean height is ~1.6x larger than mean width)\n"
5  IMAGE DATA\n\nRecall that the image data is stored in DICOM format and the annotations are stored in our **`train_df`** Dataframe.
"# PRINCIPAL COMPONENT ANALYSIS\nPrincipal Component Analysis is a technique used in order to reduce the dimensionality of a dataset while preserving as mush information as possible. Data is reprojected in a lower dimensional space, in particular we need to find a projection that minimizes the squared error in reconstructing the original data. \n\n![Principal Component Analysis](https://sebastianraschka.com/images/faq/lda-vs-pca/pca.png)\n\n\nThere are 3 different technique in order to apply PCA:\n1. **Sequential**  \n2. **Sample Covariance Matrix**\n3. **Singular Value Decomposition (SVD)** \n\nI'll explain the Sample Covariance Matrix technique:\n* The first thing to do is to standardize the data, so for each sample we need to substract the mean of the full dataset and then divide it by the variance, so as having an unitary variance for each istance. This last process is not completly necessary but it is usefull to let the CPU work less.\n$$\n    Z = \frac{X-\mu}{\sigma^2} \n$$\n\n* Then we need to compute Covariance Matrix, given data { $x_1 ,x_2, ..., x_n$ } with $n$ number of samples, covariance matrix is obtained by:\n$$\n\Sigma = \frac {1}{n}\sum_{i=1}^n (x_i - \bar{x})(x - \bar{x})^T $$    $\;\;$  where  $$\bar{x} = \frac {1}{n}\sum_{i=i}^n x_i $$ \nOr simply by multiplying the standardized matrix Z by it self transposed\n$$ COV(X) = Z Z^T $$\n\n* Principal Components will be the eigenvectors of the Covariance Matrix sorted in order of importance by the respective eigenvalues.**Larger eigenvalues $\Rightarrow$ more important eigenvectors.** They represent the most of the useful information on the entire dataset in a single vector"
## PCA EXAMPLE
#### LINEAR SVM
#### SVM + K-FOLD
#### LINEAR SVM + PCA
#### KERNEL SVM + PCA
# **3. Import Libraries**\n\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks.
# **4. Reading the Data**
"Above we can see that 38% out of the training-set survived the Titanic. \n\nWe can also see that the passenger ages range from 0.4 to 80. \n\nOn top of that we can already detect some features, that contain missing values, like the ‘Age’ and 'Cabin' feature."
### **Age and Sex:**
**Observations:**\n\n1)The number of children increases with Pclass and the survival rate for passenegers below Age 10(i.e children) looks to be good irrespective of the Pclass.\n\n2)Survival chances for Passenegers aged 20-50 from Pclass1 is high.\n
"In order to be more accurate, Sex feature is used as the second level of groupby while filling the missing Age values.\n\n**Let's see why**"
"You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully.\n\nFor women the survival chances are higher between 14 and 40.\n\nFor men the probability of survival is very low between the age of 5 and 18, but that isn’t true for women. \n\nAnother thing to note is that infants also have a little bit higher probability of survival.\n\nWhen passenger class increases, the median age for both males and females also increases. However, females tend to have slightly lower median Age than males. The median ages below are used for filling the missing values in Age feature."
\n\nImport necessary libraries
\n\nLoad dataset and get initial information
"**Natural Language Processing or NLP** is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way.\n\n**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include Positive, Neutral, and Negative, Review Ratings and Happy, Sad. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject.\n\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.\n\n![Natural-Language-Processing.png](attachment:Natural-Language-Processing.png)\n\nIn this kernel we are going to focus on text classification and sentiment analysis part. In the next lessons we will study Information retrival, Question answering, etc"
"As we can see, the classes are imbalanced, so we can consider using some kind of resampling. We will study later. Anyway, it doesn't seem to be necessary."
"As we can see, the `ham` message length tend to be lower than `spam` message length."
- All the categorical features are near about **Normally Distributed**.
- **Oldpeak's** data distribution is rightly skewed.\n- **Cholestrol** has a bidmodal data distribution. 
### Target Variable Visualization (HeartDisease) : 
- The dataset is pretty much **evenly balanced!**
### Categorical Features vs Target Variable (HeartDisease) :
"- **Male** population has more heart disease patients than no heart disease patients. In the case of **Female** population, heart disease patients are less than no heart disease patients. \n- **ASY** type of chest pain boldly points towards major chances of heart disease.\n- **Fasting Blood Sugar** is tricky! Patients diagnosed with Fasting Blood Sugar and no Fasting Blood Sugar have significant heart disease patients. \n- **RestingECG** does not present with a clear cut category that highlights heart disease patients. All the 3 values consist of high number of heart disease patients.\n- **Exercise Induced Engina** definitely bumps the probability of being diagnosed with heart diseases.\n- With the **ST_Slope** values, **flat** slope displays a very high probability of being diagnosed with heart disease. **Down** also shows the same output but in very few data points. "
"## PreTrained Model : VGG16\n\n\n\nKeras library also provides the pre-trained model in which one can load the saved model weights, and use them for different purposes : transfer learning, image feature extraction, and object detection. We can load the model architecture given in the library, and then add all the weights to the respective layers. \n\nBefore using the pretrained models, lets write a few functions which will be used to make some predictions. First, load some images and preprocess them. "
"Now, we can perform following steps : \n1. import VGG16 architecture from keras.applications  \n2. Add the saved weights to the architecture \n3. Use model to perform predictions "
Let's predict the output on new images and check the outcome.
"So a simple neural network with only 20 rows of training data is able to correctly classify the two images on test set. \n\n### EndNotes \nThanks for viewing this kernel, If you liked it, please upvote. "
pd.plotting.scatter_matrix:\n* green: *normal* and red: *abnormal*\n* c:  color\n* figsize: figure size\n* diagonal: histohram of each features\n* alpha: opacity\n* s: size of marker\n* marker: marker type 
"Okay, as you understand in scatter matrix there are relations between each feature but how many *normal(green)* and *abnormal(red)* classes are there. \n* Searborn library has *countplot()* that counts number of classes\n* Also you can print it with *value_counts()* method\n\n This data looks like balanced. Actually there is no definiton or numeric value of balanced data but this data is balanced enough for us.\n Now lets learn first classification method KNN"
" \n###  K-NEAREST NEIGHBORS (KNN)\n* KNN: Look at the K closest labeled data points\n* Classification method.\n* First we need to train our data. Train = fit\n* fit(): fits the data, train the data.\n* predict(): predicts the data\n If you do not understand what is KNN, look at youtube there are videos like 4-5 minutes. You can understand better with it.\n Lets learn how to implement it with sklearn\n* x: features\n* y: target variables(normal, abnormal)\n* n_neighbors: K. In this example it is 3. it means that Look at the 3 closest labeled data points\n"
"Accuracy is 86% so is it good ? I do not know actually, we will see at the end of tutorial.\n Now the question is why we choose K = 3 or what value we need to choose K. The answer is in model complexity\n\n Model complexity:\n* K has general name. It is called a hyperparameter. For now just know K is hyperparameter and we need to choose it that gives best performace. \n* Literature says if k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n* If k is big, model that is less complex model can lead to underfit. \n* At below, I range K value from 1 to 25(exclude) and find accuracy for each K value. As you can see in plot, when K is 1 it memozize train sets and cannot give good accuracy on test set (overfit). Also if K is 18, model is lead to underfit. Again accuracy is not enough. However look at when K is 18(best performance), accuracy has highest value almost 88%. \n\n"
"### Up to this point what you learn:\n* Supervised learning\n* Exploratory data analysis\n* KNN\n    * How to split data\n    * How to fit, predict data\n    * How to measure medel performance (accuracy)\n    * How to choose hyperparameter (K)\n    \n** What happens if I chance the title KNN and make it some other classification technique like Random Forest?**\n* The answer is **nothing**. What you need to is just watch a video about what is random forest in youtube and implement what you learn in KNN. Because the idea and even most of the codes (only KNeighborsClassifier need to be RandomForestClassifier ) are same. You need to split, fit, predict your data and measue performance and choose hyperparameter of random forest(like max_depth). "
" \n### REGRESSION\n* Supervised learning\n* We will learn linear and logistic regressions\n* This orthopedic patients data is not proper for regression so I only use two features that are *sacral_slope* and *pelvic_incidence* of abnormal \n    * I  consider feature is pelvic_incidence and target is sacral_slope \n    * Lets look at scatter plot so as to understand it better\n    * reshape(-1,1): If you do not use it shape of x or y becaomes (210,) and we cannot use it in sklearn, so we use shape(-1,1) and shape of x or y be (210, 1). "
"Now we have our data to make regression. In regression problems target value is continuously varying variable such as price of house or sacral_slope. Lets fit line into this points.\n\n Linear regression\n* y = ax + b       where  y = target, x = feature and a = parameter of model\n* We choose parameter of model(a) according to minimum error function that is lost function\n* In linear regression we use Ordinary Least Square (OLS) as lost function.\n* OLS: sum all residuals but some positive and negative residuals can cancel each other so we sum of square of residuals. It is called OLS\n* Score: Score uses R^2 method that is ((y_pred - y_mean)^2 )/(y_actual - y_mean)^2"
" \n### CROSS VALIDATION\nAs you know in KNN method we use train test split with random_state that split exactly same at each time. However, if we do not use random_state, data is split differently at each time and according to split accuracy will be different. Therefore, we can conclude that model performance is dependent on train_test_split. For example you split, fit and predict data 5 times and accuracies are 0.89, 0.9, 0.91, 0.92 and 0.93, respectively. Which accuracy do you use? Do you know what accuracy will be at 6th times split, train and predict. The answer is I do not know but if I use cross validation I can find acceptable accuracy.\n Cross Validation (CV)\n* K folds = K fold CV.\n* Look at this image it defines better than me :)\n* When K is increase, computationally cost is increase\n* cross_val_score(reg,x,y,cv=5): use reg(linear regression) with x and y that we define at above and K is 5. It means 5 times(split, train,predict)\n"
