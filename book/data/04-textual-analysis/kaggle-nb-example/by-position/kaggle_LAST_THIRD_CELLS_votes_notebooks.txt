" \n## 5-5  Partial Dependence Plot\nIn this section, we see the impact of the main variables discovered in the previous sections by using the [pdpbox](https://pdpbox.readthedocs.io/en/latest/)."
 \n## 5-6 Chart analysis\n1. The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n1. A blue shaded area indicates level of confidence
#### Residence_type vs Discrete Features :
- The graphs of **Rural Residence_type** & **Urban Residence_type** against discrete features w.r.t **stroke** are identical.\n- They cannot be separated from each other. They repeat the insights that have been highlighted uptill now.
#### smoking_status vs Discrete Features :
"- Irrespective of **smoking_status**, people **suffering from stroke** have been detected at **age around 60**.\n- Similar to **age**, same values of **avg_glucose_level** have been found in **stroke** cases irrespective of **smoking_status**.\n- However, because of the **smoking_status**, range of values for which cases of **stroke** differ. Range of values of people that **smokes** is slightly higher than everyone else."
### Discrete features vs Discrete features w.r.t Target variable (stroke) :
"- Due to the imbalance nature of the data, cases of **stroke** & **no stroke** cannot be separated.\n- No insights can be interpreted from the above graphs."
"- In order to visualize the correlation matrix, we create a new dataframe that contains values from **x_train** & **y_train**.\n- Thus, we reject anything outside the training data to avoid data leakage."
"- Clearly, we can see the difference in values between **Data Leakage** & **No Data Leakage**.\n- In the case of **No Data Leakage**, **age** displays a strong positive correlation with **stroke**. **avg_glucose_level** & **ever_married** display some kind of positive correlation. Opposite to positive correlation, **gender**, **Residence_type** & **work_type** have negative correlation with the **stroke**.\n- In the case of **Data Leakage**, none of the features display an extreme positive or negative correlation with **stroke**.\n- **age**, **heart_disease**, **avg_glucose_level**, **hypertension** & **ever_married** display some kind of positive correlation. Overall, all the features have a value very close to 0, displaying neutral correlation with **stroke**."
#### Mutual Information Test :
"- Mutual Information Score of **stroke** with categorical features display very low scores irrespective of **Data Leakage** or **No Data Leakage**.\n- According to the above scores, none of the features should be selected for modeling."
#### Chi Squared Test :
"- For **No Data Leakage**, we should reject the features that have low values. We will reject features with scores less than 20. Hence, we will not use : **smoking_status**, **heart_disease** & **hypertension**. This does contradict with the **Domain Information**.\n- For **Data Leakage**, **heart disease** & **hypertension** need to be selected for modeling and reject the other features due to low Chi Squared Score. "
#### ANOVA Test :
"- From the above ANOVA Scores, we ignore the features with values less than 20. Hence, we reject **bmi** for modeling irrespective of **Data Leakage** or **No Data Leakage**.\n- We ready the datasets for data scaling by dropping the features based on the above statistical tests.\n- We will ignore the **Domain Information**!"
- Selecting the features from the above conducted tests and splitting the data into **85 - 15 train - test** groups.
### 1] XGBoostClassifier :
"The confusion matrix shows `3289 + 230 = 3519 correct predictions` and `17 + 44 = 61 incorrect predictions`.\n\n\nIn this case, we have\n\n\n- `True Positives` (Actual Positive:1 and Predict Positive:1) - 3289\n\n\n- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 230\n\n\n- `False Positives` (Actual Negative:0 but Predict Positive:1) - 17 `(Type I error)`\n\n\n- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 44 `(Type II error)`"
# **17. Classification metrices** \n\n[Table of Contents](#0.1)
"# **18. ROC - AUC** \n\n[Table of Contents](#0.1)\n\n\n\n### ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of `TP to (TP + FN)`.\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of `FP to (FP + TN)`.\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n"
ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.
# Visualize classified images\n\n## Correctly classified images\n\n\nWe visualize few images correctly classified.
## Incorrectly classified images\n\nLet's see also few images incorrectly classified.
"### We are not done yet ! What about the results ? I've tried to make 30 submissions with this classifier, here are the results :\n    \n| Score   	 |   Nb of occurrences 	|\n|---------	    |  -------------------------- |\n| 0.77511 	|                2                |\n| 0.77990 	|                3             	  |\n| 0.78468 	|               10               |\n| 0.78947 	|               11    	         |\n| 0.79425 	|                2                |\n| 0.79904 	|                2     	          |\n\n\n### Although I know that we can do much better, the 0.79904 still places us in the top 16%. On the other hand, we see that our homemade classifier has reduced the variance so it is reliable and constant in its performance (something that is not tested by the leaderboard but still important for a data sceintist). Moreover, our solution remains simple and accessible even for beginners. \n\n### Thank you for your reading, feel free to fork this kernel and improve it, and enjoy datascience :D"
*Check out this beautiful distribution of kaggle scoring for our homemade classifier !*
### Correlation Matrix
"\n## 2- Drop Features\n* Ticket, Cabin, Name, PassengerId and Age are deleted according to the result of the correlation matrix."
"### Smoothening of a time series may be useful in:\n\n- Reducing the effect of noise in a signal get a fair approximation of the noise-filtered series.\n- The smoothed version of series can be used as a feature to explain the original series itself.\n- Visualize the underlying trend better\n\n### So how to smoothen a series? Let’s discuss the following methods:\n\n- Take a moving average\n- Do a LOESS smoothing (Localized Regression)\n- Do a LOWESS smoothing (Locally Weighted Regression)\n- Moving average is nothing but the average of a rolling window of defined width. But you must choose the window-width wisely, because, large window-size will over-smooth the series. For example, a window-size equal to the seasonal duration (ex: 12 for a month-wise series), will effectively nullify the seasonal effect."
 📄 ARIMA Model
#### 6.1.3 Plot learning curves\n\nLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.
GradientBoosting and Adaboost classifiers tend to overfit the training set. According to the growing cross-validation curves GradientBoosting and Adaboost could perform better with more training examples.\n\nSVC and ExtraTrees classifiers seem to better generalize the prediction since the training and cross-validation curves are close together.
"#### 6.1.4 Feature importance of tree based classifiers\n\nIn order to see the most informative features for the prediction of passengers survival, i displayed the feature importance for the 4 tree based classifiers."
"I plot the feature importance for the 4 tree based classifiers (Adaboost, ExtraTrees, RandomForest and GradientBoosting).\n\nWe note that the four classifiers have different top features according to the relative importance. It means that their predictions are not based on the same features. Nevertheless, they share some common important features for the classification , for example 'Fare', 'Title_2', 'Age' and 'Sex'.\n\nTitle_2 which indicates the Mrs/Mlle/Mme/Miss/Ms category is highly correlated with Sex.\n\nWe can say that: \n\n- Pc_1, Pc_2, Pc_3 and Fare refer to the general social standing of passengers.\n\n- Sex and Title_2 (Mrs/Mlle/Mme/Miss/Ms) and Title_3 (Mr) refer to the gender.\n\n- Age and Title_1 (Master) refer to the age of passengers.\n\n- Fsize, LargeF, MedF, Single refer to the size of the passenger family.\n\n**According to the feature importance of this 4 classifiers, the prediction of the survival seems to be more associated with the Age, the Sex, the family size and the social standing of the passengers more than the location in the boat.**"
The prediction seems to be quite similar for the 5 classifiers except when Adaboost is compared to the others classifiers.\n\nThe 5 classifiers give more or less the same prediction but there is some differences. Theses differences between the 5 classifier predictions are sufficient to consider an ensembling vote. 
\n# Confusion Matrix function\n\n[Back to Contents](#top)\n\n[Next](#t6_2)\n
\n# Gradient Boosting Model function\n\n[Back to Contents](#top)\n\n[Next](#t6_3)\n
\n# XGBoost\n\n[Back to Contents](#top)\n\n[Next](#t6_4)\n
 Visualization of UnderSampling Technique\n
Visualization of OverSampling Techinque\n
Choosing SMOTE oversampling data for modeling as the number of datapoints generated are in equal proportion from this technique.
#### Mainland China has recorded highest number of Closed cases as thier Recovery Rate is staggering recording 85%+\n#### Confirmed Cases/Day is clear indication of why US has highest number of Active Cases currently. The rate is 11000+ cases per day. Showing increase in that value every day.
"#### Survival Probability is the only graph that looks the most promising! Having average survival probability of 95%+ across all countries. The difference between Mean and Median Death Probability is an clear indication that there few countries with really high mortality rate e.g. Italy, Algeria, UK etc."
"#### When we see daily news reports on COVID-19 it's really hard to interpret what's actually happening, since the numbers are changing so rapidly but that's something expected from Exponential growth. Since almost all the pandemics tend to grow exponentially it's really hard to understand for someone from a non-mathematical or non-statistical background.\n\n#### We are more concerned about how we are doing and where we are heading in this pandemic rather than just looking at those exponentially growing numbers. The growth won't be exponentially forever, at some point of time the curve will become flat because probably all the people on the planet are infected or we human have been able to control the disease.\n\n#### When we are in the middle of the exponential growth it's almost impossible to tell where are we heading.\nHere, I am trying to show how we can interpret the exponential growth which is the common trend among all the countries\n\nReferences:\nHow To Tell If We're Beating COVID-19: https://www.youtube.com/watch?v=54XLXg4fYsc\n\nExponential growth and epidemics: https://www.youtube.com/watch?v=Kas0tIxDvrg"
"It's pretty evident that the disease is spreading in same manner everywhere, but if particular country is following pandemic controlling practices rigrously the results are evident in the graph.\n\nMost of the countries will follow the same trajectory as that USA, which is **""Uncontrolled Exponential Growth""** \n\nThere are few countries where the pandemic controlling practices seems to be working accurately, few classic examples are China, Germany, Italy, Spain, Turkey has started showing that dip indicating there are somehow got control over COVID-19\n\nCountries like United Kingdom, Russia are following similar lines as that of United States, indicating the growth is still exponential among those countries.\n\nIran is showing some occasional drops."
"#### Comparison of Daily Increase in Number of Cases of Italy, Spain, USA and India, where maximum number of Confirmed Cases are equivalent to maximum number of Confirmed Cases in India"
#### Videos related to COVID-19 Pandemic in India\nWuhan Coronavirus: WION breaks down the growing numbers | Gravitas: \nhttps://www.youtube.com/watch?v=xqAPDD8sw-g
"#### Squared Variables\n\nFirst, the easiest step: we'll remove all of the squared variables. Sometimes variables are squared or transformed as part of feature engineering because it can help linear models learn relationships that are non-linear. However, since we will be using more complex models, these squared features are redundant. They are highly correlated with the non-squared version, and hence can actually hurt our model by adding irrelevant information and also slowing down training.\n\nFor an example, let's take a look at `SQBage` vs `age`."
"These variables are highly correlated, and we don't need to keep both in our data."
"The final household feature we can make for now is a `bonus` where a family gets a point for having a refrigerator, computer, tablet, or television."
## Per Capita Features\n\nAdditional features we can make calculate the number of certain measurements for each person in the household.
The largest discrepancy in the correlations is `dependency`. We can make a scatterplot of the `Target` versus the `dependency` to visualize the relationship. We'll add a little jitter to the plot because these are both discrete variables.
"It's hard to see the relationship, but it's slightly negative: as the `dependency` increases, the value of the `Target` decreases. This makes sense: the `dependency` is the number of dependent individuals divided by the number of non-dependents. As we increase this value, the poverty severty tends to increase: having more dependent family members (who usually are non-working) leads to higher levels of poverty because they must be supported by the non-dependent family members. "
"#### Correlation Heatmap \n\nOne of my favorite plots is the correlation heatmap because it shows a ton of info in one image. For the heatmap, we'll pick 7 variables and show the correlations between themselves and with the target. "
"This plot shows us that there are a number of variables that have a weak correlation with the `Target`. There are also high correlations between some variables (such as `floor` and `walls+roof+floor`) which could pose an issue because of collinearity. \n\n### Features Plot\n\nFor the final exploration of the household level data, we can make a plot of some of the most correlated variables with the Target. This shows scatterplots on the upper triangle, kernel density estimate (kde) plots on the diagonal, and 2D KDE plots on the lower triangle."
"We'll leave the feature engineering of the household variables for now. Later, we can come back to this step if we are not pleased with the model performance."
"### Feature Construction\n\nWe can make a few features using the existing data. For example, we can divide the years of schooling by the age."
"We can also take our new variable, `inst`, and divide this by the age. The final variable we'll name `tech`: this represents the combination of tablet and mobile phones."
It looks like households where the head is female are slightly more likely to have a severe level of poverty.
We can also look at the difference in average education by whether or not the family has a female head of household.
"It looks like at every value of the `Target`, households with female heads have higher levels of education. Yet, we saw that overall, households with female heads are more likely to have severe poverty. "
We can investigate the object to see the training scores for each iteration. The following code will plot the validation scores versus the number of features for the training.
"We can see that the score improves as we add features up until 96 features. According to the selector, this is the optimal number of features.\n\nThe rankings of each feature can be found by inspecting the trained object. These represent essentially the importance of features averaged over the iterations. Features can share the same ranking, and only features with a rank of 1 are retained."
"For each fold, the `1, 2, 3, 4` columns represent the probability for each `Target`. The `Target` is the maximum of these with the `confidence` the probability. We have the predictions for all 5 folds, so we can plot the confidence in each `Target` for the different folds."
"What we see here is that the confidence for each class if relatively low. It does appear that the model has greater confidence in `Target=4` predictions which makes sense because of the _class imbalance and the high prevalence of this label._ \n\nAnother way to look at the information is as a `violinplot`. This shows the same information, with the number of observations related to the width of the plot."
"Overall, these results show the issue with imbalanced class problems: our model cannot distinguish very well between the classes that are underrepresented. Later we'll look at predictions themselves and see where our model is ""confused"". For now, we can generate a submission file and submit it to the competition.\n\nWhen we actually make predictions for each household, we average the predictions from each of the folds. Therefore, we are essentially using multiple models since each one is trained on a slightly different fold of the data. The gradient boosting machine is already an ensemble machine learning model, and now we are using it almost as a meta-ensemble by averaging predictions from several gbms. \n\nThis process is shown in the code below."
"We can have the function instead return the actual submission file. This takes the average predictions across the five folds, in effectm combining 5 different models, each one trained on a slghtly different subset of the data."
Now for the next four hyperparameters versus the score.
"There are not any strong trends here. Next we will try to look at two hyperparameters simultaneously versus the score in a 3-dimensional plot. This makes sense for hyperparameters that work in concert, such as the learning rate and the number of esimators or the two regularization values."
"## 3D Plots \n\nTo try and examine the simultaneous effects of hyperparameters, we can make 3D plots with 2 hyperparameters and the score. A truly accurate plot would be 10-D (one for each hyperparameter) but in this case we will stick to 3 dimensions. 3D plots can be made in matplotlib by import `Axes3D` and specifying the `3d` projection in a call to `.add_subplot`"
First up is `reg_alpha` and `reg_lambda`. These control the amount of regularization on each decision tree and help to prevent overfitting to the training data.
The next plot is learning rate and number of estimators versus the score. __Remember that the number of estimators was selected using early stopping for 100 rounds with 5-fold cross validation__. The number of estimators __was not__ a hyperparameter in the grid that we searched over. Early stopping is a more efficient method of finding the best number of estimators than including it in a search (based on my limited experience)!
Here there appears to be a clear trend: a lower learning rate leads to higher values! What does the plot of just learning rate versus number of estimators look like?
"This plot is very easy to interpret: the lower the learning rate, the more estimators that will be trained. From our knowledge of the model, this makes sense: each individual decision trees contribution is lessened as the learning rate is decreased leading to a need for more decision trees in the ensemble. Moreover, from the previous graphs, it appears that decreasing the learning rate increases the model score."
"### Function for 3D plotting\n\nAny time you write code more than twice, it should be encoded into a function! That's what the next code block is for: putting this code into a function that we can use many times! This function can be used for __any__ 3d plotting needs."
The bayesian optimization results are close in trend to those from random search: lower learning rate leads to higher cross validation scores.
"## Correlation Heatmap\n\nNow we can make a heatmap of the correlations. I enjoy heatmaps and thankfully, they are not very difficult to make in `seaborn`."
That's a lot of plot for not very much code! We can see that the number of estimators and the learning rate have the greatest magnitude correlation (ignoring subsample which is influenced by the boosting type).
Feel free to use this code for your own heatmaps! (Also send me color recommendations because I am not great at picking out a palette).
"As we see, eight variables were kept. "
"\n## 4.2. Review of model evaluation procedures\n\nMotivation: Need a way to choose between machine learning models\n* Goal is to estimate likely performance of a model on out-of-sample data\n\nInitial idea: Train and test on the same data\n* But, maximizing training accuracy rewards overly complex models which overfit the training data\n\nAlternative idea: Train/test split\n* Split the dataset into two pieces, so that the model can be trained and tested on different data\n* Testing accuracy is a better estimate than training accuracy of out-of-sample performance\n* Problem with train/test split\n    * It provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy\n    * Testing accuracy can change a lot depending on a which observation happen to be in the testing set\n\nReference: \nhttp://www.ritchieng.com/machine-learning-cross-validation/ "
\n### 4.2.1. Model evaluation based on simple train/test split using `train_test_split()` function
\n### 4.2.2. Model evaluation based on K-fold cross-validation using `cross_val_score()` function 
\n## 4.3. GridSearchCV evaluating using multiple scorers simultaneously
"\n## 4.4. GridSearchCV evaluating using multiple scorers, RepeatedStratifiedKFold and pipeline for preprocessing simultaneously\n\nWe can applied many tasks together for more in-depth evaluation like gridsearch using cross-validation based on k-folds repeated many times, that can be scaled or no with respect to many scorers and tunning on parameter for a given estimator!  "
#### Visualize `income` wrt `race`
#### Interpretation\n\n\n- We can see that whites make more money than non-whites in both the income categories.
#### Visualize `workclass` variable
#### Interpretation\n\n\n- We can see that there are lot more private workers than other category of workers.
#### Visualize `workclass` variable wrt `income` variable
#### Interpretation\n\n\n- We can see that workers make less than equal to 50k in most of the working categories.\n\n- But this trend is more appealing in Private `workclass` category.
#### Visualize `workclass` variable wrt `sex` variable
#### Interpretation\n\n\n- We can see that there are more male workers than female workers in all the working category.\n\n- The trend is more appealing in Private sector.
#### View the distribution of `age` variable
We can see that `age` is slightly positively skewed.
We can use Pandas series object to get an informative axis label as follows :-
We can shade under the density curve and use a different color as follows:-
#### Detect outliers in `age` variable with boxplot
We can see that there are lots of outliers in `age` variable.
#### Explore relationship between `age` and `income` variables
"#### Interpretation\n\n- As expected, younger people make less money as compared to senior people."
#### Visualize relationship between `race` and `age`
#### Interpretation\n\n- Whites are more older than other groups of people.
#### Find out the correlations
#### Interpretation\n\n- We can see that there is no strong correlation between variables.
#### Plot pairwise relationships in dataset
#### Interpretation\n\n- We can see that `age` and `fnlwgt` are positively skewed.\n\n- The variable `education_num` is negatively skewed while `hours_per_week` is normally distributed.\n\n- There exists weak positive correlation between `capital_gain` and `education_num` (correlation coefficient=0.1226). 
"## 14. Visualize feature scores of the features \n\n\n[Back to Table of Contents](#0.1)\n\n\nNow, I will visualize the feature scores with matplotlib and seaborn."
#### Interpretation\n\n\n- The above plot confirms that the most important feature is `fnlwgt` and least important feature is `native_country_41`.
and I output the result as wordclouds:
"From this representation, we can see that for example, one of the clusters contains objects that could be associated with gifts (keywords: Christmas, packaging, card, ...). Another cluster would rather contain luxury items and jewelry (keywords: necklace, bracelet, lace, silver, ...). Nevertheless, it can also be observed that many words appear in various clusters and it is therefore difficult to clearly distinguish them.\n\n** c / _Principal Component Analysis_ **\n\nIn order to ensure that these clusters are truly distinct, I look at their composition. Given the large number of variables of the initial matrix, I first perform a PCA:"
and then check for the amount of variance explained by each component:
"We see that the number of components required to explain the data is extremely important: we need more than 100 components to explain 90% of the variance of the data. In practice, I decide to keep only a limited number of components since this decomposition is only performed to visualize the data:"
and I represent the amount of variance explained by each of the components:
___\n#### 4.2.2 Creation of customer categories
in order to create a representation of the various clusters:
"From this representation, it can be seen, for example, that the first principal component allow to separate the tiniest clusters from the rest. More generally, we see that there is always a representation in which two clusters will appear to be distinct.\n\n** b/ _Score de silhouette intra-cluster_ **\n\nAs with product categories, another way to look at the quality of the separation is to look at silouhette scores within different clusters:"
"** d / _Customers morphology_ **\n\nFinally, I created a representation of the different morphotypes. To do this, I define a class to create ""Radar Charts"" (which has been adapted from this [kernel](https://www.kaggle.com/yassineghouzam/don-t-know-why-employees-leave -read-this)):"
This allows to have a global view of the content of each cluster:
"It can be seen, for example, that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. Other clusters will differ from basket averages (** mean **), the total sum spent by the clients (** sum **) or the total number of visits made (** count **).\n\n____\n## 5. Classification of customers\n\nIn this part, the objective will be to adjust a classifier that will classify consumers in the different client categories that were established in the previous section. The objective is to make this classification possible at the first visit. To fulfill this objective, I will test several classifiers implemented in `scikit-learn`. First, in order to simplify their use, I define a class that allows to interface several of the functionalities common to these different classifiers: "
"___\n#### 5.1.1 Confusion matrix\n\nThe accuracy of the results seems to be correct. Nevertheless, let us remember that when the different classes were defined, there was an imbalance in size between the classes obtained. In particular, one class contains around 40% of the clients. It is therefore interesting to look at how the predictions and real values compare to the breasts of the different classes. This is the subject of the confusion matrices and to represent them, I use the code of the [sklearn documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html):"
from which I create the following representation:
"___\n#### 5.1.2 Learning curve\n\nA typical way to test the quality of a fit is to draw a learning curve. In particular, this type of curves allow to detect possible drawbacks in the model, linked for example to over- or under-fitting. This also shows to which extent the mode could benefit from a larger data sample. In order to draw this curve, I use the [scikit-learn documentation code again](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr- self-examples-model-selection-pad-learning-curve-py)"
from which I represent the leanring curve of the SVC classifier:
#### ps_car_12 and ps_car_13
#### ps_car_12 and ps_car_14
#### ps_car_13 and ps_car_15
"Allright, so now what? How can we decide which of the correlated variables to keep? We could perform Principal Component Analysis (PCA) on the variables to reduce the dimensions. In the AllState Claims Severity Competition I made [this kernel](https://www.kaggle.com/bertcarremans/reducing-number-of-numerical-features-with-pca) to do that. But as the number of correlated variables is rather low, we will let the model do the heavy-lifting."
"Since fare is all filled up, we compare the fare distribution as below. Also when you look carefully at the plot below, I compared the test and training set's distribution of Fare vs Survival rate side by side and realized that the training set is not so ""smooth"" as compared to the test set. This has an underlying problem, and the idea is that when our algorithm is searching for patterns in the training set, our algorithm might adapt to certain underlying patters in the training set that DO NOT exist in the test set  - as a result, the machine learning algorithm may not be able to generalize well to the test set - which is why when we use a certain algorithm on our training set, it seems to give us a high accuracy like 80% but when we do it on the test set, it becomes much lower at maybe 75%."
### 4.1.2) Missing Values: Embarked 
1. Comparing the KDE plot for the age of those who perished before imputation against the KDE plot for the age of those who perished after imputation.
2. Comparing the KDE plot for the age of those who survived before imputation against the KDE plot for the age of those who survived after imputation.
#### **Contrasting the first imputation method with the imputation by median method**
Median imputation: Comparing the KDE plot for the age of those who perished before imputation against the KDE plot for the age of those who perished after imputation. Here we can see that the distribution between pre-imputation and after-imputation using this method is quite large - much more difference than our first method.
Median imputation: Comparing the KDE plot for the age of those who survived before imputation against the KDE plot for the age of those who perished after imputation. 
#### **Important final note for imputation**
"### Complex Case for Decision Trees\n\nTo continue the discussion of the pros and cons of the methods in question, let's consider a simple classification task, where a tree would perform well but does it in an ""overly complicated"" manner. Let's create a set of points on a plane (2 features), each point will be one of two classes (+1 for red, or -1 for yellow). If you look at it as a classification problem, it seems very simple: the classes are separated by a line. "
"However, the border that the decision tree builds is too complicated; plus the tree itself is very deep. Also, imagine how badly the tree will generalize to the space beyond the $30 \times 30$ squares that frame the training set."
"We got this overly complex construction, although the solution is just a straight line $x_1 = x_2$."
The method of one nearest neighbor does better than the tree but is still not as good as a linear classifier (our next topic).
"###  Decision Trees and k-NN in a Task of MNIST Handwritten Digits Recognition\n\nNow let's have a look at how these 2 algorithms perform on a real-world task. We will use the `sklearn` built-in dataset on handwritten digits. This task is an example where k-NN works surprisingly well.\n \nPictures here are 8x8 matrices (intensity of white color for each pixel). Then each such matrix is ""unfolded"" into a vector of length 64, and we obtain a feature description of an object.\n \nLet's draw some handwritten digits. We see that they are distinguishable."
### Train the CNN model on loop
# Submission 
### Original Dataset :\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 5**"
### Normalized Dataset :\n\n#### Elbow Method & Silhouette Score Method :
### Age - Annual Income (k\$) - Spending Score (1-100)
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 6**"
### Normalized Dataset\n\n#### Elbow Method & Silhouette Score Method :
### Results Table\n\n#### Original Dataset :\n\n|Sr. No.|Feature Combination|Number of Clusters|\n|-|-|-|\n|1.|Age - Annual Income (k\$)|4|\n|2.|Age - Spending Score (1-100)|4|\n|3.|Annual Income (k\$) - Spending Score (1-100)|5|\n|4.|Age - Annual Income (k\$) - Spending Score (1-100)|6|\n\n#### Normalized Dataset :\n\n|Sr. No.|Feature Combination|Number of Clusters|\n|-|-|-|\n|1.|Age - Annual Income (k\$)|3|\n|2.|Age - Spending Score (1-100)|6|\n|3.|Annual Income (k\$) - Spending Score (1-100)|5|\n|4.|Age - Annual Income (k\$) - Spending Score (1-100)|6|
# Cantour Plots
# Scatter Geo plot
- These are the variance values of the each feature present in the dataset.
"- This is a very effective method where we add up the variances of all the features in cummulative format.\n- Typically **eigen values with more than 95% of ratio of variance** are selected.\n- They correspond to the columns of the PCA generated dataframe.\n- In this case, we select the **Eigen Value : 2** as the steps generated have significant variances and thus the other features get dominated by their variances. "
#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 3**"
"- Now we have got the clusters but we don't know which value corresponds to what! \n- Hence, we draw a boxplot of **income** & **child_mort** w.r.t labelled clusters!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**."
- From the above plot we can conclude :\n    - **0 : No Help Needed**\n    - **1 : Help Needed**\n    - **2 : Might Need Help**
### PCA Data 
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 3**        "
- We again check the scatterplot of **income** & **child_mort** w.r.t labelled clusters for confirming the cluster values!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**.
- From the above plot we can conclude :\n    - **0 : Might Need Help**\n    - **1 : Help Needed**\n    - **2 : No Help Needed**
"## Density Based Spatial Clustering of Application with Noise Clustering\n\n**DBSCAN Clustering** is a density based clustering algorithm that is used for unsupervised learning problems.\n- In a bid to eliminate the problems of **K-Means Clustering** with nested data and high-dimensional data, **DBSCAN Clustering** eliminates it! It's has 3 important terms & 2 important hyperparameters :\n    - Terms :\n        - **Core Point** : It is the center point that has **minPts** number of data points present in it's area and the points under it's area can extend the cluster.\n        - **Non-Core Point** : It is the center point that does not have **minPts** number of data points present in it's area and it cannot extend the cluster.\n        - **Outliers / Noise** : It is the data points that are not a part of any cluster.\n    - Hyperparameters :\n        - **minPts** : It is the minimum number of data points that need to be present in the area of a point to be considered as a core point.\n        - **Epsilon** : It is the radius of the area of a center point.\n- **DBSCAN Clustering** creates the clusters in the following way :        \n    - Select a random point and consider it as a center point from the data. \n    - Check the other data points that present in the area of this center point with the **Epsilon** value as it's radius.\n    - If this area has **minPts** number of data points in it, then the center point is considered as a **core point**. \n    - The data points in this area are then considered as **core point** that further extends by the same rules creating a cluster. \n    - If any of the point does not satisfy the rules, it is considered as **non-core point** that remains the part of the cluster but it cannot extend the cluster further.\n    - When all the **core points** are done with engulfing the other data points, **non-core points** are included & the cluster is completed.\n    - The next cluster then starts forming by the same rules. Some of the points don't be a part of any of the clusters, such points are known as **outliers / noise**.\n- As you might have noticed, the hyperparameters play a crucical role in this process. Thus, they have certain rules for assigning them values :\n    - **minPts** : If D represents the number of dimensions / features of a dataset, then **minPts** >= D + 1. Typically **minPts** >= 2 * D is selected for smaller or noisy datasets. \n    - **Epsilon** : It's value is usually decided using the **k-distance graph** that is determined from the **KNN model**. The value at which the graph changes sharply is selected."
### Feature Combination : Health - Trade - Finance 
"- We select **minPts** = 8 i.e >= 2 * 3 features \n- The value assigned to the **n_neighbors** : **minPts** - 1.\n- From the graph above, we select :\n    - **eps** : 0.08"
"- Now we have got the clusters but we don't know which value corresponds to what! \n- Hence, we draw a boxplots of **income** & **child_mort** w.r.t labelled clusters!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**."
- By rule **-1** is associated with **Noise / Outliers**!\n- From the above plot we can conclude :\n    - **-1 : Noise / Outliers**\n    - **0 : Might Need Help**\n    - **1 : No Help Needed**\n    - **2 : Help Needed**
- We again draw boxplots of **income** & **child_mort** w.r.t labelled clusters for identifying the assistance required by the nations!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**.
- From the above plot we can conclude :\n    - **-1 : Noise / Outliers**\n    - **0 : Help Needed**\n    - **1 : Might Need Help**\n    - **2 : No Help Needed**
"## Hierarchical Clustering\n\n**Hierarchical Clustering** is a distanced based algorithm that is used for unsupervised learning problems. \n    \n- It develops the hierarchy of clusters in the form of a tree i.e known as the **dendrogram**. For this problem we are going to use **Agglomerative Clustering** which is a bottom-up approach that considers all the points as clusters and then merges them together based on their distances in the following ways :\n    - Initally all the points are considered as clusters.\n    - Then, clusters that are closer together they start merging as new cluster.\n    - This combined cluster then further gets compared with other clusters and the closest cluster gets merged.\n    - This process continues till a single large cluster is formed.\n- This process of forming clusters can then be viewed with a **dendrogram**. From it, we select the number of clusters by identifying the number of merges present at the penultimate stage. More is the length of the vertical lines, higher the distance between the clusters.\n- To select the number of clusters, we set a threshold value and count the number of vertical lines present above it. This number of vertical lines forms the number of clusters. Other methods like **Silhouette Score Method** and **Elbow method** can also be used.\n- For **Agglomerative Clustering**, it has 2 important hyperparameters :\n    - **linkage** : There are various connecting or linking methods for cluster i.e Single, Centroid, Average , etc.\n    - **affinity** : It is the distance formula that compares the distance before merging the clusters. "
"- In this case, we need to divide the countries into 3 categories. That is why we will select a 3 clusters directly. Dendrogram analysis for this dataset is kind of redundant. \n- Here, we can see that 1 **blue line** alongwith 2 **red lines** are the penultimate clusters that before connecting together.\n- It has 3 branches, thus indicating the **3 clusters** that it creates before merging into 1!"
### PCA Data
"- In this case, we need to divide the countries into 3 categories. That is why we will select a 3 clusters directly. Dendrogram analysis for this dataset is kind of redundant.\n- Here, again, we can see that 1 **blue line** alongwith 2 **red lines** are the penultimate clusters formed before connecting together.\n- It has 3 branches, thus indicating the **3 clusters** that it creates before merging into 1!"
- We again draw boxplots of **income** & **child_mort** w.r.t labelled clusters!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**.
- From the above plot we can conclude :\n    - **0 : Help Needed**\n    - **1 : Might Need Help**\n    - **2 : No Help Needed**
"# Conclusion\n\n- This is a great dataset that has been applied for a noble cause which highlights the scope of utilizing Data Science practices.\n\n\n- A model's performance is the reflection of the quality of the data feed to it. EDA section of this dataset provides a list of insights. Effect of normalization and standardization is massive on model performance.\n\n\n- Feature engineering is pivotal! For **feature combination & PCA data**, complexity of methods to achieve it is clearly visible. These processes highlight the various options open and thus requires us to be sensible in choosing the methods depending on the data and the problem statement.\n\n\n- **K-Means Clustering**, **Hierarchical Clustering** and **DBSCAN Clustering** are fundamentally different. They are based on different principles, thus displaying the difference in the model performances. Overall the model performances are not as great.    "
# Please upvote if you like the work!Any sort of feedback is appreciated!Thank You!
"# Logistic Generalized Additive Model [GAM]\n\nPrediction for this model reqiuires the data range to match, so I am unable to apply to the submission set.\n\nAdditional Resources:\n- [GAM: The Predictive Modeling Silver Bullet](https://multithreaded.stitchfix.com/blog/2015/07/30/gam/)\n- [pyGAM : Getting Started with Generalized Additive Models in Python](https://codeburst.io/pygam-getting-started-with-generalized-additive-models-in-python-457df5b4705f)\n- [pyGAM Github](https://github.com/dswah/pyGAM)"
"These plots showcase the marginal effect of each feature towards the depedent variable in the model. Let me interpret some to clarify:\n- **Sex:** Class 1 (Female) contributes towards the survial rate, while Class 0 (Male) has a negative effect.\n- **Age:** This variable is rescaled, but we can still see an increase rate of survival for the lower and upper age levels.\n- **Fare:** The more people payed, the more likely they are to survive.\n\n# Feedforward Neural Networks\n\nThe only “Deep Model” out of the mix. This is a characteristic of *Representation Learning*, which effectively grants the model its own feature processing and selection steps, catering to large, complex data with intertwining effects. In computer vision tasks, the convolutional neural networks is able to piece apart corners, colors, patterns and more. Recent research in Style Transfer even suggests that stylistic properties of a picture, such as art, can be extracted, and controlled.  \n**Source:** https://arxiv.org/abs/1611.07865\n\nA common explanation for Neural Networks is that it is a whole bunch of Logistic Regressions. An important thing to remembers is that the hidden-layers are fully connected, meaning that each input variables has a weight to each node (hidden-unit) in the hidden layer, thereby resulting in a black box with a whole lot of parameters, and a whole lot of matrix multiplications. Finally it's, ironic that one of the most intelligible classifiers can be transformed into the least intelligible! In *Computer Age Statistical Inference* authors Bradley Efron and Trevor Hastie are hopeful for the next Ronald Fisher to come and provide statistical intelligibility to modern day machine learning models, many of which are highly developed computationally, but lacking inferential theory.\n\nModel not ideal for such as small dataset."
**Probability Predictions:**
"## Introduction to Receiver Operating Characteristic curve [ROC]\nROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. Well said wikipedia. Closer the line is to the top left, the better its predictive ability. Non-Smooth curve suggest important thresholds, effective a cluster of probabilities near a swing point.\n\n\n# Soft and Hard Voting Ensembles of Difference Sizes"
"## Sklearn Voter Pipeline\n\nTalking about pipelines, Sklearn's ensemble voting is structured as one!"
"## 3.3. Pearson Correlation Heatmap\n\nThe Seaborn plotting package allows us to plot heatmaps showing the Pearson product-moment correlation coefficient (PPMCC) correlation between features.\nPearson is bivariate correlation, measuring the linear correlation between two features. "
"**Observations from the Pearson analysis:** \n* Correlation coefficients with magnitude between 0.5 and 0.7 indicate variables which can be considered **moderately correlated**.\n* We can see from the red cells that many features are ""moderately"" correlated: specifically, IsAlone, Pclass, Name_length, Fare, Sex.\n* This is influenced by the following two factors: 1) Women versus men (and the compounding effect of Name_length) and 2) Passengers paying a high price (Fare) have a higher chance of survival: there are also in first class, have a title. \n\n\n## 3.4. Pairplots\n\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other.\nThe Seaborn pairplot class will help us visualize the distribution of a feature in relationship to each others."
"**Observations**\n* The pairplot graph all trivariate analysis into one figure.\n* The clustering of red dots indicates the combination of two features results in higher survival rates, or the opposite (clustering of blue dots = lower survival)\nFor example:\n- Smaller family sizes in first and second class\n- Middle age with Pclass in third category = only blue dot\nThis can be used to validate that we extracted the right features or help us define new ones."
"## 4.10. Model summary\nI found that the picture illustrates the various model better than words.\nThis should be taken with a grain of salt, as the intuition conveyed by these two-dimensional examples does not necessarily carry over to real datasets.\nThe reality os that the algorithms work with many dimensions (11 in our case).\n\nBut it shows how each classifier algorithm partitions the same data in different ways.\nThe three rows represent the three different data set on the right.\nThe plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.\n\nFor instance, the visualization helps understand how RandomForest uses multiple Decision Trees, the linear SVC, or Nearest Neighbors grouping sample by their relative distance to each others.\n\n![image](http://scikit-learn.org/0.15/_images/plot_classifier_comparison_0011.png)\n"
"**Observations**\n* The above models (classifiers) were applied to a split training and x_test datasets.\n* This results in some classifiers (Decision_tree and Random_Forest) over-fitting the model to the training data. \n* This happens when the classifiers use many input features (to include noise in each feature) on the complete dataset, and ends up “memorizing the noise” instead of finding the signal.\n* This overfit model will then make predictions based on that noise. It performs unusually well on its training data, but will not necessarilyimprove the prediction quality with new data from the test dataset.\n* In the next section, we will cross-validate the models using sample data against each others. We will this by using StratifiedKFold to train and test the models on sample data from the overall dataset.\nStratified K-Folds is a cross validation iterator. It provides train/test indices to split data in train test sets. This cross-validation object is a variation of KFold, which returns stratified folds. The folds are made by preserving the percentage of samples for each class."
"## 4.11. Model cross-validation with K-Fold\n\nThe fitting process applied above optimizes the model parameters to make the model fit the training data as well as possible.\nCross-validation is a way to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available.\nIn simple words, it allows to test how well the model performs on new data.\nIn our case, cross-validation will also be applied to compare the performances of different predictive modeling procedures. \n![Cross-validation process:](https://image.slidesharecdn.com/kagglesharingmarkpeng20151216finalpresented-151216161621/95/general-tips-for-participating-kaggle-competitions-13-638.jpg?cb=1452565877)\n### Cross-validation scores"
"## 4.12 Hyperparameter tuning & learning curves for selected classifiers\n\n**1. Adaboost** is used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers.\n\n**2. ExtraTrees** implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n\n**3. RandomForest ** operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\n**4. GradientBoost ** produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n\n**5. SVMC, or Support Vector Machines.**vGiven a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier.\n\nAll descripotion adapted from Wikipedia."
"**Observations to fine-tune our models**\n\nFirst, let's compare their best score after fine-tuning their parameters:\n1. Adaboost: 80\n2. ExtraTrees: 83\n3. RandomForest: 82\n4. GradientBoost: 82\n5. SVC: 83\n\nIt appears that GBC and SVMC are doing the best job on the Train data. This is good because we want to keep the model as close to the training data as possible. But not too close!\nThe two major sources of error are bias and variance; as we reduce these two, then we could build more accurate models:\n\n* **Bias**: The less biased a method, the greater its ability to fit data well.\n* **Variance**: with a lower bias comes typically a higher the variance. And therefore the risk that the model will not adapt accurately to new test data.\nThis is the case here with Gradient Boost: high score but cross-validation is very distant.\n\nThe reverse also holds: the greater the bias, the lower the variance. A high-bias method builds simplistic models that generally don't fit well training data. \nWe can see the red and green curves from ExtraTrees, RandomForest and SVC are pretty close.\n**This points to a lower variance, i.e. a stronger ability to apply the model to new data.**\n\nI used the above graphs to optimize the parameters for Adaboost, ExtraTrees, RandomForest, GradientBoost and SVC.\nThis resulted in a significant improvement of the prediction accuracy on the test data (score).\n\nIn addition, I found out that AdaBoost does not do a good job with this dataset as the training score and cross-validation score are quite far apart. \n\n## 4.13 Selecting and combining the best classifiers\nSo, how do we achieve the best trade-off beween bias and variance?\n1. We will first compare in the next section the classifiers; results between themselves and applied to the same test data.\n2. Then ""ensemble"" them together with an automatic function callled *voting*."
"**Observations:**\n* As indicated before, Adaboost has the lowest correlations when compared to other predictors. This indicates that it predicts differently than the others when it comes to the test data.\n* We will therefore 'ensemble' the remaining four predictors.\n\n## 4.14 Ensembling\nThis is the final step, pulling it together with an amazing 'Voting' function from sklearn.\nAn ensemble is a supervised learning algorithm, that it can be trained and then used to make predictions.\nThe last line applied the ""ensemble predictor"" to the test data for submission."
## 4.15. Summary of most important features
"Nice graphics, but the obsevation is unclear in my opinion:\n* On one side, we hope as analyst that the models come out with similar patterns. An easy direction to follow.\n* At the same time, ""there have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores"". As we say in business, diversity brings better results, this seems to be true with algorithms as well!"
#### Facet Grid Plot - FirePlace QC vs.SalePrice
#### PointPlot
### Missing Value Analysis \n \n #### Numeric Features
#### Missing values for all numeric features in Bar chart Representation
#### Categorical Features
#### Missing values for  Categorical features in Bar chart Representation
### Categorical Feature Exploration
"The standard deviation of the audio features themselves do not give us much information ( as we can see in the plots below), we can sum them up and calculate the mean of the standard deviation of the lists."
"\n### Correlation Between Variables\n\nWe will correlate the feature **valence** which describes the musical positiveness with **danceability** and **energy**.\n\n\n#### Valence and Energy\nThe correlation between valence and energy shows us that there is a conglomeration of songs with high energy and a low level of valence. This means that many of my energetic songs sound more negative with feelings of sadness, anger and depression ( NF takes special place here haha). whereas when we look at the grays dots we can see that as the level of valence - positive feelings increase, the energy of the songs also increases. Although her data is split , we can identify this pattern which indicates a kind of 'linear' correlation between the variables."
Now let us load a sample audio file using librosa
Now let us visually inspect data and see if we can find patterns in the data
As you can see the air conditioner class is shown as random class and we can see its pattern.Let us again see another class by using the same code to randomly select another class and observe its pattern
Let us see the class distributions for this problem
\n#### 6.1 Classifier Comparision\n\nBy Classifier Comparison we choose which model best for the given data.
"#### Important Note:\n\n- If you're facing an error to see the result of below code cell, It's because there is a lot of __calcualation__ is taking place as multiple algorithms are running so for that I suggest get this notebook to edit mode and try to run cell by cell.\n\n- I am commenting two of the below code cells as they are taking a lot of notebook space, but i want you guys try to run them from your side. "
## Training Evaluation\n\nLet's take a look at our training loss over all batches:
"## Predict and Evaluate on Holdout Set\nNow we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using [Matthew's correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task."
"To further clean our text data, we’ll also want to create a custom transformer for removing initial and end spaces and converting text into lower case. Here, we will create a custom predictors class wich inherits the TransformerMixin class. This class overrides the transform, fit and get_parrams methods. We’ll also create a clean_text() function that removes spaces and converts text into lowercase."
"When we classify text, we end up with text snippets matched with their respective labels. But we can’t simply use text strings in our machine learning model; we need a way to convert our text into something that can be represented numerically just like the labels (1 for positive and 0 for negative) are. Classifying text in positive and negative labels is called sentiment analysis. So we need a way to represent our text numerically.\n\nOne tool we can use for doing this is called **Bag of Words**. **BoW** converts text into the matrix of occurrence of words within a given document. It focuses on whether given words occurred or not in the document, and it generates a matrix that we might see referred to as a BoW matrix or a document term matrix.\n\nWe can generate a BoW matrix for our text data by using scikit-learn‘s CountVectorizer. In the code below, we’re telling CountVectorizer to use the custom spacy_tokenizer function we built as its tokenizer, and defining the ngram range we want.\n\nN-grams are combinations of adjacent words in a given text, where n is the number of words that incuded in the tokens. for example, in the sentence “Who will win the football world cup in 2022?” unigrams would be a sequence of single words such as “who”, “will”, “win” and so on. Bigrams would be a sequence of 2 contiguous words such as “who will”, “will win”, and so on. So the ngram_range parameter we’ll use in the code below sets the lower and upper bounds of the our ngrams (we’ll be using unigrams). Then we’ll assign the ngrams to bow_vector."
We can also implement linear regression with a bare-bones approach. In the following example we measure the vertical distance and horizontal distance between a random data point and the regression line. \n\nFor more information on implementing linear regression from scratch [I highly recommend this explanation by Luis Serrano](https://aitube.io/video/introduction-to-linear-regression).
The coefficients of a linear model can also be computed using MSE (Mean Squared Error) without an iterative approach. I implemented Python code for this technique as well. The code is in [the second cell of this Github repository](https://github.com/CarloLepelaars/linreg/blob/master/linreg_from_scratch.ipynb)
"Tukey suggested that an observation is an outlier whenever an observation is 1.5 times the interquartile range below the first quartile or 1.5 times the interquartile range above the third quartile. This may sound complicated, but is quite intuitive if you see it visually.\n\nFor normal distributions, Tukey’s criteria for outlier observations is unlikely if no outliers are present, but using Tukey’s criteria for other distributions should be taken with a grain of salt.\n\nThe formula for Tukey's method:\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2a103bbd9233d9f8f711a7c76dfeb9694446f860)\n\nYa is the larger of two means being compared. SE is the standard error of the sum of the means.\n\n[Source](https://en.wikipedia.org/wiki/Tukey%27s_range_test)"
## Overfitting 
A Link Function is used in Generalized Linear Models (GLMs) to apply linear models for a continuous response variable given continuous and/or categorical predictors. A link function that is often used is called the inverse logit or logistic sigmoid function.\n\nThe link function provides a relationship between the linear predictor and the mean of a distribution.
"### Logistic regression\n\nWith logistic regression we use a link function like the inverse logit function mentioned above to model a binary dependent variable. While a linear regression model predicts the expected value of y given x directly, a GLM uses a link function. \n\nWe can easily implement logistic regression with [sklearn's Logistic Regression function.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
### Histogram for main features
## Step 0 : quaternions
![](https://d2gne97vdumgn3.cloudfront.net/api/file/UMYT4v0TyIgtyGm8ZXDQ)
"**Euler angles** are really important, and we have a problem with Z.\n\n### Why Orientation_Z (euler angle Z) is so important?\n\nWe have a robot moving around, imagine a robot moving straight through different surfaces (each with different features), for example concrete and hard tile floor. Our robot can can **bounce** or **balance** itself a little bit on if the surface is not flat and smooth, that's why we need to work with quaternions and take care of orientation_Z.\n\n![](https://lifeboat.com/blog.images/robot-car-find-share-on-giphy.gif.gif)"
**Useful functions**
This advanced features based on robust statistics.
"2.7 Augmentation pipeline  \n\nThe imgaug library provides a very useful feature called **Augmentation pipeline**. Such a pipeline is a sequence of steps that can be applied in a fixed or random order. This also gives the flexibility to apply certain transformations to a few images and other transformations to other images. In the following example, we are applying the flip, sharpen,crop etc transformations on some of the images. The blur and affline transformations will be applied sometimes and all these transformations will be applied in random order."
"# 4. Data Augmentation using Albumentations \n\n![](https://albumentations.readthedocs.io/en/latest/_static/logo.png)\n\n[Albumentations](https://albumentations.readthedocs.io/en/latest/index.html#) is a fast image augmentation library and easy to use wrapper around other libraries.It is based on numpy, OpenCV, imgaug picking the best from each of them.It is written by Kagglers and was used to get top results in many DL competitions at Kaggle, topcoder, CVPR, MICCAI. Read more about it here: https://www.mdpi.com/2078-2489/11/2/125"
# 5. Data Augmentation using Augmentor \n\n![](https://augmentor.readthedocs.io/en/master/_static/logo.png)\n
# 6. Keras Image Data Generator  \n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRgbD4KXC9PBSYSLHojvt-qcu99NCfy4AcN3eEGFM1YTmLIAJFo&usqp=CAU)\nThe Keras library has a built in class created just for the purpose of adding transformations to images.This class is called **ImageDataGenerator** and it generates batches of tensor image data with real-time data augmentations. 
Let's create a directory where the transformed images will be stored. The directory will be called keras_augmentation and its path is as follows:\n
"We have seven types of diagnosed cancerous growths here:\n+ Unkown: a possibly novel type of growth\n+ Nevus: (from Google) a usually non-cancerous disorder of pigment-producing skin cells commonly called birth marks or moles.\n+ Melanoma: Skin cancer's form (what we are working with)\n+ Seborrheic keratosis: Brown, waxy and patchy growths that are not related to skin cancer.\n+ Lentigo NOS: A type of skin cancer that starts from the outside of the skin and attacks by going inword.\n+ Lichenoid keratosis: It is a thin pigmented sort of plaque, if you will.\n+ Solar lentigo: Like lentigo but caused by UV rays from the sun (very common in Delhi)\n+ cafe-au-lait macule: French for ""coffee with milk"". These are brownish spots also called ""giraffe spots"".\n+ atypical melanocytic proliferation: Abnormal quantities of melanin appear on the skin."
**80 percent of all the cases here are unknown. Literally 80 percent.**
So we have a bell (Gaussian or normal distribution) of train data. What about test?
Middle-aged people are the most likely to get cancer whereas those on the RHS and LHS of the plot are least likely to get it.  We now have found out that there is a bell curve distribution for age.
"We use normalization to evenly distribute something across an image - we can use normalization to, for example, normalize lighting conditions across the image."
** 8. Image Augmentations**
"We need to use image augmentations to add to our existing set of data. Why? To help our model identify the tumors correctly, even in grayscale colormap."
Grayscale images
"We will first try to visualize in grayscale (only gray colors) so that it is possible for us to clearly visualize the varied differences in color, region, and shape."
Ben Graham's method from 1st competition
Train
Now we try it on the test set:
Test
Nice! But we can further observe the clear color distinctions by using Neuron Engineer's method (an improved version of Ben Graham's).
Neuron Engineer's method
"Here we can finally visualize the clear distinctions in our data. The clear regions, the clear color differences, the clear everything! This is probably the best preprocessing method that we can apply... maybe not?"
Circle crop
"Circular crop has successfully worked, although it may not be feasible for images where the tumor is on the edge of the image. It does not seem so feasible, so I would recommend you try to be smarter in your methods for preprocessing. Remember, you can build upon Ben Graham's work as a starting point, then try Neuron Engineer's or circle crop or even build you own method.\n\n"
"Now we are using **auto-cropping** as a method of preprocessing, which is a more ""refined"" circle crop if you will. Think of circle crop as C, and think of auto-cropping as C++. Auto-cropping indeed is powerful, but the risk is that you will lose valuable data in the image."
Background subtraction
Another thing you can do is **background subtraction.**
Oh god... looks like *the Conjuring* now. 
Image segmentation
"This is called **image segmentation**. It breals down an image into its constituent parts represented by the distinction between regions (i.e in this instance the growth is white and the halo/surrrounding area is blackened). It helps our model to visually understand the distinctions even better than using grayscale images and it also helps to let our model identify the tumor.\n\nHowever, it has a heavy downside as we lose all information inside and outside the growth and thus our model loses the capability to understand or learn something from the image."
A finer method for image segmentation
This is a finer form of image segmentation where we use a second threshold to finetune our segmented data in a sort of way. This helps us see some parts of the growth and also it lets us see the images and the halos in small amounts.
Grayscale image segmentation
"These are now the segmented grayscale images, complete with markers. It will be a bit difficult for the model to learn anything from this due to the complete and utter confusion (pardon me) in the image with regard to the clear distinctions between image segments and there is no disctinction between parts of the image."
Now we move on to something more interesting: **Fourier transforms.** 
What comes out is the magnitude spectrum of the image. It is helpful to understand where the majority of the growth is concentrated.
"Also, you can use the albumentations library to create a lot of simulated images for your model. Remember, your model **MUST BE ABLE TO GENERALIZE!**"
We have much more augmentations we can try like:
Mess around with `p`:
"Not done yet! We still have a few more tricks, namely erosion and dilation."
Erosion
Dilation\n\nThere! Now we can try dilation:
Now for both!
Combination of erosion and dilation
Roman's microscope augmentation
Albumentations + erosion
Albumentations + dilation
Albumentations + erosion + dilation
Complex wavelet transform
### Horizontal detail
### Vertical detail
## Hough transform
## Canny edges
**9. 3-dimensional augmentation techniques**
A simple enough attack would be to slightly alter an image by adding some noise.
This is a very basic way to add noise - it's known as salt and pepper noise. Here however we can implement multiple noises from a glance.
---\n\n APPENDIX A: Melanoma\n\n---
"## Let's checkout holoviews pairplot ..\nSimilar to sns pairplot, Holoviews also offers a pairplot kind of plot. This plot looks better than seaborn's but its very heavy in nature, if we make Holoview = True below. The plot will get the notebook hanged and this notebook will starttaking very long in loading. Feel free to change in Run and checkout the plot."
## Let's check correlation using a heatmap and check how the features are correlated ..
"** Findings ** - \n- Variables like pick_month, week_of_year, day_of_year etc are of very less importance and are not at all related to other variables.\n- OSRM variables are very important => correlation is as high as 0.8\n- Strong correlation is observed between different IDVs but we will be using tree based model so No need to remove those variables which are highly correlated. ( thought try removing for Robustness of model)\n-Trip Duration is slight correated with many IDVs"
\n # Visualization Test Result\n- this section will visualize the predicted classes of test data.
References:\n\n1. https://medium.com/@vijayabhaskar96/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1\n1. https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n1. https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\n1. https://jkjung-avt.github.io/keras-image-cropping/\n1. https://www.kaggle.com/aleksandradeis/aptos2019-blindness-detection-eda
## ROC Graph\n***
"# 5. Interpreting the Data\n***\n\n**Summary:** \nWith all of this information, this is what Bob should know about his company and why his employees probably left:\n 1. Employees generally left when they are **underworked** (less than 150hr/month or 6hr/day)\n 2. Employees generally left when they are **overworked** (more than 250hr/month or 10hr/day)\n 3. Employees with either **really high or low evaluations** should be taken into consideration for high turnover rate\n 4. Employees with **low to medium salaries** are the bulk of employee turnover\n 5. Employees that had **2,6, or 7 project count** was at risk of leaving the company\n 6. Employee **satisfaction** is the highest indicator for employee turnover.\n 7. Employee that had **4 and 5 yearsAtCompany** should be taken into consideration for high turnover rate\n 8. Employee **satisfaction**, **yearsAtCompany**, and **evaluation** were the three biggest factors in determining turnover."
"As we look through these scatter plots, I realized that it is time to explain the assumptions of Multiple Linear Regression. Before building a multiple linear regression model, we need to check that these assumptions below are valid.\n\n\n    Assumptions of Regression\n        \n            Linearity ( Correct functional form )\n            Homoscedasticity ( Constant Error Variance )( vs Heteroscedasticity ).\n            Independence of Errors ( vs Autocorrelation )\n            Multivariate Normality ( Normality of Errors )\n            No or little Multicollinearity.\n        \n\n   \nSince we fit a linear model, we assume that the relationship is **linear**, and the errors, or residuals, are pure random fluctuations around the true line. We expect that the variability in the response(dependent) variable doesn't increase as the value of the predictor(independent) increases, which is the assumptions of equal variance, also known as **Homoscedasticity**. We also assume that the observations are independent of one another(**No Multicollinearity**), and a correlation between sequential observations or auto-correlation is not there.\n\nNow, these assumptions are prone to happen altogether. In other words, if we see one of these assumptions in the dataset, it's more likely that we may come across with others mentioned above. Therefore, we can find and fix various assumptions with a few unique techniques.\n\nSo, **How do we check regression assumptions? We fit a regression line and look for the variability of the response data along the regression line.** Let's apply this to each one of them.\n\n**Linearity(Correct functional form):** \nLinear regression needs the relationship between each independent variable and the dependent variable to be linear. The linearity assumption can be tested with scatter plots. The following two examples depict two cases, where no or little linearity is present. "
"Here we are plotting our target variable with two independent variables **GrLivArea** and **MasVnrArea**. It's pretty apparent from the chart that there is a better linear relationship between **SalePrice** and **GrLivArea** than **SalePrice** and **MasVnrArea**. One thing to take note here, there are some outliers in the dataset. It is imperative to check for outliers since linear regression is sensitive to outlier effects. Sometimes we may be trying to fit a linear regression model when the data might not be so linear, or the function may need another degree of freedom to fit the data. In that case, we may need to change our function depending on the data to get the best possible fit. In addition to that, we can also check the residual plot, which tells us how is the error variance across the true line. Let's look at the residual plot for independent variable **GrLivArea** and our target variable **SalePrice **. "
"Ideally, if the assumptions are met, the residuals will be randomly scattered around the centerline of zero with no apparent pattern. The residual will look like an unstructured cloud of points centered around zero. However, our residual plot is anything but an unstructured cloud of points. Even though it seems like there is a linear relationship between the response variable and predictor variable, the residual plot looks more like a funnel. The error plot shows that as **GrLivArea** value increases, the variance also increases, which is the characteristics known as **Heteroscedasticity**. Let's break this down. \n\n**Homoscedasticity ( Constant Variance ):** \nThe assumption of Homoscedasticity is crucial to linear regression models. Homoscedasticity describes a situation in which the error term or variance or the ""noise"" or random disturbance in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable. In other words, there is a constant variance present in the response variable as the predictor variable increases. If the ""noise"" is not the same across the values of an independent variable like the residual plot above, we call that **Heteroscedasticity**. As you can tell, it is the opposite of **Homoscedasticity.**\n\n\n\nThis plot above is an excellent example of Homoscedasticity. As you can see, the residual variance is the same as the value of the predictor variable increases. One way to fix this Heteroscedasticity is by using a transformation method like log-transformation or box-cox transformation. We will do that later.\n\n**Multivariate Normality ( Normality of Errors):**\nThe linear regression analysis requires the dependent variable to be multivariate normally distributed. A histogram, box plot, or a Q-Q-Plot can check if the target variable is normally distributed. The goodness of fit test, e.g., the Kolmogorov-Smirnov test can check for normality in the dependent variable. We already know that our target variable does not follow a normal distribution. Let's bring back the three charts to show our target variable."
"Success!! As you can see, the log transformation removes the normality of errors, which solves most of the other errors we talked about above. Let's make a comparison of the pre-transformed and post-transformed state of residual plots. "
"Here, we see that the pre-transformed chart on the left has heteroscedasticity, and the post-transformed chart on the right has Homoscedasticity(almost an equal amount of variance across the zero lines). It looks like a blob of data points and doesn't seem to give away any relationships. That's the sort of relationship we would like to see to avoid some of these assumptions. \n\n**No or Little multicollinearity:** \nMulticollinearity is when there is a strong correlation between independent variables. Linear regression or multilinear regression requires independent variables to have little or no similar features. Multicollinearity can lead to a variety of problems, including:\n* The effect of predictor variables estimated by our regression will depend on what other variables are included in our model. \n* Predictors can have wildly different results depending on the observations in our sample, and small changes in samples can result in very different estimated effects. \n* With very high multicollinearity, the inverse matrix, the computer calculates may not be accurate. \n* We can no longer interpret a coefficient on a variable as the effect on the target of a one-unit increase in that variable holding the other variables constant. The reason behind that is, when predictors are strongly correlated, there is not a scenario in which one variable can change without a conditional change in another variable.\n\nHeatmap is an excellent way to identify whether there is multicollinearity or not. The best way to solve multicollinearity is to use regularization methods like Ridge or Lasso."
Outliers\n\nExtreme values affects the performance of a linear model. Let's find out if we have any in our variables. 
As you can see most of the continous variables seem to contaion outliers. We will get rid of some of these outliers in the feature engineering section. 
"# Modeling the Data\n \nBefore modeling each algorithm, I would like to discuss them for a better understanding. This way I would review what I know and at the same time help out the community. If you already know enough about Linear Regression, you may skip this part and go straight to the part where I fit the model. However, if you take your time to read this and other model description sections and let me know how I am doing, I would genuinely appreciate it. Let's get started. \n\n**Linear Regression**\n\n    We will start with one of the most basic but useful machine learning model, **Linear Regression**. However, do not let the simplicity of this model fool you, as Linear Regression is the base some of the most complex models out there. For the sake of understanding this model, we will use only two features, **SalePrice** and **GrLivArea**. Let's take a sample of the data and graph it."
"As we discussed before, there is a linear relationship between SalePrice and GrLivArea. We want to know/estimate/predict the sale price of a house based on the given area, How do we do that? One naive way is to find the average of all the house prices. Let's find a line with the average of all houses and place it in the scatter plot. Simple enough."
You can tell this is not the most efficient way to estimate the price of houses. The average line clearly does not represent all the datapoint and fails to grasp the linear relationship between GrLivArea & SalePrice.  Let use one of the evaluation regression metrics and find out the Mean Squared Error(more on this later) of this line.
Now that we have our predicted y values let's see how the predicted regression line looks in the graph.
Phew!! This looks like something we can work with!! Let's find out the MSE for the regression line as well.
"A much-anticipated decrease in mean squared error(mse), therefore better-predicted model. The way we compare between the two predicted lines is by considering their errors. Let's put both of the model's side by side and compare the errors."
"On the two charts above, the left one is the average line, and the right one is the regression line. Blue dots are observed data points and red lines are error distance from each observed data points to model-predicted line. As you can see, the regression line reduces much of the errors; therefore, performs much better than average line. \n\nNow, we need to introduce a couple of evaluation metrics that will help us compare and contrast models. One of them is mean squared error(MSE) which we used while comparing two models. Some of the other metrics are...\n\n* RMSE (Root Mean Squared Error)\n### $$ \operatorname{RMSE}= \sqrt{\frac{1}{n}\sum_{i=1}^n(\hat{y_i} - y_i)^2} $$\n\nHere\n* $y_i$ = Each observed data point. \n* $\bar{y}$ = Mean of y value.\n* $\hat{y_i}$ = Predicted data point for each $x_i$ depending on i. \n\n\n* MSE(Mean Squared Error)\n### $$\operatorname{MSE}= \frac{1}{n}\sum_{i=1}^n(\hat{y_i} - y_i)^2$$\n\n* MAE (Mean Absolute Error)\n### $$\operatorname{MAE} = \frac{\sum_{i=1}^n|{\bar{y} - y_i}|}{n}$$\n\n* RSE (Relative Squared Error)\n### $$\operatorname{RSE}= \frac{\sum_{i=1}^n(\hat{y_i} - y_i)^2}{\sum_{i=1}^n(\bar{y} - y_i)^2}$$\n\n* RAE (Relative Absolute Error) \n### $$\operatorname{RAE}= \frac{\sum_{i=1}^n |\hat{y_i} - y_i|}{\sum_{i=1}^n |\bar{y} - y_i|}$$\n\n> and \n* $R^2$ (Coefficient of the determination)"
**Plotting feature importances**
"**Make the submission**\n\nIf you remember for EDA, feature engineering and training I had melted the provided data from wide format to long format. Now, I have the predictions in long format but the format to be evaluated for the competition is in long format. Therefore, I'll convert it into wide format using `pivot` function in pandas. Below is an image explaining the pivot function.\n"
"Wth SVM I was able to increase the accuracy to upto 82%. However, we need to take a deeper look at the true positive and true negative rates, including the Area Under the Curve (AUC) for a better prediction. I will explore this soon. Stay Tuned!"
**4. ADA Boost**
### Industry and Job title \n
"As I said earlier, there are some cases where both plots (proportion and raw number of coders) can give valuable informations : this is one of those case and I kept both of them here on purpose, here's what we notice : \n\n**The plot on the left** shows that even if there are more Python users than R users in this survey as we saw earliers, we notice on this plot that there are industries where R is still dominant or as competitive as Python.\n\n* R fares as good as / better than Python in the following industries : Government, Insurance, Non-profit, Pharmaceutical, Retail and Marketing.\n* Python outdoes R, by a large margin, in the tech industry which is also the industry with the most respondents in this survey (which makes sense since Data Scientist is a tech job).\n\n**The plot on the right** shows that when it comes to industries, it's R who seems to be more versatile ! When we see the proportions, we notice that it's way more balanced for R than it is for Python."
"* We notice that most of R coders and Python coders are data scientists, well the opposite would have been very surprising !     \n* A great proportion of Python coders (a cumulative 25%) are either Software Engineers or Machine Learning Engineers while only 5% of R coders occupy those two roles ! Seems that really few R practitioners are considered ML masters.\n* 30% of R coders are Statisticians, Data Analysts or Business Analysts while only 11% of Python coders have one these 3 job titles.\n\n**Except for the Data Scientist role (and Student / Researcher) it seems that the roles where we have the highest % of R coders are the roles where the % of Python coders is the lowest and vice versa !**\n"
"Ouf ! This confrontation between R and Python is really intense, let's take a quick break from it to get our break back by asking ourselves the following question : *How dependant is the job titile of the major of a worker*\n\nUp until now, we've been analyzing the dataset through visualizations and significant plots. I would like to introduce to beginners how we evaluate dependacy between two categorical variables in statistics. Ladies and gentlemen, let me introduce to you the one and only **chi2 test of independence**!\n\nI'd recommend reading this [this](http://stattrek.com/chi-square-test/independence.aspx?Tutorial=AP) for people who never heard about this test, it's a really simple explanation followed by an example of its use.\n\nLet's apply it here, first comes the contingency table : "
"**To be entirely accurate and honest, we shouldn't be applying the chi-2 test here because one of the hypothesis is that all values should be higher than 5. Here we notice for example that we have 0 computer scientist that had Psychology as a Major.**       \nThat being said, my goal here is to show you guys how to conduct such a test during your future analysis so we'll let that go, no offense to our beloved rigourous statisticians :)      \nSo now the chi-2 test : "
### Main function and percentage of time for specific tasks  \n
"It seems that most R users tend to perform tasks related to **business /data analytics** part. Let's recall what we found earliers : More than 70% of R coders use data visualization (ML Methods) and more than 20% of them are either business analysts or data analysts (Role). Thus, it makes sense to see here that most of R users' role at work is to analyze data for business purpose.                                                                                     \nOn the other hand, Python users are doing a bit of everything and we find pretty high percentages for three tasks : Business analytisc, building ML prototypes and build ML services that improve the product.\n\nSo what's the volume of work for each task (gathering data, visualizing it ...) in a DS job ? \n"
"Both type of coders spend most of their time gathering data (38% for Python users, 40% for R) and building models (19% for Python, 18% for R).                            \nGenerally, both type of coders seem to invest the same time for nearly all tasks. For me, this means that using R or Python is more about a preference than an obligation towards some specific usage.     \n\nThat being said, **the biggest difference observed comes for putting work into production (12% for Python users, 7% for R)**. I remember the first time I asked my manager during my internship why do we use Python rather than R and he simply replied *'We always want to put our models into production and doing that using R can really be a pain in the ass'*.         \nI've personnaly never used R production-wise so I can't relate, but I guess that these statistics support my manager's words ! \n\n> EDIT : I actually recalled that there was a specific question in the survey for workers which  was *At work, how often do the models you build get put into production?* and decided to delve deeper into this aspect.       \nPossible answers were frequencies so I tried to check the % of each frequency for Python (resp. R) users and compare between the percentages for both communities."
### Experience as code-writers \n
"Here's what we observe :\n1. The most voted duration of coding, for both R and Python coders, is 3-5 years.\n2. For Python coders, the second most voted duration is 1-2 years while for R coders, the second most voted duration is actually 'More than 10 years' !\n\nWe conclude that R coders tend to be more experienced as they've been writing code for a longer period of time whil Python started to be more widely used during the 5 last years !"
"The distribution shows that most salaries lie between 50k and 140k USD. According to Glassdoor, the average annual salary for a data scientist is 128k$ so it's coherent with what we've just got here.   \n\nSeaborn's *'distplot'* fits a univariate distribution using kernel density estimation [KDE](http://en.wikipedia.org/wiki/Kernel_density_estimation). We notice with the bins that most kagglers have an income between 70k and 130k and that the fitted distribution is skewed right which means there's much more outliers towards the right (unusually high incomes) than towards the left.\n\nHere's a more sophisticated plot for the distribution of the annual income."
### Salary VS Gender\n
"It seems that the salary gap between the two genders isn't too big but is still in favour of men.       \nThe average for male kagglers is a bit higher than the average for female kagglers.         \nThat being said, there's no woman with an income of 400k or higher while there are some outliers in the men part."
### Salary VS Formal Education\n
"Let's recall what the boxes mean in seaborn's boxplot, the documentation says : *The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, except for points that are determined to be “outliers”*.    \n\nThe median follows a reasonable trend :** the higher the education, the higher the median annual income** except for doctoral education that is shadowed by professional degrees. That being said, the PhD box contain outliers + its whiskers are more extended than those of the Professional degree box so all in all, doctors are the best paid kagglers.  \n\nThe median for people who attended college but hold no degree is higher than the median for Bachelors and Masters holders BUT the first quartile (Q1) of first community is way smaller than the first quartile of the other two communities.   \nSo looking at the turquoise blue box and comparing it with the light pink and mustard yellow boxes, we notice that the majority of people with a professional degree are between Q1 and the median while the majority of Bachelors and Masters holders are between the median and Q3."
### Salary VS Job Title\n
"People labeled as **Machine Learning Engineers or Data Scientst have an annual average income higher than Data Analysts, Business Analysts, Statisticians or Programmers. **    \nOne should be aware of the job's title when looking for work because the salaries seem to be really different even if many resepondents  identify as data scientists ! "
"That's really good scores considering that : \n* Logistic regression is the easiest and most simple classifier to build and understand\n* We only selected 18 demographic features and one hot encoded them\n\n**So with just basic preprocessing and a basic model, we get 68% precision and 60% recall !**         \nAccuracy (#correct outputs / #total outputs) is not so bad too but that's expected since the dataset is a bit imbalanced.\nTo learn more about precision, recall and why those two measures are more accurate than accuracy (see what I did there?), check the following [link](http://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/).\n\nLet's recall that logistic regression outputs probabilities. A treshold (0.5 by default) is used to binarize the output, i.e if probability>0.5, predict positive class, else predict negative class.     \nAs the dataset is imbalanced, maybe reducing the threshold can yield better result because it's less probable to have a very high income. That's why we plot the [ROC curve](http://www.quora.com/Whats-ROC-curve) which basically helps to observe the trade-off between precision and recall when reducing/increasing the threshold, so here we go !"
"The AUC score isn't very high, that suggest that modifying the threshold wouldn't bring a big improvement to the model.   \n\nThat's it for the ML model part! I hope this can help beginners in DS to gain more experience and informations and I would encourage you to copy the same preprocessing and use more sophisticated models like SVM or Tree based models (Random Forest / Gradient boosting)."
"First thing first, let's see which countries has the most learners, check their age and the gender distribution amongst them."
"We notice that there's a little progress compared to the general proportions (16.71% Female and 81.88% Male). This means that there are more and more women involved and interested in learning Data Science, that's good news."
"We notice that some of the highest proportions of learners are found in Nigeria, Egypt, Kenia and Indonesia. Those are countries where data science is at its very start and it finding as much learners as practitioners make sens. In the other hand, only 26% of american kagglers are still in the learning phase.\n\nWhat I found more surprising was the proportion of learners in China (47.34%). I would have guessed that the proportion of learners in China is similar to the one of the US but that's not true at all according to the survey responses.\n\nLet's see what's the formal education of our learners."
"Most learners hold a bachelor degree (earlier, for all respondents, the most frequent education was Master level). That's primarly because amongst learners we find students who are yet to finish their studies.\n\nLet's now see what platforms are these learners using in their quest to becoming data scientists."
### Platforms used for learning \n
"Interesting!    \nKaggle and Online courses seem to be the favorite platforms for data science learners. \nAt first,** I was a bit surprised when I saw that College/University comes as the 7th most used platform** but then I remembered that : \n- learners include professionals who are looking to career switch\n- learners' median age is 26.\n\nLet's check the frequency of use for younger learners (younger than 23) : "
"Hum, there's some progress (5th instead of 7th) but nothing flagrant. Let's check young kagglers leaving in the US : "
"**Aha ! College / University becomes the second most used platform and it isn't anymore outnumbered by Kaggle.  **\n\nThe thing is in several countries, universities didn't have and still don't have specific data science training. Often, programs are either focused on mathematics or on computer science so a student must complete his training alone. In the US, more and more universities (MIT, Columbia, Stanford...) offer specific data science masters or at least machine learning courses for students who are interested.\n\nThis is why generally, in most countries, students feel like they learn about data science a lot more outside their college.\n\nLet's now see how useful are these same platforms for learners."
"We've seen that Kaggle and Coursera were the most used platforms for DS learning. We notice with this plot that they're also the platforms that are considered to be the most useful amongst learners and they're far ahead of the third most useful platform (Projects).     \nWe also notice that the relative usefulness of a platform is strongly related to the number of learners using that same platform. Indeed, the most used platforms are the most useful according to the last 2 plots."
Let's see which online platform is the most popular amongst data science learners.
"Coursera has a clear edge over Udacity, edX and DataCamp. This must have something to do with Andrew NG's famous course who helped introducing Machine Learning to a huge amount of people.\n\nLet's now see what the learners are looking forward to learn next year ! "
### Method and Tool to learn next year\n
"At first, I drew the two plots separately but when I saw the results I was like no way, I'm going to put one next to the other.\nThe deep learning hype is real ! Methods Kagglers want to learn most are Deep Learning (I consider Neural nets to be included in Deep Learning) by a really big margin. And when we review the tools, well the results are quite the same since people want to master TensorFlow which is the most used tool for deep learning !\n\nLet's now see how much time kagglers learners spend on DS learning on those platforms and for how many years have they been learning DS/ML."
### Time invested on DS training \n
"Most learners spend between 2 and 10 hours a week learning data science, which is kind of the amount you would spend if your learning consists of one Online Course at a time.  \nThat being said, nearly 1000 learners (from a total of 5494) spend more than 11 hours a week. Those are probably students enrolled in data science program or learners fully dedicated to DS learning which is not the case for workers looking for a career switch who naturally have less time at their disposal to invest in DS learning.\n\nLet's see for how many years kagglers have been learning data science."
"84.1% of learners started their DS training at most 2 years ago when only 3.5% of started more than 5 years ago !        \nThis shows the effect of the hype around Data Science over the last few years and also its exponential growth.\n\nWhat comes next focuses on how learners perceive the professional aspect of Data Science, starting with ranking skills according to necessity and ranking proofs of DS knowledge."
### Skills and proofs of knowledge for landing a DS job \n
"For Data Science learners, there's no doubt, mastering Python is the most important skill for a job in Data Science ! (we could add that in our Python VS R duel).\nMost learners think that a degree is more a nice asset to have than a necessary one but I'm not so sure about that. \n\nWhat about ways you can prove your knowledge of ML/DS? \n"
"Learners were asked *What's the most important way you can prove your knowledge of ML/DS?*. \n\nMaster's degree and PhD aren't well ranked, that's expected since most learners consider a that a degree is not necessary to land a job in DS. That being said, I wouldn't agree with the fact than MOOCs are more important that a Master's degree and I don't think that companies value Online Courses certificates more than a Master in Mathematics / Computer science or any related field.\n\nPrior work experience in ML comes in first position and I have agree with that. Hands-on experience is always valuable for recruiters, especially when it comes to coding stuff.\n"
### Learners' job hunt \n
"So the top-3 resources according to learners are :\n1. Companies' job listing pages,\n2. Tech-Specific job boards (Stack Overflow recruitment platform for example)\n3. General job boards (LinkedIn)\n\nNow to the time spent looking for a job."
"* 40.2% of learners aren't actually looking for a job. Let's not forget that some are still enrolled in College/University and others (workers who answered 'yes' for career switch) may be looking to switch position within the company they work for.   \n* 34.5% spend an hour or two a week looking for a job, which means that there's no urge to look for a job. It may be students looking for internships or employed people checking from time to time if an exciting opportunity is out there somewhere.\n* A little more than 10% are actively working for a job, spending at least 6 hours per week job hunting."
"**In this plot, the factors are ordered according to the criteria 'Most Important'.**\n\n1. DS learners are constantly striving for development ! The most important factor during their job hunt, by far, is whether the job would hand them opportunities of professional development.   \n2. The second most important factor is the office environment they would be working in.    \n3. The third one is the programming languages and frameworks they'd be working with. That shows that data scientist aren't really open to work with whatever technology the company is using, they have strong preferences that are crucial for chosing one job over another.  \n4. The salary comes fourth and is close to other factors so it doesn't seem to be too problematic for aspiring data scientists, maybe that's because they're already quite assured that the salary would be great !"
### Visualize decision-trees
### Visualize decision-trees with graphviz
"**Findings:** Over 73% passengers had ticket of category N, followed by nearly 7.5% passengers ticket category were S and P. Passengers with W ticket category were as low as 1.45%.\n\n# 6.Outliers Detection \n**How outliers affect the distribution:** If a value of a variable is significantly above the expected range, it will drag the distribution to the right, making the graph right-skewed or positive-skewed (like Fare). Alternatively, If a value is significantly below the expected range, it will drag the distribution to the left, making the graph left-skewed or negative-skewed.\n\nAnother useful plot for visualizing a continuous variable is box plot. Box plot is particularly helpful to understand the spread of the continus data and whether there are potential unusual observations (outliers) in that variable. It presents information of min, 1st quartile, 2nd quartile(median), 3rd quartile, and max of a variable.**We will use IQR method to detect the outliers for variable Age and Fare though we won't remove them.**"
## 6.1 Outliers Detection for Age 
"## 7.2 Impute Age \nTo impute Age with grouped median, we need to know which features are highly correlated with Age. Let's find out the variables correlated with Age."
"**Findings:** \n1. Age distribution seems to be the same in male and female subpopulations of Sex and S, C, Q subpopulations of Embarked. So Sex and Embarked aren't good predictors for Age.\n2. On the other hand, Age distribution seems to be distinct in Pclass's 1, 2 and 3 subpopulations, so Pclass is informative to predict Age.\n3. Finally, Age distribution seems to be distinct in different categories for nameProcessed, familySize, SibSp, Parch, and cabinProcessed. So they might be good predictors for Age as well."
"**Findings:** As expected Sex, Embarked, and ticketProcessed have the weakest correlation with Age what we could guess beforehand from boxplot. Parch and familySize are moderately correlated with Age. nameProcessed, Pclass, Cabin, and SibSp have the highest correlation with Age. But we are gonna use nameProcessed and Pclass only in order to impute Age since they have the strongest correlation with Age. So the tactic is to impute missing values of Age with the median age of similar rows according to nameProcessed and Pclass."
"# 8.Bivariate Analysis \nBeing the most important part, bivariate analysis tries to find the relationship between two variables. We will look for correlation or association between our predictor and target variables. Bivariate analysis is performed for any combination of categorical and numerical variables. The combination can be: Numerical & Numerical, Numerical & Categorical and Categorical & Categorical. Different methods are used to tackle these combinations during analysis process. The methods are:\n1. Numerical & Numerical: Pearson's correlation, or Spearman correlation (the later doesn't require normal distribution).\n2. Numerical & Categorical: Point biserial correlation (only  if categorical variable is binary type), or ANOVA test. For this problem, you can use either biserial correlation or ANOVA. But I will perform both test just to learn because ANOVA will come in handy if categorical variable has more than two classes.\n3. Categorical & Categorical: We would use Chi-square test for bivariate analysis between categorical variables.\n\n## 8.1 Numerical & Categorical Variables \nFirst we create a boxplot between our numerical and categorical variables to check if the distribution of numerical variable is distinct in different classes of nominal variables. Then we find the mean of numerical variable for every class of categorical variable. Again we plot a histogram of numerical variable for every class of categorical variable. Finally anova or point biserial correlation (in case of two class categorical variable) is calculated to find association between nominal and numerical variables.   "
### 8.1.1 Fare & Survived 
"**Note:** Choose either biserial correlation (if categorical variable has two groups) or Anova. If anova states main interaction effect(i.e.,p<0.05) and categorical variable has more than two categories ( like good, better, best), then perform tukey test to find out the pair or pairs that cause the difference(i.e., main interaction effect).\n\n**Interpretation of ANOVA result:**\nSince p>0.05, we can say that survival chance is not statistically associated with Age.\n\n## 8.2 Categorical & Categorical Variables \nWe will calculate and plot absolute and relative frequency of output categorical variable by predictor nominal variables. We would calculate the chi square test between target nominal and predictor nominal variables. Finally we will calculate Bonferroni-adjusted P value if the contingency table has dimension more than 2x2."
### 8.2.1 Sex & Survived 
"**Interpretation of chi-square test result**: Since all of the expected frequencies aren't greater than 5, the chi2 test results can't be trusted.\n\n# 9.Multivariate Analysis \nIn multivariate analysis, we try to find the relationship among more than two variables. Number of predictor variable in bivariate analysis was one. On the contrary, number of predictor variables for multivariate analysis are more than one. More specifically, we will try to associate more than one predictor variable with the response variable. We will just visualize the impact of different predictor variables (3 variables) at a time on variable Survived."
"## 9.1 (Pclass, Sex, cabinProcessed) vs Survived "
## 11.2.3  Model Selection \nLet's compare our models according to their accuracy score after tunning hyperparameters with cross validation scores to select the best models for further study on this classification problem.
"**Findings:** Among the classifiers, RF and GBC have the highest accuracy after  tunning hyperparameters. So RF and GBC are perhaps worthy of further study on this classification problem. Hence we choose RF and GBC.\n\n**Note:** Please note that if we chose our classifier based on cross validation scores, we would not get RF and GBC as our best classifiers instead we would end up choosing LR and SVC. So it is recommended to select best classifiers based on accuracy after tunning hyperparameters though it is computationally intensive.\n\n## 11.3 Retrain and Predict Using Optimized Hyperparameters \nSo we have our best classifiers with their best hyperparameters that produces best accuracy out of a model. That means if we retrain the classifiers using their best hyperparameters, we will be able to get the very same score that we got after tunning hyperparameters (see part 14.4). Let's retrain our classifiers and then use cross validation to calculate the accuracy of the trained model. That's how we will have the same accuracy score as after tunning hyperparameters. Let's retrain models with optimized hyperparameters."
## 11.4 Feature Importance \nDo the classifiers give the same priority to every feature? Let's visualize the features importance given by our classifiers.
"**Findings:** RF, DT, ETC, and ABC (in particular) give some features no importance (zero importance). On the other hand, GBC give all the features more or less importance but it doesn't give zero importance to any features. These are the tree based models that have 'feature_importances_' method by default. LR, KNN and SVC don't have this method. In this problem, SVC uses rbf kernel (only possible for linear kernel to plot feature importance), so its not possible to view feature importance given by SVC. Though its trickier, we would try to get the feature importance given by LR."
"**Findings:** We can see some negative values that means that higher value of the corresponding feature pushes the classification more towards the negative class (in our case 0) that is, of course, something we're already aware of. Some features like Family_size_single, Embarked_Q, Embarked_C, and Cabin_F were given zero importance by lr.\n\n## 11.5 Learning Curves  \nLet's plot the learning curves for the optimized classifiers to see their bias-variance tradeoff."
"**We can see precision, recall, f1 score and class count for both class (0 and 1) of our two models.**\n## 12.7 Precision-Recall vs Threshold Curve  \nSometimes we want a high precision and sometimes a high recall depending on our classification problem. The thing is that an increasing precision results in a decreasing recall and vice versa. This is called the precision-recall tradeoff that can be illustrated using precision-recall curve as a function of the decision threshold."
"**We can see for RF, the recall falls quickly at a precision of around 84%. So therefore, we need to select the precision-recall tradeoff before 84% of precision which could be at around 82%. Now, for example, if we want a precision of 80% off RF we would need a threshold of around 0.4**\n\n**On the other hand, for GBC, the recall falls fast at a precision of around 84% and hence we would select precision-recall tradeoff at around 80% of precision. If we want a precision of around 81% off GBC, we would need a threshold of around 0.38**\n\n## 12.8 Precision-Recall Curve  \nWe can also plot precision against recall to get an idea of precision-recall tradeoff where y-axis represents precision and x-axis represents recall. In my plot, I plot recall on y-axis and precision on x-axis."
"**We can see recall falls rapidly at around a precision of 0.84 for both RF and 0.82 for GBC that we've observed in the previous section.**\n\n## 12.9 ROC  Curve & AUC Score  \nROC (Reicever Operating Characteristic Curve) is a plot of the true positive rate against the false positive rate of a classifier. It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity). AUC (Area under the ROC Curve) score is the corresponding score to the AUC Curve. It is simply computed by measuring the area under the ROC curve, which is called AUC. We will plot ROC curve and AUC score together for our two classifiers."
"This two plots tells few different things:\n\n1. A model that predicts at chance will have an ROC curve that looks like the diagonal red line. That is not a discriminating model.\n\n2. The further the curve is off the diagonal red line, the better the model is at discriminating between positives and negatives in general.\n\n3. There are useful statistics that can be calculated from this curve, like the Area Under the Curve (AUC). This tells you how well the model predicts and the optimal cut point for any given model (under specific circumstances).\n\n**Comparing the two ROC curves, we can see the distance between blue and red line of RF is greater than the distance between blue and red line of GBC. Hence it can safely be said that RF, in general, is better at discriminating between positives and negatives than GBC. Also RF(92.11%) auc score (which is the area under the roc curve) is greater than gbc(91.94%). It seems the higher the area, the further the classifier is off the red diagonal line and vice versa and hence more accurate. Since RF has more area under the ROC curve than GBC, RF is more accurate.**\n\n# 13.Prediction & Submission  \nFirst we will predict using both rf and gbc. Then we will create two prediction files in csv format for kaggle submission."
"**Correlation among Base Models Predictions:** How base models' predictions are correlated? If base models' predictions are weakly correlated with each other, the ensemble will likely to perform better. On the other hand, for a strong correlation of predictions among the base models, the ensemble will unlikely to perform better. To sumarize, diversity of predictions among the base models is inversely proportional to the ensemble accuracy. Let's make prediction for the test set."
"**Findings:** The prediction looks quite similar for the 8 classifiers except when DT is compared to the others classifiers. Now we will create an ensemble with the base models RF, GBC, DT, KNN and LR. This ensemble can be called heterogeneous ensemble since we have three tree based, one kernel based and one linear models. We would use **EnsembleVotingClassifier method from mlxtend module** for both hard and soft voting ensembles. The advantage is it requires lesser codes to plot decision regions and I find it a bit faster than sklearn's voting classifier."
"**Not so much! But considering the number of features we have, its not either too less. Let's visualize our two components (transformed features) in a scatter plot.**"
"**Looking at this plot, one thing we can say that a linear decision boundary will not be a good choice to separate these two classes. Now we would train our models on this 2d transformed samples to visualize decision regions created by them.**\n\n**Note:** PCA gives you an intuition if a linear or non-linear algorithms would be suitable for a problem. For example, if we look at the scatter plot, we see a non-linear trend between the two class that is, of course better seperable by a non-linear decision boundary. So a non-linear model would be a better bet than a linear one. That's why rf(non-linear) performs better than lr(linear model) for this problem."
**Findings:** There seems to be lesser misclassifications made by hard voting decision region compared to both rf and gbc's decision regions. Let's see how and where hard voting ensemble corrects base learners prediction in a data frame together.
"**Findings:** Soft voting ensemble fails to beat our two best models (rf and gbc). In fact, it produces way to inferior results compared to hard voting ensemble (83.95 vs 84.18). So hard voting ensemble, for this problem, seems to be superior to soft voting ensemble method. WE can visualize soft voting ensemble decision region along with base models decision regions."
**Findings:** Soft voting decision region just seems to be creating more misclassification than rf and gbc.
"The boosting type should be evenly distributed for random search. \n\nAgain, we have to remake this chart in seaborn to have the visualization appear in the rendered notebook (if anyone knows how to address this issue, please tell me in the comments!)"
"Next, for the numeric hyperparameters, we will plot both the sampling distribution (the hyperparameter grid) and the results from random search in a kernel density estimate (KDE) plot. (The grid search results are completely uninteresting). As random search is just drawing random values, we would expect the random search distribution to align with the sampling grid (although it won't be perfectly aligned because of the limited number of searches). \n\nAs an example, below we plot the distribution of learning rates from both the sampling distribution and the random search results. The vertical dashed line indicates the optimal value found from random search."
The following code repeats this plot for all the of the numeric hyperparameters. 
"## Sequence of Search Values\n\nFinally, we can plot the sequence of search values against the iteration for random search. Clearly there will not be any order, but this can let us visualize what happens in a random search!\n\nThe star indicates the best value of the hyperparameter that was found."
"As a final plot, we can show the score versus the value of each hyperparameter. We need to keep in mind that the hyperparameters are not changed one at a time, so if there are relationships between the values and the score, they do not mean that particular hyperparameter is influencing the score. However, we might be able to identify values of hyperparameters that seem more promising. Mostly these plots are for my own interest, to see if there are any trends! "
"We want to avoid placing too much emphasis on any of these relationships because we were not changing one hyperparameter at a time (although we could carry out experiments where we only change one hyperparameter and observes the effects on the score) and so the trends are not due solely to the single hyperparameter we show. If we could plot this in higher dimensions, it might be interesting to see if there are more promising regions of the search space but here we are limited to one dimension (a single hyperparameter versus the score).  If we want to observe the effects of one hyperparameter on the cross validation score, we could alter only that hyperparameter while holding all the others constant. However, the hyperparameters do not act by themselves and there are complex interactions between the model settings."
"Most of the targets almost have the value between +8 or -8,please check the plot below. and some of data have value (-30)"
we should be careful about them!
 \n## 6-3-2  distplot
 \n## 6-3-3 violinplot
 \n## 6-3-4 Scatter plot\nScatter plot Purpose to identify the type of relationship (if any) between two quantitative variables
 \n## 6-3-5 Box\n
 \n## 6-4 Data Preprocessing\n\n\n>What methods of preprocessing can we run on  Elo?! \n###### [Go to top](#top)
Important features\n==================
*Gradient Boosting*\n-------------------
***BsmtFinSF2***\n- Type 2 finished square feet.
"- There are a large number of data points with this feature = 0. Outside of this, there is no significant correlation with SalePrice and a large spread of values.\n- Hence, I will replace this feature with a flag."
***BsmtUnfSF***\n- Unfinished square feet of basement area
"- This feature has a significant positive correlation with SalePrice, with a small proportion of data points having a value of 0. This tells me that most houses will have some amount of square feet unfinished within the basement, and this actually positively contributes towards SalePrice. \n- The amount of unfinished square feet also varies widely based on location and style. \n- Whereas the average unfinished square feet within the basement is fairly consistent across the different lot shapes.\n- Since this is a continuous numeric feature with a significant correlation, I will bin this and create dummy variables. "
***TotalBsmtSF***\n- Total square feet of basement area.
"- This will be a very important feature within my analysis, due to such a high correlation with Saleprice.\n- We can see that it varies widely based on location, however the average basement size has a lower variance based on type, style and lot shape.\n- Due to this being a continuous numeric feature and also being a very significant feature when describing SalePrice, I believe there could be more value to be mined within this feature. Hence, I will create some binnings and dummy variables. "
***1stFlrSF***\n- First floor square feet.
"- Clearly this shows a very high positive correlation with SalePrice, this will be an important feature during modeling.\n- Once again, this feature varies greatly across neighborhoods and the size of this feature varies across building types and styles. \n- This feature does not vary so much across the lot size.\n- Since this is a continuous numeric feature, once again I will bin this feature and create dummy variables."
***2ndFlrSF***\n- Second floor square feet.
"- Interestingly we see a highly positively correlated relationship with SalePrice, however we also see a significant number of houses with value = 0.\n- This is explained with the other visuals, showing that some styles of houses perhaps do not have a second floor, hence cannot have a value for this feature - such as ""1Story"" houses.\n- We also see a high dependance and variation between neighborhoods, building types and lot sizes.\n- It is evident that all the variables related to ""space"" are important in this analysis. Since this feature is a continuous numeric feature, I will bin this and create dummy variables."
***LowQualFinSF***\n- Low quality finished square feet (all floors)
"- We can see that there is a large number of properties with a value of 0 for this feature. Clearly, it does not have a significant correlation with SalePrice.\n- For this reason, I will replace this feature with a flag."
***Bedroom***\n- Bedrooms above grade (does not include basement bedrooms)
"- We see a lot of houses with 2 3 and 4 bedrooms above ground, and a very low number of houses with 6 or above.\n- Since this is a continuous numeric feature, I will leave it how it is."
***Kitchen***\n- Kitchens above grade.
"- Similarly to last previous feature, we see just a small number of houses with a large number of kitchens above grade. This shows that most houses have 1 kitchen above grade.\n- Since this is a continuous numeric feature, I will leave it as it is."
***KitchenQual***\n- Kitchen quality.
"- There is a clear positive correlation with the SalePrice and the quality of the kitchen.\n- There is one value for ""Gd"" that has an extremely high SalePrice however.\n- For this feature, since it is categorical with an order, I will replace these values by hand."
***TotRmsAbvGrd***\n- Total rooms above grade (does not include bathrooms)
"- Generally we see a positive correlation, as the number of rooms increases, so does the SalePrice.\n- However due to low frequency, we do see some unreliable results for the very large and small values for this feature.\n- Since this is a continuous numeric feature, I will leave it as it is."
***Fireplaces***\n- Number of fireplaces.
"- Once again we have a positive correlation with SalePrice, with most houses having just 1 or 0 fireplaces.\n- I will leave this feature as it is."
***FireplaceQu***\n- Fireplace quality.
"- We also see a positive correlation and the fireplace quality increases. Most houses have either ""TA"" or ""Gd"" quality fireplaces. \n- Since this is a categorical feature with order, I will replace the values by hand."
***GrLivArea***\n- Above grade ground living area in square feet.
"- We see a very high positive correlation with SalePrice.\n- We also see the values varying very highly between styles of houses and neigborhood.\n- Since this will be an important feature in our modeling, I will create bins and dummy features."
\n#### 4.2.3 - Architectural & Structural\n\n***MSSubClass***\n- Identifies the type of dwelling involved in the sale.
"- Each of these classes represents a very different style of building, as shown in the data description. Hence, we can see large variance between classes with SalePrice. \n- This is a numeric feature, but it should actually be categorical. I could cluster some of these categories together, but for now I will create a dummy feature for each category."
***BldgType***\n- Type of dwelling.
"- The different categories exhibit a range of average SalePrice's. The class with the most observations is ""1Fam"". \n- We can also see that the variance within classes is quite tight, with only a few extreme values in each case.\n- There could be a possibility to cluster these classes, however for now I am going to create dummy features."
***HouseStyle***\n- Style of dwelling.
"- Here we see quite a few extreme values across the categories and a large weighting of observations towards the integer story houses.\n- Although the highest average SalePrice comes from ""2.5Fin"", this has a very high standard deviation and therefore more reliably, the ""2Story"" houses are also very highly priced on average.\n- Since there are some categories with very few values, I will cluster these into another category and create dummy variables."
***OverallQual***\n- Rates the overall material and finish of the house.
"- This feature although being numeric is actually categoric and ordinal, as the value increases so does the SalePrice. Hence, I will keep it as a numeric feature.\n- We see here a nice positive correlation with the increase in OverallQual and the SalePrice, as you'd expect."
***OverallCond***\n- Rates the overall condition of the house.
"- Interestingly, we see here that it does follow a positive correlation with SalePrice, however we see a peak at a value of 5, along with a high number of observations at this value.\n- The highest average SalePrice actually comes from a value of 5 as opposed to 10, which may be a reasonable assumption.\n- For this feature, I will leave it as being numeric and ordinal.\n\n***YearRemodAdd***\n- Remodel date (same as construction date if no remodeling or additions)."
"- Here we can see that the newer the remodelling of a house, the higher the SalePrice.\n- From the data description, I believe that creating a new feature describing the difference in number of years between remodeling and construction may be a good choice."
"- Clearly we can see that there are some values which have a much higher SalePrice than others. I will leave this feature as it is, without any binnings."
***YearBuilt***\n- Original construction date.
"- Here we can see a fairly consistent upward trend for the SalePrice as houses are more modern. \n- For this feature, I am going to create bins and dummy features"
***Foundation***\n- Type of foundation.
"- We have 3 classes with high frequency, however we have 3 of low frequency.\n- Due to the large difference in median and mean SalePrice's across the 3 lower frequent classes, I am not going to cluster these together. \n- Also since this feature is not ordinal, labelling does not make sense. Instead I will create dummy variables."
***Functional***\n- Home functionality.
"- This categorical feature shows that most houses have ""Typ"" functionality, and looking at the data description leads me to believe that there is an order within these categories, ""Typ"" being of the highest order.\n- Therefore, I will replace the values of this feature by hand with numbers."
\n#### 4.2.4 - Exterior\n\n***RoofStyle***\n- Type of roof.
"- This feature has two highly frequent categories but the values of SalePrice differ between each.\n- Since this is a categorical feature without order, I will create dummy variables."
***RoofMatl***\n- Roof material.
"- Interestingly, there are very few observations in the training data for several classes. However, these will be dropped during feature reduction if they turn out to be insignificant.\n- Hence, I will create dummy variables."
***MasVnrType***\n- Masonry veneer type.
"- Each class has quite a unique range of values for SalePrice, the only class that stands out is ""BrkCmn"", which has a low frequency.\n- Clearly ""Stone"" demands the highest SalePrice on average, although there are some extreme values within ""BrkFace"".\n- Since this is a categorical feature without order, I will create dummy variables here."
***MasVnrArea***\n- Masonry veneer area in square feet.
"- From this we can see that this feature has negligible correlation with SalePrice, and the values for this feature vary widely based on house type, style and size. \n- Since this feature is insignificant in regards to SalePrice, and it also correlates highly with ""MasVnrType"" (if ""MasVnrType = ""None"" then it has to be equal to 0), I will drop this feature."
***ExterQual***\n- Evaluates the quality of the material on the exterior.
"- We can see here that this feature shows a clear order and has a positive correlation with SalePrice. As the quality increases, so does the SalePrice. \n- We see the largest number of observations within the two middle classes, and the lowest observations within the lowest class.\n- Since this is a categorical feature with order, I will replace these values by hand."
***ExterCond***\n- Evaluates the present condition of the material on the exterior. 
"- Interestingly we see the largest values of SalePrice for the second and third best classes. This is perhaps because of the large frequency of values within these classes, whereas we only see 3 observations within ""Ex"" from the training data.\n- Since this categorical feature has an order, but thr SalePrice does not necessarily correlate with this order... I will create dummy variables."
***GarageType***\n- Garage location.
"- Here we see ""BuiltIn"" and ""Attched"" having the 2 highest average SalePrices, with only a few extreme values within each class.\n- Since this is categorical without order, I will create dummy variables."
***GarageYrBlt***\n- Year garage was built.
- We can see a slight upward trend as the garage building year becomes more modern.\n- For this feature I am going to create bins and the dummy variables.
***GarageFinish***\n- Interior finish of the garage.
"- Here we see a nice split between the 3 classes, with ""Fin"" producing the highest SalePrice's on average.\n- I will create dummy variables for this feature."
***GarageCars***\n- Size of the garage in car capacity.
"- We generally see a positive correlation with an increasing garage car capacity. However, we see a slight dip for 4 cars I believe due to the low frequency of houses with a 4 car garage."
***GarageArea***\n- Size of the garage in square feet.
"- This has an extremely high positive correlation with SalePrice, and it is highly dependant on Neighborhood, building type and style of the house.\n- This could be an important feature in the analysis, so I will bin this feature and create dummy variables."
***GarageQual***\n- Garage quality.
"- We see a lot of homes having ""TA"" quality garages, with very few homes having high quality and low quality ones.\n- I am going to cluster the classes here, and then create dummy variables."
***GarageCond***\n- Garage condition.
"- We see a fairly similar pattern here with the previous feature. We see a slight positive correlation and then a dip, I believe due to the low number of houses that have ""Ex"" or ""Gd"" garage conditions. \n- Similarly to before, I am going to cluster and then dummy this feature."
***WoodDeckSF***\n- Wood deck area in SF.
"- This feature has a high positive correlation with SalePrice.\n- We can also see that it varies widely with location, building type, style and size of the lot.\n- There is a significant number of data points with a value of 0, so I will create a flag to indicate no Wood Deck. Then, since this is a continuous numeric feature, and I believe it to be an important one, I will bin this and then create dummy features. "
***PoolArea***\n- Pool area in square feet. 
"- We see almost 0 correlation due to the high number of houses without a pool.\n- Hence, I will create a flag here."
***PoolQC***\n- Pool quality.
"- Due to not many houses having a pool, we see very low numbers of observations for each class.\n- Since this does not hold much information this feature, I will simply remove it."
***Fence***\n- Fence quality.
"- Here we see that the houses with the most privacy have the highest average SalePrice.\n- There seems to be a slight order within the classes, however some of the class descriptions are slightly ambiguous, therefore I will create dummy variables here from this categorical feature. "
\n#### 4.2.5 - Location\n\n***MSZoning***\n- Identifies the general zoning classification of the sale. 
"- Since this a categorical feature without order, and each of the classes has a very different range and average for SalePrice, I will create dummy features here."
***Neighborhood***\n- Physical locations within Ames city limits.
"- Neighborhood clearly has an important contribution towards SalePrice, since we see such high values for certain areas and low values for others.\n- Since this is a categorical feature without order, I will create dummy features."
#### 4.2.6 - Land\n\n***LotFrontage***\n- Linear feet of street connected to property.
"- This feature seems to be fairly randomly distributed against SalePrice without any significant correlation.\n- LotFrontage doesn't seem to vary too much based on ""Neighborhood"", but the ""BldgType"" does seem to have a affect on the average LotFrontage.\n- Since this feature doesn't seem to show any significance to bin into groupings, I will leave this feature as it is until I scale the features."
***LotArea***\n- Lot size in square feet.
"- This feature shows a high correlation but it is very positively skewed. \n- Hence, I will create quantile bins and dummy features. Quantile bins are not based on approximately equal sized bins, instead creating bins with a similar frequency of data points within each bin."
***LotShape***\n- General shape of property.
"- Clearly we see some extreme values for some categories and a varying SalePrice across classes.\n- ""Reg"" and ""IR1"" have the highest frequency of data points within them.\n- Since this is a categorical feature without order, I will create dummy features."
***LandContour***\n- Flatness of the property
"- Most houses are indeed on a flat contour, however the houses with the highest SalePrice seem to come from properties on a hill interestingly.\n- Since this a categorical feature without order, I will create dummy features."
***LotConfig***\n- Lot configuration.
"- Cul de sac's seem to boast the highest average prices within Ames, however most houses are positioned inside or on the corner of the lot.\n- To simplify this feature I wil cluster ""FR2"" and ""FR3"", then create dummy features."
***LandSlope***\n- Slope of property.
"- We see that most houses have a gentle slope of land and overall, the severity of the slope doesn't appear to have much of an impact on SalePrice.\n- Hence, I am going to cluster ""Mod"" and ""Sev"" to create one class, and create a new flag to indicate a gentle slope or not."
\n#### 4.2.7 - Access\n\n***Street***\n- Type of road access to the property.
"- With such a lower number of observations being assigned to the class ""Grvl"" it is redundant within the model.\n- Hence, I will drop this feature."
***Alley***\n- Type of alley access to the property.
"- Here we see a fairly even split between to two classes in terms of frequency, but a much higher average SalePrice for Paved alleys as opposed to Gravel ones.\n- Hence, this seems as though it could be a good predictor. I will create dummy features from this."
***PavedDrive***\n- Paved driveway.
"- Here we see the highest average price being demanded from houses with a paved driveway, and most houses in this srea seem to have one.\n- Since this is a categorical feature without order, I will create dummy variables."
\n#### 4.2.8 - Utilities\n\n***Heating***\n- Type of heating.
"- We see the highest frequency and highest average SalePrice coming from ""GasA"" and a very low frequency from all other classes.\n- Hence, I will create a flag to indicate whether ""GasA"" is present or not."
***HeatingQC***\n- Heating quality and condition.
"- Here we see a positive correlation with SalePrice as the heating quality increases. With ""Ex"" bringing the highest average SalePrice.\n- We also see a high number of houses with this heating quality too, which means most houses had very good heating!\n- This is a categorical feature, however because it exhibits an order, I will replace the values by hand with numbers."
***CentralAir***\n- Central air conditioning.
"- We see that houses with central air conditioning are able to demand a higher average SalePrice than ones without.\n- For this feature, I will simply replace the categories with numbers 0 and 1."
***Electrical***\n- Electrical system.
"- We see the highest average SalePrice coming from houses with ""SBrkr"" electrics, and these are also the most frequent electrical systems installed in the houses from this area. \n- We have 2 categories in particular that have very low frequencies, ""FuseP"" and ""Mix"".\n- I am going to cluster all the classes related to fuses, and the ""Mix"" class will probably be removed during feature reduction."
\n#### 4.2.9 - Miscellaneous\n\n***MiscFeature***\n- Miscellaneous feature not covered in other categories.
"- We can see here that only a low number of houses in this area with any miscalleanous features. Hence, I do not believe that this feature holds much.\n- Therefore I will drop this feature along with MiscVal."
***MoSold***\n- Month sold (MM).
"- Although this feature is a numeric feature, it should really be a category. \n- We can see that there is no real indicator as to any months that consistetly sold houses of a higher price, however there does seem to be a fairly even distribution of values between classes.\n- I will create dummy variables from each category."
***YrSold***\n- Year sold (YYYY).
"- Here we see just a 5 year time period of which the houses in this dataset were sold.\n- There is a n even distribution of values between each class, and each year has a very similar average SalePrice.\n- Even though this is numeric, it should be categorical. Therefore I will create dummy variables."
***SaleType***\n- Type of sale.
"- Most houses were sold under the ""WD"" category, being a conventional sale, however the highest SalePrice was seen from houses that were sold as houses that were brand new and just sold.\n- For this feature, I will cluster some categories together and then create dummy features."
***SaleCondition***\n- Condition of sale.
"- Here we see the largest average SalePrice being associated with partial sales, and the most frequent sale seems to be the normal sales.\n- Since this is a categorical feature without order, I will create dummy features."
"\n### 4.3 - Target Variable\n\n- Unlike classification, **in regression we are predicting a continuous number**. Hence, the prediction could be any number along the real number line.\n- Therefore, it is always useful to check the distribution of the target variable, and indeed all numeric variables, when building a regression model. Machine Learning algorithms work well with features that are **normally distributed**, a distribution that is symmetric and has a characteristic bell shape. If features are not normally distributed, you can transform them using clever statistical methods.\n- First, let's check the target variable."
"The distribution of the target variable is **positively skewed**, meaning that the mode is always less than the mean and median. \n\n- In order to transform this variable into a distribution that looks closer to the black line shown above, we can use the **numpy function log1p** which applies log(1+x) to all elements within the feature."
"We can see from the skewness and the plot that it follows much more closely to the normal distribution now. **This will help the algorithms work most reliably because we are now predicting a distribution that is well-known, i.e. the normal distribution**. If the distribution of your data approximates that of a theoretical distribution, we can perform calculations on the data that are based on assumptions of that well-known distribution. \n\n- ***Note:*** Now that we have transformed the target variable, this means that the prediction we produce will also be in the form of this transformation. Unless, we can revert this transformation..."
"\n### 4.4 - Treating skewed features\n\nAs touched on earlier, skewed numeric variables are not desirable when using Machine Learning algorithms. The reason why we want to do this is move the models focus away from any extreme values, to create a generalised solution. We can tame these extreme values by transforming skewed features."
"Clearly, we have a variety of positive and negative skewing features. Now I will transform the features with skew > 0.5 to follow more closely the normal distribution.\n\n- **Note**: I am using the Box-Cox transformation to transform non-normal variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn't normal, applying a Box-Cox means that you are able to run a broader number of tests."
"### 6.2.3 | After Imputation 🔨\n\n    👉 After imputation for ""Outlet_Size"" and ""Item_Weight"" columns, a final check will be done to check if there is still missing values left in the dataset.\n"
"## 6.3 | Handling Outliers 🔧\n\n    👉 As mentioned in previous section, ""Item_Visibility"" and ""Item_Outlet_Sales"" have outliers.\n    👉 In this section, those columns will be transformed using log or square root transformation. After that, those technique will be compared first before applied into real dataset.\n"
## 7.1 | Heatmap 🔥
## 7.2 | Item Fat Content based on Item Type 🥧
## 7.3 | Outlet Location Type based on Outlet Type 📉
## 7.4 | Outlet Identifier based on Outlet Size 💹
## 7.5 | Outlet Identifier based on Outlet Type 📦
# 8. | Hypothesis Tests 🧪\n\n    👉 This section will perform hypothesis testing on pre-processed data set.\n\n\n    
"## 8.1 | Hypothesis 1 1️⃣\n\n    \n      \n          H0: There is no heteroscedasticity between ""Item_MRP"" and ""Item_Weight"".\n          H1: There is heteroscedasticity between ""Item_MRP"" and ""Item_Weight"". \n      \n    \n"
\n    \n        ▶ Conclusion: H0 accepted\n    \n
"## 8.2 | Hypothesis 2 2️⃣\n\n    \n      \n          H0: Tier 3 does not have the most numbers in ""High"" size outlets.\n          H1: Tier 3 has the most numbers in ""High"" size outlets. \n      \n    \n"
## 8.3 | Hypothesis 3 3️⃣\n\n    \n      \n          H0: There was a decrease in the number of outlets from 1985 to 1998.\n          H1: There was an increase in the number of outlets from 1985 to 1998. \n      \n    \n
## 5.14.1 Income sources of Applicant's in terms of loan is repayed or not in %
## 5.14.2 Family Status of Applicant's in terms of loan is repayed or not in %
## 5.14.3 Occupation of Applicant's in terms of loan is repayed or not in %
## 5.14.4 Education of Applicant's in terms of loan is repayed or not in %
## 5.14.5 For which types of house higher applicant's applied for loan in terms of loan is repayed or not in %
## 5.14.6 Types of Organizations in terms of loan is repayed or not in %
## 5.14.7 Distribution of Name of type of the Suite in terms of loan is repayed or not in %
# 5.15 Exploartion of previous application data
## 5.15.2 On which day highest number of clients applied in prevoies application
"* What a coincedence, Approximately 15 % clients applied in each 5 days a week i.e, Tuesday, Wednesday, Monday, Friday and Thrusday. "
## 5.15.3 Purpose of cash loan in previous application
* **Main purpose of the cash loan was  :**\n  * XAP - 55 %\n  * XNA - 41 %
## 5.15.7 Who accompanied client when applying for the previous application
"* **Who accompanied client when applying for the previous application :**\n  * Unaccompanied : Approx. 60 % times\n  * Family : Approx. 25 % times\n  * Spouse, Partner : Approx. 8 %\n  * Childrens : Approx. 4 %"
## 5.15.9 What kind of goods did the client apply for in the previous application
"## 5.15.10 Was the previous application for CASH, POS, CAR, …"
## 5.15.12 Top channels  through which they acquired the client on the previous application
* **Top channels  through which they acquired the client on the previous application :**\n  * Credidit and cash offices : 43 % times\n  * Country_wide : 30 % times\n  * Stone : 13 % times
## 5.15.13 Top industry of the seller
## 5.15.14 Grouped interest rate into small medium and high of the previous application
## 5.15.15 Top Detailed product combination of the previous application
# 6. Pearson Correlation of features
# 7. Feature Importance using Random forest
"## **Age**\n\n\n### **Original distribution**\n\n\n- We can visualise the distribution of the `Age` variable, by plotting a histogram and the Q-Q plot."
"- The variable `Age` is almost normally distributed, except for some observations on the lower value tail of the distribution. Note the slight skew to the left in the histogram, and the deviation from the straight line towards the lower values in the Q-Q- plot. \n\n- In the following cells, I will apply the above mentioned transformations and compare the distributions of the transformed `Age` variable."
"- We can observe the buckets into which each Age observation was placed. For example, age 27 was placed into the 20-40 bucket."
- We can see that there are different passengers in each age bucket label.
"### **Outliers in continuous variables**\n\n- We can see that `Age` and `Fare` are continuous variables. So, first I will cap the outliers in those variables."
- Both Age and Fare contain outliers. Let's find which valuers are the outliers.
"Age is quite Gaussian and Fare is skewed, so I will use the Gaussian assumption for Age, and the interquantile range for Fare."
"The confusion matrix shows `5999 + 1897 = 7896 correct predictions` and `1408 + 465 = 1873 incorrect predictions`.\n\n\nIn this case, we have\n\n\n- `True Positives` (Actual Positive:1 and Predict Positive:1) - 5999\n\n\n- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 1897\n\n\n- `False Positives` (Actual Negative:0 but Predict Positive:1) - 1408 `(Type I error)`\n\n\n- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 465 `(Type II error)`"
# **16. Classification metrices** \n\n[Table of Contents](#0.1)
"# **18. ROC - AUC** \n\n[Table of Contents](#0.1)\n\n\n\n### ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of `TP to (TP + FN)`.\n\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of `FP to (FP + TN)`.\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n"
"For training a sentimental analysis model, we must need the label to apply in a supervisioned Machine Learning approach. The dataset we doesn't have a clearly label saying wich comment is positive or negative. For doing that, probably the best approach is to look at individual comments and label it handly with 1 (positive comment) and 0 (negative comment) but, thinking in a fast implementation, we will use the `review_score` column to label our data into those two classes. Let's take a look."
"In this approach, let's consider that every comment with scores 1, 2 and 3 are negative comments. In the other hand, comments with score 4 and 5 will be considered as positive. Again, probably this is not the best way to train a sentimental analysis model, but for fastness, we will do this assumption and see if we can extract value from it."
___\n* _What's the main n-grams presentes in corpus on positive and negative classes?_\n___
"By the end, let's plot a WordCloud for positive and negative words on our dataset."
\n7. Conclusion\n\nGo to TOC
# Wordcloud of Top N words in each topic
# Sentence Coloring of N Sentences
"## **Feature Importance**\n\n- Decision Trees models which are based on ensembles (eg. Extra Trees and Random Forest) can be used to rank the importance of the different features. Knowing which features our model is giving most importance can be of vital importance to understand how our model is making it’s predictions (therefore making it more explainable). At the same time, we can get rid of the features which do not bring any benefit to our model."
"- Now we know which features are most important in the Random Forest model, we can train our model just using these features. \n\n- I have implemented this in the kernel - [Random Forest Classifier + Feature Importance : Section 15 - Build the Random Forest model on selected features](https://www.kaggle.com/prashant111/random-forest-classifier-feature-importance). It resulted in improved accuracy."
I used f_classif method to determine best features
"In the graph above, we can see 4 or 5 features gives higher train accuracies. "
**StratifiedKFold:** Provides train/test indices to split data in train/test sets.\n\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n\nFollowing barchart shows Cross Validation Scores for SVC with 10 fold. And average mean score is 0.957\n
 \n#### BUILDING MODEL
### Correlation Matrix :
- It is a huge matrix with too many features. We will check the correlation only with respect to **DEATH_EVENT**. 
"- Features like **high_blood_pressure**, **anaemia**, **creatinine_phosphokinase**, **diabetes**, **sex**, **smoking**, and **platelets** do not display any kind of correlation with **DEATH_EVENT**."
"- According to the above tests, none of the features should be selected for modeling."
"- According to the test, **platelets** and **creatinine_phosphokinase** need to be left out for modeling. \n\n\n- We will create 2 models : 1) Based on the statistical test 2) Based on the domain information. \n\n\n- According to the statistical tests, we will drop the following features : **anaemia, diabetes, high_blood_pressure, sex, smoking, creatinine_phosphokinase, platelets.**\n\n\n- According to the Domain Information, we will drop the following features : **sex, platelets.**"
Now that we have our function let's visualize
"## Overlaying masks on the slides\nNow That we have learned how to visualize masks and display them side by side , Let's Overlay them on one another\n\nAs the masks have the same dimension as the slides, we can overlay the masks on the tissue to directly see which areas are cancerous. This overlay can help you identifying the different growth patterns. To do this, we load both the mask and the biopsy and merge them using PIL."
We will look at only 6 examples for better visuals
\n\n#### 5.8 |Lmplot
\n\n#### 5.9 |Hexagonal binning plot
"**A hexagonal bin plot is created by covering the data range with a regular array of hexagons and coloring each hexagon according to the number of observations it covers. As with all bin plots, the hex-binned plots are good for visualizing large data sets for which a scatter plot would suffer from overplotting.**\n\n**Reference:** https://blogs.sas.com/content/iml/2014/09/02/hexagonal-bin-plot.html#:~:text=A%20hexagonal%20bin%20plot%20is,plot%20would%20suffer%20from%20overplotting."
\n\n#### 5.10 |2D Histogram
\n\n#### 5.11 |Pandas Crosstab
\n\n#### 5.12 |Lineplot
\n\n## 6|DATASET PREPROCESSING
\n\n#### 7.6 |Importance Levels of the Variables
\n\n#### 7.7 |Classification Report
\n\n#### 7.8 |ROC AUC - Random Forests
\n\n#### 7.9 |Confusion Matrix Visualization
\n\n#### 7.12 |ROC AUC - Logistic Regression
![](http://t3.ftcdn.net/jpg/02/92/36/76/360_F_292367633_TwzHFo2XLSSbcihgxESm2sKcQ0NlwrIG.jpg)
### 1. Color Palettes
### 2.Look at how these colors look on a plot
### 3. Change the color palette
### 4. Impact on the plot
### 5. seaborn palettes
### 6. matplotlib colormaps as color palettes
### 7. Let's set the palette to a matplotlib colormap
### 8. Impact on the plot
### 9. Building custom color palettes
### 10. Let's see how the plot has changed
"# 22.Controlling plot aesthetics \n---\n[**Go To TOP**](#00)\n\n![](https://tgmstat.files.wordpress.com/2013/11/tips1.png)\n\n### In this section, you can learn\n\n1. First plot with seaborn\n2. Changing the plot style with set_style\n	1. Set plot background to a white grid\n	1. Set the plot background to dark\n	1. Set the background to white\n	1. Adding 'ticks\n3. Customizing the styles\n	1. Style parameters\n4. Plotting Context Presets\n	1. Plotting Context Preset - paper\n	1. Plotting Preset - talk\n	1. Plotting Preset - poster"
### 2. Changing the plot style with set_style\n#### 1. Set plot background to a white grid
#### 2. Set the plot background to dark
#### 3.Set the background to white
#### 4.Adding 'ticks
### 3.Customizing the styles\n#### 1.Style parameters
### 4.Plotting Context Presets\n#### 1.Plotting Context Preset - paper
#### 2.Plotting Preset - talk
#### 3.Plotting Preset - poster
"# 23.Plotting categorical data \n---\n[**Go To TOP**](#00)\n\n![](https://i.stack.imgur.com/IsxzL.png)\n\n### In this section, you can learn\n\n1. Scatterplots\n2. Swarmplot\n3. Boxplot\n4. Violinplot\n5. Barplot\n6. Countplot\n7. Wide form plots"
#### 2.Swarmplot
#### 3.Boxplot
#### 4.Violinplot
#### 5.Barplot
#### 6.Count Plot
#### 7.Wide form plot
### 3.Plot it with PairGrid()
### 4.Plot it with PairGrid()
### Thanks for Reading this notebook...🙏...🙏...🙏!!!
### Pick one patient for FVC vs Weeks
## Age Distribution of Unique Patients
### Distribution of Age vs Gender In Patient Dataframe
## Gender Distribution
## Heatmap for train.csv
Please compare with the previous visualization information. And we may compare to Pandas Profiling below.
"Heatmap of Correlation between different features:\n\n>Positive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.\n>\n>Negative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the *Survived* feature."
"## Feature Extraction\n\nIn this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form."
"In the example code below, we plot a confusion matrix for the prediction of ***Random Forest Classifier*** on our training dataset. This shows how many entries are correctly and incorrectly predicted by our classifer."
## Comparing Models\n\nLet's compare the accuracy score of all the classifier models used above.
# Correlations Between Features
# Modelling Based on Tabular Meta Features
"# Meta Feature Importances\n\nImage size seems pretty important on our model, but don't forget this can be misleading for final scoring. Don't forget about missing image sizes in test set and size correlation with targets in train data!"
# First Step: Creating Meta Submission
"* One of course would see how the more 'dense' the box is, the more difficult it would be for a rusher to gain considerable yards\n* And now you know why the NFL tracks this stat:\n  * **8+ Defenders in the Box (8+D%):**    'On every play, Next Gen Stats calculates how many defenders are stacked in the box at snap. Using that logic, DIB% calculates how often does a rusher see 8 or more defenders in the box against them.'\n  * And thus we can insert a fairness factor, where rushers should be judged by how often they had a larger defensive 'density' employed against them..."
"* When there are nine defensive players in the box, 25% of the runs gained LESS than 0 yards, and half the runs were for LESS than 2 yards. \n* It is very rare for defenses to line up with four or less players, but when they do, the Offense seems to gain a fair amount of yards."
* Yards Gained vs Defensive Personnel 'Groupings':\n* Plotting the distribution of yards gained vs Defensive Formation...\n* First lets start by looking at the combined 2017/2018 dataset formations by play count (i.e. how many times in the two year season data that particular Defensive Schema were run)\n* We will then look exclusively look at 2018 stats
"Let's examine the same values, but broken out by percentage, i.e. what percentage of the time did the run play go against a particular DefensePersonnel Schema, and lets grab the top 10, since after that there is an extremely small percentage of plays incorporating that style: "
"**The Top Five Formations:**\n\n1. 4 DL, 2 LB, 5 DB\n  * 4 linemen, 2 linebackers, and 5 defensive backs (6 in the 'box')\n2. 4 DL, 3 LB, 4 DB	\n  * your conventional *4:3* type defense (7 in the 'box')\n3. 3 DL, 4 LB, 4 DB\n  * your conventional *3:4* type defense  (7 in the 'box')\n4. 2 DL, 4 LB, 5 DB	\n  * four linebackers, with only 2 guys on the line  (6 in the 'box')\n5. 3 DL, 3 LB, 5 DB\n  * a type of formation build to stop the pass (6 in the 'box')\n "
"**Very very interesting...**  \n* This analysis is important to understanding how defense is set up, its critical to understanding how to predict run productivity\n* The `#1` used defensive scheme actually resulted in slightly longer yardage plays, but also slightly higher yards lost for the offense\n* You can see how the first and second scheme is relatively good at containing the run, and as you get lower on the y-axis, you are giving up higher and higher yards\n* The most common is a 4-2-5, which is a good coverage against the pass (you have 5 DB). And since roughly 35-40% of NFL plays are runs, and the other percentage are pass, you can see why this is common. \n* With a median yardage gain allowed of 4 yards, its pretty good against the run, AND you can see that in some cases you can get losses of up to -6 yards. 75% of the runs against this defense are held to 6 yards or less. \n* The next most popular is your typical 4-3 defense, where you can see it holds runs to a bit shorter yardage, obviously since you have an extra linebacker involved in the tackling. But what i find interesting is that the third most common defense (3-4) has almost PRECISELY characteristics based on the data, look at the boxplots.  The 3-4 is run less than the 4-3 and the 4-2, but it seems to hold up pretty well against the run. The 3-4 in a flexible defense, and provides some great advantages when it comes to rushing the quarterback and defending against the pass. The 3-4 can be confusing for opposing quarterbacks, who find that wherever they look downfield there is a linebacker.  IF one could argue that the 3-4 is a better defense than the 4-3 in terms of rushing the QB, AND it holds up relatively well against the run (as it appears it does), then it would appear more teams SHOULD be running the 3-4 !  \n* The `#2` and `#3` most occurring run defense resulted in almost precisely the same running yards allowed distribution\n* Having 6 men on the line may appear to be a great idea against the run (and it does seems to 'squash' the run), you see that although it lowers the potential yards a runner could get, it offers no real ability to gain you negative yards on run plays, and its penetration ability are limited.  It does work well against runs, BUT if the play is a pass, you are devasted as you have very few DB to stop the pass. \n* When there are nine defensive players in the box, 25% of the runs gained LESS than 0 yards, and half the runs were for LESS than 2 yards. \n* It is very rare for defenses to line up with four or less players, but when they do, the Offense seems to gain a fair amount of yards."
"Number of run plays called per NFL game per team:\n* Histogram plot of the total number of run plays called per game per season.  \n* This takes into consideration every game played, where each team takes turns calling run plays (and contains both 2017 and 2018 data)\n"
"* The median number of times there is a run play per team in a game is 22, i.e. if a single running back was used, he would be running roughly 22 plays per game, but there is a fairly wide variation here from 10 up to about 40 plays in a game.  30 is considered a fair number of plays for a running back, beyond 40 is considered *extreme* for a single player...\n* Distribution appears to be bi-modal, where there is a peak at 20 and a peak at about 28 carries.  One could argue this could even be the difference between teams that run the ball a fair amount (as part of their offensive strategy), and those that choose to prefer the pass with a balance of some running plays to keep the defense off guard...\n* This does bring up the fact that to play in the NFL, as a premier running back you will be getting the ball many times, and **durability** becomes a major factor as the season goes on ! \n* Also, remember that the RB does not run every run play, sometimes there are substitutes made"
"Diving Deeper:  \n\nThe below plot is an exact count visualization of the number of run plays that occurred in a game, specifically in the entire 2018 season \n* By using the swarmplot, we see the precise distribution - and this gives a better representation of the distribution of values (where 1:1 viz, i.e. one dot is one game that had a specific count of run plays)\n* We also can **quickly** see the second, third, and fourth most run play count in a random game"
"* What we find interesting is the peaks are not **that** pronouced though, i.e. there are many teams that will run the ball 17, 18, 19, 20, up to 22 times in a single game, and also a fair amount of teams that will run the ball 24, 25, 26, up to 27 times in a single game...\n* It should be noted that this is an intriguing factor:\n  * In a single game, there are not a tremendous number of run plays either way, meaning our sample size per team per game of run plays is somewhat limited, so deriving a predictive model will contain many factors with a number of samples that is relatively small, offering a challenge..."
"Total Rushing Yards per NFL Team:\n* The following shows the total run yards per team, over the course of two individual seasons.  \n* I specifically use the total over two years to show the effect the running game can have on a team's performance.  I will eventually plot the yards on a per game average basiss, but but the point here is to show the vast amount of offensive yards that the top teams had over the others.  \n* The New England Patriots won the 2018 season superbowl (against the LA Rams).  I believe the running offense was a major factor in that. \n* **Note:** I include a new plotting term called **`icicles`** to enhance the visualization of barplots.  Using `icicles`, one can not clutter the plot excessively but still relay x-axis values superimposed onto the chart.  Thus it is not necessary to cover the entire plot with a grid, but rather only the section that specifically needs it and where it is pertinent.  \n  * *This term does not currently exist in mainstream visualization, I'm creating it.*"
"Correlation between PlayerWeight and JerseyNumber:\n*  If you run a correlation between generalized player weight and jersey number and position, you see high correlation, but why ? \n*  We accidentally have a good feature to use, which is on the surface jersey number shouldn't matter in any of this, **but** if we change JerseyNumber into a categorical bin (1-19), (20-29), we see that it can be quite helpful.  Because only certain positions are actually allowed to wear certain ranges of jersey numbers.  Thus during our modelling we will in fact include jersey number into bins.  \n"
Offense Formation Strategies:\n* We will now examine which offense formation strategies appeared to result in the best yardage gained
"* I would say there was a relatively large difference in the yards gained based on offensive scheme\n* **Wildcat** - doesn't appear to be that effective.  But it should be noted that it is not run very often in the NFL.  But when it is, it performs pretty poorly.\n* **Shotgun** - performs surprisingly low compared to the other offensive schemes.  One could argue it is a kinda pass play, but more offense* **Empty** - is the clear winner. 'Empty' simply means there is no back in the backfield.  All five eligible receivers are assembled at the line of scrimmage in some fashion.\n* The **I-formation** is one of the more tried and true offensive formations seen in football, and you will likely see it used in short-yardage running siturations.  The I-formation places the runningback 6 - 8 yards behind the line of scrimmage with the quarterback under the center and a fullback splitting them in a three-point stance;  which also means that it is highly likely the defense can see where the runningback is going, but then again, he will probably have a fair amount of speed by the time he hits the line of scrimmage.  "
Weight Distribution:\n* Let's dive into examining player weight information \n
"**Read very carefully:**\n* I specifically am *not* looking at the unique player distribution here and plotting their weights.  That is not what I am doing here.  I am gathering up the total number of running plays of all of the combined teams over the entire 2018 seasons, and I am creating a distribution of the weight of the runner who made the play (in orange), and also during those SAME plays, gathering up the weight distribution of those who did NOT run the ball.  I believe this thus gives us a very good idea of the weights of the players that were **on** the field during the season (broken out by rushing player versus non-rushing player), and starts to paint a picture of being able to predict the expected yards gained during running plays. \n  * I care about **who** is on the playing field here, that is the key for future prediction models. \n  * As long as the weights of the players are updated throughout the season, this also is an extremely granular way of determining kinetic energy on the field as well.\n  * I guess my real point is this - if they aren't on the field, or aren't on the field much, do I really care what their weight is when i figure out my model ? \n* Thus, of those that ran the ball in the 2018 NFL season, they had an average weight of **217 lbs**, and a median weight of 220 lbs. \n* Non-Runners had a pretty wide distribution, obviously depends on position they played...\n  * There is a pronounced peak at 310lbs, which is our linemen..."
"* I like this view (in order of descending median weight, by position).  You immediately see that all of the linemen are just over 300lbs. And they make up a LARGE distribution of the players on the field, i.e. there are some BIG BOYS on that field.  \n* I find it suprising that FB (fullbacks) are as heavy as they are.  I would imagine one could argue that two things are pretty critical to determining the performance of a running back: \n  * How big are the offensive linemen ???  (ideally we knew how strong they were as well, but no information contained about that)\n  * How big is the fullback ?  A fullback with some size would really help blocking for the running back and I believe would be directly proportional to the success of the runningback.  \n  * Look at how large the OTs (offensive tackles) are.  One would imagine a run off the OT being a smart play, IF the defensive linebacker at that area was smaller as well..."
"Collisions:\n* Collisions between dynamic bodies can either be elastic or inelastic\n* We will defined an `angle of attack`, similar to airfoil design.  This angle of attack will be the angle at which contact is made from the defender onto the offensive runningback.  We will then able able to also break out momentum and force into components with one common reference frame\n  * Assumption is that runner is running from left to right m\n* Every runningback should PREFER inelastic collisions !  \n  * Why ? **Because it creates the separation that is their advantage**\n  * How ? Ideally with an alpha less than 45 degrees - this way they bounce off and keep going somewhat in the x-axis direction, but ideally it is not an inelastic collision with alpha of 0 or near it, that is going to completely stop runningback momentum\n  * Effectively a runningback wants to maneuver, and when maeuvering is no longer much of an option, to 'bounce' off the tackler\n"
"* There is something here but I can't put my finger on it.  Inherent Cauchy distribution ?  Some version of Lorentz distribution ?  I think drinking might help here. \n* I think if we were able to dive deeper we would see this is something like a fire distribution\n   * I'm making the term up, but something to the effect of I make a fire and it has an intensity centerfied, but rising embers, or else there is nothing really here and its actually two sep distinct distributions superimposed\n   * But it seems to mimic this [click](https://www.shutterstock.com/video/clip-4927760-large-fire-burning-night-smoke-sparks-rising)"
Kinetic Energy:\n* An interesting thing occurs when we look at Kinetic Energy
Machine Learning Model
"*I'm going to have to spin up a second jupyter notebook to cover my machine learning model here, I still plan on adding a signficant amount of stuff above for EDA-related visuals...*  \n\n*I will include the link to the second notebook, but as it stands I still want to do more EDA here, so this notebook will be dedicated to EDA exclusively, but you see the Feature Engineering perspective as well here*"
## 4.13 Mean Value of some features over time
## 4.14 Mean Value of resp features over time
## 4.15 Rolling mean of Mean Value of resp features over time
## 4.16 Value of resp over 10 random days
## 4.17 White Noise Time Series Analysis\n\n**What is white noise ? **\nwhite noise in time series is random number which can not be predicted.\nWe can make model for the data if data is not white noise.\n\nEvery signal consist of some form of white noise.\ny(t) = signal + noise.\n\nWhite noise are series of variable which are independent and they have same variance.\nthat means the value in present has no relation with value of past or future.\n\nIf your signal is just white noise it means you can not model it they are just random numbers.\n\nWhen a model makes prediction all the errors made by models are just white noise which means.\nmodel has used all the signal available to make the prediction and all that left is white noise.\n\nTime series is not white noise if following conditions are satisfied.\n1. Mean is not-zero or near zero.\n2. Mean change over time\n3. Variance change over time\n4. Values correlate with lag values\n
### 4.17.1 Correlation with lag values.\n\nTo check that the values are correlated with lag values we will use pandas autocorrelation function.
Acctually looking at it. I looks like white noise.\nstraight grey line is 95% confidence bar.\ndashed grey line is 99% confidence bar.\n\nThere are few points which are crossing that line.\nSo is the data just white noise or did I missed something.
Let's take a look at the augmentations:
For validation we have only used the image resizing.
## Building the training loop 
## Searching for an optimal cyclical learning rate \n\nThe learning rate is one of the most important hyperparameters for tuning neural networks. A rate that is too high will lead to jumps to higher values in the training loss during optimization. If it's too small the learning process is too slow and will probably stop too early in the case we have defined a minimum required loss change. Take a look at the paper [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186).
"To fix this, we should pass every prediction through the *sigmoid function*, which has a minimum at zero and maximum at one, and is defined as follows:"
"PyTorch already defines that function for us, so we can modify `calc_preds` to use it:"
> ### Some plots
"The sum, facetted for each energy aspect, shows some aberrant values."
"Looking at the max value for each day, and for each energy aspect, shows that only a single building (for day and energy aspect) is causing the aberrant peaks"
The max values of electricity are caused by only 6 buildings.
> ### Chilledwater
The max values of electricity are caused by only 10 buildings.
> ### Steam
The max values of electricity are caused by only 4 buildings.
> ### Hotwate
"The max values of electricity are caused by only 7 buildings. Practically, two of them"
"## 6.3 Examine Missing Values\n\n\n\n\n    Taking only the buildings that consume more than the others, could be seen that there are a lot of measure scale errors.\n    The error could be:\n\n    The meter is not configured correctly. E.g., a bad voltage or current primary to secondary ratio.\n    The software has not the units configured correctly. E.g., MJ/kg for steam.\n    The software has not the decimal digits configured correctly.\n    Using a power variable instead of an energy one.\n    The measure could be done with an unique meter, or the sum of several of them.\n\n    Some changes over time, values go to zero or the scale is changed, indicates that some buildings have more than one meter. One error in one meter and the overall measure is garbage.\n\n    This notebook has only analised the outliers that influence the maximum consumption in a daily basis. This is only the tip of the iceberg. A sound analysis should be done to detect and correct these outliers.\n\n    A solution to avoid scale errors is to normalize the values from 0 to 1, for each building and for each energy aspect.\n\n\n"
"Below, I am displaying the count and percent correct by part. As you can see, Part 5 has a lot more question_id's and is also the most difficult."
"# 1.3 Exploring Lectures\n\nMetadata for the lectures watched by users as they progress in their education.\n* lecture_id: foreign key for the train/test content_id column, when the content type is lecture (1).\n* part: top level category code for the lecture.\n* tag: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n* type_of: brief description of the core purpose of the lecture\n"
Let's have a look at the type_of.
"Since there are not that many lectures, I want to check if it helps if a user watches lectures at all. As you can see, it helps indeed!"
"Batches (task_container_id) may also contain lectures, and I want to find out if there are any batches with high numbers of lectures."
"Is there a correlation between the percent_lecture and the percent_correct? No, I don't really see it. If anything, the percent_correct actually seems to go down slightly."
"The last thing that I want to check is if having a lecture in a batch helps. As you can see, it does not. Batches without lectures have about 8% more correct answers than batches with lectures."
"One particularly nice feature of random forests is they can tell us which independent variables were the most important in the model, using `feature_importances_`:"
"We can see that `Sex` is by far the most important predictor, with `Pclass` a distant second, and `LogFare` and `Age` behind that. In datasets with many columns, I generally recommend creating a feature importance plot as soon as possible, in order to find which columns are worth studying more closely. (Note also that we didn't really need to take the `log()` of `Fare`, since random forests only care about order, and `log()` doesn't change the order -- we only did it to make our graphs earlier easier to read.)\n\nFor details about deriving and understanding feature importances, and the many other important diagnostic tools provided by random forests, take a look at [chapter 8](https://github.com/fastai/fastbook/blob/master/08_collab.ipynb) of [our book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)."
We shall check if the provided images have the same dimension or not. I'll use Dask to parallelize the operation and speed things up.
### K-means Clustering
## Binary Cross Entropy Loss Curve
"## Notes about Frequency Domain modeling\n\n* As expected the CNN model could converge faster 3x than the ViT model as the available dataset size isn't the ideal for such architecture(we's talking about 600 instances here) which is based on vision transformers which in turn needs more data than CNN to work properly. \n\n* The model loss curve is descending but it doesn't mean that the model is learning well, that's why further INVESTIGATION and IMPROVEMENTS shall be done on this work to find out if the imbalanced classes is the main issue here or if we need to try feature extraction techniques(noise cancelation filters, etc.) other than using the SFT's available. One of the possible improvements here is to use pre-trained model and only fine tune it, this shall overcome data limitation  and other issues."
# Months to Campaign
The most part of projects have 1 month of campaign. We can see that the ratio of successful one month campaigns is better than projects with 1.5 or 2 months of campaign
## Launched Year Distributions
Cool. We can note that 
## Launched Months Distributions
We can note that all months are very similar. 
# Goals 
"Humm... Its an very interesting information.\nOn the first chart, we can clearly see that projects with more than 30 to 60 days have highest vales pledged, what make many sense. \nAlso, we can see that the median of goal reached is like to 120;  Let's s"
# Models Pipeline
"Cool Gradient Boosting, XGB have the best results so I will select them. \nAlso, I will seelect the Logistic Regression too. \n\nThe Decision tree and Extra trees are the models with the lowest roc_auc scores."
### 8) Age vs. Survival
"NOTE:1) From *`Pclass`* violinplot, we can see that:\n- 1st Pclass has very few children as compared to other two classes.\n- 1st Plcass has more old people as compared to other two classes.\n- Almost all children (between age 0 to 10) of 2nd Pclass survived.\n- Most children of 3rd Pclass survived.\n- Younger people of 1st Pclass survived as compared to its older people.\n\n2) From *`Sex`* violinplot, we can see that:\n- Most male children (between age 0 to 14) survived.\n- Females with age between 18 to 40 have better survival chance."
## Correlating Features
"NOTE:Heatmap of Correlation between different features:\n\n>Positive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.\n>Negative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the *Survived* feature."
"> In the example code below, we plot a confusion matrix for the prediction of ***`Random Forest Classifier`*** on our training dataset. This shows how many entries are correctly and incorrectly predicted by our classifer."
## Comparing Models\n> Let's compare the accuracy score of all the classifier models used above.
Let's compare the PPS matrix to the basic correlation matrix
"These were some of the interesting and  useful python libraries for data science, that I have come across recently. In case you know about others which can be added to the list, do mention them in the comments below. "
"\nHere we are seeing how many persons answered None to cloud platforms (meaning that they don't use a cloud platform on a regular basis). And it is decreasing over time! So... Cloud adoption is increasing amongst professionals with Modern Data Scientists being the ones that use cloud services the most. This is very good news, meaning that everyone is having more access to the best Data Science tools and they are also getting closer to productionizing Data Science!\n\nNow there is one thing I think it's curious... I would expect ML Veterans to have a lot of experience with cloud, but they don't. Are they too cool for using the cloud?\n\nHey Kaggle! This a good question for next years survey: How many years of experience with cloud platforms?\n\nNow how about we have a look at cloud adoption per provider?\n\n"
"\nNo big surprises in the cloud providers adoption. Google Cloud and Microsoft are increasing marketshare due to discounts and policies for both startups and large corporations. AWS is the biggest provider and usually adopted by business that were ""cloud first"" a few years ago.\n"
*Pool*\n-----------------------
*Fence*\n-----------------------
Fence has got 1179 null values.\nWe can safely assume that those houses doesn't have a Fence and replace those values with None.
*MSZoning*\n-----------
*1st Floor in square feet*\n--------------------------
*Ground Living Area w.r.t SalePrice*\n--------------------
*SalePrice per square foot*\n--------------------
*Garage Area*\n-------------
"*Building , remodelling years and age of house*\n----------------------------------------"
*Heating and AC arrangements*\n-----------------------------
Having AC definitely escalates price of house.
*Total rooms above grade*\n-------------------------
*Kitchen Quality*\n=================
Having 1 Kitchen of Excellent quality hikes house price like anything.
*Neighbourhood*\n--------------
*Overall Quality*\n-----------------
*2nd Floor with SalePrice*\n--------------------------
*Street*\n--------
More to come .. Watch this space.
So that step took a while! Let's take a look at what our data looks like when compressed into 2 dimensions. 
"This looks pretty bland. t-SNE was able to reduce the dimensionality of the texts, but now clustering is required. \nLet's use the clusters found by k-means as labels. This will help visually separate different concentrations of topics."
"The labeled plot gives better insight into how the papers are grouped. Interestingly, both k-means and t-SNE can find independent clusters even though they were run independently. This shows that structure within the literature can be observed and measured to some extent. \n\nNow there are other cases where the colored labels are spread out on the plot. This is a result of t-SNE and k-means finding different connections in the higher dimensional data. The topics of these papers often intersect so it was hard to cleanly separate them. "
## Setup
## Widgets
## 5.1. Logistic curve fit 
* **Italy**
* **Spain**
Other Related Notebooks
References
"## 1. Random Forest\nThere are many categorical features, so I have chosen random forest to do the classification."
## 2. Logistic Regression\n
The lowest fare is **0.0**. Wow!! a free luxorious ride. 
"There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces. As this is also continous, we can convert into discrete values by using binning."
## Correlation Between The Features
"### Interpreting The Heatmap\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\n**POSITIVE CORRELATION:** If an **increase in feature A leads to increase in feature B, then they are positively correlated**. A value **1 means perfect positive correlation**.\n\n**NEGATIVE CORRELATION:** If an **increase in feature A leads to decrease in feature B, then they are negatively correlated**. A value **-1 means perfect negative correlation**.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as **MultiColinearity** as both of them contains almost the same information.\n\nSo do you think we should use both of them as **one of them is redundant**. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between **SibSp and Parch i.e 0.41**. So we can carry on with all features."
"True that..the survival rate decreases as the age increases irrespective of the Pclass.\n\n## Family_Size and Alone\nAt this point, we can create a new feature called ""Family_size"" and ""Alone"" and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not."
"**Family_Size=0 means that the passeneger is alone.** Clearly, if you are alone or family_size=0,then chances for survival is very low. For family size > 4,the chances decrease too. This also looks to be an important feature for the model. Lets examine this further."
"It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family.\n\n## Fare_Range\n\nSince fare is also a continous feature, we need to convert it into ordinal value. For this we will use **pandas.qcut**.\n\nSo what **qcut** does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges."
"### Dropping UnNeeded Features\n\n**Name**--> We don't need name feature as it cannot be converted into any categorical value.\n\n**Age**--> We have the Age_band feature, so no need of this.\n\n**Ticket**--> It is any random string that cannot be categorised.\n\n**Fare**--> We have the Fare_cat feature, so unneeded\n\n**Cabin**--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n\n**Fare_Range**--> We have the fare_cat feature.\n\n**PassengerId**--> Cannot be categorised."
"Now the above correlation plot, we can see some positively related features. Some of them being **SibSp andd Family_Size** and **Parch and Family_Size** and some negative ones like **Alone and Family_Size.**"
Now the accuracy for the KNN model changes as we change the values for **n_neighbours** attribute. The default value is **5**. Lets check the accuracies over various values of n_neighbours.
### Gaussian Naive Bayes
"The classification accuracy can be sometimes misleading due to imbalance. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong.\n\n## Confusion Matrix\n\nIt gives the number of correct and incorrect classifications made by the classifier."
"### Interpreting Confusion Matrix\n\nThe left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong prredictions made. Lets consider the first plot for rbf-SVM:\n\n1)The no. of correct predictions are **491(for dead) + 247(for survived)** with the mean CV accuracy being **(491+247)/891 = 82.8%** which we did get earlier.\n\n2)**Errors**-->  Wrongly Classified 58 dead people as survived and 95 survived as dead. Thus it has made more mistakes by predicting dead as survived.\n\nBy looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived."
### Confusion Matrix for the Best Model
## Feature Importance
"We can see the important features for various classifiers like RandomForests, AdaBoost,etc.\n\n#### Observations:\n\n1)Some of the common important features are Initial,Fare_cat,Pclass,Family_Size.\n\n2)The Sex feature doesn't seem to give any importance, which is shocking as we had seen earlier that Sex combined with Pclass was giving a very good differentiating factor. Sex looks to be important only in RandomForests.\n\nHowever, we can see the feature Initial, which is at the top in many classifiers.We had already seen the positive correlation between Sex and Initial, so they both refer to the gender.\n\n3)Similarly the Pclass and Fare_cat refer to the status of the passengers and Family_Size with Alone,Parch and SibSp."
"* Explore the directionality of these effects, using SHAP values:\n    - Findings:\n        - The amount spent on fish increases with higher total amount spent ('TotalMnt')\n        - The amount spent on fish decreases with higher amounts spent on wine, meat, gold, fruit, or sweets ('MntWines', 'MntMeatProducts', 'MntGoldProds', 'MntSweetProducts', 'MntFruits')\n    - Interpretation:\n        - Customers who spend the most on fish are those who spend less on other products (wine, meat, gold, fruit, and sweets)"
### Is there a significant relationship between geographical regional and success of a campaign?\n\n* Plot success of campaigns by region:\n    - Findings:\n        - The campaign acceptance rates are low overall\n        - The campaign with the highest overall acceptance rate is the most recent campaign (column name: `Response`)\n        - The country with the highest acceptance rate in any campaign is Mexico\n    - Is the effect of region on campaign success statistically significant? See below.
"* Statistical summary of regional effects on campaign success:\n    - Methodology: Performed logistic regression for Campaign Accepted by Country, reporting Chisq p-value for overall model.\n    - Findings: The regional differences in advertising campaign success are statistically significant."
Please plot and visualize the answers to the below questions.\n\n### Which marketing campaign is most successful?\n\n* Plot marketing campaign overall acceptance rates:\n    - Findings: The most successful campaign is the most recent (column name: `Response`)
"### What does the average customer look like for this company?\n\n* Basic demographics: The average customer is...\n    - Born in 1969\n    - Became a customer in 2013\n    - Has an income of roughly \$52,000 per year\n    - Has 1 dependent (roughly equally split between kids or teens)\n    - Made a purchase from our company in the last 49 days"
"### Which products are performing best?\n\n* The average customer spent...\n    - \$25-50 on Fruits, Sweets, Fish, or Gold products\n    - Over \$160 on Meat products\n    - Over \$300 on Wines\n    - Over \$600 total\n* Products performing best:\n    - Wines\n    - Followed by meats"
"### Which channels are underperforming?\n\n* Channels: The average customer...\n    - Accepted less than 1 advertising campaign\n    - Made 2 deals purchases, 2 catalog purchases, 4 web purchases, and 5 store purchases\n    - Averaged 14 total purchases\n    - Visited the website 5 times\n* Underperforming channels:\n    - Advertising campaigns\n    - Followed by deals, and catalog"
# Conclusion
" Inference: \n\n* We can see that the target column, i.e **'treatment'** has almost equal values for both the categories. This means that we do not have to perform undersampling or oversampling.\n* Now let us make a heatmap and try to understand the correlation of various features with the target variable."
  Evaluating Models \n
### Relation to SalePrice for all categorical features
"**Conclusion from EDA on categorical columns:**\n\nFor many of the categorical there is no strong relation to the target.  \nHowever, for some fetaures it is easy to find a strong relation.  \nFrom the figures above these are : 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType'\nAlso for the categorical features, I use only those that show a strong relation to SalePrice. \nSo the other columns are dropped when creating the ML dataframes in Part 2 :  \n 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1',  'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', \n'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition' \n "
### Checking correlation to SalePrice for the new numerical columns
"There are few columns with quite large correlation to SalePrice (NbHd_num, ExtQ_num, BsQ_num, KiQ_num).  \nThese will probably be useful for optimal performance of the Regressors in part 3."
"On closer inspection, we can see that the `Rainfall`, `Evaporation`, `WindSpeed9am` and `WindSpeed3pm` columns may contain outliers.\n\n\nI will draw boxplots to visualise outliers in the above variables. "
The above boxplots confirm that there are lot of outliers in these variables.
"### Check the distribution of variables\n\n\n- Now, I will plot the histograms to check distributions to find out if they are normal or skewed. \n\n- If the variable follows normal distribution, then I will do `Extreme Value Analysis` otherwise if they are skewed, I will find IQR (Interquantile range)."
"We can see that all the four variables are skewed. So, I will use interquantile range to find outliers."
### Heat Map 
"#### Interpretation\n\n\n\nFrom the above correlation heat map, we can conclude that :-\n\n- `MinTemp` and `MaxTemp` variables are highly positively correlated (correlation coefficient = 0.74).\n\n- `MinTemp` and `Temp3pm` variables are also highly positively correlated (correlation coefficient = 0.71).\n\n- `MinTemp` and `Temp9am` variables are strongly positively correlated (correlation coefficient = 0.90).\n\n- `MaxTemp` and `Temp9am` variables are strongly positively correlated (correlation coefficient = 0.89).\n\n- `MaxTemp` and `Temp3pm` variables are also strongly positively correlated (correlation coefficient = 0.98).\n\n- `WindGustSpeed` and `WindSpeed3pm` variables are highly positively correlated (correlation coefficient = 0.69).\n\n- `Pressure9am` and `Pressure3pm` variables are strongly positively correlated (correlation coefficient = 0.96).\n\n- `Temp9am` and `Temp3pm` variables are strongly positively correlated (correlation coefficient = 0.86).\n"
"Now, I will draw pairplot to depict relationship between these variables."
"#### Interpretation\n\n\n- I have defined a variable `num_var` which consists of `MinTemp`, `MaxTemp`, `Temp9am`, `Temp3pm`, `WindGustSpeed`, `WindSpeed3pm`, `Pressure9am` and `Pressure3pm` variables.\n\n- The above pair plot shows relationship between these variables."
"The confusion matrix shows `20892 + 3285 = 24177 correct predictions` and `3087 + 1175 = 4262 incorrect predictions`.\n\n\nIn this case, we have\n\n\n- `True Positives` (Actual Positive:1 and Predict Positive:1) - 20892\n\n\n- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 3285\n\n\n- `False Positives` (Actual Negative:0 but Predict Positive:1) - 1175 `(Type I error)`\n\n\n- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 3087 `(Type II error)`"
## 16. Classification Metrices 
"## 18. ROC - AUC \n\n\n\n### ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of `TP to (TP + FN).`\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of `FP to (FP + TN).`\n\n\n\n\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n\n\n\n"
"I decided to chose these lags 16, 20, 30, 45, 365, 730 from PACF. I don't know that they will help me to improve the model but especially, 365th and 730th lags may be helpful. If you compare 2016 and 2017 years for sales, you can see that they are highly correlated."
# 10. Simple Moving Average
We'll be be showing the marginal distributions along x and y
Next let's overlay the number of passengers in the Heatmap cells and show the Year over Year growth as a Bar Chart.  \nWe'll be doing some data transformation to get the YoY growth.
3D Plots allow you to add a 3rd axis to your plots and rotate/pan/zoom on the plot with your mouse and keyboard.  \nLet's see them in action on the flights dataset.
## 3D Scatter Plot
## 3D Line Plot
## 3D Surface Plot
A scatter matrix creates multiple 2d scatter plots to explore relationships there might be between column variables.  \n$$n = \# columns \implies \# plots = n^2$$
You can highlight a certain variable with the color parameter.
This tool is very useful for quickly exploring correlation between variables
## Scatter Polar
## Line Polar
We can define an additional facet with the facet_row parameter
Last example
"### Visualizing Agglomerative Clustering results using the PCA reduced Dataset\n\nTo check out the clusters that were created by the Agglomerative Clustering algorithm, we can use the PCA 2-dimensional dataset to visualize points and it's respective segmentations."
"## 2.3 - DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nThis is another useful algorithm for clustering tasks. As it's name says, it bases itself on the density of the points distributed in the feature space to create clusters. Basically, DBSCAN categorizes data points into 3 types: **Noise** points, **Border** points and **Core** points. It's main parameters are **""n_samples""** and **""eps""**.\n\n![DBSCAN.png](attachment:DBSCAN.png)\n\nLet's think about an example where we defined ""eps"" value as 1.5, and ""n_samples"" as 4. Imagine that you could draw circles (like the ones showed in the image above) around each data point represented in the feature space. The **radius** of these circles is defined by the parameter **""eps""** (1.5, in this example). \n\nNow, to simulate how the algorithm works: first, take an arbitrary point and look at it. If there are at least **""n_samples"" (in this example, 4) points** inside it's ""1.5 eps"" radius drawn circle, this point is considered as **""Core""**, and DBSCAN assigns it to cluster 0. \n\nThen, for each point inside this Core point's drawn circle, do the same task: if there are at least ""n_samples"" points inside their own circles, they're also categorized as ""Core"" points, and also assigned to cluster 0.\n\n![dbscan_2.png](attachment:dbscan_2.png)\n\nIn a case where a data point has **less than ""n_samples""** points inside it's circle (2, for example), the algorithm checks if at least one of these points are considered ""Core"" ones. If so, this data point is classified as **""Border""**. \n\nIf the condition above is false (meaning there are less than ""n_samples"" points inside this point's circle, **and none of them are core ones**), it is classified as **""Noise""** (it can be interpreted as an outlier, that doesn't belong to any cluster). This process continues untill all points are analyzed and clusterized. \n\nThe main benefits of using DBSCAN, when compared to the other algorithms we have seen so far, are: you don't need to previously set the number of clusters to be created; it will be automatically found, depending (mainly) on the values set for ""eps"" and ""n_samples"". Also, it can correctly detect more complex cluster shapes and points that doesn't belong to any clusters (outliers).\n\nBenefits aside, the algorithm also has it's own disadvantadges. It fails to clusterize points when the variance inside each cluster is too different. Aditionally, just like Agglomerative Clustering, it doesn't allow predictions on new data."
### ***What products or platforms did Data Architect find to be most helpful when they first started studying datascience?***
"Online Platforms Which can help to study about Data architect job tittle. Through this graph we described some online platforms in which included or on top list platforms are Coursera, Edx, Youtube etc. then comes other some social media platforms."
### ***Which Programming Languages Use by Data Architect***
This graph describe the most frequantly use of language in Data_Architect field. According to graph description there are two languages Python or SQL are most commanly used in that field.
### ***Which of the following integrated development environments (IDE's) do Data_Architect use on a regular basis?***\n
"IDE stands for Integrated Development Environment. It’s a coding tool which allows you to write, test, and debug your code in an easier way, as they typically offer code completion or code insight by highlighting, resource management, debugging tools. According to this graph presentations three IDEs are used most frequantly by Data Architects. VS code, Pycharm or Motepad."
### ***Do you use any of the following hosted notebook products?***
"Notebooks have been commonplace in data science for most of the last 10 years and every data scientist has worked with one. They allow data scientists to rapidly experiment and share insights through quick environment creation, interactive output, and code snippets that can be executed in any order. According to this graph's results the kaggle notebook or colab notebook are on top position."
### ***Data Visualization Libraries Used by Data_Architect on regular basis***
These are one of the most popular data visualization libraries being used by developers across the globe and is used to manipulate documents based on data. Data architects are not excessive deal with visualization libraries but if they needed to use of any library then this graph defined some top libraries that are used by data architects. in which matplotlib/seaborn or plotly are on top level.
### ***Which of the following ML algorithms Data_Architect use on a regular basis?***
"A machine learning algorithm is the method by which the AI system conducts its task, generally predicting output values from given input data. This graph defines that algorithms that are mostly used by Data architects in which linear or logistic regression on the top."
### ***Which of the following machine learning frameworks Data Architect use on a regular basis***
A machine learning framework is an interface that allows developers to build and deploy machine learning models faster and easier. A tool like this allows enterprises to scale their machine learning efforts securely while maintaining a healthy ML lifecycle. According to this graphical representations the library of sk-learn is most comanly used by data architects.
### ***Do Data_Architects download pre-trained model weights from any of the serivces***
"A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task. Accoding to this servey most of people in the field of Data Architect are not preffer to use of any pre-trainned, and if it is necessary then they preffer tensorFlow hub, Pycharm, ONNX models or kaggle data_sets."
### ***Which of the following ML model hubs/repositories do Data_Architects use most often?***
"According to this graphical presentation kaggle data_set, Tensor-flow hub and then other google drives are most commonly used by Data architect for ml models hubs and repositories."
### ***Which categories of computer vision methods does Data_Architect use on a regular basis?***
"Computer vision is a field of artificial intelligence that trains computers to interpret and understand the visual world. Using digital images from cameras and videos and deep learning models, machines can accurately identify and classify objects. according to this graphical presentation if some computer vision metheds needed by data architects that are General purpose image/videos tools."
### ***Which of the following natural language processing (NLP) methods Data Architects use on a regular basis?***
"Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can. Word embeddings/vectors, transfomer languages models, BERT, XLnet, etc are on top priorities by Data architects."
### ***Which of the following cloud computing platforms do Data_Architects use?***
"the practice of using a network of remote servers hosted on the internet to store, manage, and process data, rather than a local server or a personal computer. Most of Data Architects used the services of AWS, microsofrt Azure or google cloud platforms according to this graphical result."
### ***Do Data_Architects use any of the following business intelligence tools?***
"Business intelligence (BI) tools are types of application software which collect and process large amounts of unstructured data from internal and external systems, including books, journals, documents, health records, images, files, email, video and other business sources. According to this graphical results most Data Architects are preffered to use of microsoft power BI And on 2nd option is Tableau."
### ***Do Data_Architect use any of the following managed machine learning products?***
Acoording to this graph most of Data Architect did not use ML products and if they are needed then they preffer to use of amazon sageMaker/Azure ML studio or Google cloud Vertex Al.
### ***Do Data_Architect use any of the following automated machine learning tools?***
Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. According to this server the results of this graph describes that most of data architects are not used ML automated tools and if they need to useing that tools they preffer amazon sagemaker autopilot.
### ***Do Data_Architect use any of the following products to serve your machine learning models?***
According to this interpretation Machine learning flow then tensorflow Exterded are likely to popular products to serve your machine learning models.
### ***Who/what are data_Architects favorite media sources that report on data science topics?***
"Favorite media sources from which anybody learn and then apply that learning into a practical way. Data Architect is also a skill and for this some social media platforms might be helpfull for this type of learning. According to this interpretation youtube, kaggle, journal publications are most commamly usable platforms for the learning of any kind of skill or specially Data Architect."
##### ***What products or platforms did Data Scientist find to be most helpful when they first started studying datascience?***
"- Most of the data sicentist are using a combination of Online courses, kaggle(comptions and notebooks) and youtube for learning data science."
##### ***What programming languages Data Scientist use on a regular basis?***
Python with SQL are the best languages for completing data analytics course. This graph shows python as top rated language.
##### ***Which of the following integrated development environments (IDE's) do Data Scientists use on a regular basis?***
- Jupyter notebooks with VS code are the most used IDE's by data scientists.
##### ***Do you use any of the following hosted notebook products?*** 
Noted Books are used for coding practices or projects. these notebooks are also available online for team work or for save or run your coding work in better way. according to this graph colab note books are on the top.
##### ***Data Visualization Libraries Used by Data Scientist on regular basis***
"- matplotlib, seaborn and plotly are the most used libraries for data visualization."
##### ***Which of the following machine learning frameworks Data Scientist use on a regular basis?***\n
"- Scikit learn, XG boost and Tensorflow are the most used frameworks for data science."
##### ***Which of the following ML algorithms Data Sicentist use on a regular basis?***
"- Linear or Logistic Regression, Decision TreesRandom Forest and Boosting algorithms are the most used algorithms for data science."
##### ***Which categories of computer vision methods does Data Scientists use on a regular basis?***
- Most of the Data Scientists are not using computer vision methods but if they are using then they are using Image classfication and general purpose image/video tools.
##### ***Which of the following natural language processing (NLP) methods Data Scientists use on a regular basis?***
- Word embeddings/vectors and Transformer Langugage models are the most used NLP methods by data scientists.
##### ***Do Data Scientists download pre-trained model weights from any of the serivces?***
- Most of the data scientists are not using pre-trained model weights but if they are using then they are using Kaggle and Hugging Face.
##### ***Which of the following ML model hubs/repositories do Data Scientist use most often?***
"- Most of the Data Scientst are using Kaggle, Hugging Face and Tensorflow hub for ML model hubs/repositories."
##### ***Which of the following cloud computing platforms do Data Scientist use?***
"- Most of the Data Scientists are using AWS, GCP and Azure for cloud computing platforms and a big number of data scientists are using none of these platforms."
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
- Most of the data Scientists have best experience with AWS and GCP.
##### ***Do Data Scientist use any of the following cloud computing products?***
"- Amazon Elastic Compute Cloud (EC2) and Google Compute Engine are the most used cloud computing products by data scientists, but big number of the data scientists are not using any of these products."
##### ***Do Data Scientists use any of the following data storage products?***
"- Most of the data scientists are using Amazon Simple Storage Service (S3) and Google Cloud Storage (GCS) for data storage products, but a big number of data scientists are not using any of these products."
"##### ***Do Data Scientist use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
"- Most of the data scientists are using MySQL and PostgreSQL for data products, but a big number of data scientists are not using any of these products."
##### ***Do Data Scientist use any of the following business intelligence tools?***
- Most of the Data Scientist are not using any business intelligence tools but if they are using then they are using Tableau and Power BI.
##### ***Do Data Scientist use any of the following managed machine learning products?***
- Most of the Data Scientist are not using any managed machine learning products but if they are using then they are using Amazon SageMaker and Google Cloud AI Platform.
##### ***Do Data Scientist use any of the following automated machine learning tools?***
- Most of the Data Scientist are not using any automated machine learning Models but if they are using then they are using Google Cloud AutoML and Azure Automated Machine Learning.
##### ***Do Data Scientist use any of the following products to serve your machine learning models?***
- Most of the Data Scientist are not using any products to serve their machine learning models but if they are using then they are MLFLow and TFX
##### ***Do Data Scientist use any tools to help monitor your machine learning models and/or experiments?***
- Most of the Data Scientist are not using any tools to help monitor their machine learning models and/or experiments but if they are using then they are using ML flow and TensorBoard
##### ***Do Data Scientists use any of the following responsible or ethical AI products in your machine learning practices?***
- Most of the Data Scientist are not using any responsible or ethical AI products in their machine learning practices but if they are using then they are using Google Responsible AI Toolkit and Microsoft AI Resources.
##### ***Do Data Scientists use any of the following types of specialized hardware when training machine learning models?***
- Most of the Data Scientist are using GPUs and TPUs for training machine learning models but a big number of data scientists are not using any of these products.
##### ***Who/what are data scientists favorite media sources that report on data science topics?***
"- Most of the data scientists favourite sources are Blogs, kaggle and Youtube for media sources that report on data science topics."
##### ***Mostly use products or platforms find to be most helpful when they first started studying***
"#### ***Discription :***\n if you want to presue your in field of data science as Manager. As you see in the graph, mostly people starting learning from online courses, video plateforms and kaggle."
##### ***Most use programming languages***
"#### ***Discription :***\nAs you see in the graph, Every Manager mainly use python for data analysis other langauges are you but in common.  "
##### ***Mostly use integrated development environments (IDE's)***
#### ***Discription :***\nIDE are very nessory component for progrmming where we write and debug our program. Graph show that mostly manager use jupyter notebook and visual studio code(VSCode).
##### ***Mostly use hosted notebooks*** 
"#### ***Discription :***\nNotebook are use for online code writing and debugging. As you see in graph, Colab and Kaggle Notebook are commonly in manager. "
##### ***Mostly use Data Visualization Libraries***
"#### ***Discription :***\nAs you see in the graph, Mostly data visulization libraries use by manager are Matplotlib and seaborn"
##### ***Mostly machine learning frameworks***\n
#### ***Discription :***\nMachine Learning Libraries or frameworks presents in above graph that shows Scikit-Learn and TensorFlow are widely use in manager 
##### ***Mostly use ML Algorithms***
"#### ***Discription :***\nAs you see in the graph,Mostly use Machine Learning algorthim is Linear or Logistic Regression by Managers "
##### ***Mostly use categories of computer vision methods***
"#### ***Discription :***\nAs you see in the graph,In Computer Vision Image classification and other general purpose networks are widely use. "
##### ***Mostly use natural language processing (NLP) methods***
"#### ***Discription :***\nIn Natural Language processing methods, Transformer language models are most populer in the managers"
##### ***Most Download pre-trained model weights from any of the serivces?***
"#### ***Discription :***\nAs you see in the graph, Kaggle datasets in group of pre-trained models weight is the most populer in the managers"
##### ***Mostly use ML model hubs/repositories***
"#### ***Discription :***\nAs you see in the graph, Kaggle datasets in group of hub/Repositories is the most populer in the managers because moslty data managers use kaggle which are easy tu use and save and compile the data very effciently"
##### ***Mostly use cloud computing platforms***
"#### ***Discription :***\nCloud plateform is very common in data related field because of high performence computing. So, graph showes that more the 50% of Manager use amazon Web services(AWS) for high performence computing and large data storge."
"##### ***The cloud platforms are familiar with, which has the best developer experience (mostenjoyable to use)***"
#### ***Discription :***\nwhen we see the previous graph mostly managers use AWS for our work and now we see this graph also shows AWS have provide friendly and enjoyable enviroment for their customers. 
##### ***Mostly use the following cloud computing products***
"#### ***Discription :***\nIn previous Two Graphs AWS is familier with manager, So, this graph shows that Amzaon Elastic Computing Cloud(EC2) is also very known in Manager because managers run high computer task in AWS"
##### ***Mostly use the following data storage products***
"#### ***Discription :***\nData Storage products is very common in Manager. So, Amazon Simple Storage Servies is widely use in Data storage."
"##### ***Mostly use the following data products (relational databases, data warehouses, data lakes,or similar)***"
#### ***Discription :***\nData Storage products work with data products. mean how you want to storge the data. My SQL is vwidely use in Managers. 
##### ***Mostly Use Following business intelligence tools***
#### ***Discription :***\nMS Power BI is very populer in Managers Community.
##### ***Following managed machine learning products***
"#### ***Discription :***\nGraph shows that, In Managed Machine Learning products Amazon SageMarker is very common in managers."
##### ***Following automated machine learning tools***
#### ***Discription :***\nGoogle Cloud Auto ML is widely use in Manager.
##### ***Following products to serve your machine learning models***
#### ***Discription :***\nML Flow is the product that serve Machine Learning models for the managers
##### ***Tools to help monitor your machine learning models and/or experiments***
#### ***Discription :***\nTensorboard is most populer tools to monitor your machine learning models. 
##### ***following are the responsible or ethical AI products in your machine learning practices***
##### ***following types of specialized hardware when training machine learning models***
#### ***Discription :***\nGPU is the specilized hardware which are mostly use in managers 
##### ***favorite media sources that report on data science topics***
"#### ***Discription :***\nAs you see in the graph, kaggle and youtube are use very famous media plateform in the managers."
"#### ***Discription :***\nOnline Courses plateform and Video Plateform are very common in newbie of every field. So, if you want to research scientist you want to learn from these plateform "
#### ***Discription :***\nEvery Research Scientist which is connected to Data science domain it is nessory to learn the python as primary programming Language.
#### ***Discription :***\nif you want to talk about IDE jupyter notebook is most populer bcause of python almost 80% use this.
"#### ***Discription :***\nNotebook are use for online code writing and debugging. As you see in graph, Colab and Kaggle Notebook are common. "
"#### ***Discription :***\nAs you see in the graph, Mostly data visulization libraries use by Research Scientist are Matplotlib and seaborn for visulized your Research."
#### ***Discription :***\nMachine Learning Libraries or frameworks presents in above graph that shows Scikit-Learn and TensorFlow are widely use Research Field
"#### ***Discription :***\nIn Natural Language processing methods, Transformer language models are most populer"
"#### ***Discription :***\nAs you see in the graph, Kaggle datasets in group of hub/Repositories is the most populer because moslty data managers use kaggle which are easy tu use and save and compile the data very effciently"
"#### ***Discription :***\nCloud plateform is very common in data related field because of high performence computing. So, graph showes that more the 30% use amazon Web services(AWS) for high performence computing and large data storge."
#### ***Discription :***\n we see this graph shows Google cloud plateform have provide friendly and enjoyable enviroment for their customers. 
"#### ***Discription :***\nIn previous Two Graphs GCC is familier in Research Scienist, So, this graph shows that Google cloud Compute Engine is also very known "
#### ***Discription :***\nGoogle Cloud Storage Servies is widely use in Data storage.
#### ***Discription :***\nData Storage products work with data products. mean how you want to storge the data. My SQL is vwidely use in Research. 
#### ***Discription :***\nMS Power BI is very populer in Community.
#### ***Discription :***\nML Flow is the product that serve Machine Learning models
##### ***following responsible or ethical AI products in your machine learning practices***
#### ***Discription :***\nGPU is the specilized hardware which are mostly use in research Scientist
"#### ***Discription :***\nAs you see in the graph, Resarh Scientist have the most interest on th journal Publication and other type of publication."
##### ***What products or platforms did Machine Learning/ MLops Engineer find to be most helpful when they first started studying datascience?***
"- The most helpful platform for Machine Learning/ MLops Engineer is Kaggle, Youtube, and Coursera."
##### ***What programming languages Machine Learning/ MLops Engineer use on a regular basis?***
- The most used programming language by Machine Learning/ MLops Engineer is Python.
##### ***Which of the following integrated development environments (IDE's) do Machine Learning/ MLops Engineer use on a regular basis?***
- the most used IDE by Machine Learning/ MLops Engineer is Jupyter Notebook
##### ***Do Machine Learning/ MLops Engineer use any of the following hosted notebook products?*** 
- Most of the Machine Learning/ MLops Engineer use Kaggle Notebook and Google Colab.
##### ***Data Visualization Libraries Used by Machine Learning/ MLops Engineer on regular basis***
"- The most used Data Visualization Libraries by Machine Learning/ MLops Engineer is Matplotlib,Seaborn and plotly"
##### ***Which of the following machine learning frameworks Machine Learning/ MLops Engineer use on a regular basis?***\n
"- Scikit learnm Tensorflow, Keras, Pytorch are the most used machine learning frameworks by Machine Learning/ MLops Engineer."
##### ***Which of the following ML algorithms Machine Learning/ MLops Engineer use on a regular basis?***
"- Linear or Logistic Regression, Decision Trees or Random Forests, Gradient Boosting Machines (GBM's), Convolutional Neural Networks (CNN's) are the most used ML algorithms by Machine Learning/ MLops Engineer."
##### ***Which categories of computer vision methods does Machine Learning/ MLops Engineer use on a regular basis?***
"- Most of the Machine Learning/MLops are not familiar with computer vision methods, but those who are familiar with it use Image classification and other general purpose networks."
##### ***Which of the following natural language processing (NLP) methods Machine Learning/ MLops Engineer use on a regular basis?***
"- Most of the Machine Learning/MLops are usinf word embeddings and encoder-decoder models, but a large portion of them are not familiar with NLP methods."
##### ***Do Machine Learning/ MLops Engineer download pre-trained model weights from any of the serivces***
"- Most of the MLops are not familiar with pre-trained model weights, but those who are familiar with it use Hugging face model and Tensroflow Hub."
##### ***Which of the following ML model hubs/repositories do Machine Learning/ MLops Engineer use most often?***
"- Hugging face model, Kaggle Dataset and Tensroflow Hub are the most used ML model hubs/repositories by Machine Learning/ MLops Engineer."
##### ***Which of the following cloud computing platforms do Machine Learning/ MLops Engineer use?***
"- Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure are the most used cloud computing platforms by Machine Learning/ MLops Engineer."
- AWS has the best developer experience for Machine Learning/ MLops Engineer.
##### ***Do Machine Learning/ MLops Engineer use any of the following cloud computing products?***
"- Most of the Machine Learning/ MLops Engineer use Amazon Elastic Compute Cloud (EC2), Google Cloud Compute Engine and Microsoft Azure Virtual Machines."
##### ***Do Machine Learning/ MLops Engineer use any of the following data storage products?***
"- Most of the Machine Learning/ MLops Engineer use Amazon S3, Google Cloud Storage and Microsoft Azure Blob Storage"
"##### ***Do Machine Learning/ MLops Engineer use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
"- Most of MLops are not using any of the following data products, but those who are using it use MySQL, PostgreSQL, Google BigQuery and Microsoft SQL Server."
##### ***Do Machine Learning/ MLops Engineer use any of the following business intelligence tools?***
##### ***Do Machine Learning/ MLops Engineer use any of the following managed machine learning products?***
"- Most of the MLops dont use any managed Ml products but thosem who are using  uses Amazon SageMaker, Google Cloud AI Platform and Microsoft Azure Machine Learning Studio."
##### ***Do Machine Learning/ MLops Engineer use any of the following automated machine learning tools?***
##### ***Do Machine Learning/ MLops Engineer use any of the following products to serve your machine learning models?***
- Most of MLops Dont use any of the product but those who are using they uses MLflow and TFX
##### ***Do Machine Learning/ MLops Engineer use any tools to help monitor your machine learning models and/or experiments?***
"- Most of MLops Dont use any of the tools but those who are using they uses TensorBoard, Ml flow and Weights & Biases "
##### ***Do Machine Learning/ MLops Engineer use any of the following responsible or ethical AI products in your machine learning practices?***
"- Most of the Mlops engineer dont use any of the responsible or ethical AI products but those who are using they uses Google Responsible AI, Amazon AI ethics and Microsoft AI."
##### ***Do Machine Learning/ MLops Engineer use any of the following types of specialized hardware when training machine learning models?***
- Most of the Mlops engineer uses GPu's and TPu's
##### ***Who/what are Machine Learning/ MLops Engineer favorite media sources that report on data science topics?***
"- Most of the Mlops engineer uses Kaggle, Youtube and Blogs."
##### ***What products or platforms did Statistician find to be most helpful when they first started studying Statistics?***
##### ***What programming languages Statistician use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Statistician use on a regular basis?***
##### ***Do Statistician use any of the following hosted notebook products?*** 
##### ***Data Visualization Libraries Used by Statistician on regular basis***
##### ***Which of the following machine learning frameworks Statistician use on a regular basis?***\n
##### ***Which of the following ML algorithms Statistician use on a regular basis?***
##### ***Which categories of computer vision methods does Statistician use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Statistician use on a regular basis?***
##### ***Do Statistician download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do Statistician use most often?***
##### ***Which of the following cloud computing platforms do Statistician use?***
"##### ***Of the cloud platforms that Statistician are familiar with, which has the best developer experience (mostenjoyable to use)?***"
##### ***Do Statistician use any of the following cloud computing products?***
##### ***Do Statistician use any of the following data storage products?***
"##### ***Do Statistician use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do Statistician use any of the following business intelligence tools?***
##### ***Do Statistician use any of the following managed machine learning products?***
##### ***Do Statistician use any of the following automated machine learning tools?***
##### ***Do Statistician use any of the following products to serve your machine learning models?***
##### ***Do Statistician use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Statistician use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Statistician use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are Statistician favorite media sources that report on data science topics?***
##### ***What products or platforms did Software Engineer find to be most helpful when they first started studying Software Engineering?***
##### ***What programming languages Software Engineer use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Software Engineer use on a regular basis?***
##### ***Data Visualization Libraries Used by Software Engineer on regular basis***
##### ***Which of the following machine learning frameworks Software Engineer use on a regular basis?***\n
##### ***Which of the following ML algorithms Software Engineer use on a regular basis?***
##### ***Which categories of computer vision methods does Software Engineer use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Software Engineer use on a regular basis?***
##### ***Do Software Engineer download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do Software Engineer use most often?***
##### ***Which of the following cloud computing platforms do Software Engineer use?***
##### ***Do Software Engineer use any of the following cloud computing products?***
##### ***Do Software Engineer use any of the following data storage products?***
"##### ***Do Software Engineer use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do Software Engineer use any of the following business intelligence tools?***
##### ***Do Software Engineer use any of the following managed machine learning products?***
##### ***Do Software Engineer use any of the following automated machine learning tools?***
##### ***Do Software Engineer use any of the following products to serve your machine learning models?***
##### ***Do Software Engineer use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Software Engineer use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Software Engineer use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are Software Engineer favorite media sources that report on data science topics?***
##### ***What products or platforms did Engineer (Non-software) find to be most helpful when they first started studying Engineering (Non-software)?***
##### ***What programming languages Engineer (Non-software) use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Engineer (Non-software) use on a regular basis?***
##### ***Do Engineer (Non-software) use any of the following hosted notebook products?*** 
##### ***Data Visualization Libraries Used by Engineer (Non-software) on regular basis***
##### ***Which of the following machine learning frameworks Engineer (Non-software) use on a regular basis?***\n
##### ***Which of the following ML algorithms Engineer (Non-software) use on a regular basis?***
##### ***Which categories of computer vision methods does Engineer (Non-software) use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Engineer (Non-software) use on a regular basis?***
##### ***Do Engineer (Non-software) download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do Engineer (Non-software) use most often?***
##### ***Which of the following cloud computing platforms do Engineer (Non-software) use?***
"##### ***Of the cloud platforms that Engineer (Non-software) are familiar with, which has the best developer experience (mostenjoyable to use)?***"
##### ***Do Engineer (Non-software) use any of the following cloud computing products?***
##### ***Do Engineer (Non-software) use any of the following data storage products?***
"##### ***Do Engineer (Non-software) use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do Engineer (Non-software) use any of the following business intelligence tools?***
##### ***Do Engineer (Non-software) use any of the following managed machine learning products?***
##### ***Do Engineer (Non-software) use any of the following automated machine learning tools?***
##### ***Do Engineer (Non-software) use any of the following products to serve your machine learning models?***
##### ***Do Engineer (Non-software) use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Engineer (Non-software) use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Engineer (Non-software) use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are Engineer (Non-software) favorite media sources that report on data science topics?***
##### ***What products or platforms did data administrator find to be most helpful when they first started studying data administrator?***
##### ***What programming languages data administrator use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do data administrator use on a regular basis?***
##### ***Data Visualization Libraries Used by data administrator on regular basis***
##### ***Which of the following machine learning frameworks data administrator use on a regular basis?***\n
##### ***Which of the following ML algorithms data administrator use on a regular basis?***
##### ***Which categories of computer vision methods does data administrator use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods data administrator use on a regular basis?***
##### ***Do data administrator download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do data administrator use most often?***
##### ***Which of the following cloud computing platforms do data administrator use?***
##### ***Do data administrator use any of the following cloud computing products?***
##### ***Do data administrator use any of the following data storage products?***
"##### ***Do data administrator use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do data administrator use any of the following business intelligence tools?***
##### ***Do data administrator use any of the following managed machine learning products?***
##### ***Do data administrator use any of the following automated machine learning tools?***
##### ***Do data administrator use any of the following products to serve your machine learning models?***
##### ***Do data administrator use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do data administrator use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do data administrator use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are data administrator favorite media sources that report on data administrator topics?***
##### ***What products or platforms did Data Engineer find to be most helpful when they first started studying Data Engineer?***
##### ***What programming languages Data Engineer use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Data Engineer use on a regular basis?***
##### ***Data Visualization Libraries Used by Data Engineer on regular basis***
##### ***Which of the following machine learning frameworks Data Engineer use on a regular basis?***\n
##### ***Which of the following ML algorithms Data Engineer use on a regular basis?***
##### ***Which categories of computer vision methods does Data Engineer use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Data Engineer use on a regular basis?***
##### ***Do Data Engineer download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do Data Engineer use most often?***
##### ***Which of the following cloud computing platforms do Data Engineer use?***
##### ***Do Data Engineer use any of the following cloud computing products?***
##### ***Do Data Engineer use any of the following data storage products?***
"##### ***Do Data Engineer use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do Data Engineer use any of the following business intelligence tools?***
##### ***Do Data Engineer use any of the following managed machine learning products?***
##### ***Do Data Engineer use any of the following automated machine learning tools?***
##### ***Do Data Engineer use any of the following products to serve your machine learning models?***
##### ***Do Data Engineer use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Data Engineer use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Data Engineer use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are Data Engineer favorite media sources that report on data science topics?***
##### ***What products or platforms did Teacher / professor find to be most helpful when they first started studying Teacher / professor?***
##### ***What programming languages Teacher / professor use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Teacher / professor use on a regular basis?***
##### ***Data Visualization Libraries Used by Teacher / professor on regular basis***
##### ***Which of the following machine learning frameworks Teacher / professor use on a regular basis?***\n
##### ***Which of the following ML algorithms Teacher / professor use on a regular basis?***
##### ***Which categories of computer vision methods does Teacher / professor use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Teacher / professor use on a regular basis?***
##### ***Do Teacher / professor download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do Teacher / professor use most often?***
##### ***Which of the following cloud computing platforms do Teacher / professor use?***
##### ***Do Teacher / professor use any of the following cloud computing products?***
##### ***Do Teacher / professor use any of the following data storage products?***
"##### ***Do Teacher / professor use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do Teacher / professor use any of the following business intelligence tools?***
##### ***Do Teacher / professor use any of the following managed machine learning products?***
##### ***Do Teacher / professor use any of the following automated machine learning tools?***
##### ***Do Teacher / professor use any of the following products to serve your machine learning models?***
##### ***Do Teacher / professor use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Teacher / professor use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Teacher / professor use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are Teacher / professor favorite media sources that report on Teacher / professor topics?***
##### 2.	How many people required for data science workload?
## Leading Platforms 🤦‍♂️
##### Most tool for monitoring machine learning models
The Plot shows that 460+ people use TensorBoard as Tool for monitoring machine Learning models 
##### Most Used automated machine learning tools
The Plot shows that 340+ people use MLflow as Automated Tool for Machine Learning
##### Most used Product based on Machine learning models
The Plot shows that 250+ people use Google cloud auto ML as Product based on Machine Learning models
##### Most used product management tools for machine learning
The Plot shows that 300+ people use Amazon SageMaker as Product Mangement Tool for Machine Learning
##### Most used ML model hubs/repositories
The Plot shows that 1600+ people use kaggle dataset as Machine Learning model hubs/repositorie
##### Most used pre-trained model weights
The Plot shows that 2700+ people use kaggle dataset as Machine Learning pre trained Models 
##### Most used machine learning frameworks
The Plot shows that 2200+ people Scikit-Learn as Machine Learning Plateform  
## Database
"### **Report for Business Analytics**\n  In this survey we analyze the business aspect and impact of having applying Machine Learning and Data Science in the coo-operate or IT sectors and what are the benefits we get from it.\n  \n  We concluded the following results in survey.\n## Scope of the Industry\n1. According to the survey analysis on average 1 out of 4 users are spending their money on Machine Learning & Cloud Computing services at home or at work.\n  \n2. According to the survey analysis on average 8% of the businesses are still using Machine Learning method and average 5% of the businesses recently start using it.\n   \n3. According to the survey analysis on average minimum 1 to 4 or more data scientist required based on the organization to implement the Machine Learning Methods.\n## Machine Learning\n1. According to the survey analysis, Tensor Board becomes the most popular tool for monitoring machine learning models its capture more than 1/3rd of the Market.\n   \n2. According to the survey analysis, ML flow and Tensor flow Extended (TFX) the most popular automated machine learning tools has approx. 60% of the collective user of the total users.\n   \n3. According to the survey analysis, the most popular product based on Machine learning models is Google Cloud AutoML has a market share of average 28% users.\n   \n4. According to the survey analysis, Amazon Sage Maker the most popular product management tools for machine learning has average 30% of the users used it.\n   \n5. According to the survey analysis, the most popular ML model hubs/repositories are Kaggle Datasets capture approx. 1/2 of market users.\n   \n6. According to the survey analysis, the Kaggle Dataset has no competitor in users to beat in pre-trained model weights.\n  \n7. According to the survey analysis, the most popular cloud computing services are AWS, Azure, and Google Cloud Platform.\n   \n8. According to the survey analysis, the Scikit-Learn are the most popular Machine Learning Frame Work as approx. 30% of the user’s shares. "
"## 2.2. Height distribution of men and women\n\nFrom our exploration of the unique values we know that the gender is encoded by the values *1* and *2*. And though you don't have a specification of how these values correspond to men and women, you can figure that out graphically by looking at the mean values of height and weight for each value of the *gender* feature.\n\n### Task:\n\nCreate a violin plot for the height and gender using [`violinplot()`](https://seaborn.pydata.org/generated/seaborn.violinplot.html). Use the parameters:\n- `hue` to split by gender;\n- `scale` to evaluate the number of records for each gender.\n\nIn order for the plot to render correctly, you need to convert your `DataFrame` to *long* format using the `melt()` function from `pandas`. Here is [another example](https://stackoverflow.com/a/41575149/3338479) of this.\n\n### Solution:"
"### Task:\n\nCreate two [`kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html)s of the *height* feature for each gender on the same chart. You will see the difference between the genders more clearly, but you will be unable to evaluate the number of records in each of them.\n\n### Solution:"
"## 2.3. Rank correlation\n\nIn most cases, *the Pearson coefficient of linear correlation* is more than enough to discover patterns in data. \nBut let's go a little further and calculate a [rank correlation](https://en.wikipedia.org/wiki/Rank_correlation). It will help us to identify such feature pairs in which the lower rank in the variational series of one feature always precedes the higher rank in the another one (and we have the opposite in the case of negative correlation).\n\n### Task:\n\nCalculate and plot a correlation matrix using the [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient).\n\n### Solution:"
"**Question 2.2. (1 point).** Which pair of features has the strongest Spearman correlation?\n\n1. Height, Weight\n2. Age, Weight\n3. Cholesterol, Gluc\n4. Cardio, Cholesterol\n5. Ap_hi, Ap_lo\n6. Smoke, Alco"
"### Task:\n\nCreate a *count plot* using [`countplot()`](http://seaborn.pydata.org/generated/seaborn.countplot.html), with the age on the *X* axis and the number of people on the *Y* axis. Each value of the age should have two columns corresponding to the numbers of people of this age for each *cardio* class.\n\n### Solution:"
**Question 2.4. (1 point).** At what age does the number of people with CVD outnumber the number of people without CVD for the first time?\n\n1. 44\n2. 55\n3. 64\n4. 70
### Numerical Features w.r.t Target Variable (Outcome) :
"- Considering **tenure**, a high number of customers have left after the 1st month. This high cancellation of services continues for **4 - 5** months but the churn customers have reduced since the 1st month. As the **tenure** increases, customers dropping out decreases. \n- This results in low customer churning as the **tenure** increases. It displays a symmetrical graph with the left side dominating with churn numbers and right side dominating with low churn numbers.  \n- Because of too many unique data points in **MonthlyCharges** & **TotalCharges**, it is difficult to gain any type of insight. Thus, we will scale these numerical features for understandable visualization and gaining insights purposes. This brings the varied data points to a constant value that represents a range of values.\n- Here, we divide the data points of the numerical features by 5 or 500 and assign its quotient value as the representative constant for that data point. The scaling constants are decided by looking into the data & intuition."
"- For **MonthlyCharges** group, churn rate is high for the values between **65** (13x5) - **105** (21x5). This **MonthlyCharges** range of values caused the customers to switch.\n- A very high number of customers opted out of the services for the **TotalCharges** below **500**. This customer churning continues for a **TotalCharges** range of values from **0** (0x500) - **1000** (2x500). "
- ### tenure vs Categorical Features :\n\n#### tenure vs Group 1 : Customer Information : gender | SeniorCitizen | Partner | Dependents |
"- **Male** & **Female** customer churn graphs are very similar. \n- **SeniorCitizen** opted out from the services for a tenure values of **0 - 35** months. **20 - 35** months is the kind of decision making period about whether to continue or swtich for **SeniorCitizen**.\n- Similarly, customers with partners continued with the service for a **tenure** of **5 - 45** months."
#### tenure vs Group 2: Services Subscribed by the Customer : PhoneService | MultipleLines | InternetService | StreamingTV | StreamingMovies |
"- Presence of **MutipleLines** pushes the median **MonthlyCharges** irrespective if the customers opt out of the services or not.\n- For the graph of **tenure vs PhoneService**, availability of **PhoneService** or not display a mirroring visuals. Customers were probably not heavy phone (call - message) users.\n- For **InternetService**, customers seem to be very skeptical about the usage of **Optic Fibre** cables as the churning lasted for about **30 - 35** months before either carrying it forward or switching to a new one!\n- Similary for **StreamingTV** & **StreamingMovies**, a churn tenure period of about **10 - 40** months can be observed! "
#### tenure vs Group 2: Services Subscribed by the Customer : OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport |
"- For **OnlineSecurity**, **OnlineBackup**, **DeviceProtection** & **TechSupport**, median churn tenure value is of **25** months. The highest value of this churn tenure is of around **45** months.\n- **30 - 35** month period is where the customers take a call about whether to continue with the current services or switch w.r.t above features!"
#### tenure vs Group 3 : Contract | PaperlessBilling | PaymentMethod |
"- When customers sign **One year** and **Two year** contracts for the services, they seem to continue with the services for about **25** and **45** months respectively! However, they start questioning the services and think about switching from the **35** month and **55** month mark respectively!\n- Irrespective of the **PaperlessBilling**, customers think of switching right from the 1st month.\n- When it comes to **PaymentMethod**, median churn tenure of **Bank Transfer (automatic)** & **Credit Card (automatic)**, **above 20 months**, is nearly double than that of **Electronic check** & **Mailed check**, **around 10 months** & **around 5 months** respectively."
- ### MonthlyCharges vs Categorical Features :\n\n#### MonthlyCharges vs Group 1 : Customer Information : gender | SeniorCitizen | Partner | Dependents |
"- For all the features mentioned above, the median value of the **not-churn** customers is very close to the lower limit of the **churn** customers. \n- **Male** & **Female** customers have the same median **MonthlyCharges** of around **60**. For **SeniorCitizen**, this value is pushed to **80**.\n- Customers living with **Partner** have a higher lower limit of churning, **MonthlyCharges** of **70**, than those living alone, **MonthlyCharges** of **just below 60**!  "
#### MonthlyCharges vs Group 2: Services Subscribed by the Customer : PhoneService | MultipleLines | InternetService | StreamingTV | StreamingMovies |
"- **MonthlyCharges** of **Fiber Optic** cables is very high. Thus, it might be the reason for such high churn of customers.\n- Similarly, **MonthlyCharges** of **StreamingTV** & **StreamingMovies** is quite high.\n- Range of **MonthlyCharges** for **PhoneService** is from **25 - 85** but customers think of unsubscribing from **75** value of **MonthlyCharges**."
#### MonthlyCharges vs Group 2: Services Subscribed by the Customer : OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport |
"- For **OnlineSecurity**, **OnlineBackup**, **DeviceProtection** & **TechSupport**, range of values is from **around 50 to 100**.\n- Customers who subscribe who to these services, probably don't think about cancelling the subscription due to **MonthlyCharges** as the range of values of customers who unsubscribe & continue is near about the same!"
#### MonthlyCharges vs Group 3 : Contract | PaperlessBilling | PaymentMethod |
- Lower limit of the **MonthlyCharges** is higher for **Month-to-Month** contract than **One year** & **Two year** contracts. However the lower limit of the customers who discontinue the services is lower for **Month-to-Month** contract.\n- Lower limit of the **Electronic check** is very high and it can be a huge factor due to which customers resent using it! Whereas **Mailed check** has the lowest starting values of customers who left and continued.
- ### TotalCharges vs Categorical Features :\n\n#### TotalCharges vs Group 1 : Customer Information : gender | SeniorCitizen | Partner | Dependents |
- **TotalCharges** for **male** & **female** customers is quite the same! **SeniorCitizen** that continued with the services have a higher starting and closing values of **TotalCharges**.\n- Customers housing with their **Partner** have a higher median value of **TotalCharges** as compared to those living alone!
#### TotalCharges vs Group 2: Services Subscribed by the Customer : PhoneService | MultipleLines | InternetService | StreamingTV | StreamingMovies |
"- **TotalCharges** of **PhoneService** range from **0 - 4000**. However, customers start getting 2nd thoughts about **PhoneService** due to **TotalCharges** from **around 1000**.\n- Similarly, customers start to hesitate to pay **around 2000** for **MultipleLines**. However, some customers seem to be desperate for **MultipleLines** as they paid a value of **around 6000** for it!\n- When it comes to paying for **Fiber Optic** cables, customers test out the products by paying **around 2000**!\n- Similar to **Fiber Optic**, **StreamingTV** & **StreamingMovies**, customers that continue with the services pay from **3000 - 6000**."
#### TotalCharges vs Group 2: Services Subscribed by the Customer : OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport |
"- For all the features mentioned above, customers become skeptical about paying for them around the **2000** mark. This median value of churn customers is very close to the lower limit of the customers that carry on with this service.\n- Customers that do not churn out are ready to pay from **2000 - 6000** of **TotalCharges**."
#### TotalCharges vs Group 3 : Contract | PaperlessBilling | PaymentMethod |
"- Median values of customers that decide to opt out from the services that have **One year** & **Two year** contracts is high at **around 4000 & 6000**. Some of the customers with **Two year** contracts even paid around **7000**.\n- For **PaymentMethod**, customers are skeptical to pay using **Electronic check** for a shorter range of **0 - 2000** whereas for **Bank transfer (automatic)** & **Credit Card (automatic)** this range is around **0 - 4000**. "
### Numerical features vs Numerical features w.r.t Target variable (Churn) :
"- For **tenure** of **0 - 20 months** period, churning of customers quite at any **MonthlyCharges** values. For a **tenure** period from **20 - 60** months, customers at the top end of the **MonthlyCharges** values, **70 - 120**, start to drop out from the services.\n- For **TotalCharges vs tenure**, as **tenure** increases, **TotalCharges** increase as well! Customers opting out from their plans are the ones who are charged the highest of their **tenure** period alongwith a few customers whose **Total Charges** rank in the middle!\n- Customers seemed to have decided to cancel their subscriptions when the **MonthlyCharges** reach **70 and above**."
- It is a huge matrix with too many features. We will check the correlation only with respect to **Churn**. 
"- **MulipleLines**, **PhoneService**, **gender**, **StreamingTV**, **StreamingMovies** and **InternetService** does not display any kind of correlation. We drop the features with correlation coefficient between **(-0.1,0.1)**.\n- Remaining features either display a significant **positive or negative correlation**."
#### Chi-Squared Test :
"- **PhoneService**, **gender**, **StreamingTV**, **StreamingMovies**, **MultipleLines** and **InternetService** display a very low relation with **Churn**."
"- According to the **ANOVA test**, **higher the value of the ANOVA score, higher the importance of the feature**.\n- From the above results, we need to include all the numerical features for modeling."
- Selecting the features from the above conducted tests and splitting the data into **80 - 20 train - test** groups.
#### 1] Xgboost Classifier :
"The test distribution seems to be similar to the training distribution.\n\nAs a final step, we can find the correlations between distances and fares."
"All the measures of distance have a _positive_ linear correlation with the fare, indicating that as they increase, the fare tends to increase as well. \n\nThe correlation coefficient measures the strength and direction of a linear relationship. Because the linear relationship with the target variable is so strong, we may be able to just use a linear model (regression) to accurately predict the fares. "
"As a sanity check, we can plot the predictions."
"The predicted distribution appears reasonable. Because the competition uses root mean squared error as the metric, any predictions that are far off will have an outsized effect on the error. Let's look at predictions that were greater than \$100."
"Using this one more feature improved our score slightly. Here's another chance for improvement using the same model:\n\n* __Potential Improvement 3: find an optimal set of features or construct more features__. This can involve [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html) or trying different combinations of features and evaluating them on the validation data. You can build additional features by looking at others' work or researching the problem.\n\n#### Collinear Features\n\nOne thing we do want to be careful about is highly correlated, known as [collinear](https://en.wikipedia.org/wiki/Multicollinearity), features. These can decrease the generalization performance of the model and lead to less interpretable models. Many of our features are already highly correlated as shown in the heatmap below. This plots the Pearson Correlation Coefficient for each pair of variables."
You might not want to use two variables that are very highly correlated with each other (such as `euclidean` and `haversine`) because of issues with interpretability and performance.
"The random forest does much better than the simple linear regression. This indicates that the problem is not completely linear, or at least is not linear in terms of the features we have constructed. From here going forward, we'll use the same random forest model because of the increased performance. \n\n#### Overfitting\n\nGiven the gap between the training and the validation score, we can see that our model is __overfitting__ to the training data. This is one of the most common problems in machine learning and is usually addressed either by training with more data, or adjusting the hyperparameters of the model. This leads to another recommendation for improvement:\n\n* __Potential Improvement 4: Try searching for better random forest model hyperparameters__. You may find Scikit-Learn's `RandomizedSearchCV` a useful tool.\n\nI'll provide some starter code for hyperparameter optimization later in the notebook. \n\nNext we can make predictions with the random forest for uploading to the competition."
This time we don't see any extreme predictions as we saw with the first linear regression. The random forest tends to not produce outlying predictions because the voting of the trees means that any single tree that estimates an extreme value will be balanced by the other predictions. \n\nLet's look at the 3 predictions the original simple linear regression estimated as over \$100.
"It appears that using more features helps the model! We can look at the feature importances to see which the model considers ""most relevant"".\n\n#### Feature Importances"
"The `haversine` distance is by far the most important with the other features showing considerably less relevance to the model. This suggests that distance is key, and we might want to find a more accurate way of calculating distances."
"# Additional Feature Engineering\n\nWe saw that adding more features improves the performance of the model. A natural progression is therefore to use even more features! We have not made any use of the `pickup_datetime` which provides the precise moment of pickup and that's where we'll turn our attention to next. \n\n## Extract Datetime Information\n\nWe can write a simple function that extracts as much date and time information from a datetime as possible. This is adapted from the excellent fast.ai library, in particular, the structured module (available at https://github.com/fastai/fastai/blob/master/fastai/structured.py). I have made a few changes based on what's worked best for me in the past on time-series problems."
"## Explore Time Variables\n\nWe now have a ton of time-variables to explore! First, let's ask the question if fares have increased over time. To do this, we can plot the `time_elapsed` versus the fare."
There appears to be a minor increase in prices over time which might be expected taking into account inflation. Let's look at the average fare amount by the hour of day.
We can make the same plot by day of the week.
"Both of these plots do not seem to show much difference between the different times. \n\n### Fractional Time Plots\n\nAs a final exploration of the time variables, we can plot the fare amount versus the fractional time. "
"None of these graphs are very decisive. One interesting thing to note is the horizontal bars at different fare amounts. This suggests there may be certain routes that always have the same fare amount. We explored the fare distribution earlier, but it might be a good idea to revisit the abnormalities in the fares."
#### Correlations with Target\n\nAgain we can show the correlations of all features with the target. 
It seems the most useful time variables may be the `Year` or `Elapsed` because most of the time features have a small correlation with the target. The `Elapsed` correlation is positive indicating that fares have tended to increase over time.
It seems that the new features helped both the random forest and the linear regression. Let's take a look at the random forest feature importances.
"Once again, the `haversine` distance dominates the importance. The time elapsed since the first record seems to be relatively important although the other time features do not seem to be of much use. "
"The best model from random search exhibits less overfitting, but also does not do as well on the validation data. There are probably further benefits from more hyperparameter tuning using additional data. "
"## Better Model\n\nAlthough the random forest has high performance, it does not always work the best for every problem. There are still more [models to try](http://scikit-learn.org/stable/supervised_learning.html):\n\n* __Potential improvement 5: try more models.__ You might find the Gradient Boosting Machine or Deep Neural Networks to be capable learners.\n\n# Your Challenge\n\nFrom here, we can still engineer more features, perform feature selection, or we can try upgrading the model. At this point, I'm going to leave it up to you! I've given you a decent start and some recommendations for improvement so I'll hand things over to you. __Try to build a model that outperforms those in this notebook on the validation data.__\n\nThe best performance of the models is as follows:\n\n| Model         | Train RMSE | Validation RMSE |\n|---------------|------------|-----------------|\n| Linear        | 4.87       | 4.86            |\n| Random Forest | 2.62       | 3.38            |\n\nIt's important to try and beat the __validation score__ as opposed to the training score because you can always get a better training score by training a model with greater capacity (more flexibility to fit the data). However, this won't improve the generalization performance, which is the goal of machine learning: build a model that generalizes to new data! You can also submit the predictions from this notebook and try to better the testing predictions. I look forward to seeing what you can come up with.\n\nI'm working on some more notebooks with additional performance gains, but see if you can take these methods and improve! To wrap up, here are my recommendations:\n\n1. Train with more data\n2. Experiment with different methods for filtering outliers\n3. Find an optimal set of features or build more features\n4. Hyperparameter tuning of the random forest\n5. Try more complex models such as the gradient boosting machine\n\nI have to add that these are all __potential__ improvements because of course we don't know if they will work until we try! I wish you the best of luck, and I'll be back in another kernel trying some improvements of my own. \n\nBest,\n\nWill"
\n# Step 10 | Correlation Analysis\n\n⬆️ [Tabel of Contents](#contents_tabel)
\n\nConclusion:\n\n* Target has the highest correlation first with `Sex` and then with `Pclass`.
"\n\nNow we define a function that provides a complete report of the model's performance on the training and test data, plus the confusin_matrix of the model on the test data and the summary of the model performance using the above function:"
\n    \nLet's evaluate our XGBoost classifier using the above function:
A heatplot below shows how many clusters were generated by the DBSCAN algorithm for the respective parameters combinations.
"The heatplot above shows, the number of clusters vary from 17 to 4. However, most of the combinations gives 4-7 clusters.\nTo decide which combination to choose I will use a metric - a silhuette score and I will plot it as a heatmap again."
Global maximum is 0.26 for `eps`=12.5 and `min_samples`=4.
DBSCAN created 5 clusters plus outliers cluster (-1). Sizes of clusters 0-4 vary significantly - some have only 4 or 8 observations. There are 18 outliers.
The graph above shows that there are some outliers - these points do not meet distance and minimum samples requirements to be recognised as a cluster.
\nLogistic Regression\n
\nDecision Tree Classifier\n
\nRandom Forest Classifier\n
\nXGB Classifier\n
\nSupport Vector Machines\n
 Comparing Different Models 
Let's look at the achieved acceleration:
*Please upvote if you liked it.*
\n## ___Train and valid loops with tqdm bar
\n## ___CFG
### Correlation Heat Map
"### Interpretation\n\n\nFrom the above correlation heat map, we can conclude that :-\n\n1. `Class` is highly positive correlated with `Uniformity_Cell_Size`, `Uniformity_Cell_Shape` and `Bare_Nuclei`. (correlation coefficient = 0.82).\n\n2. `Class` is positively correlated with `Clump_thickness`(correlation coefficient=0.72), `Marginal_Adhesion`(correlation coefficient=0.70), `Single_Epithelial_Cell_Size)`(correlation coefficient = 0.68) and `Normal_Nucleoli`(correlation coefficient=0.71).\n\n3. `Class` is weekly positive correlated with `Mitoses`(correlation coefficient=0.42).\n\n4. The `Mitoses` variable is weekly positive correlated with all the other variables(correlation coefficient < 0.50)."
"### Comment\n\n\nSo, kNN Classification model with k=7 shows more accurate predictions and less number of errors than k=3 model. Hence, we got performance improvement with k=7."
# **18. Classification metrices** \n\n[Table of Contents](#0.1)
"# **19. ROC-AUC** \n\n[Table of Contents](#0.1)\n\n\n\n### ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of **TP to (TP + FN)**.\n\n\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of **FP to (FP + TN)**.\n\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n\n"
### Positivity vs. Country
"In the world plot, we can see that the languages with the highest average positivity are English, Spanish, Portuguese, and Danish."
### Positivity vs. Toxicity
"I have plotted the distribution of positivity for toxic and non-toxic comments above. We can see that both the distributions are very similar, indicating that positivity is not an accurate indicator of toxicity in comments. "
"### Neutrality sentiment\n\nNeutrality sentiment refers to the level of bias or opinion in the text. It is a score between 0 and 1; the greater the score, the more neutral/unbiased the abstract is."
"From the above plot, we can see that the neutrality sentiment distribution has a strong leftward (negative) skew, which is in constrast to the negativity and positivity sentiment distributions. This indicates that the comments tend to be very neutral and unbiased in general. This also suggests that most comments are not highly opinionated and polarizing, meaning that most comments are non-toxic."
### Neutrality vs. Country
"In the world plot, we can see that the languages with the highest neutrality are Persian, Hindi, and Russian. Few western European languages like German and English seem to have a lower average neutrality than most other languages."
### Neutrality vs. Toxicity
"We can see that non-toxic comments tend to have a higher neutrality value than toxic comments on average. The probability density of the non-toxic distribution experiences a sudden jump at 1, and the probability density of the toxic distribution is significantly lower at the same point. This suggests that a comment with neutrality close to 1 is more likely to be non-toxic than toxic."
"### Compound sentiment\n\nCompoundness sentiment refers to the total level of sentiment in the sentence. It is a score between -1 and 1; the greater the score, the more emotional the abstract is."
"From the distribution above, we can see that compound sentiment is evenly distributed across the specturm (from -1 to 1) with very high variance and random peaks throughout the range."
### Average compound sentiment vs. Country
"In the world plot above, we can see that western European countries, south-east Asia, and Turkey have a lower average compound sentiment than most other countries. India, Russia, and Iran are among the countries with the maximum compound sentiment."
### Compound sentiment vs. Toxicity
"We can see that compound sentiment tends to be higher for non-toxic comments as compared to toxic comments. The non-toxic distribution has a leftward (negative) skew, while the toxic distribution has a positive (rightward) skew. This indicates that non-toxic comments tend to have a higher compound sentiment than toxic comments on average."
### Distribution of Flesch reading ease
"The Flesch reading ease distribution has a slight leftward (negative) skew, although the distribution is roughly normal. The most common values of the metric lie between 66 and 66.5."
### Flesch reading ease vs. Country
"In the world plot above, we can see that the Flesch readability ease is maximum in the Russian and Vietnamese languages. These languages have few words per sentence and few syllables per word, indicating that they are ""easier"" to read."
### Flesch reading ease vs. Toxicity
"I have plotted the distribution of Flesch reading ease for toxic and non-toxic comments above. We can see that both the distributions are very similar, indicating that Flesch reading ease is not an accurate indicator of toxicity in comments. "
### Distribution of automated readability
"The automated readability distribution has a slight rightward (positive) skew, although the distribution has a roughly normal shape. The most common value of automated readability in the dataset is approximately 9. Very few comments have a readability ease greater than 60."
### Automated readability vs. Country
"In the world plot above, we can see that automated readability is maximum in Hindi, Arabic, and Somali comments. Whereas, Turkish, English, and south-east Asian comments seem to have relatively lower automated readability value than most countries."
### Automated readability vs. Toxicity
"I have plotted the distribution of automated readability for toxic and non-toxic comments above. We can see that both the distributions are very similar, indicating that automated readability is not an accurate indicator of toxicity in comments. "
### Distribution of Dale-Chall readability
"The Dale-Chall readability distribution has a slight rightward (positive) skew, although the distribution has a roughly normal shape. There are also peaks to the left of the main distribution. The most common value of automated readability in the dataset is approximately 7. Very few comments have a readability ease greater than 20."
### Dale-Chall readability vs. Country
"The Dale-Chall readability score seems to be maximum in middle-eastern and south-east Asian languages. Russian and Arabic, on the other hand, have a lower Dale-Chall readability than most other languages in the dataset."
### Dale-Chall readability
"Dale-Chall readability seems to be higher (on average) for non-toxic comments, indicating that non-toxic comments use more ""sophisticated"" or ""difficult"" language. Toxic comments, on the other hand, are more blunt. We can see this from the fact that the non-toxic distribution peaks at a higher value than the toxic distribution."
### Non-toxic vs. Toxic
"We can see from the above wordclouds, that toxic comments use more insluting or hateful words such as ""f**k"", while the non-toxic comments do not usually use such words."
### Obscene vs. Severe Toxic vs. Threat vs. Insult
"In the above wordclouds, we can see that most of these categories use insulting/hateful language. But, the threat category seems to be slightly different from the remaining categories, as it uses words like ""kill"" and ""die"", indicating that most threats involve threats to kill someone."
### Pie chart of targets
"From the pie chart above, we can see that the most common target is toxic, and the other targets, such as insult and threat are relatively uncommon."
### Bar chart of targets
"From the bar chart above, we can once again see that toxic is the most common target, while threat is the rarest."
### Toxicity vs. Country
"From the above world plot, we can see that Irish and Afrikaans are the countries with maximum average toxicity, while most other countries have a similar average toxicity value."
"### Visualize model predictions\n\nNow, I will visualize the performance of the model on few validation samples."
We can see that the model gets only two out of four answers correct. We need to find a better model to achieve a higher accuracy.
# How does years of experience coding impact salaries?
"We did expect a increase in salaries with the increase in experience, and there it is! However do you see that people who don't code at all earn a little bit more than those who are starting to code? This is probably due to managers sitting there on the first group. Once you get some momentum coding, your salary will consistently increase. \n\nCan we test this hypothesis that managers (without coding experience) are driving increase in the salaries? Well... kind of... We expect managers to earn more and also to be... older, right? Let's see if that is true."
"It is reasonable to infer that those who have no coding experience are earning more than those with little experience because they are in managerial positions. Except for Data Engineering, where we have younger people, without coding experience, earning more than those who are starting to code."
# How does company size impact salaries?
The average salary usually increases with the company size. Data Scientists and Statisticians/Research Scientists see the biggest difference in their payslips when moving from a startup to a big corp. Business Analysts can't expect to much increase in salary just by moving to a bigger company.\n\nSalary isn't everything. Consider the pros and cons before going after jobs in large corporations.\n\n![gif career choice](https://media.giphy.com/media/gfV5GoEAaHwnC/giphy.gif)
"Python has been consolidated as the main language used by people with all job titles, about 3/4 of all respondents use it on a regular basis. R Language is much less popular (and almost no Software Engineers use with it)."
"If you want to find a job in Data Science, then Python is the language of choice. More than 75% of all Data Science positions on Glassdoor mention Python in their job descriptions.\n\n Python is the prefered language for Data Science by the market!\n\n\nStatisticians and Research Scientists use less SQL than other professions. **In fact only 1/4 of then use SQL!** This is a sign that academia is much more used to flat files than to databases. Unfortunately the real world is not served in `.csv` files.\n\n Hey you that are doing a Masters or PhD and want to find a job! Look at those two charts above! Learn SQL!\n\nIf you want to start working with data, then knowing SQL will help you get jobs as Business and Data Analyst. Now if you want to go down the Data Engineering path, SQL and Python are musts. Software engineers will find jobs more easily by knowing Java.\n\nTo get a job in the data world, the recommended language to learn is SQL! "
# Programming Languages: Kaggle vs Glassdoor
"We see that Kagglers are well prepared when it comes to Python usage! Although many respondents use Bash and R, job descriptions don't mention those languages frequently. SQL is not frequently required for Business Analysts, but knowing it (as Kagglers do) will put you one step ahead. In general Software Engineers and Data Engineers from Kaggle might find a hard time to find jobs, as Java is frequently required and many of them don't posses that skill.\n\n\nLearning Java will help you to move into Software Engineering or Data Engineering "
# How many programming languages should you learn?
"Learning (and using) multiple languages does not look like a competitive advantage in terms of salary. Pick up to three and be a real *Pro* when it comes to using them. Limit your languages especially if you don't have experience in working with data!\n\nStop learning multiple languages! Python, SQL and one more language of your choice should be enough."
# Who should learn and use Cloud Computing Platforms?
"We see a lot of Kagglers Data Scientists who are used to working with Cloud Platforms! This is good news for productionizing Machine Learning models, as it is much easier go live with a model if data scientists are not doing everything in their local environments."
I can confirm that this is true! Since I migrated from Data Science to Data Engineering I started to work heavily with AWS products. Cloud computing platforms have a lot of managed services for the whole data pipeline. Knowing where to apply (and how to use) them is of prime importance for any data engineer.\n\nBe aware! To work in Data and Software Engineering you'll need a lot of knowledge about cloud computing platforms!
"Kagglers from academia! Again, working with relational databases should be done more frequently if you wish to be prepared for the marketplace"
"Although they don't appear frequently in the job descriptions, we see that listings for Data Engineers and Software Engineers often require knowledge of database engines such as PostgreSQL, MySQL and SQL Server. Data Analysts with SQL Server Knowledge might benefit from it when looking for a job."
![](https://morvanzhou.github.io/static/results/torch/1-1-3.gif)
### Generate Fake Data
### Model design
"**Neural Network** - Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data.\n\n![](https://www.researchgate.net/profile/Reza_Rezaee8/publication/230905093/figure/fig1/AS:300580867198976@1448675471259/a-Schematic-diagram-of-an-artificial-neural-network-structure-which-consists-of-an.png)"
### Design Model
### Model Saving Function in Pickle Format
### Load save model
### Parameter Restore for net1 to net3
### Plot the Models
### Put Dataset in torch dataset
### Model Training
# Section : 3. Advance Neural Network\n---
### Show data
### Data Convert into Variable
"![](https://i.stack.imgur.com/WSOie.png)\n\n### Sequence Modeling\n\n![](https://morvanzhou.github.io/static/results/ML-intro/rnn2.png)\n* We imagine that there is now a set of sequence data **data 0,1,2,3.** When **predicting result0**, we are based on **data0, and when predicting other data,** we are only based on a single data. The neural networks are all the same NN. However, the data is in an ascending order, just like cooking in the kitchen. Sauce A is placed earlier than Sauce B, otherwise it will be scented. So the ordinary neural network structure can not let NN understands the association between these data.\n\n### Neural network for processing sequence\n\n![](https://morvanzhou.github.io/static/results/ML-intro/rnn3.png)\n\n* So how do we let the association between data be analyzed by NN? Think about how humans analyze the ***relationship between things.*** The most basic way is to remember *what happened before.* Then we let the neural network also have this. ***The ability to remember what happened before***. \n* When analyzing **Data0, we store the analysis results in memory. Then when analyzing data1, NN will generate new memories, but new memories and old memories are unrelated. Simply call the old memory and analyze it together. If you continue to analyze more ordered data, RNN will accumulate the previous memories and analyze them together.**\n\n![](https://morvanzhou.github.io/static/results/ML-intro/rnn4.png)\n\n* Let's repeat the process just now, but this time to add some mathematical aspects. Every time RNN is done, it will produce a description of the **current state**. We replace it with a **shorthand S(t), then the RNN starts Analyze x(t+1), which produces s(t+1) from x(t+1), but y(t+1) is created by s(t) and s(t+1)** So the RNN we usually see can also be expressed like this.\n\n### Application of RNN\n\n* The form of RNN is not limited to this. His structure is very free. If it is used for classification problems, for example, **if a person says a sentence, the emotional color of this sentence is positive or negative. Then we will You can use the RNN that outputs the judgment result only at the last time.**\n* Or this is the picture description RNN, we only need an X to replace the input picture, and then generate a paragraph describing the picture.\n* Or the **RNN of the language translation, give a paragraph of English, and then translate it into Any Langauge.**\n* With these different forms of RNN, RNN becomes powerful. There are many interesting RNN applications. **For example, let RNN describe the photo. Let RNN write the academic paper, let RNN write the program script, let RNN compose. We The average person can't even tell if this is written by the machine.**"
### Hyper Parameters
### Training and Testing
### Predicted and Actual Value match
"## RNN-Regression\n---\nNote : **Regression Concept I have already explaned above, so here I have applied direct RNN on Regression.**"
###  Hyper Parameters
### Model Design
"## AutoEncoder\n---\n\n* ***Nueral Network Unsupervised form is Known as AutoEncoder.***\n\n* Autoencoders (AE) are neural networks that aims to copy their inputs to their outputs. They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation. This kind of network is composed of two parts :\n\n* **1) Encoder:** This is the part of the network that compresses the input into a latent-space representation. It can be represented by an encoding function h=f(x).\n* **2) Decoder:** This part aims to reconstruct the input from the latent space representation. It can be represented by a decoding function r=g(h).\n\n![](https://cdn-images-1.medium.com/max/880/1*V_YtxTFUqDrmmu2JqMZ-rA.png)\n\n### What are autoencoders used for ?\n* Data denoising and Dimensionality reduction for data visualization are considered as two main interesting practical applications of autoencoders. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.\n\n### Types of autoencoder :\n* **Vanilla autoencoder**: In its simplest form, the autoencoder is a three layers net, i.e. a neural net with one hidden layer. The input and output are the same, and we learn how to reconstruct the input, for example using the adam optimizer and the mean squared error loss function.([Code](https://gist.githubusercontent.com/nathanhubens/604fd9cfc2d7f3d022e3ef0cf4b787de/raw/bc437b49f974b767e488a4a896b0e869d87a39d6/vanilla%20autoencoder))\n\n* **Multilayer autoencoder** :  If one hidden layer is not enough, we can obviously extend the autoencoder to more hidden layers.([Code](https://gist.githubusercontent.com/nathanhubens/219ab6efcfbab95508495eb6d6e41884/raw/d50fe4cb3b5d361da68156c789d5bd25f5dad321/multilayer%20autoencoder))\n* **Convolutional autoencoder**: We may also ask ourselves: can autoencoders be used with Convolutions instead of Fully-connected layers ?\nThe answer is yes and the principle is the same, but using images (3D vectors) instead of flattened 1D vectors. The input image is downsampled to give a latent representation of smaller dimensions and force the autoencoder to learn a compressed version of the images. ([Code](https://gist.github.com/nathanhubens/2f11dd9257263874b94966eb48e42922/raw/48f545769116607f713cded00c82d698ce9fb25a/convolutional%20autoencoder))\n\n* **Regularized autoencoder**:  There are other ways we can constraint the reconstruction of an autoencoder than to impose a hidden layer of smaller dimension than the input. Rather than limiting the model capacity by keeping the encoder and decoder shallow and the code size small, regularized autoencoders use a loss function that encourages the model to have other properties besides the ability to copy its input to its output. In practice, we usually find two types of regularized autoencoder: the sparse autoencoder and the denoising autoencoder.\n\n    * **Sparse autoencoder**: Sparse autoencoders are typically used to learn features for another task such as classification. An autoencoder that has been regularized to be sparse must respond to unique statistical features of the dataset it has been trained on, rather than simply acting as an identity function. In this way, training to perform the copying task with a sparsity penalty can yield a model that has learned useful features as a byproduct.([Code](https://gist.github.com/nathanhubens/c6000eee8d6f919d01465183f79a62b6/raw/2e5085740299cf1d9f0a28ddfb438eee4bfe5903/Sparse%20autoencoder))\n\n    * **Denoising autoencoder :** Rather than adding a penalty to the loss function, we can obtain an autoencoder that learns something useful by changing the reconstruction error term of the loss function. This can be done by adding some noise of the input image and make the autoencoder learn to remove it. By this means, the encoder will extract the most important features and learn a robuster representation of the data.([Code](https://gist.github.com/nathanhubens/2c2a7cc138e3d170956c109b10f5a7f7/raw/3b44ff899fe23f2224edf782e8dd068551d83ec5/denoising%20ae))\n"
### Visualize in 3D plot
"## DQN Reinforcement Learning\n---\n\n*  **Intensive learning**, Deep Q Network is referred to as DQN for short. The Google Deep Mind team is relying on this DQN to make computers play more powerful than us.\n\n![](https://cdn-images-1.medium.com/max/1600/1*M8RWevLxhus56RABFEGYYQ.png)\n\n### Reinforcement learning and neural networks\n---\n\n* **The reinforcement learning methods are more traditional methods.** Nowadays, with the various applications of **machine learning in daily life**, various machine learning methods are also integrated, merged and upgraded. The intensive study explored is a method that combines **neural network and Q learning , called Deep Q Network.** Why is this new structure proposed? Originally, traditional form-based reinforcement learning has such a bottleneck.\n\n### The role of neural networks\n---\n![](https://morvanzhou.github.io/static/results/ML-intro/DQN2.png)\n* We use a table to store each state state, and the Q value that each behavior action has in this state. The problem is that it is too complicated, and the state can be more than the stars in the sky (such as Go). Using tables to store them, I am afraid that our computer has not enough memory, and it is time consuming to search for the corresponding state in such a large table. However, in machine learning, there is a way It is very good for this kind of thing, that is the neural network. \n* We can use the state and action as the input of the neural network, and then the neural network analysis to get the Q value of the action, so we do not need to record the Q value in the table, and It is directly using the neural network to generate Q values. Another form is that we can only input the state value, output all the action values, and then directly select the action with the maximum value as the next step according to the principle of Q learning. \n* The action. We can imagine that the neural network accepts external information, which is equivalent to collecting information from the eyes, nose and ears, and then outputting each action through brain processing. The value, finally select the action by means of reinforcement learning.\n\n### Q-Learning [Reference](https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4)\n---\n* Q-learning learns the action-value function Q(s, a): how good to take an action at a particular state. For example, for the board position below, how good to move the pawn two steps forward. Literally, we assign a scalar value over the benefit of making such a move.\n\n![](https://cdn-images-1.medium.com/max/800/1*srmv0GScAs6vObPfPj0-uQ.png)\n\n* In Q-learning, we build a memory table Q[s, a] to store Q-values for all possible combinations of s and a. If you are a chess player, it is the cheat sheet for the best move. In the example above, we may realize that moving the pawn 2 steps ahead has the highest Q values over all others. (The memory consumption will be too high for the chess game. But let’s stick with this approach a little bit longer.)\n\n* Technical speaking, we sample an action from the current state. We find out the reward R (if any) and the new state s’ (the new board position). From the memory table, we determine the next action a’ to take which has the maximum Q(s’, a’).\n\n![](https://cdn-images-1.medium.com/max/800/1*yh8Z2t41HBvY5gwBPTqVVQ.jpeg)\n\n* In a video game, we score points (rewards) by shooting down the enemy. In a chess game, the reward is +1 when we win or -1 if we lose. So there is only one reward given and it takes a while to get it.\n\n* We can take a single move a and see what reward R can we get. This creates a one-step look ahead. R + Q(s’, a’) becomes the target that we want Q(s, a) to be. For example, say all Q values are equal to one now. If we move the joystick to the right and score 2 points, we want to move Q(s, a) closer to 3 (i.e. 2 + 1).\n\n![](https://cdn-images-1.medium.com/max/800/1*9CdBkaFzFRyACvj5P2Af2Q.png)\n\n* As we keep playing, we maintain a running average for Q. The values will get better and with some tricks, the Q values will converge.\n\n---\n### Q-Learning Algorithm\n\n---\n![](https://cdn-images-1.medium.com/max/800/1*5ffOxpSgIJCYn0XccfFYUQ.png)\n\n\n* However, if the combinations of states and actions are too large, the memory and the computation requirement for Q will be too high. To address that, we switch to a deep network Q (DQN) to approximate Q(s, a). This is called Deep Q-learning. With the new approach, we generalize the approximation of the Q-value function rather than remembering the solutions.\n\n![](https://rubenfiszel.github.io/posts/rl4j/qmodeling.png)\n\n---\n### Deep Q-Network Algorithm with experience replay\n\n---\n\n**Algorithms**\n![](https://cdn-images-1.medium.com/max/800/1*8coZ4g_pRtfyoHmsuzMH6g.png)\n"
"## Generative Adversarial Network\n---\n![](https://cdn-images-1.medium.com/max/2000/1*AZ5-3WdNdYyC2U0Aq7RhIg.png)\n\n* In **2014, Ian Goodfellow and his colleagues at the University of Montreal published** a [stunning paper](https://arxiv.org/pdf/1406.2661.pdf) introducing the world to **GANs, or generative adversarial networks.** Through an innovative combination of computational graphs and game theory they showed that, given enough modeling power, two models fighting against each other would be able to co-train through plain old backpropagation.\n\n![](https://cdn-images-1.medium.com/max/800/1*-gFsbymY9oJUQJ-A3GTfeg.png)\n\n\n* The models play **two distinct (literally, adversarial) roles.** Given some real data set **R, G** is the **generator**, trying to **create fake data** that looks just like **the genuine data,** while **D** is the **discriminator**, getting data from either the real set or **G** and labeling the difference. **Goodfellow’s metaphor** (and a fine one it is) was that **G** was like a **team of forgers trying to match real paintings** with their output, while **D** was the team of **detectives trying to tell the difference**. (Except that in this case, the forgers G never get to see the original data — only the judgments of D. They’re like blind forgers.)\n\n* **GANs** or **Generative Adversarial Networks** are a kind of neural networks that is composed of **2** separate deep neural networks competing each other: the **generator** and **the discriminator**.\n\n* **GAN** to generate various things. It can **generate realistic images, 3D-models, videos, and a lot more.Like this Example below**\n![](https://cdn-images-1.medium.com/max/800/1*NmRWSaTpBydHKnGIEAyWMw.png)\n\n---\n### Idea of GAN\n\n---\n#### Architecture of GAN\n![](https://cdn-images-1.medium.com/max/2000/1*39Nnni_nhPDaLu9AnTLoWw.png)\n**1) Generator**\n![](https://cdn-images-1.medium.com/max/1000/1*7i9iCdLZraZkrMy1-KADrA.png)\n**2) Discriminator:**\n![](https://www.researchgate.net/profile/Sinan_Kaplan2/publication/319093376/figure/fig20/AS:526859935731712@1502624605127/Architecture-of-proposed-discriminator-network-which-is-part-of-GAN-based-on-CNN-units.png)\n\n### Conceptual Diagram\n\n![](https://cdn-images-1.medium.com/max/1200/1*M2Er7hbryb2y0RP1UOz5Rw.png)\n\n> * **""The generator will try to generate fake images that fool the discriminator into thinking that they’re real. And the discriminator will try to distinguish between a real and a generated image as best as it could when an image is fed.”**\n\n* They both get stronger together until the **discriminator** cannot **distinguish** between the **real and the generated images anymore**. It could do nothing more than **predicting real or fake with only 50% accuracy.** This is no more useful than **flipping a coin and guess.** \n* This inaccuracy of the **discriminator occurs because the generator generates really realistic face images** that it seems like they are actually real. So, it is normally expected that it wouldn’t be able to distinguish them. When that happens, the most educated guess would be as equally useful as an uneducated random guess.\n\n---\n### The optimal generator\n---\n* Intuitively, the Code vector that I shown earlier in the generator will represent things that are abstract. For example, if the Code vector has 100 dimensions, there might be a dimension that represents “face age” or “gender” automatically.\n* Why would it learn such representation? Because knowing people ages and their gender helps you draw their face more properly!\n\n---\n### The optimal discriminator\n---\n* When given an image, the discriminator must be looking for components of the face to be able to distinguish correctly.\n* Intuitively, some of the discriminator’s hidden neurons will be excited when it sees things like eyes, mouths, hair, etc. These features are good for other purposes later like classification!"
"\n### 5) Tree based feature selection and random forest classification\n\nIn random forest classification method there is a **feature_importances_** attributes that is the feature importances (the higher, the more important the feature). **!!! To use feature_importance method, in training data there should not be correlated features. Random forest choose randomly at each iteration, therefore sequence of feature importance list can change.**\n"
"As you can seen in plot above, after 5 best features importance of features decrease. Therefore we can focus these 5 features. As I sad before, I give importance to understand features and find best of them. "
"\n## Feature Extraction with PCA\n\nWe will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.\n "
"* According to variance ration, 3 component can be chosen.\n* If you have any doubt about PCA, you can check my intuitive way of PCA tutorial."
"We see on the above chart that the two classes are very well defined, and distinct from each other. This is confirmed when we plot the ROC curve.\n\n> In a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner (100% sensitivity, 100% specificity). Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test (Zweig & Campbell, 1993)."
## Probability of Being in the Top 20% per Score\nNow let's calculate the probability of belonging to the Top 20% given a certain score. To do that we will create score ranges. We calculate the probability based on how the model performed on test data. Below we show the probability for each bin.
Here we can **plot data**:
"By lagging a time series, we can make its past values appear contemporaneous with the values we are trying to predict (in the same row, in other words). This makes lagged series useful as features for modeling serial dependence. To forecast series, we could use y_lag_1 and y_lag_2 as features to predict the target y."
"# 4.1 Lag plot\nA lag plot of a time series shows its values plotted against its lags. Serial dependence in a time series will often become apparent by looking at a lag plot. The most commonly used measure of serial dependence is known as **autocorrelation**, which is simply the correlation a time series has with one of its lags. The **partial autocorrelation** tells you the correlation of a lag accounting for all of the previous lags -- the amount of ""new"" correlation the lag contributes, so to speak. Plotting the partial autocorrelation can help you choose which lag features to use.\n"
Let's take a look at the **lag** and **autocorrelation plots** first:
And we can build plot with **predictions**:
"# 5. Hybrid Models\nLinear regression excels at extrapolating trends, but can't learn interactions. XGBoost excels at learning interactions, but can't extrapolate trends. Here we'll learn how to create **""hybrid"" forecasters** that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other."
After that we train and plot
# 6. Machine learning forecasting
"Also, we need to define helpfull function, **plot_multistep**:"
"So, now, we can **plot results**:"
"# 7. Conclusion\nThank you for reading my new article!\n\nHope, you liked it and it was interesting for you! There are some more my articles:\n* [SMS spam with NBC | NLP | sklearn](https://www.kaggle.com/maricinnamon/sms-spam-with-nbc-nlp-sklearn)\n* [House Prices Regression sklearn](https://www.kaggle.com/maricinnamon/house-prices-regression-sklearn)\n* [Automobile Customer Clustering (K-means & PCA)](https://www.kaggle.com/maricinnamon/automobile-customer-clustering-k-means-pca)\n* [Credit Card Fraud detection sklearn](https://www.kaggle.com/maricinnamon/credit-card-fraud-detection-sklearn)\n* [Market Basket Analysis for beginners](https://www.kaggle.com/maricinnamon/market-basket-analysis-for-beginners)\n* [Neural Network for beginners with keras](https://www.kaggle.com/maricinnamon/neural-network-for-beginners-with-keras)\n* [Fetal Health Classification for beginners sklearn](https://www.kaggle.com/maricinnamon/fetal-health-classification-for-beginners-sklearn)\n* [Retail Trade Report Department Stores (LSTM)](https://www.kaggle.com/maricinnamon/retail-trade-report-department-stores-lstm)"
"There are a lot of proteins to examine with the correlation matrix. Let's start by defining what we would consider to be a somewhat significant correlation (positive or negative). Values that are 0.1 or below are likely to have little correlation to the UPDRS target scores, and are likely just noise. A quick scan reveals that there are several candidates that may not be useful in our regression:\n\n* `O00533`\n* `O14498`\n* `O15240`\n* `O15394`\n* `O43505`\n* `O60888`\n* `P00738`\n* `P01034`\n* `P01042`\n* `P01717`\n* `P02452`\n* `P02649`\n* `P02751`\n* `P02753`\n* `P02787`\n* `P04075`\n* `P04156`\n* `P04180`\n* `P04216`\n* `P05060`\n* `P05067`\n* `P05155`\n* `P05408`\n* `P05452`\n* `P06396`\n* `P07195`\n* `P07225`\n* `P07602`\n* `P07711`\n* `P07858`\n* `P08133`\n* `P08253`\n* `P08571`\n* `P09104`\n* `P09486`\n* `P09871`\n* `P10645`\n* `P11142`\n* `P13521`\n* `P13591`\n* `P13611`\n* `P13987`\n* `P14313`\n* `P14618`\n* `P17174`\n* `P19021`\n* `P23083`\n* `P23142`\n* `P39060`\n* `P40925`\n* `P43121`\n* `P49908`\n* `P54289`\n* `P55290`\n* `P61278`\n* `P61769`\n* `P61916`\n* `P98160`\n* `Q02818`\n* `Q06481`\n* `Q08380`\n* `Q12907`\n* `Q13332`\n* `Q14118`\n* `Q14508`\n* `Q14515`\n* `Q15904`\n* `Q16610`\n* `Q6UXB8`\n* `Q7Z3B1`\n* `Q8NBJ4`\n* `Q92520`\n* `Q92823`\n* `Q96KN2`\n* `Q99435`\n* `Q99674`\n* `Q9BY67`\n* `Q9NQ79`\n* `Q9NYU2`\n* `Q9UHG2`\n* `P01594`\n* `Q13449`\n* `Q99829`\n\nThere are some proteins that are weak correlates _only_ to `updrs_4`. These are:\n\n* `P00746`\n* `P02749`\n* `P02774`\n* `P04211`\n* `P04217`\n* `P05155`\n* `P06681`\n* `P19827`\n* `P20774`\n* `P31997`\n* `P61626`\n* `Q96BZ4`\n* `Q96PD5`\n\nThe challenge is going to be how we use this knowledge. Our correlation analysis only worked because we were able to ignore values that were missing. For machine learning regression to work, we'll need to satisfy one of the following conditions to use the data:\n\n* Have complete records for every protein type for every visit.\n* Figure out a way of imputing missing values.\n* Use a machine learning algorithm that implicitly handles missing data.\n\nLet's take a look at how our proteins appear across visits. Specifically, we know that there are are 2,615 unique visits. The question is how much of each protein we see given the total number of visits we have. If we only see a protein three or four times, even if it is correlated with an UPDRS score, it's likely not going to help out too much."
"It appears that for all of the protein data, proteins measurement data exists for at most 40% of the visits we have on record. This is going to be somewhat problematic to track trends. The instances where we don't have measurements for a specific protein are going to overwhelm examples where we do have protein measurements, creating a confounding effect. Even more problematic is the visit months where those measurements come from. "
We should also look at it in terms of patients at each month. What percentage of patients are lacking protein data? Do we have representation of proteins at every month for at least some of our patients?
"As we can see, at months 3, 9, 18, 30, 42, 54, and 96, we are lacking protein data for nearly all of the patients in the study."
"# 3 - Research Papers\n\nThere is a body of real-world research that may provide some targeted insights into the data that we have at hand.\n\n# 3.1 - Cerebrospinal Fluid Peptides as Potential Parkinson Disease Biomarkers (2015)\n\nThe research by Shi et al (2015) looks specifically at CSF proteins and peptides that are potential indicators of Parkinson's Disease. Of those identified, the following are available in the training data that we have available:\n\n* Proteins:\n    * `P00450` - Ceruloplasmin (CP)\n    * `P07333` - Macrophage colony-stimulating factor 1 receptor (CSF1R)\n    * `P10451` - Osteopontin (SPP1)\n    * `P01033` - Metalloproteinase inhibitor 1 (TIMP1)\n    * `P01008` - Antithrombin-III (SERPINC1)\n    * `P02647` - Apolipoprotein A-I (APOA1)\n    * `P01024` - Complement C3 (C3)\n    * `Q92876` - Kallikrein-6 (KLK6)\n* Peptides:\n    * `GAYPLSIEPIGVR` - associated with protein Ceruloplasmin (CP)\n    * `EPGLC(UniMod_4)TWQSLR` - associated with protein Metalloproteinase inhibitor 1 (TIMP1)\n    * `WQEEMELYR` - associated with protein Apolipoprotein A-I (APOA1)\n    * `QPSSAFAAFVK` - associated with protein Complement C3 (C3)\n    * `GLVSWGNIPC(UniMod_4)GSK` - associated with protein Kallikrein-6 (KLK6)\n    \nWe should check to see how these protein levels impact UPDRS scores. Let's start by checking correlation of these peptide and protein levels against various UPDRS scores."
"Some interesting observations here. None of the peptides or proteins have any significant positive or negative correlation to UPDRS scores except for:\n\n* Protein `P07333` has a weak negative correlation to `updrs_3`\n* Peptide `GLVSWGNIPC(UniMod_4)GSK` has a weak negative correlation to `updrs_1`. \n\nThese observations fall in line with the findings from the paper. In brief, the authors noted that no single protein or peptide in isolation had a clear correlation to UPDRS scores - it was only in combination with other peptides and proteins that a stronger signal was present. The paper outlines a 2-peptide model that had strong correlations with overall disease severity. Unfortunately, the training dataset does not have the correct combination of proteins and peptides as described in the paper. Nonetheless, these two findings above may be enough to provide minimal lift."
"# 4.2 - Constant UPDRS 4\n\nOverall, our model currently cannot learn enough information about UPDRS 4 due to missing and null values. A simple way to handle this is to zero it out. This isn't very good from a clinical standpoint, but it will likely generate lift for us, since we won't get penalized for the many times we're likely to predict a value where there should be none. For example, see discussion [here](https://www.kaggle.com/competitions/amp-parkinsons-disease-progression-prediction/discussion/393104)."
"As we can see, simply setting the UPDRS 4 value to 0 improves our SMAPE score. There is obviously room here to make improvements, especially if we could leverage information to make better UPDRS 4 predictions. For models moving forward, we'll continue to use the zeroed out UPDRS score."
"# 4.3 - Supplemental Data\n\nIf we are going to continue to ignore protein and peptide data, then we can add in the additional clinical data to the mix. This is likely to generate lift for us, since we give the regressor more information to work with."
We see a little improvement over using the clinical data alone. This is likely going to help us in instances where the hidden test data contains no protein information.
"# 4.4 - Medication State\n\nWhile the medication state of a patient isn't available on our test data, there may still be some interest in it, as we saw that the medication state did have a direct impact on UPDRS scores over time. Let's take a look and see if it could potentially provide lift if we had it."
"We can see that there is lift to our model if we take into account medication state. This suggests that maybe a classifier capable of soft-labeling medication state may be applicable, assuming there is signal in the protein data to provide context for the classifier to work with."
"# 4.5 - Protein Data\n\nUp until now, we haven't looked at all at protein or peptide information. Let's see what happens when we add in raw numbers."
# 4.x - Model Comparisons\n\nWe can compare the performance of each of our models to see which one performs the best. The red dashed line indicates baseline performance that we are looking to beat.
"# 5 - Conclusions\n\nThere are several conclusions based on our observations:\n\n* The size of the dataset means that memory pressure will likely not be too great. \n* There are a total of 248 patients in the clinical data, and 771 patients in the supplemental data.\n* In terms of missing information:\n    * There are no null entries in the protein or peptide data.\n    * There are null entries in the clinical data and supplemental data.\n        * We cannot assume that null values indicate a 0 value for features such as UPDRS assessment parts, as values of 0 indicate a ""normal"" response.\n        * Null values in the medication state of the patient cannot be assumed to be either `On` or `Off`.\n* Duplicated data appears very infrequently, and is unlikely to impact machine learning models if not explicitly filtered out.\n* Clinical data appears to have a greater range in `visit_month` when compared to supplemental data (0 - 108 months compared to 0 - 36 months).\n    * In general, clinical and supplementary data is very different in terms of data distributions, as confirmed statistically, and through adversarial validation.\n* In reference to research by Shi et al (2015):\n    * The protein and peptide samples obtained do not not occur in the combinations needed to act as clear indicators as disease progression or severity.\n    * Our correlation analysis suggested that protein `P07333` has a weak negative correlation to `updrs_3`, and peptide `GLVSWGNIPC(UniMod_4)GSK` has a weak negative correlation to `updrs_1`."
\n\n##### 5.8| Visualization with AutoViz
\n\n### 6| Statistical ANOVA Test for Feature Selection 
\n\n##### 8.5| ROC AUC
\n\n##### 8.6| Confusion matrix
## PCA Visualizations:
\n### 1-D Visualization:
The plot below displays our three original clusters on the single *principal component* created for 1-D visualization:
\n### 2-D visualization:
The next plot displays the three clusters on the two *principal components* created for 2-D visualization:
\n### 3-D Visualization:
This last plot below displays our clusters on the three *principal components* created for 3-D visualization:
"## PCA Remarks:\n\nAs we can see from the plots above: if you have data that is highly *clusterable*, then PCA is a pretty good way to view the clusters formed on the original data. Also, it would seem that visualizing the clusters is more effective when the clusters are visualized using more principle components, rather than less. For example, the 2-D plot did a better job of providing a clear visual representation of the clusters than the 1-D plot; and the 3-D plot did a better job than the 2-D plot!"
The plot below displays our three original clusters on the single dimension created by T-SNE for 1-D visualization:
\n### 2-D Visualization:
The next plot displays the three clusters on the two dimensions created by T-SNE for 2-D visualization:
This last plot below displays our clusters on the three dimensions created by T-SNE for 3-D visualization:
"## T-SNE Remarks:\n\n\nThe T-SNE algorithm did a fairly decent job in visualizing the clusters, too. But, there were a few noticable differences when comparing it's resulting plots to PCA's resulting plots. \n\nOne major difference between the plots produced by PCA and T-SNE is that T-SNE's plots seemed to have it's clusters overlapping with eachother more so than in PCA's plots. For example, if you look at the [**2-D plot**](#PCA_2D) fomed from PCA, you see three distinct sections of the data-points with strict, visible borders separating each colour into groups. Whereas, if you look at the [**2-D**](#T-SNE_2D) plot formed from T-SNE, you, again, see three sections formed within the data-points, but this time, datapoints between each cluster seem to 'intermingle' and overlap more.\n\nThe other major difference between the plots created by PCA and the plots created by T-SNE, is the shape. Because both PCA and T-SNE perform dimensionality reduction in very different ways (and with different objectives), the resulting shape or distibution of the points produced by the algorithms will almost always be very different.\n\nBear in mind that the plots resulting from the T-SNE algorithm are quite variable, in that they depend very heavily on the value chosen for `perplexity`."
"\n# Decision Trees\nSince we were talking about decision tree based models I wanted to visualize how they work. For that I'm going to choose random forest classifier. RF has multiple decision trees in it, we clas display them all but I think first one should be enough for this notebook. You can learn more about decision tree plotting in this great notebook [here](https://www.kaggle.com/willkoehrsen/visualize-a-decision-tree-w-python-scikit-learn). \n\nYou can see the tree is somewhat matching our feature importances ranking just above, again this is just for gaining some knowledge and you might not need it for the competition...\n\n### [Back To Table of Contents](#toc_section)"
"\n# Feature Selection by Recursive Feature Elimination\n\nAgain a part where my main motivation is learning and applying what I know. Here I'm gonna reduce dimensionality with two sklearn tools. First one is recursive feature elimination (RFE). The goal of RFE is to select features by recursively considering smaller and smaller sets of features. I'm going to prune half of the features we have, again this is just for experience and might not be needed for Titanic competition.\n\n### [Back To Table of Contents](#toc_section)"
"- The code below is edited version of [this example from sklearn official page](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py) I really enjoyed while tinkering it, kudos to creators!"
\n# Plotting Decision Boundaries\n\nI just wanted to visualise how different models act on given 2D data. I can say that good fitting models produced similar results with small differences. I take this as a good theoretical information...\n\n### [Back To Table of Contents](#toc_section)
and plot it:
"Let's color this t-SNE representation according to the churn (blue for loyal customers, and orange for those who churned)."
"We can see that customers who churned are concentrated in a few areas of the lower dimensional feature space.\n\nTo better understand the picture, we can also color it with the remaining binary features: *International Plan* and *Voicemail*. Orange dots here indicate instances that are positive for the corresponding binary feature."
"Now it is clear that, for example, many dissatisfied customers who canceled their subscription are crowded together in one cluster representing the people with the international plan but no voice mail.\n\nFinally, let's note some disadvantages of t-SNE:\n- High computational complexity. The [implementation](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) in `scikit-learn` is unlikely to be feasible in a real task. If you have a large number of samples, you should try [Multicore-TSNE](https://github.com/DmitryUlyanov/Multicore-TSNE) instead.\n- The plot can change a great deal depending on the random seed, which complicates interpretation. [Here](http://distill.pub/2016/misread-tsne/) is a good tutorial on t-SNE. In general, you shouldn't make any far-reaching conclusions based on such graphs because it can equate to plain guessing. Of course, some findings in t-SNE pictures can inspire an idea and be confirmed through more thorough research down the line, but that does not happen very often.\n\nOccasionally, using t-SNE, you can get a really good intuition for the data. The following is a good paper that shows an example of this for handwritten digits: [Visualizing MNIST](https://colah.github.io/posts/2014-10-Visualizing-MNIST/).\n\n"
"### Components of Time Series :\n- A Time Series consists of the following components :\n\n    - **Trend** : Long term direction of the data.\n    \n    E.g : Year on year rising temperature of the Earth due to Global Warming.\n    \n    - **Seasonality** : Short term repetitve patterns of the data due to the weather seasons.\n    \n    E.g : Sale of sweaters specifically in the winter season.\n    \n    - **Cyclic Variations** : Short term repetitive patterns of the data over a period of 1 year.\n    \n    E.g : It usually consists of the Business Quarters i.e Q1, Q2, Q3 & Q4.\n    \n    - **Irregularities** : Random and unforseen fluctuations in the data.\n    \n    E.g : Occurrences of Earthquakes or Floods, etc.\n    \n    \n- **In order to assess a Time Series, we need to consider the above components and make sure that our data is free from all these components in order to make a forecast.** \n\nLet's visualize the **AveragePrice** data for the above components!\n- For this purpose, we use a function **seasonal_decompose** from the **statsmodel** library.\n- This function has a parameter, **model**, that needs to be assigned the value **additive** or **multiplicative**.\n    - **Additive Model** : Data has same width and height of the seasonal patterns or peaks. Trend of the data is linear.\n    - **Multiplicative Model** : Data has increasing / decreasing width and height of the seasonal patterns or peaks. Trend of the data is non-linear.\n\nFrom the visualizations of **AveragePrice** executed in EDA section, we can say that the data represents a **Multiplicative Model**."
- Data clearly has a **non-linear uptrend**.\n- A clear cut **seasonal** pattern is present in the data.\n- The last plot is the **Residual** plot. It is the plot that describes the data if the **trend** and **seasonal** components of the data are completely eliminated.\n- We also need to check the statistical parameters w.r.t time. 
"- For the **AveragePrice** time series data, \n    - **Rolling Mean** is clearly variable with time. It is very close to the data. Thus, it can be a good descriptor of the data.\n    - **Rolling Standard Deviation** is pretty consistent in the initial stages however changes with time are observed in the later stages.\n    - **Test Statistic : (-2.36)** > **Critical Value (5%) : (-2.88)**\n    - **p-value (0.15)** > 0.05\n- Hence, **Null Hypothesis** cannot be rejected and we can conclude that the above **AveragePrice** time series is **not stationary**.\n- In order to eliminate trend, seasonality and make the time series stationary, we will use **differencing** i.e subtracting the previous value from it's next value. "
- We have taken the **log** of the data to deal with **stationarity** and **differencing** is done to handle **trend** and **seasonality**.\n- **Trend** and **Seasonality** of the data have near about died down & their values have been reduced as well.\n- We now check the **stationarity** of the time series.
"- Model fitting is a measure of how well a machine learning model generalizes to similar data to that on which it was trained.\n- After fitting the ARIMA model, we now check this fit and compare this series with the original values of **AveragePrice**.\n- In this process, we store the model fitted values in a series. This series then undergoes the process of cummulative summation i.e opposite of differencing.\n- Result of cummulative summation is the expected log values fitted by the model. These log values are then exponentiated that creates the fitted original series."
- RMSE value between **Fitted AveragePrice** and original **AveragePrice** is **21.83**.\n- This RMSE is slightly high and does creates the possibility of overfitting the model.
- We use the **np.exp()** as we fit the model on log values and hence we unlog the values using exponentiation function.
- Values generated by the **forecast_function** are constant.
"## SARIMA\n### Seasonal Auto Regressive Integrated Moving Average \n\n- **SARIMA** model is an extension of the ARIMA model that can handle the seasonal effects of the data.\n- It has kind of 2 orders **(p,d,q) x (P,D,Q,M)**. \n- **(p,d,q)** is the order that is similar to the order of the **ARIMA** model. \n- **(P,D,Q,M)** is known as the Seasonal Order where **(P,D,Q)** are similar to the **(p,d,q)** of the ARIMA model. \n- It's selection criteria is similar as well with an important condition i.e to handle the seasonality by differencing the data with the frequency of seasonal period or periodicity, **M**. "
"- For our data, it is in **weekly format** and the **seasonal period is of 1 year**.\n- Hence, we difference the already differenced data by a periodicity, **M**, value of 52.\n- We can observe that the seasonality of the data nearabout gone with y-axis values ranging from -0.1 to 0.1.\n- We will check this seasonal differenced data for stationarity."
**HomePlanet and CabinDeck**
"*Notes:*\n* Passengers on decks A, B, C or T came from Europa.\n* Passengers on deck G came from Earth.\n* Passengers on decks D, E or F came from multiple planets."
**HomePlanet and Surname**
**Fantastic!** Everyone with the same surname comes from the same home planet.
**HomePlanet and Destination**
"Most people heading towards TRAPPIST-1e came from Earth so it makes sense to guess they came from there. But remember from earlier, no one on deck D came from Earth so we need to filter these out."
\nThe reason we are filling missing surnames is because we will use surnames later to fill missing values of other features. It also means we can improve the accuracy of the family size feature.
The majority (83%) of groups contain only 1 family. So let's fill missing surnames according to the majority surname in that group.
**CabinSide and Group**
**Another rule!** Everyone in the same group is also on the same cabin side. For cabin deck and cabin number there is also a fairly good (but not perfect) correlation with group.
**CabinNumber and CabinDeck**
There is an interesting pattern here. The cabin_number and group_number share a linear relationship on a deck by deck basis. We can therefore extrapolate the missing cabin numbers using linear regression on a deck by deck basis to get an approximate cabin number.
Let's look at the distribution of the predicted probabilities.
It is interesting to see that the models are either very confident or very unconfident but not much in between.
A right skewed normal distribution with a peak at 1 GB.\n\n**Computing Hardware used in work:**
Basic laptop - Macbook is the most commonly used hardware.\n\n**Data type used in work:**\n\nNow let us check the different types of data people use in work.
Relational data is the most commonly used data type followed by Text data.
So 35% of the people responded that sometimes the ML models go to production. Only 7% of the respondents say that it always go to production.\n\n**Algorithms used in work:**
Logistic Regression is the most commonly used algorithm is work followed by Decision trees still. But it is worthy to note that neural networks occupy the 4th position and is fast catching up.\n\n**Tools used in work:**
Python seems to be the most commonly used tool at work followed by R and SQL.\n\n**Methods used in work:**
Data visualisation is the most common method used in work followed by logistic regressiona dn cross validation.\n\n**Challenges faced in work:**
The top challenges are dirty data followed by lack of data. Company politics comes in third.\n\n**Data Visualizations in work:**
"""None"" category is the lowest of all. So most of the companies are using visualization in atleast some of the projects. \n\n**Usage of Internal and External tools at work:**"
Mostly internal tools are prefereed over external tools.\n\n**Where does DS team sit in work:**\n
Standalone is the top answer followed by IT team.\n\n**Data Storage Models at work:**\n\nNow let us look at the data storage models people use at work.
"Flat files are the most commonly used data storage methodology. \n\n**Data sharing at work:**\n\nAt work, how do people share source data with each other."
Most used method is shared drive / sharepoint followed by email. Let us now check how people share the codes at work.
Github is the popular way of sharing the codes with each other at work.\n\n**Mathematical Understanding of the Algos used in Work:**
Most people feel that they are able to explian the algorithm to someone non technical.\n\n**Usage of different tools at work:**\n\nNow we can have a look at how often different tools are being used by the people at work. Please hoven over the bars to get the actual numbers.
**Job Satisfaction:**\n\nHere comes one more important questions. How satisfied are you with your job? The results are on a scale from 0 (Highly dissatisfied) to 10 (Highly Satisfied). I have removed the people who preferred not to share from the below plot.
The peak occurs at 7 and 8 and most people are fairly satisfied with the job. Now let us check how the job satisfation level varies based on the country. Please bear in mind that different countries have different number of respondents as seen from the previous map plot.
"\n\n**Top Data Science Blogs / Podcasts / Newsletters:**\n\nLet us see the responses for ""What are your top 3 favorite DS blogs / podcass / newsletters?"""
"Kdnuggets is the most popular blog followed by R bloggers.\n\n**Compensation Analysis:**\n\nIn this section, let us take a deeper look at the compensation details. \n\nBefore we delve deep, a quick look into the data revealed that there are 11492 null values out 16716 rows. *So in the following analysis, we are only using the rows where the compensation is provided. So please note that there might be some bias in the following analysis.*\n\nLet us first do a scatter plot to see if there are any outliers."
We could clearly see that there are couple of points which are almost 100B. We can impute them with the median (probably not a good strategy since the currencies are different but since there are only 2 points let us go ahead)  
Looks better now. Also please note that there are multiple currencies in the denomination. So we can check how the distribution is for each currency.
"Most of them are not really visible since some of the currencies like IRR, IDR have very high values.\n\nNow let us convert all of them to USD and then check the scatter plot."
"Now the scale of the y-axis has come down from 1.2B to 10M. \n\nNow let us again look at the box plot by currency / country but this time let us also impute the compensations greater than 500K USD by median value of 53,812 USD for better visualization."
This plot might be affected by some currencies whose count is very less. So let us include only those currencies which occurs more than 30 times and plot this again.
**More to come.Stay tuned.!**
## R Curve
### GT Vs Pred
"### (Loss, Map) Vs Epoch\n"
### Confusion Matrix
"### Please if this kernel is useful, please upvote !!"
"- With the **default parameters, Catboost** get almost**.52 Recall and .72 Roc_Auc**"
"- By using deafult value of **`scale_pos_weight`**, **CatBoost** correctly predicted **almost half of the churned customer**\n- But model couldn't be able to correctly predict the other half."
"- With the adjusted **`scale_pos_weight=3`**, **CatBoost** got  **.84 Recall and .78 Roc_Auc**"
"- By using recommended formula for **`scale_pos_weight`**, **CatBoost** correctly predicted **84% of the churned customers**.\n- As you have seen that, we didn't change any other parameter and get almost 32% lift in our recall score."
"- With the adjusted **`scale_pos_weight=5`**, **CatBoost** got  almost **.92 Recall and .75 Roc_Auc**"
"- By using weight a little bit higher than recommended formula for **`scale_pos_weight = 5`**, **CatBoost** correctly predict almost **92% of the churned customers**.\n- On the other hand we have lost 7% for the precision and 3 points for the Roc_Auc score."
"- With the **default parameters, XGBoost** get **.47 Recall and .68 Roc_Auc**."
"- Bu using deafult value of **`scale_pos_weight`**, **XGBoost** correctly predicted **less than half of the churned customers**. "
"- With the adjusted **`scale_pos_weight`**, **XGBoost** got almost **.67 Recall and .73 Roc_Auc**"
"- Bu using recommended adjusted value of **`scale_pos_weight`**, **XGBoost** correctly predicted almost **67% of the churned customers**."
"- With the adjusted **`scale_pos_weight = 5`**, **XGBoost** got **.74 Recall and .75 Roc_Auc**"
"- When we play around a little bit with the value of  **`scale_pos_weight`**, **XGBoost** correctly predicted almost **74% of the churned customers** by using **scale_pos_weight=5** (Roc_Auc = .75)\n"
"- With the adjusted **`scale_pos_weight`**, **LightGBM** got almost **.54 Recall and .72 Roc_Auc**"
"- Bu using deafult value of **`scale_pos_weight`**, **LightGBM** correctly predicted alittle bit more than half of the churned customers. "
"- With the adjusted **`scale_pos_weight`**, **LightGBM** got **.77 Recall and .76 Roc_Auc**"
"- By using recommended formula for **`scale_pos_weight`**, **LightGBM** correctly predicted almost 77% of the churned customer.\n- As you have observed that, we didn't change any other parameter and get almost 23% lift in our recall score without overfitting.\n- We can improve this score without destroying the Roc_Auc."
"- With the adjusted **`scale_pos_weight = 5`**, **LightGBM** got **.83 Recall and almost .76 Roc_Auc**"
"- By using **`scale_pos_weight =5`**, **LightGBM** correctly predicted **83% of the churned customer**."
White noise chart:
"The process generated by the standard normal distribution is stationary and oscillates around 0 with with deviation of 1. Now, based on this process, we will generate a new one where each subsequent value will depend on the previous one: $x_t = \rho x_{t-1} + e_t$ "
Here is the code to render the plots.
"On the first plot, you can see the same stationary white noise as before. On the second plot with $\rho$ increased to 0.6, wider cycles appeared, but it still appears stationary overall. The third plot deviates even more from the 0 mean but still oscillates about the mean. Finally, with $\rho=1$, we have a random walk process i.e. a non-stationary time series.\n\nThis happens because, after reaching the critical value, the series $x_t = \rho x_{t-1} + e_t$ does not return to its mean value. If we subtract $x_{t-1}$ from both sides, we will get $x_t - x_{t-1} = (\rho - 1) x_{t-1} + e_t$, where the expression on the left is referred to as the first difference. If $\rho=1$, then the first difference gives us stationary white noise $e_t$. This is the main idea behind the [Dickey-Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test) for stationarity of time series (testing the presence of a unit root). If we can get a stationary series from a non-stationary series using the first difference, we call those series integrated of order 1. The null hypothesis of the test is that the time series is non-stationary, which was rejected on the first three plots and finally accepted on the last one. We have to say that the first difference is not always enough to get a stationary series as the process might be integrated of order d, d > 1 (and have multiple unit roots). In such cases, the augmented Dickey-Fuller test is used, which checks multiple lags at once.\n\nWe can fight non-stationarity using different approaches: various order differences, trend and seasonality removal, smoothing, and transformations like Box-Cox or logarithmic."
We can visualize the resulting features.
"Since we now have different scales in our variables, thousands for the lag features and tens for categorical, we need to transform them into same scale for exploring feature importance and, later, regularization. "
Let's look at the averages by hour.
"Finally, let's put all the transformations together in a single function ."
### This column has the ticket number of all the passengers. Here we will be taking the ticket prefix.
### Here is a tricky part. The training set and test set have a few tickets which are unique to themselves.
"I'll use **K-Means** and  **the Elbow Method** to select the number of clusters.\n\nIn the Elbow method, we are actually varying the number of clusters (K) from 1 – 6 in this caase. \n\nFor each value of K, we are calculating WCSS (Within-Cluster Sum of Square). WCSS is the sum of squared distance between each point and the centroid in a cluster. When we plot the WCSS with the K value, the plot looks like an Elbow. As the number of clusters increases, the WCSS value will start to decrease. WCSS value is largest when K = 1. When we analyze the graph we can see that the graph will rapidly change at a point and thus creating an elbow shape. From this point, the graph starts to move almost parallel to the X-axis. The K value corresponding to this point is the optimal K value or an optimal number of clusters.\n\nA good read and source for above description:\n(https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/)"
"We can now **create 3 clusters** within our dataset.\n\nThis can help for further analysis, and aid in **building our understanding even further**"
We can also view how our clusters are distributed geographically...
"# Conclusions\n\nWe see that there are many clear distinctions between happy and unhappy countries - at least at the extremes. \n\nGenerally, happier countries tend to:\n\n- be wealthier\n\n- be less populous\n\n- have fewer children\n\n- be older\n\n- be less corrupt\n\n- be more free\n\n- have a lot of social support\n\nOf course, a lot of these findings may come **as a result of** being wealthy, rather than be a cause of happiness in and of themselves.\n\nWe also know that, generally, the happy stay happy, and the unhappy are getting unhappier. \n\nHowever, as we have seen with Bulgaria - this does not have to be the case. Bulgaria improved by over 1.5 points between 2007 and 2020 - the most of any country. So there is hope that unhappy countries can break the historic trends.\n\nLastly, we clustered the data, using K-means and the elbow method. This enables us again to easily visualise and inderstand the differences between nations, and what features lead to high or low happiness index scores.\n"
"### 3.6 Visualization\n\nLet's create our own visualization of the model built by Prophet. It will comprise the actual values, forecast and confidence intervals.\n\nFirst, we will plot the data for a shorter period of time to make the data points easier to distinguish. Second, we will show the model performance only for the period that we predicted, that is the last 30 days. It seems that these two measures should give us a more legible plot.\n\nThird, we will use `Plotly` to make our chart interactive, which is great for exploring.\n\nWe will define a custom helper function `show_forecast` and call it (for more on how it works please refer to the comments in the code and the [documentation](https://plot.ly/python/)):"
"At first glance, the prediction of the mean values by our model seems to be sensible. The high value of MAPE that we got above may be explained by the fact that the model failed to catch on to increasing peak-to-peak amplitude of weakly seasonality.\n\nAlso, we can conclude from the graph above that many of the actual values lie outside the confidence interval. Prophet may not be suitable for time series with unstable variance, at least when the default settings are used. We will try to fix this by applying a transform to our data."
* It can be observed that the fraction of senior citizen is very less.\n* Most of the senior citizens churn.
"* Most customers churn in the absence of online security, "
* Customers with Paperless Billing are most likely to churn.
* Customers with no TechSupport are most likely to migrate to another service provider.
"* Very small fraction of customers don't have a phone service and out of that, 1/3rd Customers are more likely to churn."
* Customers with higher Monthly Charges are also more likely to churn
* New customers are more likely to churn
___
"Experience level, Salary, Job title"
Word Cloud of Subject Titles
\n## 7. Ask a question and solve it 💡 
Q3. How has the average salary of data science jobs change over time?\n
🎈 Observations: The average salaries increased from approximately USD96000 in 2020 to USD100000 in 2021 and finally USD125000 in 2022. This shows a positive trend and shows that data science jobs are becoming more valuable as the years pass.
"Q4. If you work full-time, is the salary higher than other types?"
"🎈 Observations: Though Data science jobs that are full time based have a higher average salary than the ones that are part time or freelance, the data science jobs that are contract based have the highest average salary.\n"
 Q5. What are the salaries of Data Science jobs based on experience level?
🎈 Observations: The entry level data science jobs have much lower average salaries than the Expert levels job.
"Q6: Working in large companies, the salary is higher than other types of companies?"
"🎈 Observations: Data science jobs in larger sized company have a higher average salary than medium sized and small companies. This does not necessarily imply that larger companies will always guarantee a higher salary on average. For example, within the company there will be varying levels of expertise and hence differing salary."
## How dense are our clusters?
"### Take-Away\n* The clusters differ a lot in their densities\n* Cluster 5 has the highest desity, which makes sense when we look at the produkt-types - beverages.  Products of this group all have in common that they contain almost no fat, proteins and salt. Only the sugar component differs due to sweet drinks like soda and unsweetened drinks like water. For this reason the sugar component has the lowest density.\n* Cluster 6 hast the lowest density. That makes sense as cluster 6 contains the most outlier products. For this reason it's the most mixed cluster of all - the product-types differ a lot and so do their nutritions.\n* There are also some other clusters with a quite high or low density:\n    * Cluster 12 has low density in carbohydrates, sugars, non-sugar-carbohydrates, salt and the g_sum. It contains candy, fruits and sauces and is of mixed type as well. As cluster 6 it contains many outliers that causes the low density.\n    * Cluster 8 and 19 have high densities in almost every feature. If we compare with the word-clouds we can see that they are very homogenous: milk & joghurts cluster and juices cluster."
"* In the energy-densities you can see a lot of different densities as well.\n* Cluster 5 and 9 have the highest densities. Cluster 5 (beverage) also has the highest density in its other features, but cluster 9 (pasta) also has a very high density - only sugars are a bit less dense.\n* Cluster 12, 6 and 17 have the lowest densities. Cluster 12 and 6 are outlier clusters. Consequently it makes sense that they are of low density. Interestingly cluster 17 (sauces & dressings) is of low density es well. It seems to hold salt outliers, as this feature is its most less dense of all features. "
## Are there some clusters with high uncertainty?\n\nPerhaps we have some cluster with more uncertain assigments than others. And perhaps similar clusters are both more uncertain. Let's find it out!
We can't find an obvious pattern!
### Certainty 3D Scatterplot\n\nJust to obtain an impression how uncertainty of cluster assignments are looking in the feature space:
"# Anomaly detection \n\nThe last and perhaps the most important part of the app is anomaly detection. But first of all, what is meant by an anomaly in the view of a gaussian mixture model? Before we start, we should be certain about the answer. Let's recap the formulation of the Gaussian Mixture Model:\n\n$$ p(x) = \sum_{k=1}^{K} \pi_{k} \cdot N(x|\mu_{k}, \Sigma_{k})$$\n\nThis sum yields a so called probability density function. If you are not very familiar with measure theory you are in good company. If we assume that all data spots are drawn randomly from that density distribution and finally take the natural log, we would end up with the following formula:\n\n$$ \ln p(x) = \sum_{n=1}^{N} ln \sum_{k=1}^{K} \pi_{k} \cdot N(x_{n}|\mu_{k}, \Sigma_{k}) $$\n\nThe stuff we need to score each data spot is now given by:\n\n$$ \ln p_{n}(x) = \ln \sum_{k=1}^{K} \pi_{k} \cdot N(x_{n}|\mu_{k}, \Sigma_{k}) $$\n\nThis is what we obtain by calling score samples of scikit-learns gaussian mixture model. But what does it tell us? We think that it is a measure how dense the region is where data spot $x_{n}$ is located. Consequently in low dense regions of our probability density function this would yield a low number. Let's go further - what can be detected as anomaly in low dense regions?\n\n* outliers that could be errors\n* seldom products\n\nWe should keep in mind that there is one kind of error we will not detect as anomaly: Imagine a user typed in nutrition table information that contains flips between features or a wrong product name. This kind of data spot can be located in dense regions and perfectly match with some cluster. Consequently we would not detect it as an anomaly and we can't find it as an error! "
## Is the percentage of anomalies dependent on the cluster?
"We found that some clusters have a very high percentage of anomalies or are completely occupied by them. We have already seen that cluster 6 has the tendency to hold all outliers. Consequently it's not strange that it mainly consists of data spots in low dense regions. Let's try to find out, how certain the cluster assignment of anomalies is and how they look like in some example clusters."
## How certain is our model about its anomalies in cluster assignment ?
"### Take-Away\n\n* We can find some very interesting patterns: In cluster 5 (water), 6 (mixed outlier cluster), 12 (sweets & sauces) and 17 (oils and cream dressings) the model is very certain in its cluster assignments in most of the anomaly cases! \n* In contrast cluster 3 (chocolate & cookies) and 1 (potatoes and beans) the model is very uncertain (below 0.6) about the cluster assignments of the majority of its anomalies. \n* Looking at all clusters we can see that most of them spread widely in their certainties."
## How do anomalies of normal clusters look like?
"### Take-Away\n\n* By looking at the most common product names of anomalies per cluster we can see different kinds:\n    *  We can find **outlier products like blackeye peas, pumpernickel, puffed grain, products without gluten**\n    * But we can also find **seldom products** that do not fit to their clusters like **sugarfree products** in pasta cluster for example\n* Besides these outliers there are probably user errors of the data type-in input process as well. We don't know! "
"### Check for any correlations between features\n![image](http://flowingdata.com/wp-content/uploads/2011/07/Cancer-causes-cell-phones-625x203.png)\nTo quantify the linear relationship between the features, I will now create a correlation matrix. \n \nThe correlation matrix is identical to a covariance matrix computed from standardized data. The correlation matrix is a square matrix that contains the Pearson product-moment correlation coefficients (often abbreviated as [Pearson's r](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)), which measure the linear dependence between pairs of features:\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/602e9087d7a3c4de443b86c734d7434ae12890bc)\nPearson's correlation coefficient can simply be calculated as the covariance between two features x and y (numerator) divided by the product of their standard deviations (denominator):\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/f76ccfa7c2ed7f5b085115086107bbe25d329cec)\nThe covariance between standardized features is in fact equal to their linear correlation coefficient.\nUse NumPy's corrcoef and seaborn's heatmap functions to plot the correlation matrix array as a heat map.\n\nTo fit a linear regression model, we are interested in those features that have a high correlation with our target variable. So, I will make a zoom in these features in order of their correlation with SalePrice."
"We can see that our target variable SalePrice shows the largest correlation with the OverallQual variable (0.79), followed by.  GrLivArea (0.71). This seems to make sense, since in fact we expect the overall quality and size of the living area to have a greater influence on our value judgments about a property.\n\nFrom the graph above, it also becomes clear the multicollinearity is an issue. \n - The correlation between GarageCars and GarageArea is very high (0.88), and has very close correlation with the SalePrice. \n - From total square feet of basement area (TotalBsmtSF) and first Floor square feet (1stFlrSF), we found 0.81 of correlation and same correlation with sale price (0.61).\n - Original construction date (YearBuilt) has a little more correlation with price (0.52) than GarageYrBlt (0.49), and a high correlation between them (0.83)\n - 0.83 is the correlation between total rooms above grade not include bathrooms (TotRmsAbvGrd) and GrLivArea, but TotRmsAbvGrd has only 0.51 of correlation with sale price.\n \nLet's see their distributions and type of relation curve between the 10th features with largest correlation with sales price,"
"As you can see, we were able at first to bring most the numerical values closer to normal. Maybe you're not satisfied with the results of MiscVal and Kitchener and want to understand if we really need to continue to transform some discrete data. So, let's take a look at the QQ test of these features."
"As you have seen, really MiscVal and Kitchener really do not seem to be good results, especially MiscVal, but it is a fact that both variables do not look good indifferent to their distribution. \n\nAs for the other discrete variables, in addition to having presented significant improvements, they also pass the QQ test and present interesting distributions as we can observe in their respective graphs.\n\nSo, we can continue to apply the BoxCox on this features and leave to feature selection algorithms to decide if we continue with some of then or not."
### Evaluate Apply Polynomials by Region Plots on the more Correlated Features 
"#### Evaluating Polynomials Options Performance\nOne way to account for the violation of linearity assumption is to use a polynomial regression model by adding polynomial terms.\n![image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQQQIQ1HTrA1PzE7sw5CwOiV3XWhKXz-rGLj7FMmxYZO_CsU1Iz)\nAlthough we can use polynomial regression to model a nonlinear relationship, it is still considered a multiple linear regression model because of the linear regression coefficients w.\n\nMoreover, as we have seen, some of our features are better when interacting with each other than with just observed ones, but some have a negative effect.\n\nSo, let's check it more carefully."
![image](http://vignette1.wikia.nocookie.net/disney/images/e/e5/Asf.gif/revision/latest?cb=20160317185039)\n\nLet's take a look at the graphs of some of the interactions of the selected features:
"As expected, we can see that prices grow with the growth of the built area, although the reform does not seem to contribute higher prices, in fact we have to remember that if a house went through renovation it is indeed old enough to have needed, and New homes tend to be more expensive. If you wish, switch from Remod to IsNew and see for yourself.\n\nSomething similar can be seen in relation to the total points segmented by the external condition, while we see that prices grow with the growth of the points, we see that although the external condition presents a small positive coefficient, the graph may be suggesting something different, but note that level 3 stands out at the beginning and around the mean, which would explain a small positive coefficient.\n\nAlso see that more important than basement conditions is its purpose in itself. Basements with living conditions present higher prices, curiously unfinished ones too, perhaps because they get the new owners to make them what they want.\n\nAs for the lot multiplied by the slope, as we already know we see the trend of price increase with lot size, but much variation, since other aspects influence the price as well as the slope itself.\n\nFinally, the total of extra points, there is nothing new when we see that the bigger the better, segmented by the format of the lot, we see that the more regular the better it is, but if the terrain is unregulated the extra high score will not work.\n\nThis is the beauty of linear models, even with many features it is possible to understand them when evaluating their coefficients, significance and graphs , as we can see how certain variables present noise and its influence on variance and bias."
"#### Sequential feature selection\n\n**Sequential feature selection algorithms** are a family of **greedy search algorithms** that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d. The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem to improve computational efficiency or reduce the generalization error of the model by removing irrelevant features or noise, ***which can be useful for algorithms that don't support regularization***.\n\nGreedy algorithms make locally optimal choices at each stage of a combinatorial search problem and generally yield a suboptimal solution to the problem in contrast to exhaustive search algorithms, which evaluate all possible combinations and are guaranteed to find the optimal solution. However, in practice, an exhaustive search is often computationally not feasible, whereas greedy algorithms allow for a less complex, computationally more efficient solution.\n\nAs you saw in the previous topic, RFE is computationally less complex using the feature weight coefficients (e.g., linear models) or feature importance (tree-based algorithms) to eliminate features recursively, whereas SFSs eliminate (or add) features based on a user-defined classifier/regression performance metric.\n\nThe SBS aims to reduce the dimensionality of the initial feature subspace with a minimum decay in performance of the regressor or classifier to improve upon computational efficiency. In certain cases, SBS can even improve the predictive power of the model if a model suffers from overfitting.\n\nSBS sequentially removes features from the full feature subset until the new feature subspace contains the desired number of features. In order to determine which feature is to be removed at each stage, we need to define criterion function J that we want to minimize. The criterion calculated by the criterion function can simply be the difference in performance of the classifier after and before the removal of a particular feature. Then the feature to be removed at each stage can simply be defined as the feature that maximizes this criterion.\n\nSo, let's see a example of SBS in our data, "
"As you saw, the SBS is straightforward code to understand, but is computationally expensive. In a nutshell, SFAs remove or add one feature at the time based on the classifier or regressior performance until a feature subset of the desired size k is reached. There are 4 different flavors of SFAs available via the SequentialFeatureSelector from [mlxtend](https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/):\n- Sequential Forward Selection (SFS)\n- Sequential Backward Selection (SBS)\n- Sequential Forward Floating Selection (SFFS)\n- Sequential Backward Floating Selection (SBFS)\n\nThe next code use the SBS from the mlxten. It has interest features to explore, but is more computationally expensive than previous code, so, take care if you try running it."
"SBS is actually computationally expensive, but also generated models with better performance when we go through the hyper parameterization phase."
"#### Univariate feature selection\nOn scikit-learn we find variety of implementation oriented to regression tasks to select features according to the [k highest scores](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html), see below some of that:\n- [f_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif) The Pearson's Correlation are covert to F score then to a p-value. So, the selection is based on the F-value between label/feature for regression tasks.\n- [mutual_info_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression) estimate mutual information for a continuous target variable. The MI between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency. The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n![image](https://blogradiusagent.files.wordpress.com/2018/07/tenor.gif?w=770)\n\nThe methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.\n\nOther important point is if you use sparse data, for example if we continue consider hot-encode of some categorical data with largest number of distinct values, mutual_info_regression will deal with the data without making it dense.\n\nLet's see the SelectKBest of f_regression and mutual_info_regression for our data:"
"### Select Features by Embedded Methods\nIn addition to the return of the performance itself, some models has in their internal process some step to features select that best fit their proposal, and returns the features importance too. Thus, they provide two straightforward methods for feature selection and combine the qualities' of filter and wrapper methods. \n![image](https://paulbromford.files.wordpress.com/2018/02/c88e5e569aa7b412bff3f848ec9f7c53.gif)\nSome of the most popular examples of these methods are LASSO, RIDGE, SVM, Regularized trees, Memetic algorithm, and Random multinomial logit.\n\nIn the case of Random Forest, some other models base on trees, we have two basic approaches implemented in the packages:\n1. Gini/Entropy Importance or Mean Decrease in Impurity (MDI)\n2. Permutation Importance or Mean Decrease in Accuracy \n3. Permutation with Shadow Features\n4. Gradient Boosting\n\nOthers models has concerns om **multicollinearity** problem and adding additional **constraints** or **penalty** to **regularize**. When there are multiple correlated features, as is the case with very many real life datasets, the model becomes unstable, meaning that small changes in the data can cause large changes in the model, making model interpretation very difficult on the regularization terms. \n\nThis applies to regression models like LASSO and RIDGE. In classifier cases, you can use 'SGDClassifier' where you can set the loss parameter to 'log' for Logistic Regression or 'hinge' for 'SVM'. In 'SGDClassifier' you can set the penalty to either of 'l1', 'l2' or 'elasticnet' which is a combination of both.\n\nLet's start with more details and examples:"
"### Residuals Plots\nThe plot of differences or vertical distances between the actual and predicted values. Commonly used graphical analysis for diagnosing regression models to detect nonlinearity and outliers, and to check if the errors are randomly distributed.\n![image.png](https://i1.wp.com/condor.depaul.edu/sjost/it223/documents/resid-plots.gif)\nSome points for help you in your analysis:\n- Since `Residual = Observed – Predicted` ***positive values*** for the residual (on the y-axis) mean the ***prediction was too low***, and ***negative values*** mean the ***prediction was too high***; 0 means the guess was exactly correct.\n\n- ***They're pretty symmetrically distributed, tending to cluster towards the middle of the plot.***\n\n    For a good regression model, we would expect that the errors are randomly distributed and the residuals should be randomly scattered around the centerline. \n    \n- ***Detect outliers, which are represented by the points with a large deviation from the centerline.***\n\n    Now, you might be wondering how large a residual has to be before a data point should be flagged as being an outlier. The answer is not straightforward, since the magnitude of the residuals depends on the units of the response variable. That is, if your measurements are made in pounds, then the units of the residuals are in pounds. And, if your measurements are made in inches, then the units of the residuals are in inches. Therefore, there is no one ""rule of thumb"" that we can define to flag a residual as being exceptionally unusual.\n\n    There's a solution to this problem. We can make the residuals **unitless**by dividing them by their standard deviation. In this way we create what are called **standardized residuals**. They tell us how many standard deviations above — if positive — or below — if negative — a data point is from the estimated regression line. \n    \n- ***They're clustered around the lower single digits of the y-axis (e.g., 0.5 or 1.5, not 30 or 150).***\n\n    Again, doesn't exist a unique rule for all cases. But, recall that the empirical rule tells us that, for data that are normally distributed, 95% of the measurements fall within 2 standard deviations of the mean. Therefore, any observations with a standardized residual greater than 2 or smaller than -2 might be flagged for further investigation. It is important to note that by using this ""greater than 2, smaller than -2 rule,"" approximately 5% of the measurements in a data set will be flagged even though they are perfectly fine. It is in your best interest not to treat this rule of thumb as a cut-and-dried, believe-it-to-the-bone, hard-and fast rule! So, in most cases it may be more practical to investigate further any observations with a standardized residual greater than 3 or smaller than -3. Using the empirical rule we would expect only 0.2% of observations to fall into this category.\n    \n- ***If we see patterns in a residual plot, it means that our model is unable to capture some explanatory information.***\n\n    A special case is  any systematic (non-random) pattern. It is sufficient to suggest that the regression function is not linear. For example, if the residuals depart from 0 in some systematic manner, such as being positive for small x values, negative for medium x values, and positive again for large x values. \n    \n- ***Non-constant error variance shows up on a residuals vs. fits (or predictor) plot in any of the following ways:***\n    - The plot has a ""fanning"" effect. That is, the residuals are close to 0 for small x values and are more spread out for large x values.\n    - The plot has a ""funneling"" effect. That is, the residuals are spread out for small x values and close to 0 for large x values.\n    - Or, the spread of the residuals in the residuals vs. fits plot varies in some complex fashion."
"### Model Hiperparametrization\n#### Lasso (Least Absolute Shrinkage and Selection Operator)\n[Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) was introduced in order to improve the prediction accuracy and interpretability of regression models by include a with L1 prior as regularizer and altering the model fitting process to select only a subset of the provided covariates for use in the final model rather than using all of them. [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)) was originally formulated for least squares models and this simple case reveals a substantial amount about the behavior of the estimator, including its relationship to ridge regression and best subset selection and the connections between Lasso coefficient estimates and so-called soft thresholding. It also reveals that the coefficient estimates need not be unique if covariates are collinear.\n\nPrior to lasso, the most widely used method for choosing which covariates to include was stepwise selection, which only improves prediction accuracy in certain cases, such as when only a few covariates have a strong relationship with the outcome. However, in other cases, it can make prediction error worse. Also, at the time, ridge regression was the most popular technique for improving prediction accuracy. Ridge regression improves prediction error by shrinking large regression coefficients in order to reduce overfitting, but it does not perform covariate selection and therefore does not help to make the model more interpretable.\n\nLasso is able to achieve both of these goals by forcing the sum of the absolute value of the regression coefficients to be less than a fixed value, which depending on the regularization strength, certain weights can become zero, which makes the Lasso also useful as a supervised feature selection technique, by effectively choosing a simpler model that does not include those coefficients. However, a limitation of the Lasso is that it selects at most n variables if m > n.\n\nThis idea is similar to ridge regression, in which the sum of the squares of the coefficients is forced to be less than a fixed value, though in the case of ridge regression, this only shrinks the size of the coefficients, it does not set any of them to zero.\n\nThe optimization objective for Lasso is: `(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1`\n\nTechnically the Lasso model is optimizing the same objective function as the Elastic Net with `l1_ratio=1.0`,  no L2 penalty.\n\nThis characteristics turn Lasso a interesting alternative approach that can lead to sparse models.\n\nFrom sklearn its most important parameters are: \n- alpha: Constant that multiplies the L1 term. Defaults to 1.0. alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using alpha = 0 with the Lasso object is not advised. Given this, you should use the LinearRegression object.\n- max_iter: The maximum number of iterations\n- selection: If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.\n- tol: The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.\n\nSo, I run many times my model, with different parameters, selection features, reduction or not and with and without log1P transformation of Sales Price. Below I preserve the code with the best options and with few possibilities for you can see the grid search cv in action, but I encourage you to make changes and see for yourself. "
### Network Graph
## Exploring the Patient Number Columns
## Exploring the Ethinicity Column
## HeatMap for hubmap
# 6. Visualising Images : Tiff 🗺️ 
Credit goes to https://www.kaggle.com/iafoss/256x256-images/data
# 8. Plot polygon from JSON Files
## Necessary Imports
![Pytorch](https://miro.medium.com/max/1200/1*4br4WmxNo0jkcsY796jGDQ.jpeg)
"Follow-up on the previous correlation plot. This plot displays that these outliers are not very strongly represented. Indeed, the average count for the **LOW QUADRANT**, as labeled at the bottom left of the plot, is only 2.3. For these reasons, hyper negative reviews may be unrepresentative outliers, and not taken as the public's general opinion. \n\nA practise I could envision tackling this problem is to include the average rating of the product class, such as ""Dress"", in order to relieve customers who may be worried about product with low, hyper negative reviews.\n\n***\n**Correlating Average Rating and Recommended IND by Class Name** \n- [Stackoverflow Annotating Outliers](https://stackoverflow.com/questions/43010462/annotate-outliers-on-seaborn-jointplot)"
**Interpretation:** \nFor the various Class cateogries there a notable correlation between average age and recommendation likelihood. I shall investigate.
"**Interpretation:** \nCheck out my [**Other Kernel: In-Depth Simple Linear Regression\n**](https://www.kaggle.com/nicapotato/in-depth-simple-linear-regression) for a deep dive into this regression.\n\n***\n\n## 5.  Working with Text \n\nNow that a general understanding of the variables have been laid out, I will begin to analysis the customer reviews.\n\n### 5.1 Text Pe-Processing"
"Evidently, the text data requires further processing ."
"**Code Explanation:** \nThis chunk of code creates a function that takes each review and combines them into one seamless text. It then applies lowercase, tokenizer, removes stopwords and punctuation, and finally uses the PorterStemmer.\n\n***\n\n**Interpretation:** \nIn order to process the data set's centerpiece, the review body, I utilized the NLTK package to lowercase, tokenize, and remove stopwords and punctuation. Tokenizing treats each word as its own value, while the other steps gets rid of the noise and irrelevant symbols in the data, standardizing the reviews for analysis. Upon reviewing the performance of text analysis, I decided to implement the Porter Stemmer on the tokens in order to combine words with tense and plurality deviance. I contemplated exploring the use of sequential models, such as Long Short-term memory, which would benefit from stop words, but unfortunately I could only find predictive applications of it, no insight extracting aspects. \n\nThe last piece of data transformation conducted was to bin the continuous variable age into a categorical variable: age category.\n\n***\n\n### 5.2 Sentiment Analysis \n\nMy first attempt at understanding the customer reviews is to see how the textual sentiment relates to the rating scores. With this method, it will be possible to distinguish outright positive and negative comments from the constructive variant.\n\nI will also explore the interaction between sentiment score:\n- Raiting\n- Recommended\n- Positive Feedback Count"
"#### **Code Explanation:** \n*Pre-processing* chunk loads the NLTK Sentiment Intensity Analyzer module, selects desired variables, and finally applies lowercasing to the column of reviews in the dataframe. The second paragraph of code *Applying Model and Variable Creation* classifies each review in the dataset on three dimensions: Positive, Neutral, and Negative. These results are stored in three respective columns. The overall sentiment is then determined and stored in the Sentiment column.\n\n- **Neutral/Negative/Positive Score:** Indicates the potency of these classes between 0 and 1. Onl\n- **Polarity Score:** Measures the difference between the Positive/Neutral/Negative values, where a positive numbers closer to 1 indicates overwhelming positivity, and a negative number closer to -1 indicates overwhelming negativity.\n\n***\n\n**Normalize Plots for Sentiment Distribution**"
**Interpretation:** \nRecommended is a variable that clearly indicates positive sentiment in the review.
"**Code Interpretation:** \nThe last chunk, Visualization, plots the frequency of sentiments in a bar plot using matplotlib.\n\n**Interpretation:** \nLike the distribution of rating, most reviews have a positive sentiment. Unlike the distribution of rating, there is a lower occurrence of neutral rating is lower in proportion to the occurrence of medium ranged ratings.\n\nThe plot on the bottom right tells and interesting story. The rating of positive sentiment reviews have an increasing occurrence as the rating gets higher. But, but negative and neutral sentiment reviews, the highest occurrence rating has 3 rating, further emphasizing that people's motivation of assigning a review score of three are multiple.\n\n***"
"**How to Interpret:** \nIn this plot, the upper and lower rows use the same variables, but the upper row is for non-recommended reviews, while the bottom row is for recommended reviews. This enables use to explore the nature of recommended reviews in terms of the mood of the writing, as well as the rating assigned by the customer.\n\n**Interpretation:** \nWhile the distribution of departments does not seem to change depending on status of recommendation, rating is almost entirely inverted. My previous theory that recommended reviews hold more criticizing  weight does not hold up in this case since recommended reviews have a highly positive sentiment occurrence."
"**Interpretation:** \nInterestingly, there appears to be a substantial negative correlation between Positive Feedback Count and Positive Score, which suggests that the most acclaimed reviews on the platform are probably in the form on constructive criticism, rather than outright positivity.\n***\n\n## 6. Word Distribution and Word Cloud  \n\n** For this section, I deviated from the book and heavily relied upon the following online resources:** \n- [Kaggle Longdoa: Word Cloud in Python](https://www.kaggle.com/longdoan/word-cloud-with-python)\n- [Word Cloud Package Forum](https://github.com/amueller/word_cloud/issues/134)\n- [Amueller Github](https://amueller.github.io/word_cloud/auto_examples/masked.html)"
"#### **Code Explanation:** \nThis code creates the word cloud visualization function. This function’s mathematical processes are hidden, since it does not explicitly state that it determines the frequency occurrence of each word in relation to the entire dictionary of words. Within the function, the Setting Function Parameter section creates the graphic structure using matplotlib. Then the text is formatted, and the word frequency is determined. Finally, the matplotlib structure is filled with words, where the larger the word size, the higher the word occurrence. "
### Visualize Reviews
"#### **Code Interpretation:** \nThe central flaw of these word clouds is that they only show the distribution of individual words. This removes the context of the word, as well as disregard negative prefixes. In order to solve this problem I will utilize n-grams, which increases the size of observed values from one word to multiple words, enabling frequency counts to be conducted to word sequences. Although I would have prefered to visualize these findings through the use of Word Clouds, I was unable to program this in, thus leaving me with a simple table."
**Taking a Different Lense: WordClouds by Department Name** 
"***\n## 7. N Grams by Recommended Feature\n\n\nAt this point, fit and product inconsistency strongly emerge as major topics in the reviews. From this information, I can infer that the dataset belongs to a online retailer, since brick and mortar stores have changing rooms to prevent this problem. The central themes in the product reviews brought to light by the n-grams are:\n- **Fit:** Whether the product’s advertised size actually corresponds to customer size and height.\n- **Love or Hate:** The customer's personal feelings towards the product.\n- **Complements:** The customer's social experience wearing the product.\n- **Product consistency:** Whether the product appears as advertised, lives up to quality expectations."
"**Interpretation:** \nIn the negative reviews, customers express their disappointment in the product, stating that they “really wanted to love” the item. This signifies that the product did not live up to the customers expectations. This occurred for multiple reasons. “Order wear size” and “Usual wear size” suggest that the fit did not suit their typical universal body size. Perhaps if better product dimension information could be provided, then the likelihood of this negative response could decrease. Furthermore, perhaps the product platform could track the user’s size through previous purchase in order to warn customer for potential size conflict.\nAnother form of negative review is in the disappointment in the product turnout. “Too much fabric” and “Looks nothing like” suggest inconsistency with online retail presentation and actual product. These reviews are especially destructive, since they damage the reputation of the store product quality, which is a online platforms biggest asset.\nOn the other hand, positive reviews are void of criticism, and are preoccupied with confirming fit and sharing social experience with the clothing. “True Size”, “Fit Perfectly”, “Fit like a glove”, on top of the multiple 2-grams with customer’s height suggest that a large part of positive reviews are employed to confirm product fit according to certain size. The high occurrence of this review suggest that height and size is usually a big issue, which this retail managed to consistently satisfy.\n“Received many compliments”, “Look forward to wearing”, “Everytime I wear”, “Looks great with jeans” are all comments which reflect the customer's experience wearing the product out in public. This not only express the relevance of trendy, jaw dropping fashion for customers in a social context, but also suggests that the product review are a highly social space, in which customers not only talk with the retailer, but with the other customers as well.\n\n***\n\n## 8. Intelligible Supervised Learning\n\nSupervised learning requires features (independent variable) and a label (dependent variable).  The Formatting section does just this by creating a tuple with the comment and customer rating label. Currently the independent variable is the entire comment. However, in order to the Naïve Bayes Algorithm to work, each word must be treated as a variable. Instead of utilizing sequential words, the model notes which words are present out of the entire dictionary of words available in the comments corpus. In order to reduce computational intensity, only the top 5000 most common words will be considered, instead of the 9000 unique words in the corpus. The find_features function does just this by checking the presence of words for a piece of text against word_features, a variable created earlier which includes the top 5000 most common words used by customers in this dataset. The Apply Function to Data section applies the find_features function to each individual customer review using a loop, while also retaining each review’s label."
\n**Converting Text to a Model-able format: One Hot Encoding**
\n  \n 2.4 Correlations With Newly Created Features 
\n💭 Thougts: This is not looking to bad. Let's build a solid pipeline and see how we perform!
"\n💭 Thougts: This is what we want for each feature. So let's create a dataframe and plot the result for a more appealing look. Furthermore, a value_count for features like Fare_per_person and Age makes little sense so we will create a distplot here."
\n📝 Note: I think we can say that a delta of the distributions of the main features between test and training data does not seem to be the main reason for our divergent performance. Only Fare_per_person looks a bit different. Let's look a little further here. 
\n📝 Note: That does not look alarming. So let's keep adding new features and hopefully get better performance.
"\n  \n 5.2 New Feature ""Salutation"" \nWe have completely ignored the name of the passengers, but the name usually contains a salutation and this could be relevant. Let's add it as a feature."
\n📝 Note: We will integrate the 4 most common salutations as a feature. So let's customize the above test function and create a corresponding pipeline function.
"\n📝 Note: So what do we have? We have the 20 most frequent Leading_tickets_numbers and their frequencies from test and training dates. It is important to note here that we have normalized the frequencies, since the size of the test data is smaller than the size of the training data. In addition, we have the associated survival probabilities based on the training data.\nI hope the steps in the code are understandable. Now let's try to represent this data in one informative plot."
"\n📝 Note: The frequency of these groups is quite similar in the test and training data set. That' s good. However, the problem is that in our current pipelines we normalize the leading ticket number and thus implicitly assume that there is a natural order of magnitude. However, if we look at the survival probabilities of these groups, we see that this is not quite correct. Although the survival probability in groups starting with a 1 is generally higher, there are also relevant exceptions. We will fix this erroneously assumed order by setting up a new pipeline that processes the leading_ticket_number via one hot encoding. This will increase the number of features tremendously but we can adjust the depth of the trees accordingly."
"Due to the low values, the violin plot was not a good choice to analyze if features are really separating the data in this case. We will see then the boxplot:"
"Difference of points (full and last 5 games), difference of points by ranking faced (full and last 5 games) and difference of rank faced (full and last 5 games) are good features. Also, some of the generated features have very similar distributions which will be analyzed using scatterplots."
"Goals difference by ranking faced and its last 5 games version has very similar distributions. So, we will use only the full version (goals_per_ranking_dif)."
#### Numerical vs Numerical correlation
 Correlations among some variables are high but not very significant apart from the dummy variable embarked_c. Since we have done one hot encoding there is a additional variable for embarked I will remove embarked_c since those 3 levels can be described using two variables.
####  Ridge Classifier
####  logistic regression
####  linear discriminant analysis
#### decision tree classifier 
#### random forest classifier 
#### gradient boost classifier  
#### adaboost classifier  
#### extreme gradient boost classifier 
#### extra tree classifier
#### voting classifier
#### stacking classifier
#### catboost classifier
##  🌌 resampling with SMOTE\n\n The response variable is imbalanced hence I will use SMOTE to resample the dataset. 
#### response variable after oversampling
#### logistic regression with resampling
#### linear discriminant analysis with resampling
#### decision tree classifier with resampling
#### RandomForest Classifier with resampling
#### Bagging Classifier with resampling
#### Gradient boosting classifier with resampling
#### Ada boosting classifier with resampling
#### Extreme gradient boosting classifier with resampling
#### Extra tree classifier with resampling
#### Voting classifier with resampling
#### Stacking classifier with resampling
From the above fitted models stacking classifier has given the best classification accuracy. The performance was not increased by using SMOTE resampling. So the best model is stacking classifier without resampling with 87.15% accuracy.
# Combined Plot
You can see that after any transformation the distributions are on a similar scale. Also notice that MinMaxScaler doesn't distort the distances between the values in each feature.
### Visualizing after Dimension Reduction
 Observation:\n\nYou can see that the blue dots are expanding their power.
### Plotting histogram Plot
 Observation:\n    \nIt can be seen that the number of data of the normal class (value = 0) is reduced. It can be seen that the distribution of the negative class (value = 0) maintains the shape of the original distribution similarly.
### Plotting Scatter Plot
 Observation:\n    \nIt can be seen that the negative values increase randomly. 
" Observation:\n    \nLooking at the historgram, it can be seen that both the V14 features increase the normal data (value = 0). For both V14 features, it can be confirmed that the overlapping section between fraud (value = 1) and normal (value = 0) is larger than other methods."
" Observation:\n    \nSimilar to SMOTE, it can be confirmed that oversampling occurs through interpolation."
 Observation:\n    \nIt can be seen that the number of data of the normal class (value = 0) is reduced. It can be seen that the distribution of the normal class (value = 0) maintains the shape of the original distribution similarly.
 Observation:\n\nIt can be seen that the fraud-level data are randomly sampled. The distribution of positive values was also changed.
" Observation:\n    \nIt can be seen that the number of data of the normal class (value = 0) is reduced. It can be seen that the distribution of the normal class (value=0) maintains the shape of the original distribution similarly, and it can be confirmed that the distribution of the fraud class (value=1) has a smaller variance compared to random undersampling."
 Observation:\n    \nIt can be seen that the number of data of the normal class (value = 0) is reduced. The distribution of the normal class (value=0) became a bimodal distribution in the shape of the original distribution. The shape of the original distribution is broken and is unlikely to be conducive to learning.
" Observation:\n    \nWhen looking at the boundary in the model using the train dataset, it was drawn very clearly."
" Observation:\n    \nIf you look at the picture above, you can check the conditions under which the decision tree is made."
# **15. Use elbow method to find optimal number of clusters** \n\n[Table of Contents](#0.1)
"- By the above plot, we can see that there is a kink at k=2. \n\n- Hence k=2 can be considered a good number of the cluster to cluster this data.\n\n- But, we have seen that I have achieved a weak classification accuracy of 1% with k=2.\n\n- I will write the required code with k=2 again for convinience."
"👉 Most male applicants are already married compared to female applicants. Also, the number of not married male applicants are higher compare to female applicants that had not married."
👉 Most not self employed applicants have good credit compared to self employed applicants.
👉 Most of loan that got accepted has property in Semiurban compared to Urban and Rural.
### 4.3.3 Categorical 📊- Numerical 📈
"👉 It can be seen that there are lots of outliers in Applicant Income, and the distribution also positively skewed"
"👉 It's clear that Co Applicant Income has a number of outliers, and the distribution is also positively skewed."
"👉 As can be seen, Co Applicant Income has a high number of outliers, and the distribution is also positively skewed."
### 4.3.4 Numerical 📈 - Numerical 📈
"*   There is **negative correlation** between Applicant income and Co Applicant Income.\n*   The correlation coefficient is **significant** at the 95 per cent confidence interval, as it has a **p-value of 1.46**\n"
## 6.2 K-Nearest Neighbour (KNN)
## 6.3 Support Vector Machine (SVM)
## 6.5 Decision Tree
## 6.6 Random Forest
## 6.7 Gradient Boosting
Bellow we define a stylized report with Plotly
## 5.5. Scores Tables 
We can complete model performance report with a table contain all results by fold
# 6. Machine Learning 
"### Check the distribution of variables\n\n\nNow, I will plot the histograms to check distributions to find out if they are normal or skewed. If the variable follows normal distribution, then I will do `Extreme Value Analysis` otherwise if they are skewed, I will find IQR (Interquantile range)."
# **16. Classification metrices** \n\n\n[Table of Contents](#0.1)
"# **18. ROC - AUC** \n\n\n[Table of Contents](#0.1)\n\n\n\n## ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of `TP to (TP + FN).`\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of `FP to (FP + TN).`\n\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n"
### Tip 6.16. Feature importance diagram 
In progress...
### Competition [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)
"### Result: the file ""submission.csv"" gives LB = 0.80382 (Top 4%)"
## Comparing Models
Our models work fine but best of them are KNN and Random Forest with 88.52% of accuracy. Let's look their confusion matrixes.
Let's check the correlation coefficient:
Find out the correlation between the medal rank and the number of mentions of the kernel:
"Finally, some numbers considering the number of mentions of the kernels on Kaggle forum:"
Let's see the feature importances:
"We can explain some of the results:\n* __Kernel version number__: this feature may be related to the kernel quality, the kernel with lot's of versions may be updated and improved,\n* __Forum topic__: the importance of this feature indicates that a forum topic helps to promote the kernel,\n* __Related competition__: we can see that the fact that there is a competition related to a kernel plays a vital role in the number of votes. Probably related competition helps to promote the kernel among the users.\n* __Author performance tier__: this feature is essential because more experienced authors create better kernels and have more followers."
"We will define the method, which fits the given model and puts all the neccesery data into different structures to use them further:\n\n* deploy_acc  - model accuracy for deploy dataset\n* train_acc   - model accuracy after the validation\n* models_dict - dictionary of the models\n"
"I've tried several Gridsearches for CVs with Kfold. And after a bunch of deploys, i chose those models.\nI chose models which gave me the best deploy accuracy. This models gonna be not ""Best model for Survival prediction"" , but ""The best model to predict the test set"", which is not the exect solution for the task."
### 4.4 Comparing results
### 4.5 K-Means on online retail data
### 5.4 Comparing results
### 5.5 Hierarchical clustering on online retail data
### 6.5 Comparing results
### 6.6 DBSCAN clustering model on online retail data
### 7.4 Comparing results
### 7.5 GMM clustering model on online retail data
# 8. All algorithm comparison
Some referrals\n\nhttps://crunchingthedata.com/when-to-use-dbscan/\n\nhttps://crunchingthedata.com/when-to-use-hierarchical-clustering/\n\nhttps://crunchingthedata.com/when-to-use-gaussian-mixture-models/\n\nhttps://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/
### SalePrice Distribution
"Distribution is skewed to the right, where the tail on the curve’s right-hand side is longer than the tail on the left-hand side, and the mean is greater than the mode. This situation is also called positive skewness.  \nHaving a skewed target will affect the overall performance of our machine learning model, thus, one way to alleviate will be to using **log transformation** on skewed target, in our case, the *SalePrice* to reduce the skewness of the distribution."
"## 4.2 Natural Language Processing\n\nNatural language processing(or NLP for short) is the branch of artificial intelligence which is concerned with the interaction between humans and machines through the use of natural language. It allows the machine to make sense of, and derive information that is of value from human language.\n\n\n    Source: Kaggle 2020 DS and ML Survey\n"
"From the above graphs we again see similar trends to what we observed in Computer vision - The bulk of our workforce is still data scientists, there is still a preference for working in smaller companies, and even the exact same countries were observed when looking at the top 10 employers.\n\nLet's now look at the NLP specific behaviours:\n* Understanding **Word Embeddings** is part of building a strong foundation in NLP. Word embeddings is a means of representing words in such a manner that words similar in meaning have similar representations.\n* **Transformer networks** function similar to Recurrent Neural Networks in the sense that they are useful to handle sequential. A key difference that makes this useful in NLP is the fact that they don't require data to be processed in order(so the beginning of the sentence needn't always be processed before the end). This makes parellisation easier, resulting in reduced training times.\n* **Encoder decoder models are used in sequence to sequence operations** like text summarising, question answering. They utilise one encoding network to encode the input sequence and another decoding network to convert this into the desired target.\n* **Contextualised word embeddings** provide additional information in the embedding regarding the context in which it was used. eg. ""ground"" has different meanings in ""ground coffee"" and ""training grounds"" which is determined based on context.\n\n##  What do they do?\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q23. Select any activities that make up an important part of your role at work.\n    \n"
"## 4.3 Machine learning in general\n\nThis section covers the users of the other machine learning algorithms that the survey group under Q17( ""*machine learning algorithms used regularly""* ). These are used for a wide range of applications and hopefully this analysis will help us understand how utilised they are in everyday applications.\n\n\n    Source: Kaggle 2020 DS and ML Survey\n"
"For the first time we see Data Analyst ranked highly in the percentage of ml users across fields, this might mean that their focus is on predictive modelling and analytics, without the use of computer vision or NLP methods.\n\nIn countries we see how France and surprisingly China, have dropped off the top 10 list and are replaced by Spain and Nigeria. As the first African country to break the top 10 employers list, it shows that there is promise for machine learning and data science in Africa.\n\nLet's take a look at some of the methods that might be worth knowing in this area.\n* **Linear and Logistic regression:** As everyone's starting point in machine learning, it's important to gain a good understanding of how linear models work which will act as a solid foundation even when working with more complex algorithms.\n* **Decision Trees and Random Forests:** Decision trees work by making a series of sequential decisions leading to a particular prediction. Random forests combine the outputs of several randomly generated trees which improves the overall performance on unseen data, as opposed to using a single decision tree.\n* **Gradient Boosting machines(xgboost, lightGBM):** While I wont go into the specifics of how each one works, in general gradient boosting involves building a predictive model using an ensemble on weaker prediction models. \n* **Convolutional Neural Networks**: CNNs are a class of deep neural networks that form the backbone of image processing in artificial intelligence. They are a great fit whenever the data has a natural 2D(or even 3D) structure.\n\n##  What do they do?\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q23. Select any activities that make up an important part of your role at work.\n    \n"
"# 5. Conclusion\n\nI understand that throughout the course of this notebook I have bombarded you with a lot of information on working with data. It is easy to feel a little overwhelmed by all the different moving parts involved and think that you might have bit off a bit more than you can chew when you decided to try your hand at data science, especially when you are just starting out. \n\nTo this, I'd like to remind you that noone enters their first job( or even switching to a new role) knowing everything on day one - you will learn as you work and you are allowed to( *and will*) make mistakes. It's just a desire to learn and improve which makes things easier.\n\nThanks for joining me in this journey and I hope you found something of use in this notebook. Stay safe and keep learning!\n"
"# What's new?\nSo I've reached stage at which I will I'll be reworking a few of the older graphs and updating the writeups in previous sections At least, I hope I've finished everything . Since it might not be obvious what was reworked, I'll list it out here so you know where to check out the new content:\n\n* [""**Distribution based on company size**""](#company_size_earnings)Wasn't a big fan of how cluttered it was and how the colours wouldn't intuitively help you find who was paid the most. Section now reworked in D3 with the new normalised pays, along with a new tab(""Overall compensation"") to see where companies are spending money.\n* Table of Contents added.\n* [""**How much Coding Experience?**""](#coding_experience) and [""**How much Machine Learning Experience?**""](#ml_experience) both edited to keep the same theme as other d3 charts. [""**Data roles**""](#roles) now highlights the secondary roles also.\n* [**Workflows of Data Professionals**](#workflows) section was added to highlight where time is spent in the actual work of a data professional.\n* [**The Skills Gap**](#skillsgap)\n\n**~ 05-01-2021 - Completed work on the notebook**"
## White Noise\n\nThe following time series is random. It consists of 48 timesteps.
"### 1. Check Stationarity\nThe sample data is stationary. Therefore, we do not need to difference the time series."
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There is only one autocorrelation that is significantly non-zero at a lag of 0. Therefore, the time series is random."
### 3. Modelling\n\nModelling white noise is difficult because we cannot retrieve any parameters from the ACF and PACF plots.
"## Random-Walk\n\nThe following time series is random like [White Noise](#White-Noise). However, the current value depends on the previous one. It consists of 48 timesteps."
"### 1. Check Stationarity\nThe sample data is non-stationary. Therefore, we need to difference the time series."
## Constant\n\nThe following time series is constant. It consists of 48 timesteps.
### 2. Check ACF and PACF\n\n- ACF/PACF was applied to non-stationary time series
"### 3. Modelling\n\nModelling a constant as an AR or MA process is diffucult because we cannot retrieve any parameters from the ACF and PACF plots. But on the other hand, if you can retrieve that a time series is constant, it should not be too difficult to forecast it, right?"
## Bitcoin
### 2. Check ACF and PACF
## Ethereum
"## Discussion on Random-Walk\nFor both Bitcoin and Ethereum, we can observe a **random-walk** behavior (see [🚀 Cheat Sheet](#🚀-Cheat-Sheet)). This is fairly common in stock prices (see [Random Walk Theory](https://www.investopedia.com/terms/r/randomwalktheory.asp))\n\n> A **random walk is unpredictable**; it cannot reasonably be predicted.\n>\n> Given the way that the random walk is constructed, we can expect that the best prediction we could make would be to use the observation at the previous time step as what will happen in the next time step.\n>\n>Simply because we know that the next time step will be a function of the prior time step.\n>\n>This is often called the naive forecast, or a persistence model. - [A Gentle Introduction to the Random Walk for Times Series Forecasting with Python](https://machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/)\n\nWell, that's an unsatisfying finding. So, where do we go from here?"
"**LSTM , XGBoost and Prophet - How good are they ?**\n\nHere lets visualize and compare the predictive results of LSTM, XGBoost and Prophet in a single plot,"
"**Time Series forecasting using ARIMA**\n\nARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. It is a class of model that captures a suite of different standard temporal structures in time series data.\nThis acronym is descriptive, capturing the key aspects of the model itself. Briefly, they are:\n* AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n* I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n* MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\nARIMA is one of the mostly used techniques for Time Series analysis. In Python,  ARIMA based forecasting models can be created either using AutoARIMA[(Pyramid ARIMA)](https://pypi.org/project/pyramid-arima/) or [StatsModel ](https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html). Here we will be using StatsModel as Kaggle do not support Pyramid ARIMA till now."
"# Technique 13: autoreload\nThe autoreload module is there to help with code structure. The module reloads the code before each execution. Once you get locked into a TDD loop and you start refactoring code from the notebook into additional files, this module will reload the code in the additional files."
"\n# Technique 14: Increase default memory limit size\n\nJupyter notebook has a default memory limit size. You can try to increase the memory limit by following the steps:\n\n**1) Generate Config file using command:**\n\n*jupyter notebook --generate-config*\n\n**2) Open jupyter_notebook_config.py file situated inside 'jupyter' folder and edit the following property:**\n\n*NotebookApp.max_buffer_size = your desired value*\n\nRemember to remove the '#' before the property value.\n\n**3) Save and run the jupyter notebook. It should now utilize the set memory value. Also, don't forget to run the notebook from inside the jupyter folder.**\n\nAlternatively, you can simply run the Notebook using below command:\n\n*jupyter notebook --NotebookApp.max_buffer_size=your_value*"
Let's look at the distribution of the target variable.
Let's eliminate the slight imbalance of classes by using the SMOTE method.
Let's see the pixel distributions for the above plot:
And how much they have changed from the original:
Not much difference here except that we've shaved a lot off the `0.0` peak which is the effect of the cutout augmentatioin.
> Before grouping
> After grouping 
* Cabin 0: ABC\n* Cabin 1: DE\n* Cabin 2: FG\n* Cabin 3: Z(missing values)
  \nSibSp vs Survived\n
  \nParch vs Survived\n\n
  \nGender vs Survived\n
  \nEmbarked and Fare vs Survived\n
  \nFare vs Survived\n
  \nAge vs Survived\n
  \nCorrelation\n
"Now this is somewhat expected since it explains the difference between ""S"" and the other ports. Therefore, it seems that between more 1st class passengers embarking at ""C"" and more men at ""S"" there doesn't seem to be much actual influence in the port of embarkation.\n\nHowever, the last plot should also indicate that ..."
... there were more males among the 3rd class passengers. Possibly travelling alone?
"Sort of, yes. This goes some way to explain features like better survival for SibSp = 1-3. But I think that it doesn't cover all the signal in the Parch feature.\n\n**We learn:**\n\n- Different percentages of passenger classes and sexes have embarked from different ports, which is reflected in the lower survival rates for ""S"" (more men, fewer 1st class) compared to ""C"" (more women and 1st class).\n\n- It's hard to say at this stage whether there is any real impact left for the *Embarked* feature once we correct for these connections. We will come back to this in the modelling stage when we will study feature importances and significances (soon)."
"Finally, let's check what's going on between *Age* and *Embarked*:"
"The curious distribution for the ""Q"" survivors somewhat follows the overall trend for 3rd class passengers (which make up the vast majority of ""Q"") but is notably narrower. Not many of the children there survived, but then there were not many children to begin with. Let's come back to this point in discussing the derived features.\n\n**We learn:**\nThere don't seem to be strong differences in *Age* among the *Embarked* categories that would point at an imbalance that goes beyond the influence of *Pclass* and *Sex*. "
Let's study the relation between *Fare* and *Pclass* in more detail:
**We learn:**\n\n- There is a broad distribution between the 1st class passenger fares (rich -> super rich)\n- There's an interesting bimodality in the 2nd class cabins and a long tail in the 3rd class ones. (*TODO: check cumulative fare question*)\n- For each class there is strong evidence that the cheaper cabins were worse for survival. A similar effect can be seen in a *boxplot*:
### *Child*
"The *Pclass == 1* plot looks interesting at first, but there are only 3 children in this group which makes the apparent pattern just random noise. The other two passenger classes are more interesting, especially for the male children. Note, that since we are selecting by *Age*, which has many missing values, a number of children will be in the *Child == False* group. Nonetheless, this seems useful.\n\n**We learn:** Male children appear to have a survival advantage in 2nd and 3rd class. We should include the *Child* feature in our model testing."
### *Cabin\_known*
"As suspected, it is more likely to know the cabin of a passenger who survived. This could be useful."
"However, we see again that a large part of this effect disappears once we control for *Sex* and *Pclass*. \n\n**We learn:** There remains a potential trend for males and for 3rd class passengers but the uncertainties are large. This feature should be tested in the modelling stage."
### *Deck*
"Ok, so what can we tell from the Deck (derived from the Cabin number)? First of all the overall survival statistics is much better than for the full sample, which is what we found above. Beyond that, the best decks for survival were B, D, and E with about 66% chance. C and F are around 60%. A and G at 50%. The only passenger on deck T died, but that's hardly robust statistics.\n\nThe largest number of cases we have is for B vs C. Let's see whether that's significant:"
Just about formally significant (i.e. < 5%). It might be worth our while to include this feature in at least the initial stages of modelling to see how it performs.
"In addition, there is some variation between the 1st class male passengers, but it doesn't look overly significant."
"Based on this plot we define a new feature called *Bad\_ticket* under which we collect all the ticket numbers that start with digits which suggest less than 25% survival (e.g. *4*, *5*, or *A*). We are aware that some of the survival fractions we see above are based on small number statistics (e.g. 2 vs 0 for *8*). It is well possible that some of our ""bad tickets"" are merely statistical fluctuations from the base survival rate of 38%.  The barplot shows mean survival fractions and the associated 95% confidence limits, which are large for the sparse samples.\n\nHowever, the significant difference between e.g. *1* and *3* (based on large enough numbers) suggests that this new feature could still contain some useful information. I think that without external information, which we are avoiding in this notebook, we can't do much better in trying to tie the ticket number to the survival statistics.\n\nOf course, it's not the tickets themselves that are ""bad"" for survival, but the possibility that the ticket numbers might encode certain areas of the ship that would have led to higher or lower survival chances."
"The factorplot suggests that bad tickets are worse for male passengers, and 3rd class passengers. The individual significances are not overwhelming, but the trend itself might be useful."
"The last plot doesn't inspire much confidence in a strong correlation between *Deck* and *Bad\_ticket*, but maybe it will be useful otherwise.\n\n**We learn:** *Bad\_ticket* might be a lower order effect that could give us some additional accuracy. We should test it out in the modelling stage."
"Similar to the known Cabin numbers, what about the *passengers for which we know the age*?"
"As we would expect intuitively, it appears that we are more likely to know someones age if the survived the disaster. There's a difference of about 30% vs 40% and it should be significant:"
"Very much so. However, we have seen before that there might be imbalances in the dominating features *Sex* and *Plcass* that create an apparent signal. Is this another of these cases?"
"It actually is. Turns out that we are more likely to know the age of higher class passengers or women, which are the strongest survival predictors we have found, so far. (Of course, the causality might as well go the other way, but that's not really the question here. What we want to find are the best predictors for survival.)\n\n**We learn:** \nThere is a strong impact of *Sex* and *Pclass* on this new feature. This might be enough to explain all the variance in the *Age\_known* variable. We should test the predictive power in our modelling."
### *Family*
"**We learn:**\nAgain, we find that having 1-3 family members works best for survival. This feature is a mix of *SibSp* and *Parch*, which increases the overall numbers we can work with, but might smooth out some more subtle effects."
### *Alone*
Travelling alone appears bad enough to be significant.
"But more men were travelling alone than women did. Especially among the 3rd class passengers. Also this feature should be evaluated in our modelling step, to see if it's still significant in the presence of the *Sex* feature."
### *Large\_Family*
"In the same way, having a large family appears to be not good for survival."
But most large families were travelling in 3rd class. The tentative imbalance between male and female 3rd class probably reflect the observation we made earlier that men were more likely to travel alone.
### *Shared\_ticket*
Sharing a ticket appears to be good for survival.
But again the sharing of tickets is more frequent with females and 1st class passengers. This is consistent with the other statistics that show that women were more likely to travel together with larger families.\n\n**We learn:** Several of these derived parameters are strongly correlated with *Sex* and *Pclass*. Whether there is actual signal in them that a model can use to improve the learning accuracy needs to be investigated.
"Ok, so we have 18 different titles, but many of them only apply to a handful of people. The dominating ones are Mr (581), Miss (210), Mrs (170), and Master (53); with the number referring to the combined data. Here are the age distributions for those:"
"We see that *Master* is capturing the male children/teenagers very well, whereas *Miss* applies to girls as well as younger women up to about 40. *Mrs* does not contain many teenagers, but has a sizeable overlap with *Miss*; especially in the range of 20-30 years old.\n\nNevertheless, *Miss* is more likely to indicate a younger woman. Overall, there is a certain amount of variance and we're not going to be able to pinpoint a certain age based on the title.\n\nTherefore, we will use 2 *Age Groups*, updating to the *Young* variable we defined above. The idea is to address the issue of missing *Age* values by combining the *Age* and *Title* features into a single feature that should still contain some of the signal regarding survival.\n\nFor this, we define everyone under 30 *or* with a title of *Master*, *Miss*, or *Mlle* (Mademoiselle) as *Young*. All the other titles we group into *Not Young*. This is a bit of a generalisation in terms of how *Miss* and *Mrs* overlap, but it might be a useful starting point. All the other rare titles (like *Don* or *Lady*) have average ages that are high enough to count as *Not Young*."
"Finally, we model a fare category, *Fare_cat*, as an ordinal integer variable based on the logarithmic fare values:"
"Because of the larger number of ""Miss"" vs ""Master"" mostly women are classified as ""Young"".  We also recover the age difference between the ticket classes that was already obvious in earlier plots. Both factors mean that the impact of *Young* has to be studied carefully."
Let's remind ourselves of the distribution of *Fare* with respect to *Pclass*:
"To simplify this broad distribution, we decide to classify the fares into *3 fare categories*: 0-10, 10-100, and above 100. This transformation can be easily achieved using the base 10 logarithm:"
"Let's investigate the *Fare affair* in more detail. First, we make sure that the passengers in each group really had the same *Fare* values:"
"Almost 100% yes. Above, we extract the standard deviation of the *Fares* among the ticket groups. A standard deviation of zero means that there's no difference. Only 2 values stand out. This is a small number that we could ignore, but we are curious, aren't we?"
"It's Mr Osen and Mr Gustafsson on Ticket 7534. Their *Fares* are close enough, though, to include them in the general treatment.\n\nNow, let's think for a moment: Identical fares could mean that the fare for a cabin was shared equally among the passengers, in which case our previous treatment would have been justified. However, it *could* also mean that the listed value is the *cumulative fare per cabin* and it was simply recorded as the same value for each passenger. Intuitively, this doesn't seem so plausible, since you typically record what is paid for a ticket and not for a cabin. But let's investigate this for a moment and check how it would transform the *Fare* distribution. For this, we create a *Fare_eff* feature above, which we derive by dividing *Fare* by the number of people sharing a ticket (*Ticket_group*; which we also newly created)."
Now **that** is interesting. We see that the distributions become significantly narrower and that the tails and bimodality become much weaker (after getting rid of the zero-fare values for both groups). The really expensive *Fares* in *Pclass == 1* are pretty much all gone. Here's how the standard deviations compare:
"And that's quite expensive for a 3rd class ticket. Maybe these two actually shared a ticket / cabin and we have another transcription / data entry error? The ticket numbers are very similar and someone could easily write ""303"" instead of ""304"". Will we ever know? Maybe not. Does it matter much? Probably not.\n\nMore importantly, there is a reasonable argument to be made for this new *Fare_eff* feature to represent the actual fare better than the original feature. For once, it splits much cleaner between the *Pclasses*:"
"So well, in fact that defining new fare categories seems almost redundant because *Pclass* already captures most of this signal. Nonetheless, we'll try; because we are optimistic people at heart. We use the dashed lines in the plot above for an (empirical) division into 3 classes, which separate the cheaper *Fare_eff* of a *Pclass* group from the more expensive ones of the next one. The new feature is called *Fare_eff_cat* and behaves as follows:"
For a final overview before the modelling stage we have another look at the correlation matrix between all old and new features:
"We designed a number of new features, and unsurprisingly several of those are correlated with the original features we used to create them. For instance *Fare\_cat* and *Fare*. Or *Family* and *SibSp/Parch*. In the modelling step, we will first determine which of the features carry the most signal (*to be done*) and then use them to train a number of different classifiers."
 LogisticRegression
 Support Vector Machines
 KNeighborsClassifier
 GaussianNB
 Perceptron
 LinearSVC 
 SGDClassifier
 DecisionTreeClassifier
 RandomForestClassifier
 MLPClassifier
 XGBClassifier
 ExtraTreesClassifier
 AdaBoostClassifier
 lgb Classifier
 NuSVC 
 HistGradientBoostingClassifier 
 GaussianProcessClassifier 
 RidgeClassifier 
 CalibratedClassifierCV 
 PassiveAggressiveClassifier 
 Comparing Different Models
"# **6. Feature importance with XGBoost** \n\n[Table of Contents](#0.1)\n\n\n- XGBoost provides a way to examine the importance of each feature in the original dataset within the model. \n\n- It involves counting the number of times each feature is split on across all boosting trees in the model. \n\n- Then we visualize the result as a bar graph, with the features ordered according to how many times they appear.\n\n- XGBoost has a **plot_importance()** function that helps us to achieve this task. \n\n- Then we can visualize the features that has been given the highest important score among all the features. \n\n- Thus XGBoost provides us a way to do feature selection.\n\n- We will proceed as follows:-"
"- We can see that the feature `Delicassesn` has been given the highest importance score among all the features. \n\n- Based upon this importance score, we can select the features with highest importance score and discard the redundant ones.\n\n- Thus XGBoost also gives us a way to do feature selection."
##  Utility Functions: 
##  Cosine Decay Learning Rate Scheduler: 
## Training : 
## 14.3. Ploting
- A proper fit now.
## 16.1. Plotting MSEs
## 16.2. Potting Bias
## 16.2. Potting Variance
## 16.2. Potting Bias - Variance
Let's use it to simulate some measurements evenly distributed over time:
"Now let's see what happens if we *underfit* or *overfit* these predictions. To do that, we'll create a function that fits a polynomial of some degree (e.g. a line is degree 1, quadratic is degree 2, cubic is degree 3, etc). The details of how this function works don't matter too much so feel free to skip over it if you like!  (PS: if you're not sure about the jargon around polynomials, here's a [great video](https://www.youtube.com/watch?v=ffLLmV4mZwU) which teaches you what you'll need to know.)"
"So, what happens if we fit a line (a ""degree 1 polynomial"") to our measurements?"
"Now we'll look at a few examples of correlations, using this function (the details of the function don't matter too much):"
"OK, let's check out the correlation between income and house value:"
"There are over 93 languages represented in our dataset. As we had expected, English language films form the overwhelmingly majority. French and Italian movies come at a very distant second and third respectively. Let us represent the most popular languages (apart from English) in the form of a bar plot."
"As mentioned earlier, **French** and **Italian** are the most commonly occurring languages after English. **Japanese** and **Hindi** form the majority as far as Asian Languages are concerned."
"Do popularity and vote average share a tangible relationship? In other words, is there a strong positive correlation between these two quanitties? Let us visualise their relationship in the form of a scatterplot."
"Surprisingly, the Pearson Coefficient of the two aforementioned quantities is a measly **0.097** which suggests that **there is no tangible correlation**. In other words, popularity and vote average and independent quantities. It would be interesting to discover how TMDB assigns numerical popularity scores to its movies."
There is a very small correlation between Vote Count and Vote Average. A large number of votes on a particular movie does not necessarily imply that the movie is good.
"With these features in hand, let us now check the most popular and most successful months and days."
"It appears that **January** is the most popular month when it comes to movie releases. In Hollywood circles, this is also known as the *the dump month* when sub par movies are released by the dozen. \n\nIn which months do bockbuster movies tend to release? To answer this question, we will consider all movies that have made in excess of 100 million dollars and calculate the average gross for each month."
"We see that the months of **April**, **May** and **June** have the highest average gross among high grossing movies. This can be attributed to the fact that blockbuster movies are usually released in the summer when the kids are out of school and the parents are on vacation and therefore, the audience is more likely to spend their disposable income on entertainment.\n\nDo some months tend to be more successful than others? Let us visualise the boxplot between the return and the months."
"The months of **June** and **July** tend to yield the highest median returns. **September** is the least successful months on the aforementioned metrics. Again, the success of June and July movies can be attributed to them being summer months and times of vacation. September usually denotes the beginning of the school/college semester and hence a slight reduction in the consumption of movies.\n\nLet us now have a look at the most popular days as we did for months."
**Friday** is clearly the most popular day for movie releases. This is understandable considering the fact that it usually denotes the beginning of the weekend. **Sunday** and **Monday** are the least popular days and this can be attributed to the same aforementioned reason.
"#### Number of Movies by the year\n\nThe Dataset of 45,000 movies available to us does not represent the entire corpus of movies released since the inception of cinema. However, it is reasomnable to assume that it does include almost every major film released in Hollywood as well as other major film industries across the world (such as Bollywood in India). With this assumption in mind, let us take a look at the number of movies produced by the year."
"We notice that there is a sharp rise in the number of movies **starting the 1990s decade.** However, we will not look too much into this as it is entirely possible that recent movies were oversampled for the purposes of this dataset.\n\nNext, let us take a look at the earliest movies represented in the dataset.\n\n#### Earliest Movies Represented"
"The movie with the most number of languages, **Visions of Europe** is actually a collection of 25 short films by 25 different European directors. This explains the sheer diversity of the movie in terms of language."
The **Spearman Coefficient** is 0.018 indicating no correlation between the two quantities.
We are aware that most movies are less than 5 hours (or 300 minutes) long. Let us plot a distribution of these mainstream movies.
Is there any meaningful relationship between runtime and return? Let us find out!
"There seems to be relationship between the two quantities. **The duration of a movie is independent of its success.** However, I have a feeling this might not be the case with duration and budget. A longer movie should entail a higher budget. Let us find out if this is really the case."
"The two quantities have a much weaker correlation than I had expected. In retrospect, the genre of the movie tends to have a much greater impact on budget. A 3 hour art film will cost significantly lesser than a 90 minute Sci-Fi movie. \n\nNext, I'd like to see the average lengths of movies through time, right from the 1890s to the 2017s. It would be interesting to see the trends in what filmmakers adjudged would be the appropriate length of a movie at that time."
"We notice that films started hitting the **60 minute mark as early as 1914**. Starting **1924**, films started having the traiditonal 90 minute duration and has remained more or less constant ever since.\n\nFinally in this section, let us see the longest and the shortest movies of all time (with respect to the movies in the dataset). "
"Two **Pirates of the Carribean** films occupy the top spots in this list with a staggering budget of over **300 million dollars**. All the top 10 most expensive films made a profit on their investment except for **The Lone Ranger** which managed to recoup less than 35% of its investment, taking in a paltry 90 million dollars on a **255 million dollar** budget.\n\nHow strong a correlation does the budget hold with the revenue? A stronger correlation would directly imply more accurate forecasts."
The pearson r value of **0.73** between the two quantities indicates a very strong correlation. 
"The mean gross of a movie is **68.7 million dollars** whereas the median gross is much lower at **16.8 million dollars**, suggesting the skewed nature of revenue. The lowest revenue generated by a movie is **just 1 dollar** whereas the highest grossing movie of all time has raked in an astonishing **2.78 billion dollars.*"
The distribution of revenue undergoes exponential decay just like budget. We also found that the two quantities were strongly correlated. Let us now take a look at the highest and least grossing movies of all time.
"These figures have not been adjusted for inflation. Therefore, we see a disproportionate number of movies from very recent times in the top 10 list. To get an understanding of the revenue garnered by movies, let us plot the maximum revenue through the years."
"As can be seen from the figure, the maximum gross has steadily risen over the years. The world of movies broke the 1 billion dollar mark in 1997 with the release of **Titanic**. It took another 12 years to break the 2 billion dollar mark with **Avatar**. Both these movies were directed by James Cameron."
**Animation** movies has the largest 25-75 range as well as the median revenue among all the genres plotted. **Fantasy** and **Science Fiction** have the second and third highest median revenue respectively. 
"From the boxplot, it seems like **Animation** Movies tend to yield the highest returns on average. **Horror** Movies also tend to be a good bet. This is partially due to the nature of Horror movies being low budget compared to Fantasy Movies but being capable of generating very high revenues relative to its budget."
Let us now take a look at the actors and the directors who have raked in the most amount of money with their movies.\n\n#### Actors with the Highest Total Revenue
#### Directors with the Highest Total Revenue
"For average revenues, we will consider only actors and directors who have acted and directed in at least 5 movies respectively."
#### Actors with Highest Average Revenue
#### Directors with Highest Average Revenue
"Which actors and directors are the safest bet? For this, we will consider the average return brought in by a particular director or actor. We will only consider those movies that have raked in at least 10 million dollars. Also, we will only consider actors and directors that have worked in at least 5 films.\n\n#### Most Successful Actors"
"We see that our model performs far more superiorly than the Dummy Regressor. Finally, let us plot the feature importances in the form of a bar plot to deduce which features were the most significant in our making predictions."
"We notice that **vote_count**, a feature we *cheated* with, is the most important feature to our Gradient Boosting Model. This goes on to show the improtance of popularity metrics in determining the revenue of a movie. **Budget** was the second most important feature followed by **Popularity** (Literally, a popularity metric) and **Crew Size**."
It seems that movies that belong to a franchise have a higher probability of being a success. 
"We see that with homepages, there is not a very huge difference in probability. To avoid the curse of dimensionality, we will eliminate this feature as it is not very useful."
### 5.2.g Distribution of price of resource requested
### 5.2.h Distribution of quantity of resource requested
### 5.2.i Teacher prefix Distribution
"* Higher number of project proposal submitted by **married womens** which is approx. **53 %**  followed by **unmarried womens** which has approx. **37 %**.\n* Project proposal submitted by **Teacher** which is approx. **2 %** is vey low as compared to **Mrs., Ms., Mr**."
## 5.4 Word Cloud of resources requested
## 5.5 Various popularities in terms of project acceptance rate and project rejection rate
### 5.5.a Popular School states in terms of project acceptance rate and project rejection rate
### 5.5.b Popular Teacher Prefix in terms of project acceptance rate and project rejection rate
### 5.5.c Popular school grade levels in terms of project acceptance rate and project rejection rate
### 5.5.d Popular category of the project in terms of project acceptance rate and project rejection rate
### 5.5.e Popular subcategory of the project in terms of project acceptance rate and project rejection rate
### 5.5.f Popular project titles in terms of project acceptance rate and project rejection rate
## 5.6 Project Proposals by US States
### 5.8.a Teacher_prefix and project_is_approved Intervals Correlation
### 5.8.b Teacher_number_of_previously_posted_projects and project_is_approved Intervals Correlation
*  Number of previously posted applications by the submitting teacher was** Zero(0)** having more number of acceptance rate.
### 5.8.c Correlation Matrix and Heatmap of training data
## 5.9 Project Submission Time Analysis
### 5.9.a Project Submission Month Analysis
* **August month** has the second  number of proposals followed by **September month** .
### 5.9.b Project Submission Weekday Analysis
* The number of proposals decreases as we move towards the end of the week.
### 5.9.c Project Submission Date Analysis
"* Looks like we have approximately one years' worth of data (May 2016 to April 2017) given in the training set.\n* There is a sudden spike on a single day (Sep 1, 2016) with respect to the number of proposals (may be some specific reason?)"
### 5.9.d Project Submission Hour Analysis
"* From Hours 03 to 05, number of proposals decreases.\n* Hours 06 to 14, number of proposals increases.\n* At Hour 14 has more number of proposals."
## 5.10 Top Keywords in project_essay_1
## 5.11 Top keywords in project_essay_2
## 5.12 Top Keywords in project_resource_summary
##  5.13 Quantity V.S. Price
# 5.15 Month wise distribution of number of projects proposal submitted in each state
* USA state **WY** was having more price requested for resources in **March** month than others.
### 5.16.a Price requested for resources distribution by different states
* As we can see most of the price requested for resources is between **0 to 2k dollar**.
### 5.16.b Price requested for resources distribution by Teacher prefixes
"* Mostly price requested for resources is \n   * 0 to 2k Dollar by **teacher** prefix\n   * 0 to 4k Dolar by **Ms. , Mrs. and Mr.** prefixes \n   * 0 to 500 Dollar by **Dr.** prefix."
### 5.16.c Price requested for resources distribution by different Genders
* Mostly price requested for resources is \n   * 0 to 2k Dollar by **Unknowns**\n   * 0 to 4k Dolar by **Males** \n   * 0 to 5k Dollar by **Females**.
### 5.16.d Price requested for resources distribution by different project_grade_category
* Mostly price requested for resources is between approx. ** 0 to 4k** **Dollar**  for all type of project grade categories.
### 5.17.a Popularities of Teacher prefixes in California
### 5.17.b Popularities of school grade levels in California
### 5.17.c Top project titles in California
### 5.17.d Trend of project submission time in California
## 5.18 TX(Texas)
### 5.18.a Popularities of Teacher prefixes in Texas
### 5.18.b Popularities of school grade levels in Texas
### 5.18.c Top project titles in Texas
### 5.18.d Trend of project submission time in Texas
# 6. Feature Engineering
"**The ROC AUC Score is the corresponding score to the ROC AUC Curve. It is simply computed by measuring the area under the curve, which is called AUC.**\n\n**A classifiers that is 100% correct, would have a ROC AUC Score of 1 and a completely random classiffier would have a score of 0.5.**"
## 8.2 Precision Recall curve
## 8.3 Ploting Metrics during training of Light GBM
## 8.4 Feature importances by LightGBM
# 9. Brief Summary/Conclusion :\n--------------------------------------------------------------------------\n* I have done analysis only on training data.\n* This is only a brief summary if want more details please go through my Notebook.
## 5.17 Peoples current job-seeking status
"**Peoples current job-seeking status :**\n  * **Approx 60 % peoples** : I am not actively looking, but i am open to new opportunities.\n  * **Approx. 24 % peoples** :  I am not interested in new job opportunities.\n  * **Approx. 16 % peoples** : I am actively looking for job."
## 5.18 When was the last time that peoples took a job with a new employer?
**When was the last time that peoples took a job with a new employer :**\n  * **Approx. 35 % peoples** : Less than a year ago.\n  * **Approx. 22 % peoples** : Between 1 and 2 year ago.\n  * **Approx. 19 % peoples** : More than 4 year ago.\n  * **Approx. 19 % peoples** : Between 2 and 4 years ago.\n  * **Approx. 6 % peoples** : I've never had a job
"## 5.19  Most popular communication tools use to communicate, coordinate, or share knowledge with coworkers"
**Most popular communication tools :**\n  * **Slack** used by  approx. 19 % developers.\n  * ** Jira** used by approx. 15 % developers.\n  * **Office / productivity suite** used by approx. 14 % developers.\n  * **Stack overflow** used by only approx. 2 % developers.
## 5.20 Most popular languages 
"* **JavaScript is the most popular language** on which developer worked and want to work in over the next year. **Python** is most demandable language on which developers want to work followed by **HTML, CSS** and **SQL** etc"
## 5.21 Most popular Databases
"* **MySQL is the most popular Database** on which developer worked and want to work in over the next year. **MongoDB** is most demandable Database on which developers want to work followed by **PostgreSQL, SQLServer, redis** and **ElasticsSearch** etc."
## 5.22 Most popular platforms
"* **Linux is the most popular platform** on which developer worked and want to work in over the next year followed by  **Android** is most demandable Database on which developers want to work followed by **AWS, Raspberry pie** etc."
## 5.23 Most popular Framworks
"* **Node.js, Angular, React and .Net core are the most popular frameworks** on which developer worked and want to work in over the next year followed by  **Tansorflow** is most demandable Database on which developers want to work."
## 5.24.4 Participation on StackOverflow
**Participation on StackOverflow in Q&A :**\n  * **Approx. 39 % Respondents said** : Less than once per month or monthly\n  * **Approx. 23 % Respondents said** : A few times per month or weekly\n  * **Approx. 17 % Respondents said** : I have never participated in Q&A on StackOverflow\n  * **Approx. 12 % Respondents said** : A few times per week\n  * **Approx. 6 % Respondents said** : Daily or almost daily\n  * **Approx. 3 % Respondents said** : Multiple times per day
## 5.24.6 Up-to-date developer story on StackOverflow
"**Up-to-date developer story on StackOverflow :**\n  * **Approx. 37 % Respondents said** : No, I don't know what that is\n  * **Approx. 24 % Respondents said** : No, I know what it is but i don't have one\n  * **Approx. 23 % Respondents said** :  No, I have one but it's out of date\n  * **Approx. 17 % Respondents said** : YES"
## 5.24.11.1 StackOverflow Visit V.S. StackOverflow Recommendation
"* Majority of peoples who visit Stackoverflow **Multiple times per day**, **Daily or almost daily**, **A few times per week** or **A few times per month or weekly**	 recommending **very likely(10)**."
## 5.24.11.2 StackOverflow Visit V.S. participation in StackOverflow 
## 5.24.11.3 StackOverflow Visit V.S. peoples visiting  StackOverflow job board
* Most of the peoples who are visiting stackOverflow **Daily or almost daily**	or **Multiple times per day** they know about **StackOverflow job board**.
## 5.24.11.4 StackOverflow Visit V.S. Up-to-date developer story on StackOverflow
"* Highest number of peoples who are visiting StackOverflow **Daily or almost dail**y, **Multiple times per day** or **A few times per week	**, they don't know what is **Stackoverflow developer Story**"
## 5.24.11.5 StackOverflow job board Visit V.S. StackOverflow Jobs Recommend
"* Its good to see that who are visiting stackoverflow job board, out of these most of the peoples are recommending Stackoverflow job board very likely(10)."
## 5.25 Top Reasons of upadating a CV 
**Top Reasons of upadating a CV  :**\n  * **Approx. 42 % Respondents said** : My job status and my personal status changed.\n  * **Approx. 14 % Respondents said** : A recruiter contacted me.\n  * **Approx. 11 % Respondents said** : I had a negative experience or interaction at work.\n  * **Approx. 11 % Respondents said** : A friend told me about a job opportunity.\n  * **Approx. 10 % Respondents said** : I saw an employer's advertisement.
## 5.26 Types of Non-degree education in which people participated
"**Types of Non-degree education in which people participated :**\n  * **Approx. 29 % Respondents said** : Taught yourself a new language, framework or tool without taking a formal course.\n  * **Approx. 16 % Respondents said** : Taken an online course in programming and software developement(eg. a MOOC)\n  * **Approx. 14 % Respondents said** : Contributed to open source softwares\n  * **Approx. 12 % Respondents said** : Received on-the-job training in software developement\n  * **Approx. 9 % Respondents said** : Participated in hackathon\n  * **Approx. 8 % Respondents said** : Participated in online coding compitition(eg. HackerRank, CodeChef or TopCoder)\n  * **Approx. 6 % Respondents said** : Taken a part-time in-person course in programming or software development\n  * **Approx. 5 % Respondents said** : Completed an industry certification program(eg. MCPD)\n  * **Approx. 4 % Respondents said** :  Participated in a full-time developer training program or bootcamp"
## 5.27 Top resources used by peoples who taught yourself without taking a course
"**Top resources used by peoples who taught yourself without taking a course :**\n  * **Approx. 22 % Respondents said** : The official documentation and/or standards for the technology\n  * **Approx. 22 % Respondents said** : Q&A on the StackOverflow\n  * **Approx. 13 % Respondents said** : A book or e-book from o'Reilly, Apress or a similar publisher\n  * **Approx. 13 % Respondents said** : Online developer communities other than StackOverflow(eg, forums, listservs, IRC Channels etc)\n  * **Approx. 13 % Respondents said** :The technology's online help system\n  * **Approx. 5 % Respondents said** : A college/University computer  science or software engineering books"
## 5.28 Top reasons who participated in online coding compitition or hackathon
"**Top reasons who participated in online coding compitition or hackathon :**\n  * **Approx. 26 % Respondents said** : Because I find it enjoyable\n  * **Approx. 23 % Respondents said** : To improve my general technical skills or programming ability\n  * **Approx. 18 % Respondents said** : To improve my knowledge of a specific programming language , framework or technology\n  * **Approx. 10 % Respondents said** : To improve my ability to work on a team with others programmers\n  * **Approx. 9 % Respondents said** : To build my professional network\n  * **Approx. 7 % Respondents said** : To help me find a new job opportunities\n  * **Approx. 6 % Respondents said** : to win prizes or cash awards"
## 5.30 Top most used IDE  by the developers
**Top most used IDE  by the developers :**\n  * Approx. **12 %** developers using **Visual Studio Code**\n  * Approx. **12 %** developers using Visual Studio \n  * Approx. **11 %** developers using **Notepad++**\n  * Approx. **10 %** developers using **Sublime Text**\n  * Approx. **9 %** developers using **Vim**
## 5.31 Top Used Operating system by the developers
**Top Used Operating system by the developers :**\n  * Approx. **50 %** developers are using **Windows**\n  * Approx. **27 % **developers are using **MacOS**\n  * Approx. **23 %** developers are using **Linux-based**\n  * **Only  0.2 %** developers are using **BSD/Unix**
## 5.32 Top version control system developers regularly use
**Top version control system developers regularly use :**\n  * Approx. **63 %** developers are using **Git**(**Most popular version control system**)\n  * Approx. **12 %** developers are using **Subversion**\n  * Approx. **8 %** developers are using **Team Foundation Version Control**\n  * Approx **6 %** deveopers are using **Zip file back-ups**
## 5.33 Top methodologies developers have experience working in
**Top methodologies developers have experience working in :**\n  * Approx. **33 %** developers experience in **Agile**\n  * Approx. **24 %** developers experience in **Scrum**\n  * Approx. **14 %** developers experience in **Kanban**\n  * Approx. **11 %** developers experience in **Pair programming**
## 5.34.5 Top ergonomic furniture or devices developers use on a regular basis
**Top ergonomic furniture or devices developers use on a regular basis :**\n  * **Approx. 38 %** developers use : **Ergonomic Keyboard or mouse**\n  * **Approx. 37 %** developers use : **Standing desk**\n  * **Approx. 16 %** developers use : **Wrist/hand supports or braces**\n  * **Approx. 9 %** developers use : **Fatigue-relieving floor mat**
"## 5.34.6 In a typical week, how many times do developers exercise?"
"**In a typical week, how many times do developers exercise? **\n  * **Approx. 38 %** developers said : **I don't typically excercise**\n  * **Approx. 29 %** developers said : **1 - 2 times per week**\n  * **Approx. 20 %** developers said : **3 - 4 times per week**\n  * **Approx. 14 %** developers said : **Daily or almost every day**"
## 5.35 Age of the developers of participated in the survey
**Age of the developers of participated in the survey :**\n  * **Apprx. 49 %** developers : **25 - 34 years old**\n  * **Apprx. 24 %** developers : **18- 24 years old**\n  * ** Apprx. 18 %** developers : **35 - 44 years old**\n  * **Apprx. 5 %** developers : **45 - 54 years old**\n  * **Apprx. 3 %** developers : **under 18 years old**\n  * **Apprx. 1 %** developers : **55 - 64 years old**\n  * **Apprx. 0.2 %** developers : **65 years or older**
"## 5.38.3 In a typical week, how many times do developers exercise(Male V.S. Female)?"
## 5.38.4 Age of the developers who participated in the survey(Male V.S. Female)
## 5.38.5 Fromal Education of developers(Male V.S. Female)
## 5.38.6 Top DevType with Median Salary(Male V.S. Female)
"**Top DevType with Median Salary(Male V.S. Female) : **\n  * Female having more median salary than Male in these DevType :\n    * **DevOps specialist**\n    * **C-suite executive (CEO, CTO etc.)**\n    * **Full Stack developer**\n    * **Educator or acadmic research**"
## 5.38.7 Top Countries where respondents are > 500 with Median Salary(Male V.S. Female)
**Top Countries where respondents are > 500 with Median Salary(Male V.S. Female) :**\n  * Female having more median salary than Male : **China**\n  * Female and Male having equal median salary : **Brazil**
"## 5.39.3 Top DevType with highest median salary (India, USA , Gobal)"
"**Top 5 DevType with highest median salary Globally in USD :**\n  * **Engineering manager :** \$ 88,573\n  * **DevOps specialist :** \$ 72,469\n  * **C-suite executive(CEO, CTO) :** \$ 69,244\n  * **Product manager : ** \$ 63,174\n  * **Data scientist or machine learning specialist :** \$ 60,000\n* Thea bove Top 5 Deytype with highest median salary  are same in **India**, **USA** and **Globally**. Only Devops specialist and C-suite executive are interchangable but in india  and USA are same."
## 5.43 Time to get a full-time job offer after doing  developer training program or bootcamp
**Time to get a full-time job offer after doing developer training program or bootcamp :**\n  * **Approx. 45 %** respondents said : **I already had a full time job as a developer when i began the program**\n  * **Approx. 16 %** respondents said : **Immediately after graduating** \n  * **Approx. 10 %** respondents said : **One to three month**\n  * **Approx. 9 %** respondents said : **I haven't gotten a developer job**\n  * **Approx. 5 %** respondents said : **Less than a month 4 to 6 month**\n  * **Approx. 4 %** respondents said : **6 month to a year**\n  * **Approx. 3 %** respondents said : **Longer than a year**
## 5.44.1 Salary  of Data Scientist / Machine Learning Specialists
"* Most of the Data Scientist / Machine Learning Specialist getting salary **\$ 0 to \$2,50,000 **"
## 5.44.5 Most popular languages (Data Scientist / Machine Learning Specialists)
"* Most popular languages in which Data Scientist / Machine Learning Specialist are working or want to work is **Python** followed by **R**, **SQL**, **Bash/Shell**.\n* **C++** and **Scala** are languages which are growing where Data Scientist / Machine Learning Specialist want to work."
## 5.44.6 Most popular Databases (Data Scientist / Machine Learning Specialists)
"* Top most  Databases in which Data Scientist / Machine Learning Specialist are working **MySQL** followed by **PostgreSQL**, **SQLServer**, **MongoDB**, **Apache Hive** etc.\n* Top most  Databases in which Data Scientist / Machine Learning Specialist want to work **PostgreSQL** followed **MongoDB**, **MySQL**, **Elasticsearch**, **Apache Hive** etc."
## 5.44.7 Most popular platforms (Data Scientist / Machine Learning Specialists)
* Most popular  Platforms on which Data Scientist / Machine Learning Specialist are working or want to wok is **Linux** followed by **AWS**. **Raspberry** Pie is the Platforn after these on which most  Databases in which Data Scientist / Machine Learning Specialist want to work.
## 5.44.8 Most popular Frameworks (Data Scientist / Machine Learning Specialists)
"* **Most popular Framworks** on which Data Scientist / Machine Learning Specialists are working or want to work is **TensorFlow** followed by **Spark**, **Torch/PyTorch**, **Hadoop**, **Django** etc."
## 5.44.9 Top most used IDE  by Data Scientist / Machine Learning Specialists)
"* **Top most used IDE**  by Data Scientist / Machine Learning Specialist is **IPython / Jupyter** followed by **RStudio**, **Vim**, **PyCharm**, **Sublime Text**."
## 5.45.1  Most dangerous aspects of increasingly advanced AI technology
"**Most dangerous aspects of increasingly advanced AI technology in decreasing order :**\n  * Approx. **29 %** peoples thinks that **Algorithms making important decisions**.\n  * Approx. **28 %** peoples thinks **Artificial intelligence surpassing human intelligence**.\n  * Aprox. **24 %** peoples thinks **Evolving definations of ""fairness"" in algorithmic V.S. human decisions**.\n  * Approx. **20 %** peoples thinks **Increasing automation of jobs**."
> ## 5.45.2  Most Exciting aspects of increasingly advanced AI technology
"**Most exciting aspects of increasingly advanced AI technology in decreasing order :**\n  * Approx. **41 %** peoples thinks **Increasing automation of jobs**.\n  * Approx. **23 %** peoples thinks that **Algorithms making important decisions**.\n  * Approx. **23 %** peoples thinks **Artificial intelligence surpassing human intelligence**.\n  * Aprox. **12 %** peoples thinks **Evolving definations of ""fairness"" in algorithmic V.S. human decisions**.\n  "
## 5.45.3 Whose responsibility is it to consider the ramifications of increasingly advanced AI technology?
**Whose responsibility is it to consider the ramifications of increasingly advanced AI technology :**\n  * Approx. **48 %** peoples said that **The developers or the people creating the AI**.\n  * Approx. **28 %** peoples said that **A governmental or other regulatory body**.\n  * Approx. **17 %** peoples said that **Prominent industry leaders**.\n  * Approx. **8 %** peoples said that **Nobody**.
## 5.45.4 What's peoples take on the future of artificial intelligence ?
"**Peoples take on the future of artificial intelligence :**\n  * Approx.** 73 %** peoples said that **I'm excited about the possibilities more than worried about the dangers**.\n  * Approx. **19 %** peoples said that **I'm worried about the dangers more than I'm excited about the possibilities**.\n  * Approx. **8 %** peoples said that **I don't care about it, or I have't thought about it**."
Get the feature importances from the trained model:
"As we can see from the diagram showing feature importances, the most important features are:\n* Customer's account balance,\n* Customer's age,\n* Number of contacts performed during this campaign and contact duration,\n* Number of contacts performed before this campaign."
> Let's plot a heatmap to visualize the correlation between Attrition and these factors.
"> As shown above, ""Monthly Rate"", ""Number of Companies Worked"" and ""Distance From Home"" are positively correlated to Attrition;  while ""Total Working Years"", ""Job Level"", and ""Years In Current Role"" are negatively correlated to Attrition."
"> **Classification Accuracy** is the number of correct predictions made as a ratio of all predictions made.  \nIt is the most common evaluation metric for classification problems. However, it is often **misused** as it is only really suitable when there are an **equal number of observations in each class** and all predictions and prediction errors are equally important. It is not the case in this project, so a different scoring metric may be more suitable."
> **Area under ROC Curve** (or AUC for short) is a performance metric for binary classification problems. \nThe AUC represents a **model’s ability to discriminate between positive and negative classes**. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.
"> Based on our ROC AUC comparison analysis, **Logistic Regression** and **Random Forest** show the highest mean AUC scores. We will shortlist these two algorithms for further analysis. See below for more details on these two algos."
"> Random Forest allows us to know which features are of the most importance in predicting the target feature (""attrition"" in this project). Below, we plot features by their importance."
> Random Forest helped us identify the Top 10 most important indicators (ranked in the table below).
#### Evaluation
> The Confusion matrix provides us with a much more detailed representation of the accuracy score and of what's going on with our labels - we know exactly which/how labels were correctly and incorrectly predicted
> AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. The green line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). 
"> As shown above, the fine-tuned Logistic Regression model showed a higher AUC score compared to the Random Forest Classifier. "
"### **Generating a Confusion Matrix**\n\nNote, this code is taken straight from the SKLEARN website, an nice way of viewing confusion matrix. This is useful in classification problems. Consider this issue:\n- Your underlying data only has **10 / 90** virus to non-virus samples to detect\n- if you get an accuracy of 90%, what does that mean? Is that better than guessing? Nope. It's the same\n- **Practical sense** if you are a hospital, you want to be very careful with **False Positives: telling people they have a disease when they don't**. That's much worse than **False Negatives**, not detecting a disease. As a result, people will design around the priority of these two different metrics. \n\n#### A sample confusion matrix\n"
"\n## ** 7. Grid Search/Randomized Search: the quest for hyperparameters ** \n> \n\n#### **Look at how many options are in  logistic regression:**\n>        LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n>                  intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n>                 penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n>                  verbose=0, warm_start=False)\n\n\nMany of the advanced machine learning functions have a large number of model options that can be entered. these are often called **hyper parameters**. These address questions such as: \n- ""how long should the model run"", or \n- ""how many times should my computer re-look at the data"" or \n- ""how slow should the computer work through the problem?"" \n\nTo assist answering some of these questions, `sklearn` has `GridSearch` and `RandomizedSearch` which will try various combinations with a provided model, compare scores and return the optimal model that should be tried."
"It looks like the heatmap of injuries is somewhat different from the general heatmap of the field! Probably, there are more dangerous areas on the field, where players are more likely to get injured!\n\nWe can use this insight for the feature engineering for the injury prediction model!"
This is the KDE plot for the injury play locations on the field:
"So is the speed of the player, his position on the field and the number of the game (`PlayerGame`) correlated with the injury?"
The diagram above shows that there is no correlation between the injury and the number of games/plays per game played.
Explore the correlations:
We can see that the distances and speed per play have a negative correlation with PlayerGamePlay. Does it mean that players get tired and move slower?
Let's look at the distribution of motion features for plays with injury:
It seems that speed for plays with injuries may come from a different distribution. Let's try [Kolmogorov–Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) to check if those samples come from one distribution:
"The p-value is very small, so we can reject the null hypothesis that both samples are drawn from one distribution. It means that most certainly, we do have some difference between speed for normal plays and plays with the injury."
Apply Kolmogorov-Smirnov test:
Explore the speed:
We can see that the speed distribution for the natural and synthetic field type are almost identical!
The same thing is for the distance.
We get identical distributions for the angle too.\n\nIt means that the type of turf does not affect the main motion features.
It is also useful to look at the distributions of the features to see how their values influence predictions.\n\nFrom the histogram for the feature `loan_amnt` (top 1) the loan is more likely to be returned (good loans) if the loan amount is lower. This makes sense because smaller loan amounts usually have smaller monthly installments that are easier to pay.
"From the histogram for the feature `mths_since_recent_inq` (top 2) the loan is less likely to be returned (bad loans) if the borrower had an inquiry recently. This also makes sense because inquiries are usually done when someone applies for a loan, a credit card, etc. so recent inquiries could indicate bad financial stability of the borrower."
"From the histogram for the feature `revol_util` (top 3) the loan is less likely to be returned (bad loans) if the revolving utilization is lower. This actually doesn't make much sense because revolving utilization is the percentage of the used credit on your credit card so higher revolving utilization indicates worse financial stability. Nevertheless, this dataset shows otherwise and it could be an interesting topic for discussion."
"# 5. Model adjustment\n\nThe previously reported model was obtained by minimizing both false positive and false negative errors that contribute to precision and recall respectively. In reality, however, one of these errors might have a larger impact so it would be better to optimize for it instead. In case of loan investing, the false positive errors are the number of bad loans that were identified as good so the investor will loose money by investing in them. This is a direct loss and should be avoided. The false negative errors are the number of good loans that were identified as bad so the investor will not earn extra money by not investing in them. This is a missed opportunity and is less critical compared to the direct loss. Therefore, the false positive errors should be decreased (higher precision) even if the false negative errors will be increased (lower recall). The connection between precision and recall can be visualized using the precision-recall curve (see below). To calculate it, one requires probabilities of belonging to class 1 rather than the predicted labels. This precision-recall curve is calculated for the validation dataset because adjusting precision or recall is similar to adjusting hyperparameters. For each precision-recall pair the function `precision_recall_curve()` also returns the corresponding probability threshold. This threshold is the actual hyperparameter that will be used to obtain the best precision."
"From the precision-recall curve the best precision is 1 but then the recall would be extremely low so in the end the model might not predict good loans at all. Therefore, I exclude 1 from the precision array and find its maximum. The threshold array `t` returned by `precision_recall_curve()` is missing the threshold 0 in the beginning so I add it to match the dimension of the precision array `p`. Then I find the threshold that correspond to the maximum precision and recalculate the predicted labels. The obtained precision score for the adjusted labels is indeed the maximum (excluding 1) as can be seen from the precision-recall curve."
"Finally by using the adjusted threshold on the testing dataset, the adjusted precision 0.939 is indeed within the above 95% confidence interval. Note, however, that the recall is significantly decreased from 0.632 to 0.068 but the precision only increased from 0.931 to 0.939. Of course the gain in precision depends on the train-test split and for a different testing dataset can be closer to the right boundary of the confidence interval. Getting higher values than that, however, is unlikely."
Thanks for **UPVOTING** this kernel! Trying to become a Kernels Master. 🤘\n\nCheck out my other cool projects:\n- [📊 Interactive Titanic dashboard using Bokeh](https://www.kaggle.com/pavlofesenko/interactive-titanic-dashboard-using-bokeh)\n- [🌐 Extending Titanic dataset using Wikipedia](https://www.kaggle.com/pavlofesenko/extending-titanic-dataset-using-wikipedia)\n- [👪 Titanic extended dataset (Kaggle + Wikipedia)](https://www.kaggle.com/pavlofesenko/titanic-extended)
"### Correlation and Causation 📍\n\nReference: https://www.abs.gov.au/websitedbs/D3310114.nsf/home/statistical+language+-+correlation+and+causation\n\nTwo or more variables considered to be related, in a statistical context, if their values change so that as the value of one variable increases or decreases so does the value of the other variable (although it may be in the opposite direction). For example, for the two variables ""hours worked"" and ""income earned"" there is a relationship between the two if the increase in hours worked is associated with an increase in income earned. If we consider the two variables ""price"" and ""purchasing power"", as the price of goods increases a person's ability to buy these goods decreases (assuming a constant income).\n\nCorrelation is a statistical measure (expressed as a number) that describes the size and direction of a relationship between two or more variables. A correlation between variables, however, does not automatically mean that the change in one variable is the cause of the change in the values of the other variable.\n\nCausation indicates that one event is the result of the occurrence of the other event; i.e. there is a causal relationship between the two events. This is also referred to as cause and effect.\n\nTheoretically, the difference between the two types of relationships are easy to identify — an action or occurrence can cause another (e.g. smoking causes an increase in the risk of developing lung cancer), or it can correlate with another (e.g. smoking is correlated with alcoholism, but it does not cause alcoholism). In practice, however, it remains difficult to clearly establish cause and effect, compared with establishing correlation. "
"\n\n Correlation\n\n\n\n**IMPORTANT NOTE!** There is 'multicollinearity' problem\n\nHere we see that there is relatively high (0.67, positive) correlation \nbetween 'free sulfur dioxide' and 'total_sulfur_dioxide' variables.\nThere is relatively high (-0.68, negative) correlation between \n""pH"" and ""fixed_acidity"" variables. And there is about 0.5 correlation\nbetween some of other variables. That's why we must consider when build \nMachine Learning models."
\n\n#### 7.19 |ROC AUC - Light GBM Model 📉
# Thank you very much 🙂
---------------------------------------\n## Embarked
" Observation:\n    \n* Many passengers on board at S port died.\n* For passengers boarding at port C, the survival rate is higher than the mortality rate."
" Observation:\n    \n* Among the passengers who boarded at S port, the proportion of males is higher than that of other ports."
---------------------------------------\n# Checking Correlation
" Observation and Decision:\n    \n* There is a large correlation between FamilySize and SibSp and Parch. Since the derived variable FamilySize is made of SibSp and Parch, SibSp and Parch are removed.\n* The relationship between Cabin and Has_Cabin is high. Therefore, the derived variable Has_Cabin is left and Cabin is removed.\n* The relationship between Fare and Fare_class is high. Fare is selected because skewness is removed by nonlinear transform of the Fare feature.\n* There are many features that are not related to the survived value."
"-------------------------------------------------------------\n# Selecting Features\n\nFeatures that are not helpful in judging the above heatmap and survivors, or that have other derived variables, will be removed."
**Let's check the correlation of each feature.**
Let's check the correlation between the target value (Suvived) and other features.
"Compared to the soft blending model, the boundary does not look clean."
-------------------------------------------------------------------------------------------------\n## Calibrating the final model\n\n> This function calibrates the probability of a given estimator using isotonic or logistic regression. \n
### 3.Plot Data
### 4.Default Network
### 6.Model Training and Prediction
## Tensorboard\n---\n[**Go to Top**](#Tensorflow-Tutorial)\n\n![](https://www.tensorflow.org/images/graph_vis_animation.gif)\n\n### 1. Load dataset
### 3. Model Training with Placeholder of data
### 4.Generate TensorBoard and see the results
### 1.Load Library
### 2. Read dataset and set parameter
### 3.Plot one example
### 4.  Define Placeholder to store data
### 6. Model Run from this session and plot model with t-sne
### 7. Top 10 Prediction Results
###  1.Load data
### 2.Hyper parameter and data Load
### 3.Plot data
### 4.Placeholder data loader
### RNN Regression\n---\n[**Go to Top**](#Tensorflow-Tutorial)\n\n### RNN\n\n* This time we will use **RNN for regression training (Regression).** We will continue **to predict a cos curve using the sin curve we created. Next we will determine the various parameters of the RNN (super-parameters):)**\n \n ### 1. Load Packages
### 2. Data plot
### 3. Define Placeholder
### 5. Final Model Training
"### AutoEncoder\n---\n[**Go to Top**](#Tensorflow-Tutorial)\n\n### AutoEncoder\n\n![](https://cdn-images-1.medium.com/max/2000/1*woWzbXU2bmshM1czEur72g.gif)\n* **Autoencoder** is an **unsupervised learning algorithm** that **uses a backpropagation algorithm** to **make the target value equal to the input value**. as the picture shows:\n\n![](https://cdn-images-1.medium.com/max/1600/1*wr9QeopG3BK4Lz6DGhlqbA.png)\n\n* A **autoencoder is a neural network that has three layers:** an ***input layer, a hidden (encoding) layer, and a decoding layer.*** \n* *The network is trained to reconstruct its inputs, which forces the hidden layer to try to learn good representations of the inputs.*\n* ***An autoencoder neural network is an unsupervised Machine learning algorithm that applies backpropagation, setting the target values to be equal to the inputs. An autoencoder is trained to attempt to copy its input to its output. Internally, it has a hidden layer that describes a code used to represent the input.***\n\n* The autoencoder tries to learn a function hW,b(x)≈xhW,b(x)≈x. In other words, it is trying to learn an approximation to the identity function, so as to output x̂ x^ that is similar to xx.\n* Autoencoders belong to the neural network family, but they are also closely related to PCA (principal components analysis).\n\n#### Some Key Facts about the autoencoder:\n\n* It is an unsupervised ML algorithm similar to PCA\n* It minimizes the same objective function as PCA\n* It is a neural network\n* The neural network’s target output is its input\n\n* ***Autoencoders although is quite similar to PCA but its Autoencoders are much more flexible than PCA. Autoencoders can represent both liners and non-linar transformation in encoding but PCA can only perform linear transformation. Autoencoders can be layered to form deep learning network due to it’s Network representation.***\n\nFore More Read this article : [**Autoencoder**](https://www.jeremyjordan.me/autoencoders/)"
### 1.Load data
### 2.Generate dataset
### 3. Plot the data
### 4.Placeholder
### 6.AutoEncoder Training
### Visualozation in 3D Plot
"### Generative Adversarial Nets\n\n---\n[**Go to Top**](#Tensorflow-Tutorial)\n\n**1. The Story Behind GAN**\n\n* In the academic world, **GAN founder Ian Goodfellow** discussed academic issues with colleagues after the drunkenness of the bar. At that time, Emmanuel raised the initial **idea of GAN**, but at that time ***he did not get the approval of his colleagues. After returning from the bar, he found that his girlfriend had fallen asleep. Then, I wrote the code day and night, and found that it was really effective, so after some research, GAN was born,*** a mountain work. Attach a photo of the Great God.\n\n![](https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawCUoIVNsXpWVcLibMiaesQkjxuV9uR0D6XeOpaRbic6AzvDbLloEYYIavMicMYMlLCsic6dIrr7hPicEWoQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)\n\n**Architecture of GAN**\n\n![](https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawCUoIVNsXpWVcLibMiaesQkjxuxMTNqrJJy7A9mNicyyGwqWmKJWUseJgBhlNOKBIOc9B3Gr64umFrJA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)\n\n**2. The principle of GAN:**\n\n* **GAN's main inspiration comes from the idea of zero-sum game in game theory.** When applied to **deep learning neural network, it is through continuous generation of network G (Generator) and discriminant network D (Discriminator), so that G learns the distribution of data.** If the image generation is used, G can generate a realistic image from a random number after the training is completed. The main functions of G, D are:\n\n*  **G** is a ***generative network that receives a random noise z (random number) and generates an image from this noise.***\n* **D** is a ***discriminating network that discriminates whether a picture is ""real"". Its input parameter is x, x represents a picture, and the output D(x) represents the probability that x is a real picture. If it is 1, it means that 100% is the real picture, and the output is 0, it means that it is impossible to be true.\n* **During the training process,** the **goal of generating the network G is to generate a real picture** as much as possible to **deceive the discriminant network D. The goal of D is to identify the false images and real images generated by G as much as possible.** Thus, G and D constitute a dynamic ""gaming process"", and the final equilibrium point is the Nash equilibrium point.\n\n**3. Features of GAN:**\n\n* **Compared to the traditional model, he has two different networks instead of a single network, and the training method uses the confrontation training method.**\n* **The gradient update information of G in GAN comes from discriminator D, not from data sample.**\n\n**4. Advantages of GAN:**\n(The following section is taken from ian goodfellow's Q&A in Quora)\n* **GAN is a generative model that uses only backpropagation compared to other generation models (Boltzmann machines and GSNs) without the need for complex Markov chains.**\n* **GAN** can produce a **clearer, more realistic sample** than all other models\n* **GAN uses an unsupervised learning style training** that can be widely used in unsupervised and semi-supervised learning.\n* Compared to the **variational self-encoder**, **GANs** does not **introduce any deterministic bias, and the variational method introduces deterministic bias** because ***they optimize the lower bound of the log likelihood rather than the likelihood itself, which looks The examples that led to the generation of VAEs are more blurred than GANs***\n* **Compared to VAE, GANs has no variation lower bound.** If the **discriminator** is well trained, the **generator can perfectly learn the distribution of training samples.** In other words, GANs are gradual, but VAE is biased.\n* **GAN is applied to some scenes, such as picture style migration, super resolution, image completion, denoising, avoiding the difficulty of loss function design, regardless of the three seven twenty-one, as long as there is a benchmark, directly on the discriminator,** The rest is handed over to the confrontation training.\n\n**5. The disadvantages of GAN:**\n* **Training GAN needs to reach Nash equilibrium, sometimes it can be done by gradient descent method, sometimes it can't be done. We have not found a good way to reach Nash Equilibrium, so training GAN is unstable compared to VAE or PixelRNN. But I think in practice it is still more stable than training the Boltzmann machine.**\n* **GAN is not suitable for processing discrete forms of data, such as text**\n* **GAN has problems with unstable training, gradient disappearance, and mode collapse (currently resolved)**\n\nGenerally, when the GAN training is unstable, the result is very poor, but it cannot be improved even after the training time is lengthened.\n\n**6.Why is the optimizer in GAN not commonly used for SGD?**\n\n* 1. SGD is easy to oscillate, and it is easy to make GAN training unstable.\n* 2. The purpose of GAN is to find the Nash equilibrium point in the high-dimensional non-convex parameter space. The Nash equilibrium point of GAN is a saddle point, but SGD will only find the local minimum value, because SGD solves the problem of finding the minimum value, GAN It is a game problem.\n\n**7.Why GAN is not suitable for processing text data**\n\n1. Text data is discrete compared to image data, because for text, it is usually necessary to map a word to a high-dimensional vector, and the final predicted output is a one-hot vector, assuming that the output of softmax is ( 0.2, 0.3, 0.1, 0.2, 0.15, 0.05) then become onehot is (0,1,0,0,0,0), if the softmax output is (0.2, 0.25, 0.2, 0.1, 0.15, 0.1), one -hot is still (0, 1, 0, 0, 0, 0), so for the generator, G outputs different results but D gives the same result, and the gradient update information is not very good. Passed to G, so the judgment of the final output of D is meaningless.\n2. In addition, the loss function of GAN is JS divergence, and JS divergence is not suitable for measuring the distance between the distributions that do not want to intersect.\n\n(WGAN uses the wassertein distance instead of the JS divergence, but the ability to generate text is still limited. GAN uses seq-GAN in the generated text, and the product of reinforcement learning)\n\n**8.Some tips for training GAN**\n1. The input is normalized to (-1,1), and the activation function of the last layer uses tanh (except BEGAN)\n2. Using the loss function of wassertein GAN,\n3. If there is tag data, try to use tags. It is also suggested that using reverse tags works well. In addition, tag smoothing, single-sided label smoothing or bilateral label smoothing is used.\n4. Use mini-batch norm, if you don't use batch norm you can use instance norm or weight norm\n5. Avoid using the RELU and pooling layers to reduce the possibility of sparse gradients. You can use the leanrelu activation function.\n6. The optimizer should choose ADAM as much as possible. The learning rate should not be set too large. The initial 1e-4 can be used for reference. In addition, the learning rate can be continuously reduced as the training progresses.\n7. Adding Gaussian noise to the network layer of D is equivalent to a regular"
### 2.Plot the data
### 3.Design Art work
### 5.Model Running
### Notebook is continue updated
"# Model by Model Confusion Matrix\n\nNow we have selected our models, we can view how they performed in each prediction.\n\nA great way to visualise where your data performs well, and where it performs poorly.\n\n\n\n"
"# Model Success\n\nSo all of our models have quite a high accuracy, the highest being 95% (Tuned Random Forest).\n\nBut the recall of Strokes is quite poor across the board.\n\nResults always need to be considered carefully - ask yourself: 'Why do I need to predict this value?'\nIn our case, I would assume it would be to offer medical advice / preventative treatment to those we predict will have a stroke, therefore, in the real-world, I would probably select the model with the highest recall.\n\n\nThe model's can be considered a success - that is, healthcare professionals would be better equipped with this model than without it.\n\n\nSeeing as Random Forest did have the highest accuracy, I will delve deeper in to the model and how it works - woth feature importance & LIME.\n\nHowever, the actual selection of model would be up for debate due to the recall variance. \n\n\n\n # Which model would you choose?\n \n I am curious to hear your opinions."
"# Selection\n\nI would opt for Logistic Regression. \n\nIt Has a decent accuracy, and the best recall. I feel that on balance it provides the best overall results."
"# Model Interpretation\n\nI'll use some valuable tools which help to uncover the supposed ""Black Box"" of machine learning algorithms.\n\nAs I always say, the models we create need to be sold to business stakeholders. And if business stakeholders don't understand what we're creating, they may be less likely to back the project."
Let's plot this with the top 3 features highlighted
# We can also use SHAP\n\nSHAP Values (SHapley Additive exPlanations) break down a prediction to show the impact of each feature. \n\nIt interprets the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value (e.g. zero)\n\n\nIn this case I will use it for the Random Forest Model. It can be used for any type of model but it is by far the quickest with tree based models.\n\nIt is possible to change the colour values in SHAP plots too\n
"# SHAP explained\n\nThe plot above shows the effext of each data point on our predictions. \n\nFor example, for age, the top left point reduced the prediction by 0.6.\n\nThe color shows whether that feature was high or low for that row of the dataset\nHorizontal location shows whether the effect of that value caused a higher or lower prediction.\n\n\nWe can also see how our Random Forest Model is heavily skewed in favour of predicting no-strokes.\n\n# There's more? SHAP dependence plots\n\nWe can also focus on how the impact of each variable changes, as the variable itself changes.\n\nFor instance, Age. When this variable increases, the SHAP value also increases - pushing the patient closer to  our 1 condition (stroke). \n\nThis is also shown with colour -  pink/red representing those who suffered a stroke."
"The same plot, but with a more interesting varibale. \n\nHere we see a clear cutoff point for when strokes become far more common - after a BMI of around 30 or so.\n\nSuch is the power of SHAP visualization."
"# One-Step Further: Logistic Regression with LIME\n\nWhen it comes to model interpretation, sometimes it is useful to unpack and focus on one example at a time.\n\nThe LIME package enables just that.\n\nLime stands for Local Interpretable Model-agnostic Explanations - here's an example:"
"Now with less dimension than before, we can see how our series distributed in 2 dimensions."
"The result of PCA is basically, representation of a 333-dimensional data point as a 2-dimensional data point. As a result of that instead of a time series, we have just 2 value for each series."
"And this is the result of the basic KMeans, pretty logical and straight forward."
"And again thanks to the clever implementation of ```KMeans``` algorithm by ```sklearn``` team, labels are returned in the same order. Thus, we can use the same code to visualize our cluster in series."
"And we can see that now with the ```PCA``` algorithm, our series are much more equally distributed to clusters than before."
"Same chart, just in a different library for practice! \n\n## Learning Rate Distribution\n\nNext we can start plotting the distributions of hyperparameter values searched. We expect random search to align with the search domain, while the Bayesian hyperparameter optimization should tend to focus on more promising values, wherever those happen to be in the search domain.\n\nThe dashed vertical lines indicate the ""optimal"" value of the hyperparameter."
"## Distribution of all Numeric Hyperparameters\n\nWe can make the same chart now for all of the hyperparameters. For each setting, we plot the values tried by random search and bayesian optimization, as well as the sampling distirbution."
"## Evolution of Search\n\nAn interesting series of plots to make is the evolution of the hyperparameters over the search. This can show us what values the Bayesian optimization tended to focus on. The average cross validation score continued to improve throughout Bayesian optimization, indicating that ""more promising"" values of the hyperparameters were being evaluated and maybe a longer search would prove useful (or there could be a plateau in the validation scores with a longer search)."
The final plot is just a bar chart of the `boosting_type`. 
"The Bayes optimization spent many more iterations using the `dart` boosting type than would be expected from a uniform distribution. We can use information such as this in further hyperparameter tuning. For example, we could use the distributions from Bayesian hyperparameter optimization to make a more focused hyperparameter grid for grid or even random search. \n\n![](http://)For this chart, we can also make it in Altair for the practice."
"The 3D plot below distinguishes best between fraud and non-fraud data\nby using both of the engineered error-based features. Clearly, the\noriginal *step* feature is ineffective in seperating out fraud. Note\nthe striped nature of the genuine data vs time which was aniticipated\nfrom the figure in section 5.1."
back to top
Smoking gun and comprehensive evidence embedded in the dataset of the \ndifference between fraudulent\nand genuine transactions is obtained by examining their respective\ncorrelations in the heatmaps below.
\n##### 6.1. What are the important features for the ML model?\nThe figure below shows that the new feature *errorBalanceOrig* that we created is the most relevant feature for the model. The features are ordered based on the number of samples affected by splits on those features.
Cross Correlation chart to view collinearity within the features. Kendall's seems appropriate as the Output Variable is numerical and much of the input is categorical. Here is a chart to guide you to which method to use based on your dataset.\n\n![image.png](attachment:image.png)
Look at some correlation values in a list format
We use the scipy  function boxcox1p which computes the Box-Cox transformation of **\\(1 + x\\)**. \n\nNote that setting \\( \lambda = 0 \\) is equivalent to log1p used above for the target variable.  \n\nSee [this page][1] for more details on Box Cox Transformation as well as [the scipy function's page][2]\n[1]: http://onlinestatbook.com/2/transformations/box-cox.html\n[2]: https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.special.boxcox1p.html
"I recommend plotting the cumulative sum of eigenvalues (assuming they are in descending order). If you divide each value by the total sum of eigenvalues prior to plotting, then your plot will show the fraction of total variance retained vs. number of eigenvalues. The plot will then provide a good indication of when you hit the point of diminishing returns (i.e., little variance is gained by retaining additional eigenvalues)."
"Do some PCA for the dataset, to remove some of the collinearity. Not sure if this will have any effect as collinearity is usually not detrimental to many or most algorithms"
"We choose number of eigenvalues to calculate based on previous chart, 20 looks like a good number, the chart starts to roll off around 15 and almost hits max a 20. We can also try 30,40 and 50 to squeeze a little more variance out..."
Compare different predictions to each other:\n\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_regressor.html
Example plot\n\n![image.png](attachment:image.png)
Show adjustments
"We have bumped up the predictions and they look correct so far, now to verify on the previous chart"
Show new predictions
correlation looks much better at the high end
Show both adjustments
Final stacked model
distribution of residual errors looks normal except for ET() and SVR()
**Create File for Submission to Kaggle**
"Now, lets which category has much incorrect predictions"
"# Conclusion\nIt seems our model has maximum number of incorrect predictions for Basal cell carcinoma which has code 3, then second most missclassified type is Vascular lesions code 5 then Melanocytic nevi code  0 where as Actinic keratoses code 4 has least misclassified type.\n\nWe can also further tune our model to easily achieve the accuracy above 80% and I think still this model is efficient in comparison to detection with human eyes having 77.0344% accuracy \n\nI hope kagglers like my stepwise approach to classify cancer types. If like then kindly dont forget to hit the **like**\n"
# Creating pipeline to evaluate different models
Nice! We can see that Xgb is the best model. Let's use it with hyperopt
# XGB - HyperOpt Optimization
## HyperOpt Run
### Necessary or Not??
Clearly Python is a much more necessary skill compared to R.\n\nSpecial Thanks to [Steve Broll](https://www.kaggle.com/stevebroll) for helping in the color scheme.
### Number Of Users By Language
"The number of Python users are definetely more than R users. This may be due to the easy learning curve of Python. However there are more users who know both the languages. These responses might be from established Data Scientists,as they tend to have a knowledge in multiple languages and tools."
"Python coders have a slightly higher median salary as that compared to their R counterparts. However, the people who know both these languages, have a pretty high median salary as compared to both of them.\n\n## Language Used By Professionals"
"As I had mentioned earlier, R beats Python in visuals. Thus people with Job-Titles like Data Analyst, Business Analyst where graphs and visuals play a very prominent role, prefer R over Python. Similarly almost 90% of statisticians use R. Also as stated earlier, Python is better in Machine Learning stuff, thus Machine Learning engineers, Data Scientists and others like DBA or Programmers prefer Python over R. \n\nThus for data visuals--->R else---->Python.\n\n**Note: This graph is not for Language Recommended by professionals, but the tools used by the professionals.**"
## Job Function vs Language
"As I had already mentioned ** R excels in analytics, but Python beats in Machine Learning.** The graph shows that R has influence when it comes to pure analytics, but other ways python wins."
## Tenure vs Language Used
"As we had seen earlier, Python is highly recommended for beginners. Thus the proportion of Python users is more in the initial years of coding. The gap between the languages however reduces over the years, as the coding experience increases."
## Common Tools with Python and R
**SQL** seems to be the most common complementory tool used with both the languages.
So about 26% of the total respondents consider themselves as Data Scientist. What does Sort of mean?? Are they still learning or are they unemployed. For now lets consider them as a No.\n\n## Current Job Titles
"Surprisingly there is **no entry for the Job Title Data Scientist**. There reasons for this could be that the people with CurrentJobTitleSelect as Data Scientist(who might be working as Data Scientist) might have not answered the question: **""Do you currently consider yourself a Data Scientist?""**\n\nThere are many overlapping and common skills between the jobs like Data Analyst,Data Scientist and Machine Learning experts, Statisticians,etc. Thus they too have similar skills and consider themselves as Data Scientists even though they are not labeled the same. Now lets check if the previous assumption was True."
"So out of the total respondents, about **40%** of them are Data Scientists or have skills for the same.\n\n## Country-Wise Split"
The graph is similar to the demographic graph where we had shown number of users by country. The difference is that the numbers have reduced as we have only considered Data Scientists.\n\n## Employment Status
"About **67%** of the data scientists are employed full-time, while about **11-12%** of them are unemployed but looking for job.\n\n## Previous Job and Salary Change"
Clearly majority of people switching to Data Science get a salary hike about **6-20% or more**.\n\n## Tools used at Work
"Similar observations, Python, R and SQL are the most used tools or languages in Data Science\n\n"
"The average Job Satisfaction level is between **6-7.5** for most of the countries. It is lower in Japan(where people work for about 14 hours) and China. It is higher in come countries like Sweden and Mexico.\n\n## Time Spent on Tasks\n\nA Data Scientist is not always building predictive models, he is also responsible for the data quality, gathering the right data, analytics,etc. Lets see how much time a data scientist spends on these differnt tasks."
"Lets do it stepwise:\n\n  - **TimeGatheringData:** It is undoubtedly the most time consuming part. Getting the data is the most painstaking task in the entire process, which is followed by Data Cleaning(not shown as data not available) which is yet other time consuming process. Thus gathering right data and scrubing the data are the most time consuming process.\n  \n  - **TimeVisualizing:** It is probably the least time consuming process(and probably the most enjoyable one..:p), and it reduces even further if we use Enterprise Tools like Tableau,Qlik,Tibco,etc, which helps in building graphs and dashboards with simple drag and drop features.\n  \n  - **TimeFindingInsights:** It is followed after visualising the data, which involves finding facts and patterns in the data, slicing and dicing it to find insights for business processes.It looks to a bit more time consuming as compared to TimeVisualizing.\n  \n  - **TimeModelBuilding:** It is where the data scientists build predictive models, tune these models,etc. It is the 2nd most time consuming process after TimeDataGathering.\n\n## Importance Of Visualisations"
"Visualisations are a very integral part of Data Science Projects, and the above graph also shows the same. Almost all data science projects i.e **99%** of the projects have visualisations in them, doesn't matter how big or small. About **95%** of Data Scientists say that Visualisations skills are nice to have or necessary.Visuals help to understand and comprehend the data faster not only to the professionals but also to target customers, who may not be technically skilled.\n\n## Knowledge Of Algorithms (Maths and Stats)"
"Data Scientists have a good knowledge of mathematical concepts like Statistics and Linear Algebra, which are the most important part of Machine Learning algorithms. But is this maths really required, as many standard libraries like scikit,tensorflow,keras etc have all these things already implemented. But the experienced data scientists say that we should have a good understanding of the maths behind the algorithms. About **95%** of the data scientists say the stats is an important asset in Data Science.\n\n## Learning Platform Usefullness"
"The above donut charts shows the opinion of Data Scientists about the various platforms to learn Data Science. The plot looks best for **Projects**,where the percentage for not useful is **0%**.According to my personal opinion too, projects are the best platform or way for learning anything in the IT industry. The other excellent platforms are **Online Courses and Kaggle**. The graphs for other platforms are quite similar to each other.\n\n## What should the Resume have??"
"It is evident that Work experience in ML projects and Kaggle competitions reflects the knowledge of Data Science. Also a kaggle rank can be a good thing in one's resume.\n\n# Conclusions\n\nSome brief insights that we gathered from the notebook:\n\n1) Majority of the respondents are from USA followed by India. USA also had the maximum number of data scientists followed by India. Also the median Salary is highest in USA.\n\n2) Majority of the respondents are in the age bracket 20-35, which shows that data science is quite famous in the youngsters.\n\n3) The respondents are not just limited to Computer Science major, but also from majors like Statistics, health sciences,etc showing that Data Science is an interdisciplinary domain.\n\n4) Majority of the respondents are fully employed.\n\n5) Kaggle, Online Courses(Coursera,eDx,etc), Projects and Blogs(KDNuggets,AnalyticsVidya,etc) are the top resources/platforms for learning Data Science.\n\n6) Kaggle has the highest share for data acquisition whereas Github has the highest share for code sharing.\n\n7) Data Scientists have the highest Job Satisfaction level and the second highest median salary (after Operations Research Analyst). On the contrary, Programmers have the least Job Satisfaction level and one of the least median salary also.\n\n8) Data Scientists also get a hike of about 6-20% from their previous jobs.\n\n#### Tips For Budding Data Scientists\n\n1) Learn **Python,R and SQL** as they are the most used languages by the Data Scientists. Python and R will help in analytics and predictive modeling while SQL is best for querying the databases.\n\n2) Learn Machine Learning Techniques like **Logistic Regression, Decision Trees, Support Vector Machines**, etc as they are most commonly used Machine Learning techniques/algorithms.\n\n3) **Deep Learning and Neural Nets** will be the most sought after techniques in the future, thus a good knowledge in them will be very helpful.\n\n4) Develop skills for **Gathering Data** and **Cleaning The Data** as they are the most time consuming processes in the workflow of a data scientist. \n\n5) **Visualisations** are very important in Data Science projects and almost all projects require Visualisations for understanding the data better. So one should learn Data Visualisation as Data Scientists consider it to be a **necessary or nice to have skill.**\n\n6) **Maths and Stats** are very important in Data Science, so we should have good understanding of it for actually understanding how the algorithm works.\n\n7) **Projects** are the best way to learn Data Science according to Data Scientists.So working on projects will help you learn data science better.\n\n8) **Experience with ML Projects in company and Kaggle Competitions** are the best ways to show your working knowledge in Data Science. Working on ML projects in a company gives the experience of working with real world datasets, thereby enhancing the knowledge. Kaggle competitions are also a great medium, as you will be competing with Data Scientists over the world. Also a **Kaggle Rank** can be a good USP in the resume.\n"
"The first six values have all become blank as there wasn’t enough data to actually fill them when using a window of seven days.      \n\nSo, what are the key benefits of calculating a moving average or using this rolling mean method? Our data becomes a lot less noisy and more reflective of the trend than the data itself. Let’s actually plot this out. First, we’ll plot the original data followed by the rolling data for 30 days.     "
"The **blue line** is the original open price data. The **red line represents the 30-day rolling window**, and has less noise than the orange line. Something to keep in mind is that once we run this code, the first 29 days aren’t going to have the blue line because there wasn’t enough data to actually calculate that rolling mean."
"Above you can clearly see that the recall is falling of rapidly at a precision of around 85%.  Because of that you may want to select the precision/recall tradeoff before that - maybe at around 75 %.\n\nYou are now able to choose a threshold, that gives you the best precision/recall tradeoff for your current machine learning problem. If you want for example a precision of 80%, you can easily look at the plots and see that you would need a threshold of around 0.4.  Then you could train a model with exactly that threshold and would get the desired accuracy.\n\n\nAnother way is to plot the precision and recall against each other:"
"## ROC AUC Curve\n\nAnother way to evaluate and compare your binary classifier is provided by the ROC AUC Curve. This curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall."
## Precision Recall Curve of Logistic Regression
"# CONCLUSION: \nThe highest values of Normal transactions are 25691.16 while of Fraudulent transactions are just 2125.87. \nThe average value of normal transactions are small(USD 88.29) than fraudulent transactions that is USD 122.21\n\n\nWe got the best score when we use the SMOTE (OverSampling)  + RandomForest, that performed a f2 score of 0.8669~ \n\nThis is a considerably difference by the second best model that is 0.8252 that uses just RandomForests with some Hyper Parameters.\n\nThe worst model was Logreg where I used GridSearchCV to get the Best params to fit and predict where the recall was ~0.6666 and f2 ~0.70.\n\n\n"
**Highlights**\n\n**Above graph shows that most of the Fraud transactions are happening at night time (0 to 7 hours) when most of the people are sleeping and Genuine transaction are happening during day time (9 to 21 hours).**
### Visualising Data for detecting any particular Pattern or Anomaly using Histogram Plots\n\nFinally visulaising all columns once and for all to observe any abnormality
## Reset the index
Let's Compare one example again to verify that the normalization was done properly
## Multi-Dimensional Sliding Window
## Training \nSome enhancement  we save the best model (based on the lowest validation loss)
## Predict
" \n### VISUAL EXPLORATORY DATA ANALYSIS\n* Box plots: visualize basic statistics like outliers, min/max or quantiles"
 \n### TIDY DATA\nWe tidy data with melt().\nDescribing melt is confusing. Therefore lets make example to understand it.\n
### 5.2.1 Automagic\nIt's also possible to automatically select the optimal number of features and visualize this. This is uncommented and can be tried in the competition part of the tutorial.\n\n*Select the cell below and run it by pressing the play button.*
## 5.3 Competition time!\nIt's now time for you to get your hands even dirtier and go at it all by yourself in a `challenge`! \n\n1. Try to the other models in step 4.1 and compare their result\n    * Do this by uncommenting the code and running the cell you want to try\n2. Try adding new features in step 3.4.1\n    * Do this by adding them in to the function in the feature section.\n\n\n**The winner is the one to get the highest scoring model for the validation set**
#### Visualize the frequency distribution of `cp` variable
#### Frequency distribution of `target` variable wrt `cp`
We can visualize the value counts of the `cp` variable wrt `target` as follows -
"#### Interpretation\n\n- We can see that the values of `target` variable are plotted wrt `cp`.\n\n- `target` variable contains two integer values 1 and 0 : (1 = Presence of heart disease; 0 = Absence of heart disease)\n\n- The above plot confirms our above findings, "
"Alternatively, we can visualize the same information as follows :"
### Analysis of `target` and `thalach` variable \n
#### Visualize the frequency distribution of `thalach` variable
#### Comment\n\n- We can see that the `thalach` variable is slightly negatively skewed.
We can use Pandas series object to get an informative axis label as follows :
We can plot the distribution on the vertical axis as follows:-
#### Seaborn Kernel Density Estimation (KDE) Plot\n\n\n- The kernel density estimate (KDE) plot is a useful tool for plotting the shape of a distribution.\n\n- The KDE plot plots the density of observations on one axis with height along the other axis.\n\n- We can plot a KDE plot as follows :
We can shade under the density curve and use a different color as follows:
#### Histogram\n\n- A histogram represents the distribution of data by forming bins along the range of the data and then drawing bars to show the number of observations that fall in each bin.\n\n- We can plot a histogram as follows :
#### Visualize frequency distribution of `thalach` variable wrt `target`
#### Interpretation\n\n- We can see that those people suffering from heart disease (target = 1) have relatively higher heart rate (thalach) as compared to people who are not suffering from heart disease (target = 0).
We can add jitter to bring out the distribution of values as follows :
#### Visualize distribution of `thalach` variable wrt `target` with boxplot
#### Interpretation\n\nThe above boxplot confirms our finding that people suffering from heart disease (target = 1) have relatively higher heart rate (thalach) as compared to people who are not suffering from heart disease (target = 0).
"#### Interpretation\n\nFrom the above correlation heat map, we can conclude that :-\n\n- `target` and `cp` variable are mildly positively correlated (correlation coefficient = 0.43).\n\n- `target` and `thalach` variable are also mildly positively correlated (correlation coefficient = 0.42).\n\n- `target` and `slope` variable are weakly positively correlated (correlation coefficient = 0.35).\n\n- `target` and `exang` variable are mildly negatively correlated (correlation coefficient = -0.44).\n\n- `target` and `oldpeak` variable are also mildly negatively correlated (correlation coefficient = -0.43).\n\n- `target` and `ca` variable are weakly negatively correlated (correlation coefficient = -0.39).\n\n- `target` and `thal` variable are also waekly negatively correlated (correlation coefficient = -0.34).\n\n\n"
### Pair Plot 
"#### Comment\n\n\n- I have defined a variable `num_var`. Here `age`, `trestbps`, ``chol`, `thalach` and `oldpeak`` are numerical variables and `target` is the categorical variable.\n\n- So, I wll check relationships between these variables."
"#### Plot the distribution of `age` variable\n\nNow, I will plot the distribution of `age` variable to view the statistical properties."
#### Interpretation\n\n- The `age` variable distribution is approximately normal.
#### Visualize frequency distribution of `age` variable wrt `target`
#### Interpretation\n\n- We can see that the people suffering from heart disease (target = 1) and people who are not suffering from heart disease (target = 0) have comparable ages.
#### Visualize distribution of `age` variable wrt `target` with boxplot
#### Interpretation\n\n- The above boxplot tells two different things :\n\n  - The mean age of the people who have heart disease is less than the mean age of the people who do not have heart disease.\n  \n  - The dispersion or spread of age of the people who have heart disease is greater than the dispersion or spread of age of the people who do not have heart disease.\n
I will plot a scatterplot to visualize the relationship between `age` and `trestbps` variable.
#### Interpretation\n\n- The above scatter plot shows that there is no correlation between `age` and `trestbps` variable.
#### Interpretation\n\n- The above line shows that linear regression model is not good fit to the data.
#### Box-plot of `age` variable
### `trestbps` variable
#### Box-plot of `trestbps` variable
### `chol` variable
#### Box-plot of `chol` variable
### `thalach` variable
#### Box-plot of `thalach` variable
### `oldpeak` variable
#### Box-plot of `oldpeak` variable
#### Findings\n\n- The `age` variable does not contain any outlier.\n\n- `trestbps` variable contains outliers to the right side.\n\n- `chol` variable also contains outliers to the right side.\n\n- `thalach` variable contains a single outlier to the left side.\n\n- `oldpeak` variable contains outliers to the right side.\n\n- Those variables containing outliers needs further investigation.\n
"So, friends, our EDA journey has come to an end.\n\nIn this kernel, we have explored the heart disease dataset. In this kernel, we have implemented many of the strategies presented in the book **Think Stats - Exploratory Data Analysis in Python by Allen B Downey** . The feature variable of interest is `target` variable. We have analyzed it alone and check its interaction with other variables. We have also discussed how to detect missing data and outliers.\n\nI hope you like this kernel on EDA journey.\n\nThanks\n"
[Go to Top](#0)\n
"Sex is not informative for age prediction, age distribution seems to be same."
"1st class passengers are older than 2nd, and 2nd is older than 3rd class. "
###  4.4 | Visualising high floor area properties \n\nPROBLEM AIM\n\n- Let's revisit the [Perth Housing Dataset](https://www.kaggle.com/syuzai/perth-house-prices).\n- We are interested in different properties that were sold near the Perth CBD & their corresponding property FLOOR_AREA.\n- An interactive maps allow us to investigate the different properties using the interactive window & outline for example key regions with high FLOOR_AREA.
"# 5 | DENSITY HEATMAPS\n\n###  5.1 | Density heatmap overview \n\n- Density heatmaps allow us to visuaise __clustermap__ data in the form of a continuous function. \n- The issue with heavily concentrated __clustermaps__ is that points in close proximity tend to overlap; it may be difficult to distinguish them apart.\n- Continuous functions used to define the heatmap allows us to visualise the overall local value tendencies, similar to how a bias/variance balanced model. \n- The benefit of interactive heatmaps also lies in the ability to use hover_name which comes in handy as well.\n\nDENSITY HEATMAPS REQUIRE\n\n- longitude & latitude spatial point & size/color visualisation data.\n\n\n\n###  5.2 | Visualising high floor area property trends \n\nPROBLEM AIM\n\n- Let's revisit the **[Perth Housing Dataset](https://www.kaggle.com/syuzai/perth-house-prices)** once again. We are interested in regional trends this time of different properties that were sold near the Perth.\n- This time, we aren't too interested in individual property EDA, rather we use heatmaps to find general trends in different parts of Perth, this is quite useful to get an overall picture."
"# 6 | SUMMARY\n\n- In this notebook, we looked at different tools that can be used for geospatial data analysis, mainly Choropleth,Hexbin,Cluster & Density Heatmaps.\n- Greater attention was payed to Choropleth Maps, due to their more complex data input structure, requiring __boundary geometry__ data alongside with the data which is desired to be shown. Compared to Hexbin,Cluster & Density Heatmaps, which require only __point coordinates (longitude & latitude)__.\n\nBOUNDARY BASED VISUALISATION\n\n- Interactive Choropeth were shown to be quite effective at portraying data, especially when the difference in boundary sizes to be shown is very big. We also plotted static maps using __geopandas__, although most of the regions were visible, additional plots were needed in order to show all region data clearly.\n- One slightly issue arose when plotting Australian Choropleth maps, we needed to know specifically where to get the boundary data, which is a required step for plotting Australian based Choropleth Maps. These sources were outlined and a specific example for the __unemployment rate__ of specific demographics were shown and compared to one another.\n- One of the more compex parts of plotting choropleth maps, was the integration of two separate dataframe (boundary & visualisation), which required some data wrangling to combine and join indicies.\n\nCOORDINATE BASED VISUALISATIONS\n\n\n- Hexbin,cluster & density heatmaps, all require geospatial point data (longitude,latitude)\n- Out of the three, cluster maps are probably most useful due to their ability to pinpoint data at different locations. They are also commonly used with data interpolation methods, to estimate data at points we don't yet have data.\n- We also saw that cluster maps, especially when zoomed out tend to start overlapping as demonstrated in the __Perth Housing__ dataset, in such cases, density heatmaps are quite useful, in order to plot continuous data."
### ROC Curve\nLet's check out the performance of our model through ROC curve
From the ROC curve we can infer that our logistic model has classified the placed students correctly rather than predicting false positive. T**he more  the ROC curve(blue) lies towards the top left side the better our model** is. We can choose **0.8 or 0.9** for the threshold value which can reap us true positive results
### Looking at Feature Importance\nLet's see which feature influences more on making the decision and we should cut it off to make our model accurate
As we see the **school and undergrad specialisations** have less influence in classifying the model. But it is really wierd to acknowledge **ssc_p** influencing more in classifying
### Error rate vs K-value
There are a lot of ups and downs in our graph. If we consider any value between 10-15 we may get an overfitted model. So let's stick onto the first trough. Our **K value is 5**
"It seems that the model tends to overfit here as well, so to solve this problem, we perform parameter tuning for RF:"
"\n    📌You may see some warnings during the optimization for invalid configuration combinations. These can be safely ignored.\n\n\nThe results are summarized as follows:\n\n    Explore random forest bootstrap sample size\n    >0.1, mean:0.763, ste:0.068\n    >0.2, mean:0.824, ste:0.060\n    >0.3, mean:0.850, ste:0.049\n    >0.4, mean:0.859, ste:0.053\n    >0.5, mean:0.867, ste:0.042\n    >0.6, mean:0.867, ste:0.048\n    >0.7, mean:0.876, ste:0.048\n    >0.8, mean:0.879, ste:0.050\n    >0.9, mean:0.878, ste:0.043\n    >1.0, mean:0.877, ste:0.047\n\n\n    Explore random forest number of features effect\n    >1, mean:0.778, ste:0.061\n    >2, mean:0.866, ste:0.050\n    >3, mean:0.882, ste:0.047\n    >4, mean:0.885, ste:0.044\n    >5, mean:0.896, ste:0.035\n    >6, mean:0.899, ste:0.039\n    >7, mean:0.896, ste:0.038\n\n\n    Explore random forest tree depth effect\n    >1, mean:0.000, ste:0.000\n    >2, mean:0.119, ste:0.061\n    >3, mean:0.437, ste:0.086\n    >4, mean:0.658, ste:0.080\n    >5, mean:0.801, ste:0.059\n    >6, mean:0.840, ste:0.054\n    >7, mean:0.865, ste:0.052\n    >8, mean:0.869, ste:0.042\n    >9, mean:0.879, ste:0.044\n    >None, mean:0.877, ste:0.042\n\n"
Lets see how our predictions are done on Test Data by both networks...
"\n8. Summary \n\nWhat happened so far?\n\nSo far, I have tried to build a cat and dog classifier with help of deeplearning models, lets see the steps \n\n    Data processing and visualization\n    Build a basic CNN and make prections\n    Explored Data Agumentation and Learning rate Schedule\n    Build a Resnet based Transfer leraning model and make prections\n    final comparision of both models results\n\n\nThank you so much for reading all the way here.....Hope you enjoyed my work.....!!! I am open to suggetions. Please do comment if you any advice or critical comments... Thanks again!!!"
"### Considering highly-correlated features\nFeeding highly-correlated features to machine algorithms may cause a reduction in performance. Hence, these are addressed below:"
Highly-correlated attributes include (left attribute has higher correlation with SalePrice_log):\n* GarageCars and GarageArea (0.882)\n* YearBuilt and GarageYrBlt (0.826)\n* GrLivArea_log1p and TotRmsAbvGrd (0.826)\n* TotalBsmtSF and 1stFlrSF_log1p (0.780)\n\nPerhaps choose to drop the column with the lower correlation against SalePrice_log from the above pairs with more than 0.8 correlation.
Let's look this keys values further
Why this occurs and how to solve this problem in graph? it's a overffiting? 
"# Step 7: Evaluate Predictions #\n\nBefore making your final predictions on the test set, it's a good idea to evaluate your model's predictions on the validation set. This can help you diagnose problems in training or suggest ways your model could be improved. We'll look at two common ways of validation: plotting the **confusion matrix** and **visual validation**."
## Confusion Matrix ##\n\nA [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) shows the actual class of an image tabulated against its predicted class. It is one of the best tools you have for evaluating the performance of a classifier.\n\nThe following cell does some processing on the validation data and then creates the matrix with the `confusion_matrix` function included in [`scikit-learn`](https://scikit-learn.org/stable/index.html).
## Plot model performance report 😀
## Plot model performance report in 3D 😎
11 outliers were detected as the first principal component.
" Observation:\n    \nLooking at the figure above, points that are clearly judged as outliers in the figures on the left are also judged as outliers in PC1 after PCA.\nThat is, outlier judgment using PCA seems very effective."
----------------------------------------------------------------\n## Checking Outliers after removing outliers
" Observation:\n\nObserving the above figures, it can be seen that many of the points previously judged to be outliers have disappeared. Assuming that the outlier has been removed to some extent, let's try another feature engineer.\nOf course, we can delete outliers directly by looking at each graph, but in doing so, our model reads a lot of generality. The method using PCA is reasonable and can be used generally.\nIt can be used for this problem as well as other problems.\n\nAlso, as the outliers are removed, the regression line seems to be well-fitted to more general data."
## Question 1: Does the combination of underground and above-ground area have a high correlation with the Sale Price?\n\n* TotalBsmtSF: Total square feet of basement area\n* GrLivArea: Above grade (ground) living area square feet
"## Question 2: If you recently remodeled and have a large basement, will your sale price increase?\n\n* TotalBsmtSF: Total square feet of basement area\n* YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)"
## Question 3: Can the combined area of the 1st and 2nd floors affect the sale price?\n* 1stFlrSF: First Floor square feet \n* 2ndFlrSF: Second floor square feet
**Good derivative features come from good questions. Good questions come from a lot of domain-knowledge.**\n
"First, let's check skewness. A skewness greater than 1 is generally judged to be skewed, so check mainly those greater than 1."
 Observation:\n    \nIt is conformed that 14 features are skewed. We will do log transformation for these features.
The number of skewed features is reduced from 14 to 9. The remaining 4 skewness was greatly reduced.
"Some features still have skewness greater than 1, but further improvement seems difficult."
"## PoolQC: Pool quality \n\n**Question: Does the lack of pool quality mean that the house does not have a pool? If so, is there a relationship between the missing value and the house price?**"
## MiscFeature: Miscellaneous feature not covered in other categories\n\n**Question: Is there a difference in house price with and without miscellaneous features?**
## Alley: Type of alley access to property\n\n**Question: Is there a difference in house price with and without Alley access?**
## Fence: Fence quality\n\n**Question: Is there a difference in house price with and without fence?**
## FireplaceQu: Fireplace quality\n\n**Question: Is there a difference in house prices with and without Fireplace?**
## Garage Features\n\n**Question: Is there a difference in house price with and without Garage?**
## Basement features\n\n**Question: Is there a difference in the house price with and without a Basement?**
## MasVnrType: Masonry veneer type\n\n**Question: Is there a difference in house price with and without Masonry veneer?**
## Question 1: Is total house quality correlated with sale price?
## Question 2: Is the total number of bathrooms correlated with the sale price?
## Question 3: Can Total Condition Affect Sale Price?
## Question 4: Can area per room affect the sale price?
"If we ask a good question and generate good derivatives from it, we will train our model further. \n\n**However, I'm not a real estate agent, and I'm not very knowledgeable about it.**"
## Selecting Features
 Observation:\n\n* The newly created derivative variable AllArea feature and house price have a high correlation!\n* Pool-related features and fireplaces with many missing values have a low correlation with house price. \n
## Plotting after dimensionality reduction to 3D
"Even looking at the 3D scaled-down picture, it is difficult to observe the special rules. If we look at it in 202 dimensions, We may be able to find some rules. However, we cannot draw a 202-dimensional picture. \n\n**Now, all we can do is create a good model and make the model learn well.**"
We see here that there are two statuses that can be merged: *NONE* and *None* to improve data quality and to reduce one dimension. Most of customers do not receive any communication from H&M.\n\nLet's see now what is the age distribution.
The distribution shows that there are two main age-groups of customers: around 20-30 years old and 45-55 years old. Let's check how old is the oldest customer.
# 3. Transactions\n\nThis is the biggest database containing all transactions every day.
"Columns description:\n* ```t_dat``` - date of a transaction in format YYYY-MM-DD but provided as a string\n* ```customer_id``` - identifier of the customer which can be mapped to the ```customer_id```  column in the ```customers``` table\n* ```article_id``` - identifier of the product which can be mapped to the ```article_id```  column in the ```articles``` table\n* ```price``` - price paid\n* ```sales_channel_id``` - sales channel, 2 unique values"
No missing data!\n\nLet's investigate the price column.
It's clear from the above graph that we have a lot o outliers. Let's look at the price after cutting the values above 0.1.
Now let's see the data distribution over time. First what dates range is provided. It will be usefull to change the datatype of 
"The bar chart above show us that per day usuall number of transactions lays in range about between 25 000 and 80 000 transactions per day. We see also that sales spikes during summertime and drops during winter.\n\nNow, let's see how many transactions, on average, customers do."
Clearly there's a lot of outliers. Let's look at the distribution after cutting everything above 50 trasactions per customer.
"The graph above shows us that most of customers, on average, bought only few items during these 2 years.\n\nLet's see now the popularity of sale channels."
# 4. Combined databases EDA
**Looks like there is very strong correlation of Survival rate and Name length**
**Chance to survive increases with length of name for all Passenger classes**
**Increase of survival rate with length of name most important for male passengers**
"**RandomizedSearchCV  and GridSearchCV apply k fold cross validation on a chosen set of parameters**\n**and then find the parameters that give the best performance.**  \nFor GridSearchCV, all possible combinations of the specified parameter values are tried out, resulting in a parameter grid.  \nFor RandomizedSearchCV, a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter."
"**In the following we apply GridSearchCV and RandomizedSearchCV for these Classification models:**  \n**KNN, Decision Tree, Random Forest, SVC**"
"The null is still not rejected, but p-value has dropped - which indicates the transformations are the right way to go. Next, we can try differentiating to get rid of the trend"
"As expected, differentiation removes the trend (oscillations happen around a fixed level), but variations amplitude is magnified."
"# Which Features Influence the Result of a Term Deposit Suscription?\n## DecisionTreeClassifier:\n\nThe top three most important features for our classifier are **Duration (how long it took the conversation between the sales representative and the potential client), contact (number of contacts to the potential client within the same marketing campaign), month (the month of the year).\n\n\n"
## GradientBoosting Classifier Wins!\nGradient Boosting classifier is the best model to predict whether or not a **potential client** will suscribe to a term deposit or not.  84% accuracy!
"* As expected, data analysis is the most common data responsibility and is undertaken by more than half of all professionals who have at least one data responsibility\n* Building prototypes to explore applying machine learning to new areas is the next most common data-task at daily jobs for the professional respondents.\n\nIt is especially promising to see high proportion of professional involved in building prototypes to find new application areas for ML. With this finding in place, one can be doubly sure of the future growth prospects in the industrial use of data-science and especially machine learning."
"Size of employer companies:\n\n* More than a third of the professionals in the data community are employed with small startups with less than 50 employees.\n* Of these professionals, ML engineers stand out as more than half of these ML enineers are employed with small startups with less than 50 employees.\n* Large and very large companies are much more likely to have professionals in traditional data roles like Data Engineer, Data Scintist, Software Engineer, and Business Analyst.\n\n\n---\n\n\n💭 These observations seem to suggest that much of the cutting edge data science work is being undertaken at small startups. The largest firms (with more than 10k employees), meanwhile, are taking a more traditional approach with their data."
"By looking at each particular programming language indiviadually, we find that:\n* With 78% of the respondents using Python on a regular basis, it is the undisputed favorite programming language for the data-science community\n* SQL and R are also used regularly by a sizable portion of the population.\n* More than 25% those who use these three languages, (viz. Python, SQl, and R), do so exclusively. This goes to show how powerful these languages are, even on a standalone basis.\n* All the rest of the languages specified in teh survey are predominantly used alongside with one or more of the other programming languages.\n* Julia, and Swift, for instance, are only used by a tiny minority, and that too in unison with a host of other programming languages. "
"Over the years, Python is gaining ground as the community favorite (even as the recommended first programming language for aspiring data scientists); meanwhile, R is phasing out."
[Go back to the top](#qa)
"Among the list of IDEs provided in the 2020 survey -\n* Jupyter is the community favorite.\n* Visual studio and Pycharm are next in line.\n* ~1-in-5 users, use a single IDE exclusivel, and very few use more than 3 IDE. This clearly shows how powerful and self-sufficient each of these IDEs are.  "
"\n### 4.1. Machine Learning experience, in years\n\n* In India, 88% female/ LGBTQA+ users and 84% male users have ML experience of only 2 years or less.\n* Half of all users in India, male or female/ LGBTQA+, have less than 1 year of ML experience.\n* Compared to users in India, users in U.S.A. are more experienced in ML, especially the U.S.A. males. (Male users from U.S.A. are the most experienced in ML, among all users in India and U.S.A.)\n\n**Takeaway**\nConsistent with the previous analysis, we find that between India ans U.S.A., the male users from U.S.A. are most experienced, followed by the female/ LGBTQA+ users. Indian users are mostly younger with fewer years of coding experience and ML experience as well.\n"
and in between them is `feature_64`
"which has a big gap for values in the range 0.7 to 1.38. (Incidentally,  $\ln(2) \approx 0.693...$ and $\ln(4) \approx 1.386...$, I do not know if there is any significance to this at all).\n\nThe **Tag 22** features also have a very interesting daily pattern. For example, here are scatter and cumulative plots over three days for feature 64"
"The global minimum value of `feature_64` is \\( \approx -6.4 \\) and the global maximum value is \\( \approx 8 \\) (not all days reach these limits). It is curious that a trading day on the New York Stock Exchange spans from 9:30 until 16:00. What if the units of `feature_64` were \\( \approx 30 \\) minutes, and `feature_64 = 0` corresponds to 12:00? Just for fun let us make a plot of the *arcsin* function, renaming the *y*-axis as the hours of the day..."
"where, for some reason, the tick time is more frequent at the start and end of the day than in the middle. Also for fun let us plot the hypothetical tick frequency, *i.e.*\n\n$$ \frac{d}{dt} (2 \arcsin(t) +1) = \frac{2}{\sqrt{1-t^2}}$$"
"If this were so, then perhaps the period of missing values seen at the start of the day for some of the  features (see the section below on missing values) is actually similar to the period of missing values seen during the middle of the day? Also perhaps the higher tick frequency at the beginning and end of the day is due to a lot of buying when the day opens, and a lot of selling towards the close of the day so as to have no significant position overnight?\n\nIt was first suggested (if I am not mistaken) by [marketneutral](https://www.kaggle.com/marketneutral) in a [post](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201264#1101507) that the data *may* correspond to equities traded on the [Tokyo Stock Exchange](https://www.jpx.co.jp/english/derivatives/rules/trading-hours/index.html), whose trading hours are from 9:00 until 11:30, a break for lunch, and then from 12:30 until 15:00. This could explain the central discontinuity in the Tag 22 features.\n\nWe shall now also look at `feature 65`:"
"For a very interesting look at the Tag 22 features see the notebook [""*Important and Hidden Temporal Data*""](https://www.kaggle.com/lachlansuter/important-and-hidden-temporal-data) written by [Lachlan Suter](https://www.kaggle.com/lachlansuter).\n### 'Noisy' features\n* 3, 4, 5, 6\n* 8, 10, 12, 14, 16\n* 37, 38, 39, 40\n* 72, 73, 74, 75, 76\n* 78, 79, 80, 81, 82\n* 83\n\nHere are cumulative plots of some of these features"
"Could these represent offer prices, and those with with **Tag 9** bid prices? That said, we can see that after day 85 the value of `feature_40` actually becomes greater than the value of `feature_39`.\n\n### `feature_51` (Tag 19)\nIn the Topic [""*Weight and feature_51 de-anonymized*""](https://www.kaggle.com/c/jane-street-market-prediction/discussion/202014) by [marketneutral](https://www.kaggle.com/marketneutral) it is suggested that `feature_51` is the (log of) the average daily volume of the stock.\nHere I reproduce the plot of `feature_51` w.r.t. `weight` for non-zero weights:"
### `feature_52` (Tag 19)
having the following lag plot
and the following curious relationship with `resp`
"### 'Negative' features\nFeatures 73, 75, 76, 77 (noisy), 79, 81(noisy), 82. These are all found in the **Tag 23** section.\n\n### 'Hybrid' features (Tag 21): \n55, 56, 57, 58, 59.\n\nThese start off noisy, with prominent almost discontinuous steps around the 0.2M, 0.5M, and 0.8M trade marks, then go linear. These five features form the ""**Tag 21**"" set:"
"What if these are associated with the five `resp` values? Perhaps: \n* `feature_55` is related to `resp_1`\n* `feature_56` is related to `resp_4` \n* `feature_57` is related to `resp_2` \n* `feature_58` is related to `resp_3` \n* `feature_59` is related to `resp`\n\nIf that *is* the case then \n* **Tag 0** represents `resp_4` features\n* **Tag 1** represents `resp` features\n* **Tag 2** represents `resp_3` features\n* **Tag 3** represents `resp_2` features\n* **Tag 4** represents `resp_1` features\n\n*i.e.*\n* `resp_1` related features: 7, 8, 17, 18, 27, 28, 55, 72, 78, 84, 90, 96, 102, 108, 114, 120, and 121 (Note: 79.6% of all of the missing data is found within this set of features).\n* `resp_2` related features: 11, 12, 21, 22, 31, 32, 57, 74, 80, 86, 92, 98, 104, 110, 116, 124, and 125 (Note: 15.2% of all of the missing data is found within this set of features).\n* `resp_3` related features: 13, 14, 23, 24, 33, 34, 58, 75, 81, 87, 93, 99, 105, 111, 117, 126, and 127\n* `resp_4` related features: 9, 10, 19, 20, 29, 30, 56, 73, 79, 85, 91, 97, 103, 109, 115, 122, and 123\n* `resp` related features: 15, 16, 25, 26, 35, 36, 59, 76, 82, 88, 94, 100, 106, 112, 118, 128, and 129\n\nLet us take a look at a plot of each of these 17 features for each of the resp (Note: This is an image; right click to view and enlarge)"
"Just for fun let us re-plot the above data, but now in '8-bit' mode; totally illegible, but may perhaps serve as an overall visual aid..."
Let us sum the number of tags for each feature:
we can see that with the above formula overall we are very slightly more proactive (0.4%) than inactive. How does this look daily?
We can see that the daily action is fairly consistent; no obvious weekly/monthly/seasonal changes *etc*.
"we can see that the missing data does not appear to be random, indeed there appear to be two big chunks missing at the start and in the middle of each column. Let us assume that a [trading day](https://en.wikipedia.org/wiki/Trading_day) spans from 9:30 until 16:00. Let us also assume that the trades occur at regular intervals (which is almost certainly *not* the case) then `feature_7` has chunks of missing data from 9:30 until 10:03, and is missing ≈16 minutes from 13:17 until 13:33. `feature_11` has missing data from 9:30 until 9:35, and is missing ≈5½ minutes from 13:17 until 13:22.\n\nNow let us look at the sum of the number of missing data in each column for the whole `train.csv` file:"
"First of all, \n\n* **79.6%** of all the missing data is located in the **Tag 4** group, which represent the `resp_1` features\n* **15.2%** of the missing data is in the **Tag 3** group, which represent the `resp_2` features\n* In total, the features associated with `resp_1` and `resp_2` make up **> 95%** of all the missing data.\n\nWe can see that features 7 and 8 both have exactly the same number of missing values (393135). \n17 and 18, and 27 and 28 all have 395535 missing values each. These are all `resp_1` features.\n\nNext we have features 72, 78, 84, 90, 96, 102, 108, 114 all with 351426 missing values each. These too are all `resp_1` features.\n\nFeatures 21, 22, 31, 32 have 81444 missing values, closely followed by features 11 and 12. These are all `resp_2` features.\n\nThere are more features with even less missing values. I think the interesting thing is not so much the quantity of missing values in so much as it may tell us which features represent similar measures/metrics.\n\nIs day 0 special, or does every day have missing data?"
"Indeed we can see that there is missing data *almost* every day, with no discernible pattern (weekly, monthly, *etc*). The exceptions are days **2** and **294**, which we shall look at in the next section.\n\nIn the notebook [""*Jane Street EDA Market Regime*""](https://www.kaggle.com/marketneutral/jane-street-eda-market-regime) written by [marketneutral](https://www.kaggle.com/marketneutral) a plot is made of the number of trades per day, and is strikingly similar to the above plot. In view of this, for curiosity, we shall plot the number of missing values in the features with respect to the number of trades, for each day."
"We can see that on average there are $\approx$ 3 missing feature values per trade, per day, except for two spikes located on days 2 and 294 where there are no missing values at all. (The most missing values are on day 14).\n\nThis raises the question of [what to do with missing data in the unseen test data?](https://www.kaggle.com/c/jane-street-market-prediction/discussion/200691). Whatever one decides to do, in this competition time is of the essence, so we have to do it fast, and [Yirun Zhang](https://www.kaggle.com/gogo827jz) has made an exhaustive study of the time taken in various filling methods in the notebook [""*Optimise Speed of Filling-NaN Function*""](https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function).\n\n\n## Is there any missing data: Days 2 and 294\nIf we produce scatter plots of `feature_64` we see that each day has the same sweeping pattern. However we see that **day 2** has only 231 `ts_id`  which all seem to originate from the very end of the day. Here is a plot of day 1 (in blue), day 2 (in red) and day 3 (blue again). Day 2 has been encircled as a visual aid."
"The same goes for **day 294**, which has only 29 `ts_id`.\nThis would also explain why days 2 and 294 have none of the missing values that we usually find during breakfast and lunch the other days.\nIt is possibly worth treating these two days as outliers and dropping them. \n\n\n\n## DABL plots\n\nLet us run **day 0** through the *data analysis baseline library* [dabl](https://github.com/amueller/dabl). First using the `action` as the target:"
We can see that the classes 0 and 1 for `action` are reasonably well balanced.\n\nNow we shall use `resp` as the target:
### Plot of `resp` values with respect to time (`ts_id`) for day 0
"\n## Very quick Permutation Importance using the Random Forest\nWe shall now perform a simple [permutation importance](https://www.kaggle.com/dansbecker/permutation-importance) calculation, a basic way of seeing which features may be important. We shall perform a regression, with `resp` as the target."
Most kills are made from a distance of 100 meters or closer. There are however some outliers who make a kill from more than 1km away. This is probably done by cheaters.
Let's take a look at the players who make these shots.
**Feature importance for top features**
## Correlations
**Predictive quality of kills**
**Predictive quality of walkDistance**
# Final Random Forest Model 
## Loss for each model 
"From the above graph, we can see that the two smoothing methods: moving average and exponential smoothing are the best-scoring models. Holt linear is not far behind. The remaining models: naive approach, ARIMA, and Prophet are the worst-scoring models. I believe that the accuracy of ARIMA and Prophet can be boosted significantly by tuning the hyperparameters."
"Weight decay, or *L2 regularization*, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\n\nWhy would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, `y = a * (x**2)`, the larger `a` is, the more *narrow* the parabola is:"
"So, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.\n\nLimiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just `wd`) is a parameter that controls that sum of squares we add to our loss (assuming `parameters` is a tensor of all parameters):\n\n``` python\nloss_with_wd = loss + wd * (parameters**2).sum()\n```\n\nIn practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of `p**2` with respect to `p` is `2*p`, so adding that big sum to our loss is exactly the same as doing:\n\n``` python\nparameters.grad += wd * 2 * parameters\n```\n\nIn practice, since `wd` is a parameter that we choose, we can just make it twice as big, so we don't even need the `*2` in this equation. To use weight decay in fastai, just pass `wd` in your call to `fit` or `fit_one_cycle`:"
"So, for instance, even if you don't normally enjoy detective movies, you might enjoy *LA Confidential*!\n\nIt is not quite so easy to directly interpret the embedding matrices. There are just too many factors for a human to look at. But there is a technique that can pull out the most important underlying *directions* in such a matrix, called *principal component analysis* (PCA). We will not be going into this in detail in this book, because it is not particularly important for you to understand to be a deep learning practitioner, but if you are interested then we suggest you check out the fast.ai course [Computational Linear Algebra for Coders](https://github.com/fastai/numerical-linear-algebra). Here's what our movies look like based on two of the strongest PCA components."
"We can see here that the model seems to have discovered a concept of *classic* versus *pop culture* movies, or perhaps it is *critically acclaimed* that is represented here."
### Sidebar: kwargs and Delegates
### End sidebar
# Part-of-Speech Tagging for questions Corpus
"# Topic Modelling\n\n#### Now we will apply a clustering algorithm to the headlines corpus in order to study the topic, as well as how it has evolved through time. To do so, we first experiment with a small subsample of the dataset in order to determine which of the two potential clustering algorithms is most appropriate – once this has been ascertained, we then scale up to a larger portion of the available data."
"### Thus we have converted our initial small sample of headlines into a list of predicted topic categories, where each category is characterised by its most frequent words. The relative magnitudes of each of these categories can then be easily visualised though use of a bar chart.\n\n"
"#### However, this does not provide a great point of comparison with other clustering algorithms. In order to properly contrast LSA with LDA we instead use a dimensionality-reduction technique called  *t*-SNE, which will also serve to better illuminate the success of the clustering process."
"### All that remains is to plot the clustered questions. Also included are the top three words in each cluster, which are placed"
"### Evidently, this is a bit a of a failed result. We have failed to reach any great degree of separation across the topic categories, and it is difficult to tell whether this can be attributed to the LSA decomposition or instead the  t -SNE dimensionality reduction process. Let's move forward and try another clustering method"
"Our hypothesis is that the higher the class, the higher the chances of survival. This means that a person travelling in the first class has a higher chance of survival than a person traveling on the second or third class.\n\nTo visualize if there is a relationship between 'Pclass' and 'Survival', let's do a bar plot."
"As we can see, about 60% of the people travelling in the first class survived. In contrast, only approximately 25% of the people travelling in the third class survived. Accordingly, this plot suggests that the class in which people travel affects the chances of survival."
"'Age' is the next variable in the list. Our hypothesis is that children are more prone to survive, while people in its adult life may have a lower rate of survival. Personally, I don't have any special intuition about elders, since they are the most vulnerable. This can play for both sides: either people help elders because they are more vulnerable, or they they are not able to cope with the challenges posed by the wreck of a ship.\n\nLet's call the usual suspect (bar plot) to help us understanding the situation."
"With a little bit of creativity, we can say that the plot has three regions: \n\n1. One region that goes between age 0 and 15; \n2. One between age 15 and 48;\n3. A last one between age 48 and 80. \n\nI know that this division is arguable, especially in what concerns the last two categories. However, the point is that this categories split fits into what we know about the way our society is organized: childrens, adults and elders. For now, let's proceed this way."
"Regarding family size, our hypothesis is that those who travel alone, have a lower survival rate. The idea is that people with family can collaborate and help each other escaping.\n\nLet's see if that makes sense using our [beautiful and only friend](https://youtu.be/LsQtnBu3p7Y), the bar plot."
"As we can see, when 'FamilySize' is between 0 and 3, our hypothesis finds some support. People that are travelling alone have a lower survival rate than people who are travelling with one, two or three people more. \n\nHowever, when FamilySize is between 4 and 10, things start to change. Despite the large variability of the results, the survival rate drops. This may suggest that our hypothesis should be revised when 'FamilySize' is higher than 3. \n\nThis variable seems to be more complex than expected. Accordingly, we will not make any transformation in this variable and we will leave it as a continuous variable to preserve all the information it has."
"The same logic applied to 'Pclass' should work for 'Fare': higher fares, higher survival rate.\n\nSince now we want to establish comparisons across different levels of a categorical variable, we will use a box plot instead of a bar plot."
"The plot suggests that those who survived paid a higher fare. Since we believe this variable is connected with 'Pclass', let's see how they work together."
"Here we have an interesting result. It seems that 'Fare' doesn't make difference, in terms of survival, if you are travelling in second or third class. However, if you are travelling in first class, the higher the fare, the higher the chances of survival. Considering this, it would make sense to create interaction features between 'Fare' and 'Pclass'."
"The hypothesis regarding 'Embarked' is that it doesn't influence the chances of survival. It is hard to imagine a scenario in which people from Southampton, for instance, would such a competitive advantage, that it would make them more apt for survival than people from Queensland. Yes, in [Darwin](https://en.wikipedia.org/wiki/Natural_selection) we believe.\n\nA simple plot can enlighten us."
Ups... It seems that people embarking on C were selected by a superior entity to survive. This is strange and may be hidding some relationship that is not obvious with this plot (e.g. people embarking on C were mostly women). \n\nLet's dive deeper. 
"Box-Cox transformations aim to normalize variables. These transformations are an alternative to the typical transformations, such as square root transformations, log transformations, and inverse transformations. The main advantage of Box-Cox transformations is that they optimally normalize the chosen variable. Thus, they avoid the need to randomly try different transformations and automatize the data transformation process."
#### Polynomials
"## **Hello Guys! I'm trying Glove embed, Build lstm model and predict output**\n\n### **Features selection: I'm simply take x is target and y is score**\n"
### **Count the score attribute**
### **Feature selection**
# Score between the congestion of the Monday afternoons\n\nBelow I compute the mean absolute error between all the Monday afternoons and the nearest Monday afternoon of the test (the 23 Sept)
# Score between the congestion of the Monday afternoons w.r.t. the median congestion\n\nThe mean absolute error between all the Monday afternoons and the median congestion over all the afternoons is shown below.
# Score between the congestion of the afternoons\n\nThe mean absolute error between all the afternoons and the nearest afternoon of the test (the 29 Sept) is shown below.
# Score between the congestion of the afternoons w.r.t. the median congestion\n\nThe mean absolute error between all the afternoons and the median congestion over all the afternoons is shown below.
# The ouliers\n\nWe may consider the afternoons with high MAE w.r.t. the median congestion as outliers.
### **CNN related videos from:** https://www.appliedaicourse.com/\n### http://cs231n.github.io/convolutional-networks/\n### https://www.mathworks.com/solutions/deep-learning/convolutional-neural-network.html\n### https://blog.floydhub.com/building-your-first-convnet/\n### https://medium.com/@gopalkalpande/biological-inspiration-of-convolutional-neural-network-cnn-9419668898ac\n### https://blog.datawow.io/interns-explain-cnn-8a669d053f8b\n### https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5
\n![](https://i.imgur.com/Qaong5c.png)\n
## Categorical Encoding Class:\n\nThis is a way to encode our features in a way that it avoids the assumption that two nearby values are more similar than two distant values. This is the reason we should avoid using LabelEncoder to scale features (inputs) in our dataset and in addition the word **LabelEncoder** is used for scaling labels (outputs). This could be used more often in **binary classification problems** were no *association* exists between the outputs.
## Combine Attribute Class:\n\nThis class will help us to include the total area variable into our pipeline for further scaling.
**Let's compare the results with the ones found via Decision Tree.**
\n7.4.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)\n\nTable of Contents
\n7.4.f The Visualization of the Tree\n\nTable of Contents
\n7.5 The Implementation of K-Nearest Neighbor (KNN)\n\nTable of Contents
\n8) THE COMPARISON OF MODELS\n\nTable of Contents
\n9) CONCLUSION\n\nTable of Contents
\n Toss Decision 
\n Analysis Over by Over 
"### If these kernels impress you,give them an Upvote."
A confusion matrix is a table that is often used to describe the performance of a classification model. read more [here](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)
"* By looking at above matrices we can say that, if we are more concerned on making less mistakes by predicting survived as dead, then Naive Bayes model does better.\n* If we are more concerned on making less mistakes by predicting dead as survived, then Decision Tree model does better."
"## Feature Importance[^](#6)\n\nWell after we have trained a model to make predictions for us, we feel curiuos on how it works. What are the features model weights more when trying to make a prediction?. As humans we seek to understand how it works. Looking at feature importances of a trained model is one way we could explain the decisions it make. Lets visualize the feature importances of the Random forest model we used inside the ensemble above."
"---\n## So That is it...\n\nIts Simple isnt it?.\n\nWe started from **EDA** to see what the data can tell us. Then we moved to **Feature Engineering and Data Cleaning** step where we added few features, Removed redundant features, Converted features into suitable form for modeling. Finally in the **Predictive Modeling** part we tried basic ML algorthms, cross validated, ensemble and Important feature Extraction.\n\nThanks a lot for reading!\nSee you! :)"
"We've now taken the initial 37,000 dimension book vector and reduced it to just 2 dimensions."
"There do appear to be a few noticeable clumps. However, it's difficult to derive any meaning from this plot since we aren't distinguishing books in any way."
### Book Embeddings by Genre\n\nThe above graphs are difficult to interpret. Let's plot the embeddings by the `genre` which is contained in the `Infobox` template data for each book. We'll limit it to the 10 most popular genres.
"Finally, we can plot the embedding colored by the genre"
The books do seem to be slightly separated based on the genre. The categories aren't exactly that helpful but we did what we could! We can also try UMAP. 
"There doesn't appear to be much separation between the categories in the UMAP clustering. There are a lot of parameters to play around with in UMAP, and changing some of them might result in better clusters."
## Most Popular Books in Embedding\n\nLet's see the embedding labeled with the 10 books most often mentioned by other books. 
The `Encyclopedia`s of Science Fiction and Fantasy have nearly perfect overlap. 
The next image shows all the link embedded with the 10 most popular categories labeled.
"We do see some clumpings, but it's difficult to label them. If this was interactive, then we could get a lot more use from it. (This will be an upcoming topic for an article)."
"#### Visualize New Variables\n\nWe should explore these __domain knowledge__ variables visually in a graph. For all of these, we will make the same KDE plot colored by the value of the `TARGET`."
It's hard to say ahead of time if these new features will be useful. The only way to tell for sure is to try them out! 
"Again with the inclusion of the genre variable, the results do not seem to defer significantly as compared to the previous results."
"From the results, it would seem that the genre section actually plays an important part in the decision tree making. Yet the exclusion of it dosent seem to significantly impact results. This to me is quite interesting."
### Test Environment
### Author and License Information
"Next, please..."
"Ok, now we are dealing with the big boss. What do we have here?\n\n* Something that, in general, presents skewness.\n* A significant number of observations with value zero (houses without basement).\n* A big problem because the value zero doesn't allow us to do log transformations.\n\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.\n\nI'm not sure if this approach is correct. It just seemed right to me. That's what I call 'high risk engineering'."
"The best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).\n\nStarting by 'SalePrice' and 'GrLivArea'..."
"Older versions of this scatter plot (previous to log transformations), had a conic shape (go back and check 'Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)'). As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n\nNow let's check 'SalePrice' with 'TotalBsmtSF'."
"We can say that, in general, 'SalePrice' exhibit equal levels of variance across the range of 'TotalBsmtSF'. Cool!"
\n### Pulled- out Pie Plot
⬆️Back to Table of Contents ⬆️
\n### Pie Chart in Subplots
\n### Basic Bar Plot
\n### Bar Plot with Hover Text
\n### Bar Plot with continous Color scale
\n### Horizontal Bar Plot
\n### Stacked Bar Plot
\n###  Grouped Bar Plot
\n### Facetted Bar Subplots
\n### Animated Bar Plot
####  Press the play ▶️ button to see the animation
#### Basic Box Plot -1 
#### Basic Box Plot - 2 
\n###  Horizontal Bar Plot
\n#### Box Plots with all points
\n#### Box Plots with outlier points
\n#### Box Plots with only points
\n###  Styled Bar Plot
\n###  Rainbow Bar Plot
"### 4.3.1. Corrolation\n\nIf we fit highly corrolated data in our model, it results in the overfitting probelm. Thus, for example if there are two highly corrolated features we have to drop the one that has more corrolation with other feature.  "
There is not highly corrolated feature in this data set.
**Now let's check that heat map again!**
**Great! Let's go ahead and drop the Cabin column and the row in Embarked that is NaN.**
### **3.2 Feature Importance**
### **3.3 ROC Curve**
### **3.4 Submission**
\n    \n
Precision Recall Curve 
**References:**\n\nhttps://towardsdatascience.com/evaluation-metrics-for-classification-problems-in-machine-learning-d9f9c7313190
**Checking for number of clusters**
**Fitting Model**
**Plotting Clusters**
# CNN
**Preprocessing and Data Split**
**Model**
"**It's an important method for dimension reduction.It extracts low dimensional set of features from a high dimensional data set with a motive to capture as much information as possible and to visualise high-dimensional data, it also reduces noise and finally makes other algorithms to work better because we are injecting fewer inputs.**\n* Example: When we have to bring out strong patterns in a data set or to make data easy to explore and visualize"
# Apriori
**Library and Data**
**Model and Forecast**
**Prediction**
# **Evaluate Algorithms** \n**The evaluation of algorithm consist three following steps:- **\n1. Test Harness  \n2. Explore and select algorithms \n3. Interpret and report results \n\n
"* If 90% of the data set is cat image and 10% is dog image, your accuracy will be 90% even if you estimate the entire test set as a cat.\n* But in another aspect, the model's success in predicting dogs is 0%.\n* In this context, accuracy may not always give us realistic information about the actual performance of the model.\n* The confusion matrix shows how confused your classification model is for which classes by detailing the relationship between actual class and predicted class.\n* If there is an anomaly something like above mentioned, you can specify the problem with confusion matrix and improve accuracy by various methods like adding more data for a specific class, etc."
### **F1 Score Calculation** \n[Return Contents](#0)
![rsz_1form%C3%BCl.jpg](attachment:rsz_1form%C3%BCl.jpg)
### **Evaluate with Another Dataset** \n[Return Contents](#0)
"Thanks to https://www.kaggle.com/startupsci/titanic-data-science-solutions\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification problem. With these two criteria - Supervised Learning, we can narrow down our choice of models to a few. These include:\n\n- Linear Regression, Logistic Regression\n- Naive Bayes \n- k-Nearest Neighbors algorithm\n- Neural network with Keras\n- Support Vector Machines and Linear SVC\n- Stochastic Gradient Descent, Gradient Boosting Classifier, RidgeCV, Bagging Classifier\n- Decision Tree Classifier, Random Forest Classifier, AdaBoost Classifier, XGB Classifier, LGBM Classifier, ExtraTrees Classifier \n- Gaussian Process Classification\n- MLP Classifier (Deep Learning)\n- Voting Classifier\n\nEach model is built using cross-validation (except LGBM). The parameters of the model are selected to ensure the maximum matching of accuracy on the training and validation data. A plot is being built for this purpose with [learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html?highlight=learning_curve#sklearn.model_selection.learning_curve) from sklearn library."
### 5.1 Linear Regression \n\n[Back to Table of Contents](#0.1)
"Here is the link to the editor.\n\n- [Jupyter](https://jupyter.org/) : Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.\n- [RStudio](https://rstudio.com/) : Open source and enterprise-ready professional software for data science.\n- [PyCharm](https://www.jetbrains.com/pycharm/) : The Python IDE for Professional Developers\n- [Atom](https://atom.io/) : A hackable text editor for the 21st Century\n- [MATLAB](https://www.mathworks.com/products/matlab.html) : Math. Graphics. Programming.\n- [Visual Studio / Visual Studio Code](https://code.visualstudio.com/) : Code editing. Redefined. Free. Built on open source. Runs everywhere.\n- [Spyder](https://www.spyder-ide.org/) : Spyder is a powerful scientific environment written in Python, for Python, and designed by and for scientists, engineers and data analysts.\n- Vim / Emacs\n- [Notepad++](https://notepad-plus-plus.org/) : Notepad++ is a free source code editor and Notepad replacement that supports several languages. \n- [Sublime Text](https://www.sublimetext.com/) : A sophisticated text editor for code, markup and prose\n\nI have used jupyter, pycharm, atom, vs / vscode, vim, notepad++, sublime text.\nCurrently I am using jupyter and vs code.\n\nIn the DS world where you need to keep testing, ipython environments like jupyter are a good fit. And it's very easy to have another server. And other than that, it's lightweight and the library uses a lot of vs code."
"Recently, the Colab GPU was upgraded to P100. Personally, I prefer the Kaggel notebook because of CSS. If you have a good UI / UX environment, please recommend."
"You can find some interesting facts by checking the treemap.\n\nFor most occupations, Python ranks first and SQL second.\n\nHowever, only statisticians rank R first.\n\nOf course, people could choose multiple languages ​​for one job, but it is surprising that this trend is emerging.\n\n\n### 10-2. Jobs & Framework"
"## 11. Is salary high depending on career? How about educational background? \n\n> Q4. What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\n\n> Q10. What is your current yearly compensation?\n\n> Q15. How long have you been writing code to analyze data (at work or at school)?\n\nIn fact, money questions are always fun.\n\n![money](https://media.giphy.com/media/xTiTnqUxyWbsAXq7Ju/giphy.gif)\n> img from https://giphy.com/gifs/yosub-money-donald-duck-cash-xTiTnqUxyWbsAXq7Ju\n\n\nLet's look at the annual salary.\n\n\nMost graphs will be viewed in the following order:\n\n- Distribution over the whole figure,\n- Percentage in percent\n- Average value\n\nAs a statistician, you shouldn't have complete confidence in the mean, but the visualization below shows the trend."
First let's look at the salary distribution.
"\n### 11-1. Educational background & Average Salary\n\nThe distribution is more diverse than I thought. I expected a normal distribution, but that's a shame.\n\nNow let's see what distribution this has for each condition.\n\nLet's look at a typical counting distribution and scaled distribution based on that condition."
"The average amount is estimated as follows. (If the distribution within the interval is normally distributed, we thought we would use the median.)"
"Those who earned a Ph.D. in the second half of the graph show that the salary is somewhat high. \n\nEven if you look at the average, degree holders can see some linearity.\n\nThis is why We have to go to a Ph.D.\n\n> I think that no formal education past high school is actually a successful dropout like Steve Jobs or Bill Gates. In fact, the answer may really be outside of school. Of course it would be different if it was a Ph.D.\n\n### 11-2. Career & Averagy Salary"
"Here you can see that there is about 25% difference between male/female.\n\nBefore looking at the country differences below, let's take a look at the percentage differences between male and female.\n\n- Formula : Female Wages-Male Wages / Male Wages\n\nAnd I was surprised to do this work. In this survey, My home country, Korea, was divided into South Korea and Republic of Korea. Korea also has wage gaps, but not as bad as the statistics show. Please see South Korea."
### 12-3. Country & Salary
"## 14. Who paid a lot?\n\n> Q11. Approximately how much money have you spent on machine learning and/or cloud computing products at your work in the past 5 years?\n\nAlthough it was not graphed, it did not have much correlation with the following areas. I preprocessed to log scale and looked at corr, but it didn't matter much. (I checked th correlation by corr method)\n\n- ML Library \n- Basis Algorithm\n\n\nI thought *deep learning* would definitely spend more money, but that wasn't it.\nWho is spending a lot of money?\n\n"
I will show it as a percentage graph according to the feature.
"On average, women use more. but i don't know why"
"Obviously, you can see that the size of your spending grows with age.\n\nThere seems to be a difference between having a job and not having one."
"The transactions we perform in this section mean an average age. In this part, taking the average of all transactions is performed."
In this section we will use the groupby function. Our aim here is to obtain the average values of Thalach according to age ranges. Because we're going to do chest pain.
It seems that old people have a very hard job because their values are very high.
"MODEL, TRAINING and TESTING\nAs a result of our initial evaluations, we have used a number of artificial learning algorithms. These are logistic regression, support vector machine (SVM), k close neighborhood (kNN), GradientBoostingClassifier and RandomForestClassifier algorithms. The first algorithm is logistic regression algorithm. To implement this algorithm model, we need to separate dependent and independent variables within our data sets. In addition, we created a combination of features between different features to make different experiments. While creating these parameters, the process of finding the best results was made by giving hyper parameter values."
"---\n\n## **3.Doc2Vec**\n\n---\n\n- Python implementation and application of doc2vec with Gensim\n- Original paper: Le, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning (ICML-14) (pp. 1188-1196)."
## **Import training dataset**\n* Import Shakespeare's Hamlet corpus from nltk library
"---\n\n## **4.Word2Vec**\n\n---\n\n- Python implementation and application of word2vec with Gensim\n- Original paper: [Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.](https://arxiv.org/pdf/1301.3781)"
## **Import training dataset**\n- Import Shakespeare's Hamlet corpus from nltk library
"**Logging custom bar charts** for unigrams, bigrams and trigrams🏋️‍♀️"
Plugging in RAPIDS 🏃‍♀️ \n
 \n## Step 9: Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.
### RMSE Score
## Step 9: Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.
"Since this operation is so common, Pandas has the `diff` function that computes the differences based on the period:"
### 3.2 Percentage changes 
"The above output shows that for the first 3 dates, Apple stocks didn't change. Then, it increased by 1% of what it was on the first date ('2010–12–16'). Google's prices are more volatile, fluctuating between 1 and 2% increases during the first 10 dates.\n\nNow, let's plot them to compare growth:"
Both Apple's and Google's achieved over 300% growth from 2011 to 2017. This plot may be even more interesting if we compare their growth to other 500 Fortune Companies:
"As you can see, Apple and Google have much higher growth than other top 500 companies in the US."
"Now, let's plot the running min and max of S&P500 stocks:"
## Summary 
![image.png](attachment:image.png)
## 9. Embedding Analysis \n\n[Back to Table of Contents](#0.1)
## 10.1. 3D Plot - Matplotlib \n\n[Back to Table of Contents](#0.1)
"**The kernel 3 [Rare Visualization Tools](https://www.kaggle.com/kanncaa1/rare-visualization-tools)**\n\n**Thanks to @kanncaa1**\n\n### Basic 3D Scatter Plot (Plotly)\n\n* import data again to avoid confusion\n* go.Scatter3d: 3D scatter\n\nWe will plot iris setosa and iris virginica classes according to their Sepal Length(x), Sepal Width(y), and Petal Length(z)."
"## 10.3. Animation Plots - Matplotlib, Plotly \n\n[Back to Table of Contents](#0.1)"
**The kernel 2 [Earthquake Animation with Plotly](https://www.kaggle.com/kanncaa1/earthquake-animation-with-plotly)**\n\n**Thanks to @kanncaa1**\n\nInteractive plot with time scale - optimal for time series
**The kernel 1 [Kiva in 2 minutes Animated Story](https://www.kaggle.com/poonaml/kiva-in-2-minutes-animated-story)**\n\n**Thanks to @poonaml**\n\nInteractive maps (with Folium) with save as gif - by FuncAnimation from matplotlib.animation
## 10.4. Bringing Matplotlib to the Browser - MPLD3 \n\n[Back to Table of Contents](#0.1)
**The kernel [Geostatistical analysis with SciKit-GStat](https://www.kaggle.com/vbmokin/geostatistical-analysis-with-scikit-gstat)**\n\nIt's my kernel.\n\nThe main application for scikit-gstat is variogram analysis and Kriging (Geostatistical analysis). The basic idea of kriging is to predict the value of a function at a given point by computing a weighted average of the known values of the function in the neighborhood of the point.
**The kernel 1 [Wuhan Coronavirus : A geographical analysis](https://www.kaggle.com/parulpandey/wuhan-coronavirus-a-geographical-analysis/notebook)**\n\n**Thanks to @parulpandey**
## 15. Altair \n\n[Back to Table of Contents](#0.1)
**The kernel 1 [Cheatsheet 100+ Altair Plots - Part 1 (Basic)](https://www.kaggle.com/raenish/cheatsheet-100-altair-plots-part-1-basic/)**\n\n**Thanks to @raenish**\n\nWith Altair:\n* Scatter\n* Line\n* Area\n* Bubble\n* Bar\n* Histogram\n* Box\n* Time Series\n* Strip Plot\n* Dot Plot
"**[Altair](https://pypi.org/project/altair/)** is a declarative statistical visualization library for Python. With Altair, you can spend more time understanding your data and its meaning. Altair's API is simple, friendly and consistent and built on top of the powerful Vega-Lite JSON specification. This elegant simplicity produces beautiful and effective visualizations with a minimal amount of code. Altair is developed by Jake Vanderplas and Brian Granger in close collaboration with the UW Interactive Data Lab."
**The kernel 2 [Cheatsheet 100+ Altair Plots - Part 2 (Advanced)](https://www.kaggle.com/raenish/cheatsheet-100-altair-plots-part-2-advanced/)**\n\n**Thanks to @raenish**\n\nWith Altair:\n* Heatmap\n* Error\n* Candlestick\n* Violin\n* Gantt\n* Ridgeline\n* Map\n* Interactive
## 16. Interactive Dashboard \n\n[Back to Table of Contents](#0.1)
**Correlation Matrix**
"Survived and Fare positively correlated, Survived and Sex_male negatively correlated.  \nAlso, Survived and Pclass_3 negatively correlated. SibSp and Parch correlated"
Correlation of training dataset:-
-We can see that there isn’t much correlation among the input features . Thus there is no Multicollinearity among the features. This is good.\n-Few Features have greater than 0.5 Pearson Correlation with output feature.\n-A value closer to 0 implies weaker correlation (exact 0 implying no correlation).\n-A value closer to 1 implies stronger positive correlation.\n-A value closer to -1 implies stronger negative correlation.\n
Let's Find The Outliers:-
Categorical Variables:-
"# Evaluation & submission\n\n### Check loss and accuracy\nOkay, we made it! Let's evaluate our model, check our stats (Out-Of-Fold log-loss) and submit it! Let's start with some plots.\nThose plots can tell us, whether our model training is working as expected, or if it strongly overfits.  \nThe below **EXAMPLE-IMAGE** is an example of a strongly overfitting model:\n![image.png](attachment:image.png)\nLong before we hit the 10th epoch, the validation loss is increasing again, while the training loss keeps decreasing. We can also clearly ovserve that there is no improvement in our accuracy anymore. \n\nWhat can we do against strongly overfitting models?\nWe could do the following:\n\n* Collect more training data or use augmentation to generate new data: Sadly I have no brilliant idea on how to do this for this specific Kaggle competition.\n* Reduce the network’s size (width andor/ dept) by removing layers or reducing the number of neurons in the hidden layers\n* Use regularization like LASSO (=Least Absolute Shrinkage and Selection Operator; aka L1 regularization) or Ridge (aka L2 regularization) which results in adding a cost-term to the loss function\n* Use higher dropout-rate in the Dropout-Layers, which will randomly remove more connections by setting them to zero and forcing the network to generalize better (=avoid relying on a limitied number of strong-influence neurons).\n\nBut now check our model's result and evaluate it:"
### OOF Evaluation
# 2.2.5.4 Education vs. Role
- Hmm this one seems interesting... It looks like to me some students couldn't decide between if they should choose Bachelor's Degree or Some College Study?
# 2.2.5.5 Courses vs. Role
"- Again like education, it doesn't matter what stage you're in your career, you still could use help of online courses.\n- We can clearly see decent amount of people who already has specialized titles attended an online course in past or still attending one."
# 2.3.1.1 Role vs. Experience
"- Participants who never written code mostly are consists of business analysts, product managers and some various other roles.\n- For under one year experience group, unemployed and students are making a big jump, and some data related roles are somewhat closely distributed like ML Engineer, Data Scientist, Data Engineer etc.\n- Next we have 1-3 years of experience, which is the biggest group. 44% of students are in this group as they represent biggest role group among total participants too.\n    - 1-3 years looks like a balanced group if we don't include the students. They take around ~20-25% of every single role group. There are some roles at higher end of this average like Unemployed and Data Analysts...\n- For the 3-10 years of experience range we can observe the beginning of drastic drops at some groups like Students, Unemployed, I believe that's the part where people start establishing their career path choices.\n- 10+ years of experience groups where we can observe lowest levels of unemployment and students.\n    - Again we can see that Data Scientist, Machine Learning Engineer, Data Analyst roles are not popular among really experienced participants. This migt be due to:\n        - Getting more managerial, advocacy roles with increased experience,\n        - Experienced developers who interested in data science but havent changed their career path yet...\n- Maybe merging this relation with one more layer would give us better insights:"
"# 2.3.2.1 Role vs. Language\n- Here we're going to inspect what languages has been popular with various job titles/roles, so we can get better insights about role demands..."
"- In general -as we seen previous section- Python is popular with most of the roles. Especially popular among Data Scientists, Machine Learning Engineers and Students.\n- Looks like R commonly used by Statisticians,\n- SQL has little bit more general usage but, we can see people who work in data pipelines like Database Engineers or Data Engineers it's much more common.\n- It seems MATLAB mainly used by Research Scientists."
# 2.3.2.2 Education vs. Language
"- We have similar look above, Python is most popular one with all education levels meanwhile SQL takes the second spot,\n- It seems R is getting more popular with higher levels of education, is it due to age of the language? Maybe...\n- I don't see significant differences for the rest of the programming languages maybe with exception: You can notice people where attended/planning doctorate have used MATLAB previously more common then other groups, this might be due to engineering related education in past...\n"
"# 2.3.3.1 Recommending the Language by It's User\n\nBefore we start;\n- Please don't forget this recommendation is for aspiring Data Scientists and it's recommended for their first choice of programming language.\n    - So we are not looking for general purpose language recommendation here,\n    - Nor recommendations for experienced data scientists directly.\n    - So for example, a person who's using C++ and not recommending that actually not recommending his own language for the beginner data scientists, not because he doesn't like it :)\n    - Also people who are using C++ are likely been using other languages like Python too, since it's multi choice question...\n- Well, with leaving the warnings behind let's take a look at the data itself :D"
"- Hmm 84% of the participants who uses Python regularly recommends Python for aspiring data scientist to learn first.\n- That makes Python is the most recommended language by it's regular users by far.\n- Closest one to this one is R, which is around 20%\n- Julia also an interesting one, I think high performance makes it recommendable by it's users. Maybe with some more library support we might see increased numbers incoming years?\n- We can already see Python is popular recommendation with all programming language users, so I just want to open new window for Python itself:\n    - Who doesn't recommend Python?\n    - What other languages are recommended by people who uses Python already?"
- It looks like love for Julia and R strong against Python.\n- People who uses these two languages I mentioned above more likely to recommend you another beginner data science language than Python :)
"- It's really small part of the users recommend other languages in this group (remember they mostly recommended Python), people who uses Python but doesn't recommend python are tend to recommend SQL (around %5) and R (around 5%) for aspiring data scientists."
# 2.4.1.1 IDE vs Language\n\n- What we going to find about in next plot is whetever if there is a correlation with IDE and user's programming language preference:
"- Interesting... Even sometimes it's hard to find meaningful correlations between categorical datas we can see some noticable correlations between some categories.\n- The most obvious one is between R and RStudio,\n- With MATLAB and MATLAB IDE there we can notice similar correlation\n- There's noticeable correlation between Python and Jupyter Notebooks\n- Vim / Emacs has some degree of correlation with Bash \n- While looking at this table we can also see some kind of clusters between some languages like: Java, Javascript, C, C++, it shows when a participant uses one of these languages regularly there is a stronger chance than rest to use one of the other languages from this cluster..."
# 2.4.2 Notebooks
"- Colab and  kaggle  notebooks are the leaders here. \n- We also have significant amount of people who doesn't prefer notebooks at all too.\n- It comes back to topic we discussed before, it's important to decide what tool you going to use depending on your project before you start it..."
# 2.5.1 Visualization Libraries
# 2.5.1.1 Visualization Libraries and Languages
"- I see some decent correlations here, most obvious one is Ggplot / ggplot2 and R correlation, well it was expected, right? I know some people are using R just for ggplot...\n- Seaborn & Matplotlib are closely related. Well... One built over the other one anyways...\n- With Python most correlated ones are these two I mentioned above, followed by Plotly which gets pretty popular lately.\n\n*For Robert we can suggest taking a closer look to Seaborn & Matplotlib, since he feels competent with Python. We still don't know if he is going to like data visualization though. But anyways, knowing little bit of visualization might help a lot in his other tasks too. I can also recommend Plotly to Robert if he's interested in visualization more, lately I'm seeing lots of cool plots created with it around here :)*"
# 2.5.1.2 Visualization Libraries and Roles
"- Doesn't matter your job/role is you probably used Matplotlib once or twice. The table above shows about 60% of the participants uses that library regularly.\n- Statisticians are using ggplot more often, we can observe the Statistician, R and ggplot relation here easily.\n- Machine Learning Engineers who uses Matplotlib regularly has bigger percentage than Data Scientists. I believe it's because of matplotlib's easy to use and it's way get results in a fast and simple fashion, meanwhile seaborn and plotly a little bit more popular for Data Scientists; probably for the people who needs to present their findings in better shape :)\n\n*Well Robert, I know you are confused and still not sure what data oriented career path you should choose in future but you must know about at least one visualization library. As you can see it doesn't matter if you going to be Data Analyst or Machine Learning Engineer, you need to visualize your findings and use/present them in your job. We already suggested which ones to start learning first but these findings are strengthening the idea.*"
# 2.5.2 Machine Learning Frameworks
"- Things are getting more complicated here :)\n- With ML libraries/frameworks are entering the picture, even the clear winner barely passes the 50% limit, which is Scikit-learn.\n- Since I use most of these on regular basis I might add some personal notes about them:\n    - Scikit-learn is a library provides many learning algorithms, setting standard for many other ML libraries. It's a great tool for prototyping, I forgot how many times I started prototyping a solution for a problem and at the end found out sklearn is more than enough...\n    - Tensorflow is a library for fast numerical computings which we need them for Deep Learning. It's has low level abilities to customize your workflow as you want.\n    - Keras is higher level API for Tensorflow, it wraps most common deep learning tasks aiming for easier usage. (You can still do low level stuff though using funtional or subclassing API's)\n    - Pytorch is somewhere between these two above, again it's an open source ML library for mainly deep learning tasks. People usually say it has more ""Pythonic"" style...\n     - You can think of PyTorch Lightning like a Keras for Pytorch, not exactly same but it's aiming for similar purposes.\n    - JAX is relatively a young framework, it's designed for high-performance numerical computing. People like to call it ""NumPy on steroids"" :) I find that description pretty accurate too :)\n    - Xgboost, LightGBM and Catboost are similar libraries, aiming to get maximum use of gradient boosting. They do well on tabular data usually...\n    - You can also see some libraries has more specific usage areas like Huggingface for Natural Language Processing tasks or Prophet for time series etc."
# 2.5.2.1 Machine Learning Libraries and Roles
"- Scikit-learn seems to have wide variety users, doesn't matter what role you have it's still has some use cases for you. It has pretty common usage with Data Scientists and Machine Learning Engineers.\n- When we switch to Deep Learning Frameworks like TensorFlow, Keras, PyTorch, we see Machine Learning Engineers are getting one step ahead, while most roles seen decrease from sklearn to these frameworks, one of the most limited decrease is from ML Engineers. Followed by Research Scientists and Sofware Engineers (Interesting one)...\n- Gradient boosting algorithms are more common than other role groups among Data Scientists.\n- It seems statisticians are one of the least interested groups in machine learning (after Developer Relations), I was expecting little big jump at Tidymodels for this group but it seems even ""None"" group had sharper spike.\n\n*So Robert... I heard you getting interested specially in Data Scientist and Machine Learning Engineer roles. You better start learning Scikit-learn then, as you can see it's really popular and in demand. Also, like I mentioned above it has great workflow standard, so after getting familiar with it you can learn other libraries much faster. For example some popular libraries like Xgboost, LightGBM already having sklearn interface too, so after getting familiar with sklearn you can start using these too in no time! Good lad! If you want to advance further I'd suggest taking a look at the TensorFlow and especially Keras, because of it's higher level API and having some similarities with sklearn workflow, it would be good starting point. If I recall correctly Keras has sklearn wrapper too. To get little bit of theory and practical knowledge you can get help of some books. For Scikit-learn I'd recommend ""Introduction to Machine Learning with Python: A Guide for Data Scientists"" from Andreas C. Müller, for TensorFlow and Keras you can take a look at ""Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow"" from Aurélien Géron, in that book you can get some solid knowledge for sklearn too. If you want something Keras specific you can read ""Deep Learning with Python"" from François Chollet. Getting familiar with these going to take some time but after that you can start learning PyTorch if you like to, it's a great framework and lately on the rise, many research codes are published in PyTorch format so it's nice to know. Hey Robert! Are you still there?!*"
"# 2.5.2.2 Machine Learning Libraries and Visualization\n- So, couple sections above we have seen visualization libraries are pretty popular among Machine Learning practicioners, let's see how these relations connects with ML frameworks themselves."
"- Most popular visualization tools (Matplotlib, Seaborn) are highly related with Scikit-learn, followed by Keras/TF and XGboost.\n- ggplot and Shiny are more correlated with Tidymodels and Caret. Again strong R connections :)\n- For the rest of visualization libraries I couldn't find meaningful correlations between them and ML libraries, so let's check relations between ML libraries themselves next.\n- You can notice decent correlation between sklearn and gradient boosting algorithms and TF/Keras, so in some ways that confirms our suggestion to learn Scikit-learn first.\n- For PyTorch strongest ones are Pytorch Lightning (Surprise!) and Huggingface, I think you can use Tensorflow with Huggingface too but PyTorch seems more default setting for it. Again TF/Keras has some correlation with PyTorch too since they both serve for Deep Learning...\n\n*You see Robert? Now we have stronger foundations for the suggestions about your data science roadmap. As you can see the higher correlated libraries are creating a cluster for most of the data scientists: Matplotlib/Seaborn usage correlated with Scikit-learn, Scikitlearn usage correlated with gradient boosting algorithms and these are somewhat correlated with deep learning frameworks like TF/Keras. With these small steps we start getting a broad roadmap for what you need to learn already, how you feel about that?*"
# 2.5.3 Machine Learning Algorithms
"- Regression tasks (either linear or logistic for classification) are most popular ML algorithms by participants.\n- Tree based algorithms taking second place, again pretty common among  kaggle  users.\n- Gradient boosting and CNN's (mostly for computer vision tasks) following these two.\n- It's interesting to see DNN's are less popular than CNN's, probably classical supervised approaches (sklearn and gradient boosting stuff) are still preferable for tabular data...\n- Then comes Bayesian Approaches, RNN's and Transformers... Last two are usually used for Natural Language Processing problems but lately Transformers are getting some hype around computer vision tasks too."
# 2.5.3.1 Machine Learning Algorithms and Roles
"- The heatmap above you can see which role regularly interacts with a certain algorithm. \n- Again we can confirm that Data Scientist is a broad term, this role uses wide range of algorithms regularly.\n- Machine Learning Engineer role uses neural networks more than rest, this includes:\n    - DNN's\n    - CNN's\n    - RNN's\n    - Transformers\n    - GAN's\n- These above are more specialized algorithms for tasks like computer vision, natural language processing, autoencoders etc.\n- Research scientists also having similar algorithm usage patterns with machine learning engineers.\n\n*Hey there Robert! You probably heard enough but I'm going to say again, learn Scikit-learn! You can use most of these algorithms within the sklearn package, even some DNN's like Perceptron or MLP's and as you can see you are going to use some of these algorithms a lot and it doesn't matter what specific title you want to get in future. You were interested in Data Scientist or Machine Learning Engineer roles right? I'd definitely start learning then... I can show it to you in another way, look:*"
As you can see there is a strong path to the Data Scientist and Machine Learning Engineer roles from classical libraries.
# 2.5.3.2 Machine Learning Algorithms and Libraries
"- The plot above confirms that Scikit-learn is correlated with most of the machine learning algorithms.\n- For neural networks TF/Keras, PyTorch is getting more popular.\n- And lastly as expected from previous plots Transformer Networks are highly correlated with Huggingface"
# 2.5.4.1 ML Experience Distribution by Role
"*You see Robert? No need to worry about getting confused with data science concepts at first sight, as you can see most of the  kaggle  users are either under 1 years of ML experience or has no experience at all. It's still young field and there will be many career opportunities if you play your cards right and in time!*"
# 2.5.4.2 ML and Programmin Experience
"- We can see solid linear correlation between years of programming experience and years of ML experience.\n- Usually ML experience of participants are couple years less than their programming experience, which makes sense...\n- I noticed there are **6 people with more than 20 years of Machine Learning experience but less than 1 years of coding experience**. Well, I mean it's important to learn theory behind the ML algorithms before start coding them, right? Haha...\n\n*It's a good thing you start learning how to code with Python before you start doing Machine Learning stuff. As I mentioned above usually people are getting familiar with a programming language before they start learning the algorithms etc. You done well there Robert, good lad!*"
# 2.6.1 Work Activities\n\n- Let's stary by looking what's your daily responsibilities going to be like in future job...
"- Seems like analyzing and understanding the data for stakeholders is still most common task for data related jobs.\n- Second most popular activity seems intriguing, looks like industry is still trying to find use of machine learning techniques to take advantage of. So I believe there will be new areas to explore and conquer :) \n- If we merge these two above we can see that we're still in early parts of machine learning part of the data science, lot to discover, lot to understand...\n- Third one more relevant to data itself, creating and storing useful information is really important.\n- Next two seems like using ML techniques more practically, where participant tries to build/upgrade machine learning methods to improve his workflow.\n- Doing research probably important for research scientists or maybe machine learning engineers, we'll find about it soon.\n- And lastly there is decent portion of participants who doesn't find these activities important for his role..."
# 2.6.1.1 Roles and Responsibilities
"- This one above seems interesting... We can learn a lot about roles and their responsibilities.\n- Analyzing and understanding the data is pretty important for all roles, maybe with the exceptions like other roles or software engineers...\n    - For Business and Data analysts learning the data and influence business decisions are most important thing. (Well, surprise...)\n    - Again for data scientists exploring the data seems pretty important but as you can see it's not the only responsibility they usually take :)\n    - We can also see that understanding the data is important for managerial roles too...\n- Again an obvious one, building and running data infrastructure is pretty common for data and database engineers.\n- Building machine learning prototypes for new areas to exploit seems pretty common among the data&research scientists and ML engineers. An interesting one for me is  the product manager one which more than one third of product managers are prototyping machine learning to new areas.\n- Building and running ML services are less common than I expected, mostly taken by machine learning engineers and data scientists.\n- Experimenting or improving models taking important part of ml engineers and data&research scientists daily duties.\n- For research, research scientists are coming first (Duh!) followed by ml engineers. \n\n*Well Robert, whatever you do for work, to find a place in data field you must understand and process the data. As I told you in previous chapter best way to do this is doing exploratory data analysis and one of the best tools going to help you with this is visualization. That's why it's recommended to learn matplotlib/seaborn for you. When you understand the data you can move to the next steps like asking the questions: ""What can I do with this data, how can I use this information to create something useful, can I train a model to predict stuff with this data"". If you start asking these kind of questions you're on the right track, but again you need to get hang of the data first, then the questions are getting more specialized. After exploring, to answer some of these questions you going to use machine learning. It's going to take lot's of prototyping and as I mentioned in previous chapter, Sckit-learn going to help you a lot with this. You don't want to create complex models before you reach satisfactory results with more traditional models. If you manage to do this then you'll get to next step where you need to improve existing model, maybe doing some research to learn about state of the art stuff then deploy the model etc. Maybe you are little bit unexperienced for some of these but it's good to know and by the way please stop asking me about the wage, we'll get there...*"
# 2.6.1.2 ML Experience and Responsibilities
"- Well... With increasing the years of ml experience almost all responsibilities are getting greater. I thought things will get easier in future, oh... :(\n- So the chart above confirms our assumption that the duties you cover/work increases with your knowledge.\n- It also confirms that you can get new skills while working too."
# 2.6.2.1 Industry and Roles
"- The table above we can see distribution of roles by industry.\n- Most of the roles are employed by computers/technology and acadamics/education industries as we seen from previous plot too. But let's take short look on rest.\n- It seems accounting and finance industry employs more analyst positions.\n- For government and public services, statisticians and database engineers are more popular than others.\n- For some fields I hoped for little bit higher percentages and I believe they could take advantage of data much more:\n    - Insurance / Risk Assesment: Where you could train model with lesser human bias.\n    - Medical / Pharmaceutical: Especially computer vision could be life saver (literally) for early and accurate detections.\n    - Retail / Online sales: Better customer segmentation or maybe stock estimation.\n    - Shipping / Transportation: Where companies can optimize their workflow and increase their efficiency.\n    \n*Things looking good for you Robert, it might your options are limited at first sight but if you look closely future is promising. You might be working for tech startup or academia(if you want to continue after master's) and advance your career there and you might want to move bigger tech companies in future. Also you might get hired by a small company or small department of data science in bigger company. Options are countless as long as you add some skills that make you one step ahead of your competitors. And we discussed what you should do for that in previous chapter right? If you forgot you can just go back there and start reading again. I'll be waiting for you in next part...*\n"
# 2.7.1.1 Country of Residence and Yearly Income
"- Before I plot the results I noticed high standard deviation for these variables so I decided to use median instead of mean. I believe this would give more robust results.\n- As you can see from the table above there are huge gaps between countries, but again every country should be analysed by it's economic standards.\n- The chart looks better than I expected at first, if you are little bit familiar with world economies and demographics you can notice there are similarities between them and this chart.\n- You can see some countries are clustering toghether and they usually have similar economic parameters like money zone, gdp/c etc.\n- I'm not going to analyse median income country by country since it would take another chapter itself, but feel free to do so;\n\n*And lucky you Robert, you already residing in top pretty high median income country!*"
# 2.7.1.2 Education and Yearly Income
- Although there are some outliers we can see a clear pattern here: With increasing level of education median income by year increases too.
# 2.7.1.3 Role and Yearly Income
"- Wait what?! Machine Learning Engineer has one of the lowest median income?\n- I thought they earning little bit more than data scientists generally. There must be something wrong there, we need to dig that; but for now let's look at the rest...\n- Seems like managerial roles has the highest income although having some large gaps between quantiles.\n- Some roles like statisticians or data analysts are earning a little bit less than general averages.\n\n*Hey Robert are you there? Sorry what?! You changed your mind over machine learning engineer after this chart? Hold on mate, as I said there must be something affecting this statistic, let me show you; hey listen listen... Go check the next chart!*"
"- That's a relief, I think we found the reason behind the low median income for machine learning engineers globally.\n- If you look at the countries where having the highest number of machine learning engineers you see most of them are placed lower ranks on the list in terms of median USD income.\n- So high number of machine learning engineers from these countries are lowering the global median income statistics.\n- You can see two exceptions on this list: USA and Japan, when you look closely their income values for data scientists and machine learning engineers you can see they're almost identical, meanwhile in countries like India data scientists almost making two times more than machine learning engineers, interesting...\n- Is machine learning engineer title over-hyped in some countries and demand is low? That might be the case...\n\n*You see Robert, there's no need to worry about that, you are interested in machine learning engineer role again? Ah nice, good lad!*"
# 2.7.1.4 Industry and Yearly Income
"- A saddening chart... While Military/Security/Defense Industry giving highest median wages the lowest median one is Academics/Education (If we don't count non-profit ones), I mean security is important in it's own way but I believe Academics should get more love for the future.\n    - Personally I check high paying countries individually and the trend is similar, academics are at bottom in yearly compensation charts...\n- The rest of the Industries seems balanced in their own clusters, especially when you include the country effect into the equation. But I'm not going to inspect all of these: Firsly I'm not that familiar with all of these industries to make assumptions and secondly that would take many charts to analyze.\n- So I'd like to contine by taking picture of the industry in regards to mean yearly compensation, so let's continue with that."
"- The trend seems obvious, the bigger the company gets the more wage they pay.\n- You could interpret the size of a company with level of corporateness where corporate firms having higher benefits, more standardized wages etc."
"- Again a clear trend...\n- So, the companies where incorporated ML methods into their business are paying more for the data workers. This could be due to some reasons, like:\n    - The company understands the value of data and willing to pay even more for right person.\n    - After using ML methods for a while the requirements are getting greater and complex so companies trying to get more skillful employees.\n    - The added-value generated by ML is significant so data department getting higher budgets inside the company.\n    - The demand is high for people who can work with ML stuff but the qualified workforce is low.\n    "
- This one confirms our assumptions in previous charts:\n    - The need for people who can work with data increases how much the company is willing to pay for it's employees.
"- Here I'm going to reduce dimension of the data and try to find meaningful clusters, it'a unsupervised learning method might help us to understand more about the data itself..."
"- I'm going to use some of the data we analysed visually before while excluding the ones we didn't analyse properly, but firstly we going to reduce dimension of that data to 2D space using ""Principal Component Analysis"" so we can plot it easily and make it human readable :)\n- Then we're going to try various number of clusters to see intertia values (basically mean distance of instances to the cluster centroid), of course increasing k naturally decreases the inertia, so we looking for the ""Elbow"" where inertia decrease slows down sharply."
- So our k can be 2 or 3 but 3 seems little bit healthier. That means we're going to cluster all instances(participants) into 3 groups.\n- Let's plot that to see if it makes any sense...
"- Ok... Not great, not terrible, I can see one cluster diverges a little bit more than others meanwhile cluster 0 and 2 is just one big blob :)\n- When we plot same data with coloring instances based on their role we can see two roles are fitting pretty good on cluster 1 on left:\n    - Students and Currently Unemployed ones are mainly sitting in the cluster 1,\n    - Other clusters are just chaotic :) It might be due to data itself or 2D might not be enough to plot that for human eyes...\n    \n*Hey Robert! You see yourself there? Wave at us! Here you are! So you are fine fit for cluster 1, this is your new team, meet your friends! Cluster 1 mainly consists of students and unemployed people so cluster 1 is for people who starting their data science career recently. If you want to compare yourself with other people, using this tecnique can help you with reducing your scope so you can make fine grained analyses. Anyways as you can see you are not alone at all and with the takeaways from this notebook you can make rapid advances to other clusters you want to be in :)*"
- Since we reduced the dimensions let's check what's the percentage of variance explained by principal components:
- Oh this explains a lot... Well I mean actually it doesn't explain the variance a lot :) We used first two principal components to visualize the and it only explains 40% of the variance.\n- Let's take a look one more thing about dimension reduction then...
"- Well well, take a look at this one...\n- Looks like 4th principal component represents the Role feature really well, look how clustered the instances are! They look like cute rainbows :)\n\n*Isn't it fun to find hidden stuff inside the data Robert? You having fun there? As I told you before, with little bit knowledge of Sklearn and visualization tools you can find new ways to gain insights. Wait you already clustered Iris dataset? Nice one Robert! It's a great choice to learn about clustering, well done!*"
"# Something Personal...\n\nUntill now I used the actual data itself. I wonder how's the text written by me along this notebook, what points I kept dwell on, what words I over used :/ When I turn back and check the notebook I found out I wrote loads of things, I just crawled them and extracted it to a single .txt file, I think its worth to visualize it :) Let's see:"
"Ok, maybe I over used some words like ""see, one etc."" usually they coming from the parts where I point out a plot or finding like I just did above (facepalm)... If we count these as stopwords then we can see some more important words like:\n- ""Data"",\n- ""Data Science"",\n- ""Data Scientist"",\n- ""Machine Learning"" or ""ML"",\n- ""Machine Learning Engineer"",\n- And many more related stuff...\n\nAnd of course the **Robert**, we couldn't done it without him :)\n\nAnyways it's time to wrap up things..."
**In this section I will demonstrate 15 Torchvision Transforms with demos namely:**\n1. [Center Crop](#2.1)\n2. [Random Crop](#2.2)\n3. [Random Resized Crop](#2.3)\n4. [Color Jitter](#2.4)\n5. [Pad](#2.5)\n6. [Random Affine](#2.6)\n7. [Random Horizontal Flip](#2.7)\n8. [Random Vertical Flip](#2.8)\n9. [Random Perspective](#2.9)\n10. [Random Rotation](#2.10)\n11. [Random Invert](#2.11)\n12. [Random Posterize](#2.12)\n13. [Random Solarize](#2.13)\n14. [Random Autocontrast](#2.14)\n15. [Random Equalize](#2.15)
## **PyTorch Dataset Class**
## 3a. Gender and Survived\n\n***
This bar plot above shows the distribution of female and male survived. The ***x_label*** represents **Sex** feature while the ***y_label*** represents the % of **passenger survived**. This bar plot shows that ~74% female passenger survived while only ~19% male passenger survived.
"This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive. \n\n**Summary**\n***\n- As we suspected, female passengers have survived at a much better rate than male passengers. \n- It seems about right since females and children were the priority. "
"- It looks like ...\n    - ~ 63% first class passenger survived titanic tragedy, while \n    - ~ 48% second class and \n    - ~ only  24% third class passenger survived. \n\n"
"This KDE plot is pretty self-explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that the lower class passengers have survived more than second-class passengers. It is true since there were a lot more third-class passengers than first and second. \n\n**Summary**\n***\nThe first class passengers had the upper hand during the tragedy. You can probably agree with me more on this, in the next section of visualizations where we look at the distribution of ticket fare and survived column. "
## 3c. Fare and Survived\n\n***
"This plot shows something impressive..\n- The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive. \n- When fare is approximately more than 280 dollars, there is no gray shade which means, either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment. Let's check..."
## 3d. Age and Survived\n\n***
"There is nothing out of the ordinary about this plot, except the very left part of the distribution. This may hint on the posibility that children and infants were the priority. "
"## 3e. Combined Feature Relations\n\n***\nIn this section, we are going to discover more than two feature relations in a single graph. I will try my best to illustrate most of the feature relations. Let's get to it. "
"Facetgrid is a great way to visualize multiple variables and their relationships at once. From the chart in section 3a we have a intuation that female passengers had better prority than males during the tragedy. However, from this facet grid, we can also understand which age range groups survived more than others or were not so lucky"
"This is another compelling facet grid illustrating four features relationship at once. They are **Embarked, Age, Survived & Sex**. \n* The color illustrates passengers survival status(green represents survived, gray represents not survived)\n* The column represents Sex(left being male, right stands for female)\n* The row represents Embarked(from top to bottom: S, C, Q)\n***\nNow that I have steered out the apparent let's see if we can get some insights that are not so obvious as we look at the data. \n* Most passengers seem to be boarded on Southampton(S).\n* More than 60% of the passengers died boarded on Southampton. \n* More than 60% of the passengers lived boarded on Cherbourg(C).\n* Pretty much every male that boarded on Queenstown(Q) did not survive. \n* There were very few females boarded on Queenstown, however, most of them survived. "
"This facet grid unveils a couple of interesting insights. Let's find out.\n* The grid above clearly demonstrates the three outliers with Fare of over \$500. At this point, I think we are quite confident that these outliers should be deleted.\n* Most of the passengers were with in the Fare range of \$100. "
**Passenger who traveled in big groups with parents/children had less survival rate than other passengers.**
"**While, passenger who traveled in small groups with sibilings/spouses had better changes of survivint than other passengers.**"
\n**Squaring the correlation feature not only gives on positive correlations but also amplifies the relationships.** 
"#### Positive Correlation Features:\n- Fare and Survived: 0.26\n\n#### Negative Correlation Features:\n- Fare and Pclass: -0.6\n- Sex and Survived: -0.55\n- Pclass and Survived: -0.33\n\n\n**So, Let's analyze these correlations a bit.** We have found some moderately strong relationships between different features. There is a definite positive correlation between Fare and Survived rated. This relationship reveals that the passenger who paid more money for their ticket were more likely to survive. This theory aligns with one other correlation which is the correlation between Fare and Pclass(-0.6). This relationship can be explained by saying that first class passenger(1) paid more for fare then second class passenger(2), similarly second class passenger paid more than the third class passenger(3). This theory can also be supported by mentioning another Pclass correlation with our dependent variable, Survived. The correlation between Pclass and Survived is -0.33. This can also be explained by saying that first class passenger had a better chance of surviving than the second or the third and so on.\n\nHowever, the most significant correlation with our dependent variable is the Sex variable, which is the info on whether the passenger was male or female. This negative correlation with a magnitude of -0.54 which points towards some undeniable insights. Let's do some statistics to see how statistically significant this correlation is. "
H0: male mean is greater or equal to female mean\nH1: male mean is less than female mean. 
"#### Compare P-value with $\alpha$\n> It looks like the p-value is very small compared to our significance level($\alpha$)of 0.05. Our observation sample is statistically significant. Therefore, our null hypothesis is ruled out, and our alternative hypothesis is valid, which is ""**There is a significant difference in the survival rate between the male and female passengers.""**"
Let's take a look at the histogram of the age column. 
"## age_group\nWe can create a new feature by grouping the ""Age"" column"
we have our confusion matrix. How about we give it a little more character. 
AUC & ROC Curve
#### 4.3.1.2. Average method forecasting
#### 4.3.1.3. Seasonal Naïve forecasting
### 4.3.2. ARIMA Forecasting 
#### 4.3.2.2. Rolling forecasting with ARIMA model
#### 4.3.2.3. Long-term forecasting with ARIMA model
#### 4.3.2.4. Rolling forecasting with Auto-ARIMA model
#### 4.3.2.5. Long-term forecasting with Auto-ARIMA model
### 4.3.3. Prophet forecasting 
#### 4.3.3.2. Rolling forecasts with Prophet
#### 4.3.3.3. Long-term forecasting with Prophet
### 4.3.4. Forecasting with LSTM
#### 4.3.4.1. Long-term forecasting with Vanilla LSTM configuration
#### 4.3.4.2. Long-term forecasting with Stacked LSTM model
#### 4.3.4.3. Long-term forecasting with Bidirectional LSTM
# 5. Conclusion
**Let us use Petrol data and observe seasonality using visualization techniques**
"### Seasonal Indices\n\n* Seasonality is the presence of variations that occur at specific regular intervals less than a year, such as weekly, monthly, or quarterly. \n* Seasonality may be caused by various factors, such as weather, vacation, and holidays and consists of periodic, repetitive, and generally regular and predictable patterns in the levels of a time series."
#### Plot the average temp
#### Plot the average forecast
#### Plot the moving average forecast and average temperature
### Moving average of window size 5 for US GDP
"### Inference\n\nWe observe that the resample() function has created the rows by putting NaN values as new values for dates other than day 01. \n\nNext we can interpolate the missing  values at this new frequency. The function, interpolate() of pandas library is used to interpolate the missing values. \nWe use a linear interpolation which draws a straight line between available data, on the first day of the month and fills in values at the chosen frequency from this line. "
**Another common interpolation**\n\n* Another common interpolation method is to use a polynomial or a spline to connect the values.\nThis creates more curves and look more natural on many datasets.\n* Using a spline interpolation requires you specify the order (count of terms in the polynomial); we use 2.
"**Down-sampling Frequency**\n\n* The sales data is monthly, but we prefer the data to be quarterly. The year can be divided into 4 business quarters, 3 months a piece. \n* The resample() function will group all observations by the new frequency.\n* We need to decide how to create a new quarterly value from each group of 3 records. We shall use the mean() function to calculate the average monthly sales numbers for the quarter"
"### Example \nWe can turn monthly data into yearly data. Down-sample the data using the alias, A for year-end frequency and this time use sum to calculate the total sales each year."
"**Outliers**\nData may contain corrupt or extreme outlier values that need to be identified and handled.\n\n####  Detection of outliers in time series is difficult.\n* If a trend is present in the data, then usual method of detecting outliers by boxplot may not work.\n* If seasonality is present in the data, one particular season's data may be too small or too large compared to others.\n\n#### Decomposition helps in identifying unsual observations\n\n* If trend and seasonality are not adequate to explain the observation\n\n#### Outliers cannot be eliminated - they need to be imputed as closely as possible by using the knowledge gained from decomposition."
### Types of Trends\n\n* Deterministic Trends: They consistently increase or decrease and are easier to identify.\n* Stochastic Trends: They increase and decrease inconsistently \n\n#### Detrend a time series is by differencing
#### Inference\n\nWe don't see any particular trend in the data.
We will use Shampoo dataset.\n\n* A linear model can be fit on the time index to predict the observation. \n* Get a trend line from the predictions from this model.\n* Subtract these predictions from the original time series to provide a detrended version of the dataset.\n\nWe will use a scikit-learn LinearRegression model to train the data.
#### Inference\n\nWe have plotted the trend line in orange colour over the original dataset in blue colour.
"## Seasonal variation may be present in Time series data.\n\n* Seasonal variation, or seasonality, are cycles that repeat regularly over time.\n\n* By plotting and reviewing the data, you can determine if there is any seasonality in the data.\n* We can try with different scales and by adding a trend line.\n* Once the seasonality is identified, it can be modeled. When you remove the model of seasonality from the time series, it is called deseasonalizing or seasonal adjustment.\n\n** Seasonal adjustment with differencing**\n\nWe can test the seasonality differencing method on the daily minimum temperature data."
### Accuracy measures
Let us get the optimum value for $\alpha$ by omitting the value and leave it for the model to decide.
"### Inference\n\nWe observe that for the optimum $\alpha$ value, both RMSE and MAPE are smallest when compared to other $\alpha$ values of 0.1,0.5 and 0.99."
### Check for stationarity using dickey fuller test
"Since the test statistics is more than 5 % critical value and the p-value is larger than 0.05 , the moving average is not constant over time and the null hypothesis of the Dickey-Fuller test cannot be rejected. This shows the weekly time series is not stationary. \n\nAs such , we need to transform this series into a stationary time series. "
### Some of our key observations from this analysis:\n\n1) Trend: 12-months moving average looks quite similar to a straight line hence we could have easily used linear regression to estimate the trend in this data.\n\n2) Seasonality: Seasonal plot displays a fairly consistent month-on-month pattern. The monthly seasonal components are average values for a month after removal of trend. Trend is removed from the time series using the following formula:\n\nSeasonality_t × Remainder_t = Y_t/Trend_t\n \n3) Irregular Remainder (random): is the residual left in the series after removal of trend and seasonal components. Remainder is calculated using the following formula:\n\nRemainder_t = Y_t / (Trend_t × Seasonality_t)
We observe seasonality even after differencing.
We observe trend and seasonality even after taking log of the observations.
"Nonstationary series have an ACF that remains significant for half a dozen or more lags, rather than quickly declining to zero. You must difference such a series until it is stationary before you can identify the process\n\nThe above ACF is “decaying”, or decreasing, very slowly, and remains well above the significance range (blue band) for at least a dozen lags. This is indicative of a non-stationary series."
### Inference\n\nThe above ACF has “decayed” fast and remains within the significance range (blue band) except for a few (5) lags. This is indicative of a stationary series.
### Plot ACF and PACF for residuals of ARIMA model to ensure no more information is left for extraction
"### Inference\n\nWe need to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. If it is not that it signifies that the model can be further improved and we repeat the process with the residuals.\n\nIn this case, our model diagnostics suggests that the model residuals are normally distributed based on the following:\n\n1. The KDE plot of the residuals on the top right is almost similar with the normal distribution.\n2. The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) follows the linear trend of the samples taken from a standard normal distribution with N(0, 1). Again, this is a strong indication that the residuals are normally distributed.\n3. The residuals over time (top left plot) don't display any obvious seasonality and appear to be white noise. This is confirmed by the autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have low correlation with lagged versions of itself.\n\nThose observations coupled with the fact that there are no spikes outside the insignificant zone for both ACF and PACF plots lead us to conclude that that residuals are random with no information or juice in them and our model produces a satisfactory fit that could help us understand our time series data and forecast future values. It sems that our ARIMA model is working fine."
\nFeature Importance\n\nTable of Contents
\nModel Comparison\n\nTable of Contents
### Necesario o No??
"Claramente, Python es una habilidad mucho más necesaria en comparación con R.\n\nGracias especiales a [Steve Broll](https://www.kaggle.com/stevebroll) por ayudar en la combinación de colores."
### Número de usuarios por lenguajes
"El número de usuarios de Python es definitivamente más que los usuarios de R. Esto puede deberse a la curva de aprendizaje fácil de Python. Sin embargo hay más usuarios que conocen ambos lenguajes. Estas respuestas pueden ser de científicos de datos establecidos, ya que tienden a tener conocimientos en varios lenguajes y herramientas."
"Los codificadores de Python tienen un salario medio ligeramente más alto en comparación con sus homólogos de R. Sin embargo, las personas que conocen ambos idiomas tienen un salario medio bastante alto en comparación con los dos.\n\n## Languaje utilizado por profesionales"
"Como mencioné anteriormente, R vence a Python en visualizacion. Por lo tanto, las personas con títulos de trabajo como analista de datos, analista de negocios donde los gráficos y visuales desempeñan un papel muy importante, prefieren R sobre Python. De manera similar, casi el 90% de los estadísticos usan R. También como se indicó anteriormente, Python es mejor en materia de Aprendizaje automático, por lo que los ingenieros de Aprendizaje automático, científicos de datos y otros como DBA o programadores prefieren Python sobre R.\n\nPor lo tanto, para datos visuales ---> R de lo contrario ----> Python.\n\n**Nota: esta gráfica no es para el lenguaje recomendado por los profesionales, sino para las herramientas utilizadas por los profesionales.**"
## Función en el trabajo vs lenguaje
"Como ya mencioné, **R sobresale en analítica, pero Python vence en Aprendizaje automático.** La gráfica muestra que R tiene influencia cuando se trata de analítica pura, pero otras formas en que Python gana."
## Permanencia Vs lenguaje usado
"Como habíamos visto anteriormente, Python es muy recomendable para principiantes. Por lo tanto, la proporción de usuarios de Python es mayor en los años iniciales de codificación. Sin embargo, la brecha entre los lenguajes se reduce con los años, a medida que aumenta la experiencia de codificación.\n\n## Industria vs lenguaje utilizado"
"R vence a Python en las industrias gubernamentales, de seguros y sin fines de lucro. Del mismo modo, Python vence a R con un margen muy grande en la industria de tecnología y militar. En el resto de otras industrias, la proporción de Python parece ser aproximadamente un 15-20% más que la de R.\n\n## Herramientas comunes con Python y R"
"**SQL** parece ser la herramienta complementaria más común utilizada con ambos idiomas. SQL es el lenguaje principal para consultar grandes bases de datos, por lo que saberlo bien es una gran ventaja."
"Entonces, aproximadamente el 26% del total de los encuestados se consideran a sí mismos como científicos de datos. ¿Qué significa algo de esto? ¿Siguen aprendiendo o están desempleados? Por ahora vamos a considerarlos como un No.\n\n## Títulos de trabajo actuales"
"Sorprendentemente no hay **ninguna entrada para el científico de datos del título del trabajo**. Esto podría deberse a que las personas con CurrentJobTitleSelect como Data Scientist (que podría estar trabajando como Data Scientist) podrían no haber respondido la pregunta: **""¿Actualmente te consideras un Data Scientist?""**\n\nHay muchas habilidades comunes y superpuestas entre los trabajos como analista de datos, científico de datos y expertos en aprendizaje automático, estadísticos, etc. Por lo tanto, ellos también tienen habilidades similares y se consideran a sí mismos como científicos de datos, aunque no estén etiquetados de la misma manera. Ahora vamos a comprobar si la suposición anterior era verdadera."
"Entonces, del total de encuestados, alrededor del **40%** de ellos son científicos de datos o tienen habilidades para el mismo.\n\n## Pasion y Division de DS"
El gráfico es similar al gráfico demográfico en el que mostramos el número de usuarios por país. La diferencia es que los números se han reducido ya que solo hemos considerado científicos de datos.\n\n## Situación laboral y educación
"About **67%** of the data scientists are employed full-time, while about **11-12%** of them are unemployed but looking for job. On the education side it is evident that about **45-46%** of the data scientists hold a **master's degree**, while about **23-24%** of them have a **bachelor's degree or a doctoral degree**. Thus education seems to be an important factor for becoming a data scientist. Let's see how does the salary vary according to the education.\n\nAlrededor de **67%** de los datos se emplean a tiempo completo, mientras que alrededor de **11-12%** de ellos están desempleados pero buscando trabajo. Por el lado de la educación, es evidente que alrededor del **45-46%** de los científicos de datos tienen un **grado de maestría**, mientras que alrededor del **23-24%** de ellos tienen un **título de licenciatura o un doctorado**. Por lo tanto, la educación parece ser un factor importante para convertirse en un científico de datos. Veamos cómo varía el salario según la educación.\n\n## Compensación por la educación formal\n"
"Esto es sorprendente, ya que los rangos salariales para licenciatura, maestría y doctorado parecen muy similares. La mediana de la licenciatura parece ser un poco alta en comparación con la maestría y el doctorado. No esperaba esto ya que muchos de los científicos de datos tienen una maestría. Pero creo que **Experiencia laboral** es más importante que cualquier título. Tal vez los titulares de la licenciatura tienen más experiencia en comparación con los otros dos.\n\n## Trabajo previo y cambio salarial"
"Es evidente que la mayoría de las personas que se cambian a Data Science obtienen un aumento salarial de **6-20% o más**. Con esta creciente demanda de Data Scientists, el salario también puede aumentar con el tiempo.\n\n## Herramientas utilizadas en el trabajo"
"Observaciones similares, Python, R y SQL son las herramientas o lenguajes más utilizados en Data Science"
"Coursera es la plataforma más favorecida por los científicos de datos para el aprendizaje de la ciencia de datos. Mi voto personal también va para Coursera, donde puedes aprender cosas desde cero hasta avanzadas en la misma plataforma. No se limita a un solo lenguaje como Python o R, sino que también tiene cursos que cubren otros lenguaje como Scala, etc. Del mismo modo KDNuggets es el blog más preferido.\n\n## Tiempo empleado en tareas\n\nUn científico de datos no siempre está construyendo modelos predictivos, también es responsable de la calidad de los datos, la recopilación de los datos correctos, análisis, etc. Veamos cuánto tiempo pasa un científico de datos en estas diferentes tareas."
"La línea punteada es la línea media.\nVamos a hacerlo paso a paso:\n\n  - **TimeGatheringData:** Es, sin duda, la parte más lenta. Obtener los datos es la tarea más minuciosa de todo el proceso, a la que sigue la Limpieza de datos (que no se muestra como datos no disponibles), que es otro proceso que consume mucho tiempo. Por lo tanto, la recopilación de datos correctos y la limpieza de los datos son el proceso más lento.\n  \n  - **TimeVisualizing:** Es probablemente el proceso que consume menos tiempo (y probablemente el más agradable ...: p), y se reduce aún más si usamos Enterprise Tools como Tableau, Qlik, Tibco, etc., lo que ayuda a Construyendo gráficos y tableros con características simples de arrastrar y soltar.\n  \n  - **TimeFindingInsights:** Se sigue después de visualizar los datos, lo que implica encontrar datos y patrones en los datos, dividirlos y cortarlos en trozos para encontrar información sobre los procesos de negocios. Parece un poco más lento que el de TimeVisualizing.\n  \n  - **TimeModelBuilding:** Es donde los científicos de datos construyen modelos predictivos, sintonizan estos modelos, etc. Es el segundo proceso que consume más tiempo después de TimeDataGathering.\n"
"## Servicios en la nube\n\nCon el aumento del tamaño de los datos, no es posible procesar los datos y realizar análisis predictivos en las infraestructuras del servidor físico. Por lo tanto, la nube lleva el análisis predictivo a un nivel superior, con escalabilidad su principal ventaja. Gestionan un servicio que le permite construir fácilmente modelos de aprendizaje automático que funcionan con cualquier tipo de datos, de cualquier tamaño. Permite comprobar cuáles son las plataformas en la nube más utilizadas por los científicos de datos."
"It is evident that **AmazonAWS**, which is a public cloud service provider is the most used cloud platform, followed by Hadoop. Hadoop is an open-source software framework used for distributed storage and processing of dataset of big data. For reading more about Hadoop, **Check this**\n\nEs evidente que **AmazonAWS**, que es un proveedor de servicios de nube pública, es la plataforma de nube más utilizada, seguida por Hadoop. Hadoop es un marco de software de código abierto utilizado para el almacenamiento distribuido y el procesamiento de conjuntos de datos de big data. Para leer más sobre Hadoop, **Revise esto**"
## Importancia de las visualizaciones
"Las visualizaciones son una parte muy integral de los proyectos de Data Science, y el gráfico anterior también muestra lo mismo. Casi todos los proyectos de ciencia de datos, es decir, el **99%** de los proyectos tienen visualizaciones, no importa lo grande o pequeño que sea. Aproximadamente **95%** de los científicos de datos dicen que las habilidades de visualización son agradables de tener o necesarias. Las visuales ayudan a comprender y comprender los datos con mayor rapidez, no solo para los profesionales, sino también para los clientes objetivo, que pueden no ser técnicamente expertos.\n\n## Herramientas de BI\n\nEl software de inteligencia empresarial es un tipo de software de aplicación diseñado para recuperar, analizar, transformar e informar datos para inteligencia empresarial. Hacen que la visualización y el análisis de datos sean muy simples en comparación con la forma de codificación normal en Python o R. El único inconveniente es que son **exclusivos y costosos**. Permite comprobar cuáles son las herramientas de BI empresariales más utilizadas por los científicos de datos."
"Los científicos de datos tienen un buen conocimiento de conceptos matemáticos como Estadística y Álgebra Lineal, que son la parte más importante de los algoritmos de aprendizaje automático. Pero, ¿es esta matemática realmente necesaria, ya que muchas bibliotecas estándar como scikit, tensorflow, keras, etc. ya tienen todas estas cosas implementadas? Pero los científicos de datos experimentados dicen que deberíamos tener una buena comprensión de las matemáticas detrás de los algoritmos. Alrededor de **95%** de los científicos de datos dicen que las estadísticas son un activo importante en Data Science\n\n## Utilidad de la plataforma de aprendizaje"
"Los gráficos de anillos anteriores muestran la opinión de los científicos de datos sobre las distintas plataformas para aprender la ciencia de datos. La trama se ve mejor para **Proyectos**, donde el porcentaje no es útil casi **0%**. Según mi opinión personal, los proyectos son la mejor plataforma o forma de aprender algo en la industria de TI. Las otras plataformas excelentes son **Cursos en línea y Kaggle**. Los gráficos para otras plataformas son bastante similares entre sí.\n\n## ¿Qué debe tener el curriculum vitae?"
"Es evidente que la experiencia laboral en proyectos ML y en las competiciones Kaggle refleja el conocimiento de Data Science. También un rango de Kaggle puede ser algo bueno en el currículum. Como mencioné anteriormente, la experiencia laboral relevante podría tener un valor mayor en comparación con cualquier título de maestría o doctorado. Por lo tanto, esta afirmación es válida, ya que los científicos de datos prefieren la experiencia laboral sobre el grado, como se ve en el gráfico anterior.\n\n\n## ¿Cómo buscaron empleo?"
"Muchos científicos de datos conocen los trabajos a través de sus amigos o familiares o fueron contactados directamente por la compañía. Por lo tanto, debemos mantener adecuadamente nuestros perfiles profesionales como Linkedin y seguir actualizándolos, ya que dichos sitios de redes podrían ayudarlo a conseguir el trabajo de sus sueños."
"## Comprobando las respuestas libres\n\nEste archivo contiene las respuestas de forma libre respondidas por los encuestados. El problema con este es que al ser una respuesta de forma libre, cada usuario responderá a su manera. Lo que quiero decir es que tendremos diferentes respuestas para la misma cosa. Un ejemplo de esto que observé es la biblioteca **Pandas está escrito como pandas, Pandas, panda y en muchas formas diferentes.** Por lo tanto, trataré de analizar este archivo utilizando **nltk (Natural Language Toolkit).**"
## Motivación detrás de trabajar en Kaggle
"### Vamos a flexibilizar nuestra fuerza en la ciencia de datos ...\n\nEl wordcloud muestra la motivación de los usuarios para trabajar en kaggle. Claramente, el aprendizaje de la ciencia de datos, el aprendizaje automático, el interés en el mismo, la curiosidad, la diversión y la recuperación de conjuntos de datos son algunos de los más relevantes.\n\n## Bibliotecas más utilizadas"
"Una breve información sobre las bibliotecas:\n\n### Python:\n\n1) **Sklearn** - Para algoritmos de aprendizaje automático. Esta biblioteca tiene casi todos los algoritmos importantes de aprendizaje automático utilizados para las industrias.\n\n2) **Pandas, Matlotlib y Seaborn** Generalmente se usan juntos para el trabajo de análisis y visualización.\n\n3) **TensorFlow y Keras** Usado para Deep Learning\n\n4) **Numpy and Scipy** Usado para cálculos científicos.\n\n5) **nltk** Se utiliza para el procesamiento de lenguaje natural.\n\n### R:\n\n1) **dply** dplyr es el paquete para la manipulación rápida de datos.\n\n2) **ggplot2 and shiny** El famoso paquete de R para hacer hermosos gráficos. Los efectos visuales de Python no se parecen en nada a la calidad de los elementos visuales creados con esta biblioteca.\n\n3) **Caret and randomforest** Para fines de aprendizaje automático.\n\n4) **tidyr** Herramientas para cambiar el diseño de sus conjuntos de datos.\n\n5) **stringr** Herramientas fáciles de aprender para expresiones regulares y cadenas de caracteres.\n\n**Los prospectos (folium en Python) y Plotly son bibliotecas comunes en ambos idiomas, y se utilizan para crear gráficos interactivos como mapas geográficos, etc.**"
- Selecting the features from the above conducted tests and splitting the data into **75 - 25 train - test** groups.
"### **Continuous Features**\nBoth of the continuous features (`Age` and `Fare`) have good split points and spikes for a decision tree to learn. One potential problem for both features is, the distribution has more spikes and bumps in training set, but it is smoother in test set. Model may not be able to generalize to test set because of this reason.\n\n* Distribution of `Age` feature clearly shows that children younger than 15 has a higher survival rate than any of the other age groups\n* In distribution of `Fare` feature, the survival rate is higher on distribution tails. The distribution also has positive skew because of the extremely large outliers"
#### **Categorical Features**\nEvery categorical feature has at least one class with high mortality rate. Those classes are very helpful to predict whether the passenger is a survivor or victim. Best categorical features are `Pclass` and `Sex` because they have the most homogenous distributions.\n\n* Passengers boarded from **Southampton** has a lower survival rate unlike other ports. More than half of the passengers boarded from **Cherbourg** had survived. This observation could be related to `Pclass` feature\n* `Parch` and `SibSp` features show that passengers with only one family member has a higher survival rate
"### **Conclusion(EDA)**\nMost of the features are correlated with each other. This relationship can be used to create new features with feature transformation and feature interaction. Target encoding could be very useful as well because of the high correlations with `Survived` feature.\n\nSplit points and spikes are visible in continuous features. They can be captured easily with a decision tree model, but linear models may not be able to spot them.\n\nCategorical features have very distinct distributions with different survival rates. Those features can be one-hot encoded. Some of those features may be combined with each other to make new features.\n\nCreated a new feature called `Deck` and dropped `Cabin` feature at the **Exploratory Data Analysis** part."
# Correlation Between The Features
"### Interpreting The Heatmap\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\n**POSITIVE CORRELATION:** If an **increase in feature A leads to increase in feature B, then they are positively correlated**. A value **1 means perfect positive correlation**.\n\n**NEGATIVE CORRELATION:** If an **increase in feature A leads to decrease in feature B, then they are negatively correlated**. A value **-1 means perfect negative correlation**.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as **MultiColinearity** as both of them contains almost the same information.\n\nSo do you think we should use both of them as **one of them is redundant**. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between **SibSp and Parch i.e 0.37**. So we can carry on with all features."
"### **Frequency Encoding**\n`Family_Size` is created by adding `SibSp`, `Parch` and **1**. `SibSp` is the count of siblings and spouse, and `Parch` is the count of parents and children. Those columns are added in order to find the total size of families. Adding **1** at the end, is the current passenger. Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates.\n* Family Size with **1** are labeled as **Alone**\n* Family Size with **2**, **3** and **4** are labeled as **Small**\n* Family Size with **5** and **6** are labeled as **Medium**\n* Family Size with **7**, **8** and **11** are labeled as **Large**"
### **Ticket**
#### ROC AUC Curve\n\n**[What is an ROC AUC Curve ?](https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/)**
"The red line in the middel represents a purely random classifier (e.g a coin flip) and therefore your classifier should be as far away from it as possible. Our Random Forest model seems to do a good job.\nOf course we also have a tradeoff here, because the classifier produces more false positives, the higher the true positive rate is."
"\n\n Positive skewness and high kurtosis\n    \n    \n    \n    \n* Positive skewness, more weight is on the left side of the distribution.\n    \n* Kurtosis is greater than 3. It is leptokurtic."
"\n\n What the graph shows?\n    \n    \n    \n    \nWe see positive skewness from the graph above. As the graphs shows, more weight is on the left side of the distribution. We will try to fix it using ""log1p"" function of numpy."
- It is a huge matrix with too many features. We will check the correlation only with respect to **HeartDisease**. 
"- Except for **RestingBP** and **RestingECG**, everyone displays a positive or negative relationship with **HeartDisease**."
- We will leave out **RestingBP** from the modeling part and take the remaining features.
#### 1] Logistic Regression :
