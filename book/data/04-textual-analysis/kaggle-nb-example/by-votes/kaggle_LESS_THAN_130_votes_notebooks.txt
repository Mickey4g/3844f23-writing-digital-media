 \n ## 2- Load packages\n   \n## 2-1 Import
 \n##  2-2 Setup
 \n##  2-2 Setup
 \n## 2-3 Version\n
 \n## 4-2-2 Mean Frequency
 \n## 4-2-3 countplot
 \n## 4-2-3 countplot
" \n## 4-2-4 hist\nIf you check histogram for all feature, you will find that most of them are so similar"
 \n## 4-2-6 distplot\n The target in data set is **imbalance**
 \n## 4-2-7 violinplot
" \n## 5-5  Partial Dependence Plot\nIn this section, we see the impact of the main variables discovered in the previous sections by using the [pdpbox](https://pdpbox.readthedocs.io/en/latest/)."
 \n## 5-6 Chart analysis\n1. The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n1. A blue shaded area indicates level of confidence
#### Elbow Curve to get the right number of Clusters\nA fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.
"### Silhouette Analysis\n\n$$\text{silhouette score}=\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster."
"**Single Linkage:**\n\nIn single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two closest points.\n![](https://www.saedsayad.com/images/Clustering_single.png)"
"**Complete Linkage**\n\nIn complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points. \n![](https://www.saedsayad.com/images/Clustering_complete.png)"
"**Complete Linkage**\n\nIn complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points. \n![](https://www.saedsayad.com/images/Clustering_complete.png)"
"**Average Linkage:**\n\nIn average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters “r” and “s” to the left is equal to the average length each arrow between connecting the points of one cluster to the other.\n![](https://www.saedsayad.com/images/Clustering_average.png)"
"**Average Linkage:**\n\nIn average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters “r” and “s” to the left is equal to the average length each arrow between connecting the points of one cluster to the other.\n![](https://www.saedsayad.com/images/Clustering_average.png)"
#### Cutting the Dendrogram based on K
"We can see that there are leading spaces (spaces at the start of the string name) in the dataframe. So, I will remove these leading spaces."
I have removed the leading spaces from the column names. Let's again view the column names to confirm the same.
"On closer inspection, we can suspect that all the continuous variables may contain outliers.\n\n\nI will draw boxplots to visualise outliers in the above variables. "
The above boxplots confirm that there are lot of outliers in these variables.
"### Check the distribution of variables\n\n\nNow, I will plot the histograms to check distributions to find out if they are normal or skewed. "
We can see that all the 8 continuous variables are skewed. 
"The confusion matrix shows `3289 + 230 = 3519 correct predictions` and `17 + 44 = 61 incorrect predictions`.\n\n\nIn this case, we have\n\n\n- `True Positives` (Actual Positive:1 and Predict Positive:1) - 3289\n\n\n- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 230\n\n\n- `False Positives` (Actual Negative:0 but Predict Positive:1) - 17 `(Type I error)`\n\n\n- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 44 `(Type II error)`"
# **17. Classification metrices** \n\n[Table of Contents](#0.1)
"# **18. ROC - AUC** \n\n[Table of Contents](#0.1)\n\n\n\n### ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of `TP to (TP + FN)`.\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of `FP to (FP + TN)`.\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n"
ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.
# Get our environment set up\n________\n\nThe first thing we'll need to do is load in the libraries and datasets we'll be using. \n\n> **Important!** Make sure you run this cell yourself or the rest of your code won't work!
"Now that we're set up, let's learn about scaling & normalization. (If you like, you can take this opportunity to take a look at some of the data.)"
"# Scaling vs. Normalization: What's the difference?\n____\n\nOne of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that, in scaling, you're changing the *range* of your data while in normalization you're changing the *shape of the distribution* of your data. Let's talk a little more in-depth about each of these options. \n\n___\n\n## **Scaling**\n\nThis means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1.  You want to scale data when you're using methods based on measures of how far apart data points, like [support vector machines, or SVM](https://en.wikipedia.org/wiki/Support_vector_machine) or [k-nearest neighbors, or KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). With these algorithms, a change of ""1"" in any numeric feature is given the same importance. \n\nFor example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n\nBy scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in just a second, this is just to help illustrate my point.)\n"
"Notice that the *shape* of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n\n___\n## Normalization\n\nScaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n\n> **[Normal distribution:](https://en.wikipedia.org/wiki/Normal_distribution)** Also known as the ""bell curve"", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n\nIn general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with ""Gaussian"" in the name probably assumes normality.)\n\nThe method were  using to normalize here is called the [Box-Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Let's take a quick peek at what normalizing some data looks like:"
"Notice that the *shape* of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n\n___\n## Normalization\n\nScaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n\n> **[Normal distribution:](https://en.wikipedia.org/wiki/Normal_distribution)** Also known as the ""bell curve"", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n\nIn general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with ""Gaussian"" in the name probably assumes normality.)\n\nThe method were  using to normalize here is called the [Box-Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Let's take a quick peek at what normalizing some data looks like:"
"Notice that the *shape* of our data has changed. Before normalizing it was almost L-shaped. But after normalizing it looks more like the outline of a bell (hence ""bell curve""). \n\n___\n## Your turn!\n\nFor the following example, decide whether scaling or normalization makes more sense. \n\n* You want to build a linear regression model to predict someone's grades given how much time they spend on various activities during a normal school week.  You notice that your measurements for how much time students spend studying aren't normally distributed: some students spend almost no time studying and others study for four or more hours every day. Should you scale or normalize this variable?\n* You're still working on your grades study, but you want to include information on how students perform on several fitness tests as well. You have information on how many jumping jacks and push-ups each student can complete in a minute. However, you notice that students perform far more jumping jacks than push-ups: the average for the former is 40, and for the latter only 10. Should you scale or normalize these variables?"
"# Practice scaling\n___\n\nTo practice scaling and normalization, we're going to be using a dataset of Kickstarter campaigns. (Kickstarter is a website where people can ask people to invest in various projects and concept products.)\n\nLet's start by scaling the goals of each campaign, which is how much money they were asking for."
You can see that scaling changed the scales of the plots dramatically (but not the shape of the data: it looks like most campaigns have small goals but a few have very large ones)
"# Practice normalization\n___\n\nOk, now let's try practicing normalization. We're going to normalize the amount of money pledged to each campaign."
It's not perfect (it looks like a lot pledges got very few pledges) but it is much closer to normal!
"## 1. Introduction\n\nThis is my first kernel at Kaggle. I choosed the Titanic competition which is a good way to introduce feature engineering and ensemble modeling. Firstly, I will display some feature analyses then ill focus on the feature engineering. Last part concerns modeling and predicting the survival on the Titanic using an voting procedure. \n\nThis script follows three main parts:\n\n* **Feature analysis**\n* **Feature engineering**\n* **Modeling**"
## 2. Load and check data\n### 2.1 Load data
## 3. Feature analysis\n### 3.1 Numerical values
"Only Fare feature seems to have a significative correlation with the survival probability.\n\nIt doesn't mean that the other features are not usefull. Subpopulations in these features can be correlated with the survival. To determine this, we need to explore in detail these features"
#### SibSP
"It seems that passengers having a lot of siblings/spouses have less chance to survive\n\nSingle passengers (0 SibSP) or with two other persons (SibSP 1 or 2) have more chance to survive\n\nThis observation is quite interesting, we can consider a new feature describing these categories (See feature engineering)"
#### Parch
"Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6 ).\n\nBe carefull there is an important standard deviation in the survival of passengers with 3 parents/children "
#### Age
"Age distribution seems to be a tailed distribution, maybe a gaussian distribution.\n\nWe notice that age distributions are not the same in the survived and not survived subpopulations. Indeed, there is a peak corresponding to young passengers, that have survived. We also see that passengers between 60-80 have less survived. \n\nSo, even if ""Age"" is not correlated with ""Survived"", we can see that there is age categories of passengers that of have more or less chance to survive.\n\nIt seems that very young passengers have more chance to survive."
"Age distribution seems to be a tailed distribution, maybe a gaussian distribution.\n\nWe notice that age distributions are not the same in the survived and not survived subpopulations. Indeed, there is a peak corresponding to young passengers, that have survived. We also see that passengers between 60-80 have less survived. \n\nSo, even if ""Age"" is not correlated with ""Survived"", we can see that there is age categories of passengers that of have more or less chance to survive.\n\nIt seems that very young passengers have more chance to survive."
"When we superimpose the two densities , we cleary see a peak correponsing (between 0 and 5) to babies and very young childrens."
"Since we have one missing value , i decided to fill it with the median value which will not have an important effect on the prediction."
"As we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled. \n\nIn this case, it is better to transform it with the log function to reduce this skew. "
"Since we have two missing values , i decided to fill them with the most fequent value of ""Embarked"" (S)."
"It seems that passenger coming from Cherbourg (C) have more chance to survive.\n\nMy hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown (Q), Southampton (S).\n\nLet's see the Pclass distribution vs Embarked"
"It seems that passenger coming from Cherbourg (C) have more chance to survive.\n\nMy hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown (Q), Southampton (S).\n\nLet's see the Pclass distribution vs Embarked"
"Indeed, the third class is the most frequent for passenger coming from Southampton (S) and Queenstown (Q), whereas Cherbourg passengers are mostly in first class which have the highest survival rate.\n\nAt this point, i can't explain why first class has an higher survival rate. My hypothesis is that first class passengers were prioritised during the evacuation due to their influence."
"## 4. Filling missing Values\n### 4.1 Age\n\nAs we see, Age column contains 256 missing values in the whole dataset.\n\nSince there is subpopulations that have more chance to survive (children for example), it is preferable to keep the age feature and to impute the missing values. \n\nTo adress this problem, i looked at the most correlated features with Age (Sex, Parch , Pclass and SibSP)."
"Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n\nHowever, 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers.\n\nMoreover, the more a passenger has parents/children the older he is and the more a passenger has siblings/spouses the younger he is."
#### 6.1.3 Plot learning curves\n\nLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.
GradientBoosting and Adaboost classifiers tend to overfit the training set. According to the growing cross-validation curves GradientBoosting and Adaboost could perform better with more training examples.\n\nSVC and ExtraTrees classifiers seem to better generalize the prediction since the training and cross-validation curves are close together.
"#### 6.1.4 Feature importance of tree based classifiers\n\nIn order to see the most informative features for the prediction of passengers survival, i displayed the feature importance for the 4 tree based classifiers."
"I plot the feature importance for the 4 tree based classifiers (Adaboost, ExtraTrees, RandomForest and GradientBoosting).\n\nWe note that the four classifiers have different top features according to the relative importance. It means that their predictions are not based on the same features. Nevertheless, they share some common important features for the classification , for example 'Fare', 'Title_2', 'Age' and 'Sex'.\n\nTitle_2 which indicates the Mrs/Mlle/Mme/Miss/Ms category is highly correlated with Sex.\n\nWe can say that: \n\n- Pc_1, Pc_2, Pc_3 and Fare refer to the general social standing of passengers.\n\n- Sex and Title_2 (Mrs/Mlle/Mme/Miss/Ms) and Title_3 (Mr) refer to the gender.\n\n- Age and Title_1 (Master) refer to the age of passengers.\n\n- Fsize, LargeF, MedF, Single refer to the size of the passenger family.\n\n**According to the feature importance of this 4 classifiers, the prediction of the survival seems to be more associated with the Age, the Sex, the family size and the social standing of the passengers more than the location in the boat.**"
"I plot the feature importance for the 4 tree based classifiers (Adaboost, ExtraTrees, RandomForest and GradientBoosting).\n\nWe note that the four classifiers have different top features according to the relative importance. It means that their predictions are not based on the same features. Nevertheless, they share some common important features for the classification , for example 'Fare', 'Title_2', 'Age' and 'Sex'.\n\nTitle_2 which indicates the Mrs/Mlle/Mme/Miss/Ms category is highly correlated with Sex.\n\nWe can say that: \n\n- Pc_1, Pc_2, Pc_3 and Fare refer to the general social standing of passengers.\n\n- Sex and Title_2 (Mrs/Mlle/Mme/Miss/Ms) and Title_3 (Mr) refer to the gender.\n\n- Age and Title_1 (Master) refer to the age of passengers.\n\n- Fsize, LargeF, MedF, Single refer to the size of the passenger family.\n\n**According to the feature importance of this 4 classifiers, the prediction of the survival seems to be more associated with the Age, the Sex, the family size and the social standing of the passengers more than the location in the boat.**"
The prediction seems to be quite similar for the 5 classifiers except when Adaboost is compared to the others classifiers.\n\nThe 5 classifiers give more or less the same prediction but there is some differences. Theses differences between the 5 classifier predictions are sufficient to consider an ensembling vote. 
"## Introduction ##\n\nThis is my first work of machine learning. the notebook is written in python and has inspired from [""Exploring Survival on Titanic"" by Megan Risdal, a Kernel in R on Kaggle][1].\n\n\n  [1]: https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic"
# Feature Engineering #
# Classifier Comparison #
# Prediction #\nnow we can use SVC classifier to predict our data.
#### Active Cases = Number of Confirmed Cases - Number of Recovered Cases - Number of Death Cases\n#### Increase in number of Active Cases is probably an indication of Recovered case or Death case number is dropping in comparison to number of Confirmed Cases drastically. Will look for the conclusive evidence for the same in the notebook ahead.
#### Closed Cases = Number of Recovered Cases + Number of Death Cases \n#### Increase in number of Closed classes imply either more patients are getting recovered from the disease or more pepole are dying because of COVID-19
"#### Growth rate of Confirmed, Recovered and Death Cases "
#### Moratality and Recovery Rate analysis around the World
#### Moratality and Recovery Rate analysis around the World
"#### Mortality rate = (Number of Death Cases / Number of Confirmed Cases) x 100\n#### Recovery Rate= (Number of Recoverd Cases / Number of Confirmed Cases) x 100\n#### Mortality rate is showing a considerable for a pretty long time, which is positive sign\n#### Recovery Rate has started to pick up again which is a good sign, another supportive reason to why number of Closed Cases are increasing"
"### Growth Factor\nGrowth factor is the factor by which a quantity multiplies itself over time. The formula used is:\n\n**Formula: Every day's new (Confirmed,Recovered,Deaths) / new (Confirmed,Recovered,Deaths) on the previous day.**\n\nA growth factor **above 1 indicates an increase correspoding cases**.\n\nA growth factor **above 1 but trending downward** is a positive sign, whereas a **growth factor constantly above 1 is the sign of exponential growth**.\n\nA growth factor **constant at 1 indicates there is no change in any kind of cases**."
#### Growth Factor for Active and Closed Cases\nGrowth factor is the factor by which a quantity multiplies itself over time. The formula used is:\n\n**Formula: Every day's new (Active and Closed Cases) / new (Active and Closed Cases) on the previous day.**\n\nA growth factor **above 1 indicates an increase correspoding cases.**\n\nA growth factor **above 1 but trending downward is a positive sign.**\n\nA growth factor **constant at 1 indicates there is no change in any kind of cases.**\n\nA growth factor **below 1 indicates real positive sign implying more patients are getting recovered or dying as compared to the Confirmed Cases.**
#### Growth Factor for Active and Closed Cases\nGrowth factor is the factor by which a quantity multiplies itself over time. The formula used is:\n\n**Formula: Every day's new (Active and Closed Cases) / new (Active and Closed Cases) on the previous day.**\n\nA growth factor **above 1 indicates an increase correspoding cases.**\n\nA growth factor **above 1 but trending downward is a positive sign.**\n\nA growth factor **constant at 1 indicates there is no change in any kind of cases.**\n\nA growth factor **below 1 indicates real positive sign implying more patients are getting recovered or dying as compared to the Confirmed Cases.**
#### Growth Factor constantly above 1 is an clear indication of Exponential increase in all form of cases.
#### Mainland China has recorded highest number of Closed cases as thier Recovery Rate is staggering recording 85%+\n#### Confirmed Cases/Day is clear indication of why US has highest number of Active Cases currently. The rate is 11000+ cases per day. Showing increase in that value every day.
"#### Survival Probability is the only graph that looks the most promising! Having average survival probability of 95%+ across all countries. The difference between Mean and Median Death Probability is an clear indication that there few countries with really high mortality rate e.g. Italy, Algeria, UK etc."
"#### When we see daily news reports on COVID-19 it's really hard to interpret what's actually happening, since the numbers are changing so rapidly but that's something expected from Exponential growth. Since almost all the pandemics tend to grow exponentially it's really hard to understand for someone from a non-mathematical or non-statistical background.\n\n#### We are more concerned about how we are doing and where we are heading in this pandemic rather than just looking at those exponentially growing numbers. The growth won't be exponentially forever, at some point of time the curve will become flat because probably all the people on the planet are infected or we human have been able to control the disease.\n\n#### When we are in the middle of the exponential growth it's almost impossible to tell where are we heading.\nHere, I am trying to show how we can interpret the exponential growth which is the common trend among all the countries\n\nReferences:\nHow To Tell If We're Beating COVID-19: https://www.youtube.com/watch?v=54XLXg4fYsc\n\nExponential growth and epidemics: https://www.youtube.com/watch?v=Kas0tIxDvrg"
"It's pretty evident that the disease is spreading in same manner everywhere, but if particular country is following pandemic controlling practices rigrously the results are evident in the graph.\n\nMost of the countries will follow the same trajectory as that USA, which is **""Uncontrolled Exponential Growth""** \n\nThere are few countries where the pandemic controlling practices seems to be working accurately, few classic examples are China, Germany, Italy, Spain, Turkey has started showing that dip indicating there are somehow got control over COVID-19\n\nCountries like United Kingdom, Russia are following similar lines as that of United States, indicating the growth is still exponential among those countries.\n\nIran is showing some occasional drops."
"#### Comparison of Daily Increase in Number of Cases of Italy, Spain, USA and India, where maximum number of Confirmed Cases are equivalent to maximum number of Confirmed Cases in India"
#### Videos related to COVID-19 Pandemic in India\nWuhan Coronavirus: WION breaks down the growing numbers | Gravitas: \nhttps://www.youtube.com/watch?v=xqAPDD8sw-g
"## 1) Import Necessary Libraries\nFirst off, we need to import several Python libraries such as numpy, pandas, matplotlib and seaborn."
"## 2) Read in and Explore the Data \nIt's time to read in our training and testing data using `pd.read_csv`, and take a first look at the training data using the `describe()` function."
### Sex Feature
"As predicted, females have a much higher chance of survival than males. The Sex feature is essential in our predictions."
### Pclass Feature
"As predicted, people with higher socioeconomic class had a higher rate of survival. (62.9% vs. 47.3% vs. 24.2%)"
### SibSp Feature
"In general, it's clear that people with more siblings or spouses aboard were less likely to survive. However, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)"
### Parch Feature
"People with less than four parents or children aboard are more likely to survive than those with four or more. Again, people traveling alone are less likely to survive than those with 1-3 parents or children."
### Age Feature
Babies are more likely to survive than any other age group. 
"### Cabin Feature\nI think the idea here is that people with recorded cabin numbers are of higher socioeconomic class, and thus more likely to survive. Thanks for the tips, [@salvus82](https://www.kaggle.com/salvus82) and [Daniel Ellis](https://www.kaggle.com/dellis83)!"
"People with a recorded Cabin number are, in fact, more likely to survive. (66.6% vs 29.9%)"
"American Express Default Prediction\n\n# 1 | Competition Objective\nWhether out at a restaurant or buying tickets to a concert, modern life counts on the convenience of a credit card to make daily purchases. It saves us from carrying large amounts of cash and also can advance a full purchase that can be paid over time. How do card issuers know we'll pay back what we charge? That's a complex problem with many existing solutions—and even more potential improvements, to be explored in this competition.\n\nCredit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics. Current models exist to help manage risk. But it's possible to create better models that can outperform those currently in use.\n\nAmerican Express is a globally integrated payments company. The largest payment card issuer in the world, they provide customers with access to products, insights, and experiences that enrich lives and build business success.\n\nThe objective of [this competition](https://www.kaggle.com/competitions/amex-default-prediction) is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. In this competition, you'll apply your machine learning skills to predict credit default. Specifically, you will leverage an industrial scale data set to build a machine learning model that challenges the current model in production. Training, validation, and testing datasets include time-series behavioral data and anonymized customer profile information. You're free to explore any technique to create the most powerful model, from creating features to using the data in a more organic way within a model.\n\nIf successful, you'll help create a better customer experience for cardholders by making it easier to be approved for a credit card. Top solutions could challenge the credit default prediction model used by the world's largest payment card issuer—earning you cash prizes, the opportunity to interview with American Express, and potentially a rewarding new career.\n\n# 2 | Data Overview\nThe target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n\nThe dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:  \n**`D_*`:** Delinquency variables  \n**`S_*`:** Spend variables  \n**`P_*`:** Payment variables  \n**`B_*`:** Balance variables  \n**`R_*`:** Risk variables  \nWith the following features being categorical: `B_30`, `B_38`, `D_63`, `D_64`, `D_66`, `D_68`, `D_114`, `D_116`, `D_117`, `D_120`, `D_126`. \n\nThere are a total of 190 variables in the dataset with approximately 450,000 customers in the training set and 925,000 in the test set. Due to the dataset size, I will use the compressed version of the train and test sets provided by @munumbutt's [AMEX-Feather-Dataset](https://www.kaggle.com/datasets/munumbutt/amexfeather) and take the last statement for each customer."
"In the training set, the last statement of all customers was in March 2018, while in the test set the date of customers' last statements range from April through October 2019. "
"About 25% of customers in the training data have defaulted. This proportion is consistent across each day in the training set, with a weekly seasonal trend in the day of the month when customers receive their statements."
# 3.1 EDA of Delinquency Variables
"There are several highly correlated Delinquency variables, with a few pairs perfectly positively correlated at 1.0. There are also a number of missing correlations, particularly in `Delinquency 87`, due to null values in the data. Below are the relationships between some of the most correlated Delinquency variables. "
# 3.2 EDA of Spend Variables
"# 4.1 Feature Importance\nAmong the top 50 features, `Payment 2` has the highest average importance, which is also the feature that is the most negatively correlated with the target variable."
# 5 | Submission
"#### Imports\n\nWe'll use a familiar stack of data science libraries: `Pandas`, `numpy`, `matplotlib`, `seaborn`, and eventually `sklearn` for modeling. "
### Read in Data and Look at Summary Information
"#### Integer Columns\n\nLet's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot."
"The columns with only 2 unique values represent Booleans (0 or 1). In a lot of cases, this boolean information is already on a household level. For example, the `refrig` column says whether or not the household has a refrigerator. When it comes time to make features from the Boolean columns that are on the household level, we will _not need to aggregate_ these. However, the Boolean columns that are on the individual level will need to be aggregated. \n\n#### Float Columns\n\nAnother column type is floats which represent continuous variables. We can make a quick distribution plot to show the distribution of all float columns. We'll use an [`OrderedDict`](https://pymotw.com/2/collections/ordereddict.html) to map the poverty levels to colors because this keeps the keys and values in the same order as we specify (unlike a regular Python dictionary).\n\nThe following graphs shows the distributions of the `float` columns colored by the value of the `Target`. With these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level."
"The columns with only 2 unique values represent Booleans (0 or 1). In a lot of cases, this boolean information is already on a household level. For example, the `refrig` column says whether or not the household has a refrigerator. When it comes time to make features from the Boolean columns that are on the household level, we will _not need to aggregate_ these. However, the Boolean columns that are on the individual level will need to be aggregated. \n\n#### Float Columns\n\nAnother column type is floats which represent continuous variables. We can make a quick distribution plot to show the distribution of all float columns. We'll use an [`OrderedDict`](https://pymotw.com/2/collections/ordereddict.html) to map the poverty levels to colors because this keeps the keys and values in the same order as we specify (unlike a regular Python dictionary).\n\nThe following graphs shows the distributions of the `float` columns colored by the value of the `Target`. With these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level."
"Later on we'll calculate correlations between the variables and the `Target` to gauge the relationships between the features, but these plots can already give us a sense of which variables may be most ""relevant"" to a model. For example, the `meaneduc`, representing the average education of the adults in the household appears to be related to the poverty level: __a higher average adult education leads to higher values of the target which are less severe levels of poverty__. The theme of the importance of education is one we will come back to again and again in this notebook! "
"## Exploring Label Distribution\n\nNext, we can get an idea of how imbalanced the problem is by looking at the distribution of labels. There are four possible integer levels, indicating four different levels of poverty. To look at the correct labels, we'll subset only to the columns where `parentesco1 == 1` because this is the head of household, the correct label for each household.\n\nThe bar plot below shows the distribution of training labels (since there are no testing labels)."
"We are dealing with an imbalanced class problem (which makes it intriguing why the contest organizers choose the _macro_ F1 score as the metric instead of _weighted_ F1!). There are many more households that classify as _non vulnerable_ than in any other category. The _extreme_ poverty class is the smallest (I guess this should make us optimistic!).\n\nOne problem with imbalanced classification problems is that the machine learning model can have a difficult time predicting the minority classes because it sees far less examples. Think about this in human terms: if we are classifiying poverty and we see far more cases of no poverty than extreme poverty, it will make it more difficult for us to identify the high poverty households because of less exposure. One potential method to address class imbalanceds is through oversampling  (which is covered in more advanced notebooks)."
"In addition to looking at the missing values of the monthly rent payment, it will be interesting to also look at the distribution of `tipovivi_`, the columns showing the ownership/renting status of the home. For this plot, we show the ownership status of those homes with a `nan` for the monthyl rent payment."
"The meaning of the home ownership variables is below:\n\n    tipovivi1, =1 own and fully paid house\n    tipovivi2, ""=1 own,  paying in installments""\n    tipovivi3, =1 rented\n    tipovivi4, =1 precarious\n    tipovivi5, ""=1 other(assigned,  borrowed)""\n    \nWe've solved the issue! Well, mostly: the households that do not have a monthly rent payment generally own their own home. In a few other situations, we are not sure of the reason for the missing information. \n\nFor the houses that are owned and have a missing monthly rent payment, we can set the value of the rent payment to zero. For the other homes, we can leave the missing values to be imputed but we'll add a flag (Boolean) column indicating that these households had missing values."
"#### Squared Variables\n\nFirst, the easiest step: we'll remove all of the squared variables. Sometimes variables are squared or transformed as part of feature engineering because it can help linear models learn relationships that are non-linear. However, since we will be using more complex models, these squared features are redundant. They are highly correlated with the non-squared version, and hence can actually hurt our model by adding irrelevant information and also slowing down training.\n\nFor an example, let's take a look at `SQBage` vs `age`."
"These variables are highly correlated, and we don't need to keep both in our data."
"The final household feature we can make for now is a `bonus` where a family gets a point for having a refrigerator, computer, tablet, or television."
## Per Capita Features\n\nAdditional features we can make calculate the number of certain measurements for each person in the household.
The largest discrepancy in the correlations is `dependency`. We can make a scatterplot of the `Target` versus the `dependency` to visualize the relationship. We'll add a little jitter to the plot because these are both discrete variables.
"It's hard to see the relationship, but it's slightly negative: as the `dependency` increases, the value of the `Target` decreases. This makes sense: the `dependency` is the number of dependent individuals divided by the number of non-dependents. As we increase this value, the poverty severty tends to increase: having more dependent family members (who usually are non-working) leads to higher levels of poverty because they must be supported by the non-dependent family members. "
"It's hard to see the relationship, but it's slightly negative: as the `dependency` increases, the value of the `Target` decreases. This makes sense: the `dependency` is the number of dependent individuals divided by the number of non-dependents. As we increase this value, the poverty severty tends to increase: having more dependent family members (who usually are non-working) leads to higher levels of poverty because they must be supported by the non-dependent family members. "
"#### Correlation Heatmap \n\nOne of my favorite plots is the correlation heatmap because it shows a ton of info in one image. For the heatmap, we'll pick 7 variables and show the correlations between themselves and with the target. "
"#### Correlation Heatmap \n\nOne of my favorite plots is the correlation heatmap because it shows a ton of info in one image. For the heatmap, we'll pick 7 variables and show the correlations between themselves and with the target. "
"This plot shows us that there are a number of variables that have a weak correlation with the `Target`. There are also high correlations between some variables (such as `floor` and `walls+roof+floor`) which could pose an issue because of collinearity. \n\n### Features Plot\n\nFor the final exploration of the household level data, we can make a plot of some of the most correlated variables with the Target. This shows scatterplots on the upper triangle, kernel density estimate (kde) plots on the diagonal, and 2D KDE plots on the lower triangle."
"This plot shows us that there are a number of variables that have a weak correlation with the `Target`. There are also high correlations between some variables (such as `floor` and `walls+roof+floor`) which could pose an issue because of collinearity. \n\n### Features Plot\n\nFor the final exploration of the household level data, we can make a plot of some of the most correlated variables with the Target. This shows scatterplots on the upper triangle, kernel density estimate (kde) plots on the diagonal, and 2D KDE plots on the lower triangle."
"We'll leave the feature engineering of the household variables for now. Later, we can come back to this step if we are not pleased with the model performance."
"### Feature Construction\n\nWe can make a few features using the existing data. For example, we can divide the years of schooling by the age."
"We can also take our new variable, `inst`, and divide this by the age. The final variable we'll name `tech`: this represents the combination of tablet and mobile phones."
It looks like households where the head is female are slightly more likely to have a severe level of poverty.
We can also look at the difference in average education by whether or not the family has a female head of household.
We can also look at the difference in average education by whether or not the family has a female head of household.
"It looks like at every value of the `Target`, households with female heads have higher levels of education. Yet, we saw that overall, households with female heads are more likely to have severe poverty. "
We can investigate the object to see the training scores for each iteration. The following code will plot the validation scores versus the number of features for the training.
"We can see that the score improves as we add features up until 96 features. According to the selector, this is the optimal number of features.\n\nThe rankings of each feature can be found by inspecting the trained object. These represent essentially the importance of features averaged over the iterations. Features can share the same ranking, and only features with a rank of 1 are retained."
"For each fold, the `1, 2, 3, 4` columns represent the probability for each `Target`. The `Target` is the maximum of these with the `confidence` the probability. We have the predictions for all 5 folds, so we can plot the confidence in each `Target` for the different folds."
"What we see here is that the confidence for each class if relatively low. It does appear that the model has greater confidence in `Target=4` predictions which makes sense because of the _class imbalance and the high prevalence of this label._ \n\nAnother way to look at the information is as a `violinplot`. This shows the same information, with the number of observations related to the width of the plot."
"What we see here is that the confidence for each class if relatively low. It does appear that the model has greater confidence in `Target=4` predictions which makes sense because of the _class imbalance and the high prevalence of this label._ \n\nAnother way to look at the information is as a `violinplot`. This shows the same information, with the number of observations related to the width of the plot."
"Overall, these results show the issue with imbalanced class problems: our model cannot distinguish very well between the classes that are underrepresented. Later we'll look at predictions themselves and see where our model is ""confused"". For now, we can generate a submission file and submit it to the competition.\n\nWhen we actually make predictions for each household, we average the predictions from each of the folds. Therefore, we are essentially using multiple models since each one is trained on a slightly different fold of the data. The gradient boosting machine is already an ensemble machine learning model, and now we are using it almost as a meta-ensemble by averaging predictions from several gbms. \n\nThis process is shown in the code below."
"Overall, these results show the issue with imbalanced class problems: our model cannot distinguish very well between the classes that are underrepresented. Later we'll look at predictions themselves and see where our model is ""confused"". For now, we can generate a submission file and submit it to the competition.\n\nWhen we actually make predictions for each household, we average the predictions from each of the folds. Therefore, we are essentially using multiple models since each one is trained on a slightly different fold of the data. The gradient boosting machine is already an ensemble machine learning model, and now we are using it almost as a meta-ensemble by averaging predictions from several gbms. \n\nThis process is shown in the code below."
"We can have the function instead return the actual submission file. This takes the average predictions across the five folds, in effectm combining 5 different models, each one trained on a slghtly different subset of the data."
~20% of entries for passenger age are missing. Let's see what the 'Age' variable looks like in general.
"Since ""Age"" is (right) skewed, using the mean might give us biased results by filling in ages that are older than desired. To deal with this, we'll use the median to impute the missing values. "
\n## 3.1. Exploration of Age
"The age distribution for survivors and deceased is actually very similar.  One notable difference is that, of the survivors, a larger proportion were children.  The passengers evidently made an attempt to save children by giving them a place on the life rafts. "
"The age distribution for survivors and deceased is actually very similar.  One notable difference is that, of the survivors, a larger proportion were children.  The passengers evidently made an attempt to save children by giving them a place on the life rafts. "
"Considering the survival rate of passengers under 16, I'll also include another categorical variable in my dataset: ""Minor"""
\n## 3.2. Exploration of Fare
"As the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model.  Passengers who paid lower fare appear to have been less likely to survive.  This is probably strongly correlated with Passenger Class, which we'll look at next."
\n## 3.3. Exploration of Passenger Class
"Unsurprisingly, being a first class passenger was safest."
\n## 3.4. Exploration of Embarked Port
"Passengers who boarded in Cherbourg, France, appear to have the highest survival rate.  Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown.  This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck).  It's also worth noting the size of the whiskers in these plots.  Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest.  The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker.  It's possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts."
\n## 3.5. Exploration of Traveling Alone vs. With Family
"Individuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it's likely that individuals traveling alone were likely male."
\n## 3.6. Exploration of Gender Variable
This is a very obvious difference.  Clearly being female greatly increased your chances of survival.
\n### 4.1.2. Feature ranking with recursive feature elimination and cross-validation\n\nRFECV performs RFE in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation.
"As we see, eight variables were kept. "
"As we see, eight variables were kept. "
"\n## 4.2. Review of model evaluation procedures\n\nMotivation: Need a way to choose between machine learning models\n* Goal is to estimate likely performance of a model on out-of-sample data\n\nInitial idea: Train and test on the same data\n* But, maximizing training accuracy rewards overly complex models which overfit the training data\n\nAlternative idea: Train/test split\n* Split the dataset into two pieces, so that the model can be trained and tested on different data\n* Testing accuracy is a better estimate than training accuracy of out-of-sample performance\n* Problem with train/test split\n    * It provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy\n    * Testing accuracy can change a lot depending on a which observation happen to be in the testing set\n\nReference: \nhttp://www.ritchieng.com/machine-learning-cross-validation/ "
\n### 4.2.1. Model evaluation based on simple train/test split using `train_test_split()` function
\n### 4.2.2. Model evaluation based on K-fold cross-validation using `cross_val_score()` function 
\n## 4.3. GridSearchCV evaluating using multiple scorers simultaneously
"\n## 4.4. GridSearchCV evaluating using multiple scorers, RepeatedStratifiedKFold and pipeline for preprocessing simultaneously\n\nWe can applied many tasks together for more in-depth evaluation like gridsearch using cross-validation based on k-folds repeated many times, that can be scaled or no with respect to many scorers and tunning on parameter for a given estimator!  "
"As a first step, I load all the modules that will be used in this notebook:"
"Then, I load the data. Once done, I also give some basic informations on the content of the dataframe: the type of the various variables, the number of null values and their percentage with respect to the total number of entries:"
and show the result on a chloropleth map:
"We see that the dataset is largely dominated by orders made from the UK.\n\n___\n### 2.2 Customers and products\n\nThe dataframe contains $\sim$400,000 entries. What are the number of users and products in these entries ?"
"In order to have a global view of the type of order performed in this dataset, I determine how the purchases are divided according to total prizes:"
"It can be seen that the vast majority of orders concern relatively large purchases given that $\sim$65% of purchases give prizes in excess of £ 200.\n\n____\n## 3. Insight on product categories\n\nIn the dataframe, products are uniquely identified through the **StockCode** variable. A shrort description of the products is given in the **Description** variable. In this section, I intend to use the content of this latter variable in order to group the products into different categories.\n\n___\n### 3.1 Products Description\n\nAs a first step, I extract from the **Description** variable the information that will prove useful. To do this, I use the following function:"
"It can be seen that the vast majority of orders concern relatively large purchases given that $\sim$65% of purchases give prizes in excess of £ 200.\n\n____\n## 3. Insight on product categories\n\nIn the dataframe, products are uniquely identified through the **StockCode** variable. A shrort description of the products is given in the **Description** variable. In this section, I intend to use the content of this latter variable in order to group the products into different categories.\n\n___\n### 3.1 Products Description\n\nAs a first step, I extract from the **Description** variable the information that will prove useful. To do this, I use the following function:"
"This function takes as input the dataframe and analyzes the content of the **Description** column by performing the following operations:\n\n- extract the names (proper, common) appearing in the products description\n- for each name, I extract the root of the word and aggregate the set of names associated with this particular root\n- count the number of times each root appears in the dataframe\n- when several words are listed for the same root, I consider that the keyword associated with this root is the shortest name (this systematically selects the singular when there are singular/plural variants)\n\nThe first step of the analysis is to retrieve the list of products:"
"Using it, I create a representation of the most common keywords:"
___\n### 3.2 Defining product categories 
and I output the result as wordclouds:
"From this representation, we can see that for example, one of the clusters contains objects that could be associated with gifts (keywords: Christmas, packaging, card, ...). Another cluster would rather contain luxury items and jewelry (keywords: necklace, bracelet, lace, silver, ...). Nevertheless, it can also be observed that many words appear in various clusters and it is therefore difficult to clearly distinguish them.\n\n** c / _Principal Component Analysis_ **\n\nIn order to ensure that these clusters are truly distinct, I look at their composition. Given the large number of variables of the initial matrix, I first perform a PCA:"
and then check for the amount of variance explained by each component:
"We see that the number of components required to explain the data is extremely important: we need more than 100 components to explain 90% of the variance of the data. In practice, I decide to keep only a limited number of components since this decomposition is only performed to visualize the data:"
and I represent the amount of variance explained by each of the components:
___\n#### 4.2.2 Creation of customer categories
in order to create a representation of the various clusters:
"From this representation, it can be seen, for example, that the first principal component allow to separate the tiniest clusters from the rest. More generally, we see that there is always a representation in which two clusters will appear to be distinct.\n\n** b/ _Score de silhouette intra-cluster_ **\n\nAs with product categories, another way to look at the quality of the separation is to look at silouhette scores within different clusters:"
"** d / _Customers morphology_ **\n\nFinally, I created a representation of the different morphotypes. To do this, I define a class to create ""Radar Charts"" (which has been adapted from this [kernel](https://www.kaggle.com/yassineghouzam/don-t-know-why-employees-leave -read-this)):"
This allows to have a global view of the content of each cluster:
This allows to have a global view of the content of each cluster:
"It can be seen, for example, that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. Other clusters will differ from basket averages (** mean **), the total sum spent by the clients (** sum **) or the total number of visits made (** count **).\n\n____\n## 5. Classification of customers\n\nIn this part, the objective will be to adjust a classifier that will classify consumers in the different client categories that were established in the previous section. The objective is to make this classification possible at the first visit. To fulfill this objective, I will test several classifiers implemented in `scikit-learn`. First, in order to simplify their use, I define a class that allows to interface several of the functionalities common to these different classifiers: "
"___\n#### 5.1.1 Confusion matrix\n\nThe accuracy of the results seems to be correct. Nevertheless, let us remember that when the different classes were defined, there was an imbalance in size between the classes obtained. In particular, one class contains around 40% of the clients. It is therefore interesting to look at how the predictions and real values compare to the breasts of the different classes. This is the subject of the confusion matrices and to represent them, I use the code of the [sklearn documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html):"
from which I create the following representation:
from which I create the following representation:
"___\n#### 5.1.2 Learning curve\n\nA typical way to test the quality of a fit is to draw a learning curve. In particular, this type of curves allow to detect possible drawbacks in the model, linked for example to over- or under-fitting. This also shows to which extent the mode could benefit from a larger data sample. In order to draw this curve, I use the [scikit-learn documentation code again](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr- self-examples-model-selection-pad-learning-curve-py)"
"___\n#### 5.1.2 Learning curve\n\nA typical way to test the quality of a fit is to draw a learning curve. In particular, this type of curves allow to detect possible drawbacks in the model, linked for example to over- or under-fitting. This also shows to which extent the mode could benefit from a larger data sample. In order to draw this curve, I use the [scikit-learn documentation code again](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr- self-examples-model-selection-pad-learning-curve-py)"
from which I represent the leanring curve of the SVC classifier:
## Loading packages
## Loading data
### Categorical variables\nLet's look into the categorical variables and the proportion of customers with target = 1
"As we can see from the variables **with missing values**,  it is a good idea to keep the missing values as a separate category value, instead of replacing them by the mode for instance. The customers with a missing value appear to have a much higher (in some cases much lower) probability to ask for an insurance claim."
### Interval variables\nChecking the correlations between interval variables. A heatmap is a good way to visualize the correlation between variables. The code below is based on [an example by Michael Waskom](http://seaborn.pydata.org/examples/many_pairwise_correlations.html)
"There are a strong correlations between the variables:\n- ps_reg_02 and ps_reg_03 (0.7)\n- ps_car_12 and ps_car13 (0.67)\n- ps_car_12 and ps_car14 (0.58)\n- ps_car_13 and ps_car15 (0.67)\n\nSeaborn has some handy plots to visualize the (linear) relationship between variables. We could use a *pairplot* to visualize the relationship between the variables. But because the heatmap already showed the limited number of correlated variables, we'll look at each of the highly correlated variables separately.\n**NOTE**: I take a sample of the train data to speed up the process. "
"#### ps_reg_02 and ps_reg_03\nAs the regression line shows, there is a linear relationship between these variables. Thanks to the *hue* parameter we can see that the regression lines for target=0 and target=1 are the same."
#### ps_car_12 and ps_car_13
#### ps_car_12 and ps_car_13
#### ps_car_12 and ps_car_14
#### ps_car_12 and ps_car_14
#### ps_car_13 and ps_car_15
#### ps_car_13 and ps_car_15
"Allright, so now what? How can we decide which of the correlated variables to keep? We could perform Principal Component Analysis (PCA) on the variables to reduce the dimensions. In the AllState Claims Severity Competition I made [this kernel](https://www.kaggle.com/bertcarremans/reducing-number-of-numerical-features-with-pca) to do that. But as the number of correlated variables is rather low, we will let the model do the heavy-lifting."
"Next to create our wordclouds, I will import the python module ""wordcloud"". "
But generating a normal wordcloud is rather boring so I would like to introduce to you a technique of importing pictures (something relevant) and using the outline of that picture as a mask for our wordclouds. Therefore the pictures that I have chosen are the ones I feel most representative for their authors:\n\n1. ) The Raven for Edgar Allen Poe 2.) Octopus Cthulu-thingy for HP Lovecraft and 3.) Frankenstein for Mary Shelly\n\nThe way I am loading in the pictures on Kaggle is a sort of a feature hack although readers familiar to my work know this trick. I first derive the Base64 encoding of whatever images I want to use and then use that particular encoding and re-convert the picture back on the notebook. The cell below contains the Base64 encoding of the three images I will use but I have hidden them so that I do not pollute this notebook with just long streteches of text - unhide them if you want to see the encoding.
"And now we can use stemmer to see if it can reduce our these test words (""running"", ""runs"", ""run"") into their a single stemmed word. Conveniently we can test the stemmer on the fly as follows:"
"As we can see, the stemmer has successfully reduced the given words above into a base form and this will be most in helping us reduce the size of our dataset of words when we come to learning and classification tasks.\n\nHowever there is one flaw with stemming and that is the fact that the process involves quite a [crude heuristic in chopping off the ends of words](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) in the hope of reducing a particular word into a human recognizable base form. Therefore this process does not take into account vocabulary or word forms when collapsing words as this example will illustrate:"
"As we can see, the stemmer has successfully reduced the given words above into a base form and this will be most in helping us reduce the size of our dataset of words when we come to learning and classification tasks.\n\nHowever there is one flaw with stemming and that is the fact that the process involves quite a [crude heuristic in chopping off the ends of words](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) in the hope of reducing a particular word into a human recognizable base form. Therefore this process does not take into account vocabulary or word forms when collapsing words as this example will illustrate:"
"### Lemmatization to the rescue\n\nTherefore we turn to another that we could use in lieu of stemming. This method is called lemmatization which aims to achieve the same effect as the former method. However unlike a stemmer, lemmatizing the dataset aims to reduce words based on an actual dictionary or vocabulary (the Lemma) and therefore will not chop off words into stemmed forms that do not carry any lexical meaning. Here we can utilize NLTK once again to initialize a lemmatizer (WordNet variant) and inspect how it collapses words as follows:"
"**Revisiting our Term frequencies**\n\nHaving implemented our lemmatized count vectorizer, let us revist the plots for the term frquencies of the top 50 words (by frequency). As you can see from the plot, all our prior preprocessing efforts have not gone to waste. With the removal of stopwords, the remaining words seem much more meaningful where you can see that all the stopwords in the earlier term frequency plot "
"## 3b. Latent Dirichlet Allocation\n\nFinally we arrive on the subject of topic modelling and the implementation of a couple of unsupervised learning algorithms. The first method that I will touch upon is [Latent Dirichlet Allocation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). Now there are a couple of different implements of this LDA algorithm but in this notebook, I will be using Sklearn's implementation. Another very well-known LDA implementation is Radim Rehurek's [gensim](https://radimrehurek.com/gensim/), so check it out as well."
"## General information\n\nIn this kernel I work with IEEE Fraud Detection competition.\n\nEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.\n\nWe have a binary classification problem with a heavy imbalance which is an inherent property of such problems.\nAt first I'll explore the data and try to find valuable insights, maybe I'll do some feature engineering and then it wil be time to build models.\n\n![](https://cis.ieee.org/images/files/slideshow/abstract01.jpg)\n\n*Work in progress*"
Importing libraries
Importing libraries
## Functions used in this kernel\nThey are in the hidden cell below.
## Functions used in this kernel\nThey are in the hidden cell below.
"## Data loading and overview\n\nData is separated into two datasets: information about the identity of the customer and transaction information. Not all transactions belong to identities, which are available. Maybe it would be possible to use additional transactions to generate new features."
"## Data Exploration\nLet's start with identity information.\nid_01 - id_11 are continuous variables, id_12 - id_38 are categorical and the last two columns are obviously also categorical."
`id_01` has an interesting distribution: it has 77 unique non-positive values with skeweness to 0.
22% of values in `id_11` are equal to 100and 76% are missing. Quite strange.
"Some of features seem to be normalized. So if someone wants to normalize all variables, it would be necessary to separate such variables which seem to be already normalized."
Here we can see some information about client's device. It is important to be careful here - some of info could be for old devices and may be absent from test data.\n\nNow let's have a look at transaction data.
"A very important idea: it seems that train and test transaction dates don't overlap, so it would be prudent to use time-based split for validation.\nThis was already noted in abother kernel: https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda"
# Set up environment
"# Detect TPU\nWhat we're doing with our code here is making sure that we'll be sending our data across a TPU. What you're looking for is a printout of `Number of replicas: 8`, corresponding to the 8 cores of a TPU. If your printout instead says `Number of replicas: 1` you likely do not have TPUs enabled in your notebook.   \n\nTo enable TPUs navigate to the panel on the right and click on `Accelerator`. Choose TPU from the dropdown.  \n\nIf you'd like more TPU troubleshooting and optimization guidelines check out our **[Learn With Me: Troubleshooting and Optimizing TPUs video](https://youtu.be/BSeWHzjMHMU)**.  "
"# Introduction\n\nThis competition is hosted by the third largest insurance company in Brazil: [Porto Seguro](https://en.wikipedia.org/wiki/Porto_Seguro_S.A.) with the task of predicting the *probability that a driver will initiate an insurance claim in the next year.*\n\nThis notebook will aim to provide some interactive charts and analysis of the competition data by way of the Python visualisation library Plot.ly and hopefully bring some insights and beautiful plots that others can take and replicate. Plot.ly is one of the main products offered by the software company - [Plotly](https://plot.ly/) which specializes in providing online graphical and statistical visualisations (charts and dashboards) as well as providing an API to a whole rich suite of programming languages and tools such as Python, R, Matlab, Node.js etc.\n\nListed below for easy convenience are links to the various Plotly plots in this notebook:\n\n* Simple horizontal bar plot - Used to inspect the Target variable distribution\n* Correlation Heatmap plot  - Inspect the correlation between the different features\n* Scatter plot - Compare the feature importances generated by Random Forest and Gradient-Boosted model\n* Vertical bar plot - List in Descending order, the importance of the various features\n* 3D Scatter plot \n\nThe themes in this notebook can be briefly summarized follows:\n\n   [**1. Data Quality Checks**](#quality) - Visualising and evaluating all missing/Null values (values that are -1)\n\n**2. Feature inspection and filtering** - Correlation and feature Mutual information plots against the target variable. Inspection of the Binary, categorical and other variables.\n\n**3. Feature importance ranking via learning models** \n/n Building a Random Forest and Gradient Boosted model to help us rank features based off the learning process.\n\nLet's Go"
Let us load in the training data provided using Pandas:
"**Target variable inspection**\n\nAnother standard check normally conducted on the data is with regards to our target variable, where in this case, the column is conveniently titled ""target"". The target value also comes by the moniker of class/label/correct answer and is used in supervised learning models along with the corresponding data that is given (in our case all our train data except the id column) to learn the function that best maps the data to our target in the hope that this learned function can generalize and predict well with new unseen data."
"Hmmn, the target variable is rather imbalanced so it might be something to keep in mind. An imbalanced target will prove quite"
"## Correlation plots\n\nAs a starter, let us generate some linear correlation plots just to have a quick look at how a feature is linearly correlated to the next and perhaps start gaining some insights from here. At this juncture, I will use the seaborn statistical visualisation package to plot a heatmap of the correlation values. Conveniently, Pandas dataframes come with the corr() method inbuilt, which calculates the Pearson correlation. Also as convenient is Seaborn's way of invoking a correlation plot. Just literally the word ""heatmap""\n\n**Correlation of float features**"
"From the correlation plot, we can see that the majority of the features display zero or no correlation to one another. This is quite an interesting observation that will warrant our further investigation later down. For now, the paired features that display a positive linear correlation are listed as follows:\n\n**(ps_reg_01, ps_reg_03)**\n\n**(ps_reg_02, ps_reg_03)**\n\n**(ps_car_12, ps_car_13)**\n\n**(ps_car_13, ps_car_15)**"
"**Correlation of integer features**\n\nFor the columns of interger datatype, I shall now switch to using the Plotly library to show how one can also generate a heatmap of correlation values interactively. Much like our earlier Plotly plot, we generate a heatmap object by simply invoking the ""go.Heatmap"". Here we have to provide values to three different axes, where x and y axes take in the column names while the correlation value is provided by the z-axis. The colorscale attribute takes in keywords that correspond to different color palettes that you will see in the heatmap where in this example, I have used the Greys colorscale (others include Portland and Viridis - try it for yourself). "
"Similarly, we can observe that there are a huge number of columns that are not linearly correlated with each other at all, evident from the fact that we observe quite a lot of 0 value cells in our correlation plot. This is quite a useful observation to us, especially if we are trying to perform dimensionality reduction transformations such as Principal Component Analysis (PCA), this would require a certain degree of correlation  . We can note some features of interest are as follows:\n\n***Negatively correlated features*** : ps_ind_06_bin, ps_ind_07_bin,  ps_ind_08_bin,  ps_ind_09_bin\n\nOne interesting aspect to note is that in our earlier analysis on nullity, ps_car_03_cat and ps_car_05_cat were found to contain many missing or null values. Therefore it should come as no surprise that both these features show quite a strong positive linear correlation to each other on this basis, albeit one that may not really reflect the underlying truth for the data."
"**Plot.ly Scatter Plot of feature importances**\n\nHaving trained the Random Forest, we can obtain the list of feature importances by invoking the attribute ""feature_importances_"" and plot our next Plotly plot, the Scatter plot.\n\nHere we invoke the command Scatter and as per the previous Plotly plots, we have to define our y and x-axes. However the one thing that we pay attention to in scatter plots is the marker attribute. It is the marker attribute where we define and hence control the size, color and scale of the scatter points embedded."
"Furthermore we could also display a sorted list of all the features ranked by order of their importance, from highest to lowest via the same plotly barplots as follows:"
"Furthermore we could also display a sorted list of all the features ranked by order of their importance, from highest to lowest via the same plotly barplots as follows:"
"**Decision Tree visualisation**\n\nOne other interesting trick or technique oft used would be to visualize the tree branches or decisions made by the model. For simplicity, I fit a decision tree (of max_depth = 3) and hence you only see 3 levels in the decision branch, use the export to graph visualization attribute in sklearn ""export_graphviz"" and then export and import the tree image for visualization in this notebook."
We import the standard Keras library
"Loading the train and test files, as usual"
"### 1.5 Entities Extraction  \nThe extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of parts of speech (POS) tags. The nouns and the proper nouns would be our entities.\n\nHowever, when an entity spans across multiple words, then POS tags alone are not sufficient. We need to parse the dependency tree of the sentence.\n\nTo build a knowledge graph, the most important things are the nodes and the edges between them.\n\nThese nodes are going to be the entities that are present in the Wikipedia sentences. Edges are the relationships connecting these entities to one another. We will extract these elements in an unsupervised manner, i.e., we will use the grammar of the sentences.\n\nThe main idea is to go through a sentence and extract the subject and the object as and when they are encountered. However, there are a few challenges ⁠— an entity can span across multiple words, eg., “red wine”, and the dependency parsers tag only the individual words as subjects or objects.\n\nSo, I have created a function below to extract the subject and the object (entities) from a sentence while also overcoming the challenges mentioned above. I have partitioned the code into multiple chunks for your convenience:"
"**Chunk 1**\n\nDefined a few empty variables in this chunk. prv_tok_dep and prv_tok_text will hold the dependency tag of the previous word in the sentence and that previous word itself, respectively. prefix and modifier will hold the text that is associated with the subject or the object.\n\n**Chunk 2**\n\nNext, we will loop through the tokens in the sentence. We will first check if the token is a punctuation mark or not. If yes, then we will ignore it and move on to the next token. If the token is a part of a compound word (dependency tag = “compound”), we will keep it in the prefix variable. A compound word is a combination of multiple words linked to form a word with a new meaning (example – “Football Stadium”, “animal lover”).\n\nAs and when we come across a subject or an object in the sentence, we will add this prefix to it. We will do the same thing with the modifier words, such as “nice shirt”, “big house”, etc.\n\n**Chunk 3**\n\nHere, if the token is the subject, then it will be captured as the first entity in the ent1 variable. Variables such as prefix, modifier, prv_tok_dep, and prv_tok_text will be reset.\n\n**Chunk 4**\n\nHere, if the token is the object, then it will be captured as the second entity in the ent2 variable. Variables such as prefix, modifier, prv_tok_dep, and prv_tok_text will again be reset.\n\n**Chunk 5**\n\nOnce we have captured the subject and the object in the sentence, we will update the previous token and its dependency tag.\n\nLet’s test this function on a sentence:"
"Well, this is not exactly what we were hoping for (still looks quite a sight though!).\n\nIt turns out that we have created a graph with all the relations that we had. It becomes really hard to visualize a graph with these many relations or predicates.\n\nSo, it’s advisable to use only a few important relations to visualize a graph. I will take one relation at a time. Let’s start with the relation “composed by”:"
"That’s a much cleaner graph. Here the arrows point towards the composers. For instance, A.R. Rahman, who is a renowned music composer, has entities like “soundtrack score”, “film score”, and “music” connected to him in the graph above.\n\nLet’s check out a few more relations.\n\nNow I would like to visualize the graph for the “written by” relation:"
"That’s a much cleaner graph. Here the arrows point towards the composers. For instance, A.R. Rahman, who is a renowned music composer, has entities like “soundtrack score”, “film score”, and “music” connected to him in the graph above.\n\nLet’s check out a few more relations.\n\nNow I would like to visualize the graph for the “written by” relation:"
"This knowledge graph is giving us some extraordinary information. Guys like Javed Akhtar, Krishna Chaitanya, and Jaideep Sahni are all famous lyricists and this graph beautifully captures this relationship.\n\nLet’s see the knowledge graph of another important predicate, i.e., the “released in”:"
"This knowledge graph is giving us some extraordinary information. Guys like Javed Akhtar, Krishna Chaitanya, and Jaideep Sahni are all famous lyricists and this graph beautifully captures this relationship.\n\nLet’s see the knowledge graph of another important predicate, i.e., the “released in”:"
## 2. Conclusion  \n### I hope you have a good understanding on how to use Knowledge Graph .\n\n## Please do leave your comments /suggestions and if you like this notebook please do UPVOTE
## Training Evaluation\n\nLet's take a look at our training loss over all batches:
"## Predict and Evaluate on Holdout Set\nNow we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using [Matthew's correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task."
"To further clean our text data, we’ll also want to create a custom transformer for removing initial and end spaces and converting text into lower case. Here, we will create a custom predictors class wich inherits the TransformerMixin class. This class overrides the transform, fit and get_parrams methods. We’ll also create a clean_text() function that removes spaces and converts text into lowercase."
"When we classify text, we end up with text snippets matched with their respective labels. But we can’t simply use text strings in our machine learning model; we need a way to convert our text into something that can be represented numerically just like the labels (1 for positive and 0 for negative) are. Classifying text in positive and negative labels is called sentiment analysis. So we need a way to represent our text numerically.\n\nOne tool we can use for doing this is called **Bag of Words**. **BoW** converts text into the matrix of occurrence of words within a given document. It focuses on whether given words occurred or not in the document, and it generates a matrix that we might see referred to as a BoW matrix or a document term matrix.\n\nWe can generate a BoW matrix for our text data by using scikit-learn‘s CountVectorizer. In the code below, we’re telling CountVectorizer to use the custom spacy_tokenizer function we built as its tokenizer, and defining the ngram range we want.\n\nN-grams are combinations of adjacent words in a given text, where n is the number of words that incuded in the tokens. for example, in the sentence “Who will win the football world cup in 2022?” unigrams would be a sequence of single words such as “who”, “will”, “win” and so on. Bigrams would be a sequence of 2 contiguous words such as “who will”, “will win”, and so on. So the ngram_range parameter we’ll use in the code below sets the lower and upper bounds of the our ngrams (we’ll be using unigrams). Then we’ll assign the ngrams to bow_vector."
[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n**4.4. Finding Optimum Number of Principle Component**
"In the figure above, it can be seen that 90 and more PCA components represent the same data. Now let's make the classification process using 90 PCA components."
[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n**4.5. Show Average Face**  
[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n**4.6. Show Eigen Faces** 
[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n**4.6. Show Eigen Faces** 
[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n**4.7. Classification Results**  
Application
"This looks rather,well,odd but it helps us to get a general **picture of the outlines of our image in 3 dimensional perspective.** It helps to understand the size with a 3-dimensional spatial representatiion and at the same time helps to understand the limits of our image in a 3 dimensional scale. It was originally used for Data Science Bowl 2017 by Guido Zuldhof, but I have adapted it for here (this competition).\n\nA recommended approach would be to **resample** the images before plotting it like this. Here we can clearly see what is inside the image and where the isolated cells are (it forms a figure vaguely resemblant of a skull). We can also get a touchy-feely sense of this whole thing with Plot.ly (it is a real toll on my poor computer with 3 gigabyte RAM)"
"This looks rather,well,odd but it helps us to get a general **picture of the outlines of our image in 3 dimensional perspective.** It helps to understand the size with a 3-dimensional spatial representatiion and at the same time helps to understand the limits of our image in a 3 dimensional scale. It was originally used for Data Science Bowl 2017 by Guido Zuldhof, but I have adapted it for here (this competition).\n\nA recommended approach would be to **resample** the images before plotting it like this. Here we can clearly see what is inside the image and where the isolated cells are (it forms a figure vaguely resemblant of a skull). We can also get a touchy-feely sense of this whole thing with Plot.ly (it is a real toll on my poor computer with 3 gigabyte RAM)"
**3. Benign and malignant tumors**
Benign image viewing
These are the first 30 melanoma images with benign tumors. Let's check the distribution of values in `benign_malignant`.
Malignant image viewing
** 4. Which part of the body?**
"It seems like we have a lot of issues with the torso, and after that the extremities of the body (upper/lower). We have about 100-200 cases of cancer in the mouth or genitalia (the areas with the lowest rate of cancerous growth) and the palms and soles also are safe (probably because they are not exposed to any external sources in the day-to-day life of the person). \n\nOne could indeed fathom that the torso is frequently exposed either during the occasional workout or the occasional swim, or in some cases, the occasional extreme adventure. It also could be that the torso was exposed to UV light (which is the cause of melanoma) in highly populated regions (where pollution allows the sun's UV rays to come in)."
"Our earlier hypothesis now has more depth to it, with half (approximately?) of the skin cancer cases located in the torso. However, the torso has more square area than any other affected part, so it seems like the area on a body part is correlated with number of cases on that body part (as we can see)."
"We have seven types of diagnosed cancerous growths here:\n+ Unkown: a possibly novel type of growth\n+ Nevus: (from Google) a usually non-cancerous disorder of pigment-producing skin cells commonly called birth marks or moles.\n+ Melanoma: Skin cancer's form (what we are working with)\n+ Seborrheic keratosis: Brown, waxy and patchy growths that are not related to skin cancer.\n+ Lentigo NOS: A type of skin cancer that starts from the outside of the skin and attacks by going inword.\n+ Lichenoid keratosis: It is a thin pigmented sort of plaque, if you will.\n+ Solar lentigo: Like lentigo but caused by UV rays from the sun (very common in Delhi)\n+ cafe-au-lait macule: French for ""coffee with milk"". These are brownish spots also called ""giraffe spots"".\n+ atypical melanocytic proliferation: Abnormal quantities of melanin appear on the skin."
**80 percent of all the cases here are unknown. Literally 80 percent.**
So we have a bell (Gaussian or normal distribution) of train data. What about test?
Middle-aged people are the most likely to get cancer whereas those on the RHS and LHS of the plot are least likely to get it.  We now have found out that there is a bell curve distribution for age.
"We use normalization to evenly distribute something across an image - we can use normalization to, for example, normalize lighting conditions across the image."
** 8. Image Augmentations**
"We need to use image augmentations to add to our existing set of data. Why? To help our model identify the tumors correctly, even in grayscale colormap."
Grayscale images
"We will first try to visualize in grayscale (only gray colors) so that it is possible for us to clearly visualize the varied differences in color, region, and shape."
Ben Graham's method from 1st competition
Train
Now we try it on the test set:
Test
Nice! But we can further observe the clear color distinctions by using Neuron Engineer's method (an improved version of Ben Graham's).
Neuron Engineer's method
"Here we can finally visualize the clear distinctions in our data. The clear regions, the clear color differences, the clear everything! This is probably the best preprocessing method that we can apply... maybe not?"
Circle crop
"Circular crop has successfully worked, although it may not be feasible for images where the tumor is on the edge of the image. It does not seem so feasible, so I would recommend you try to be smarter in your methods for preprocessing. Remember, you can build upon Ben Graham's work as a starting point, then try Neuron Engineer's or circle crop or even build you own method.\n\n"
"Now we are using **auto-cropping** as a method of preprocessing, which is a more ""refined"" circle crop if you will. Think of circle crop as C, and think of auto-cropping as C++. Auto-cropping indeed is powerful, but the risk is that you will lose valuable data in the image."
Background subtraction
Another thing you can do is **background subtraction.**
Oh god... looks like *the Conjuring* now. 
Image segmentation
"This is called **image segmentation**. It breals down an image into its constituent parts represented by the distinction between regions (i.e in this instance the growth is white and the halo/surrrounding area is blackened). It helps our model to visually understand the distinctions even better than using grayscale images and it also helps to let our model identify the tumor.\n\nHowever, it has a heavy downside as we lose all information inside and outside the growth and thus our model loses the capability to understand or learn something from the image."
A finer method for image segmentation
This is a finer form of image segmentation where we use a second threshold to finetune our segmented data in a sort of way. This helps us see some parts of the growth and also it lets us see the images and the halos in small amounts.
Grayscale image segmentation
"These are now the segmented grayscale images, complete with markers. It will be a bit difficult for the model to learn anything from this due to the complete and utter confusion (pardon me) in the image with regard to the clear distinctions between image segments and there is no disctinction between parts of the image."
Now we move on to something more interesting: **Fourier transforms.** 
What comes out is the magnitude spectrum of the image. It is helpful to understand where the majority of the growth is concentrated.
"Also, you can use the albumentations library to create a lot of simulated images for your model. Remember, your model **MUST BE ABLE TO GENERALIZE!**"
We have much more augmentations we can try like:
We have much more augmentations we can try like:
Mess around with `p`:
Mess around with `p`:
"Not done yet! We still have a few more tricks, namely erosion and dilation."
Erosion
Dilation\n\nThere! Now we can try dilation:
Dilation\n\nThere! Now we can try dilation:
Now for both!
Combination of erosion and dilation
Roman's microscope augmentation
Albumentations + erosion
Albumentations + dilation
Albumentations + dilation
Albumentations + erosion + dilation
Albumentations + erosion + dilation
Complex wavelet transform
### Horizontal detail
### Vertical detail
### Vertical detail
## Hough transform
## Hough transform
## Canny edges
## Canny edges
**9. 3-dimensional augmentation techniques**
A simple enough attack would be to slightly alter an image by adding some noise.
This is a very basic way to add noise - it's known as salt and pepper noise. Here however we can implement multiple noises from a glance.
This is a very basic way to add noise - it's known as salt and pepper noise. Here however we can implement multiple noises from a glance.
---\n\n APPENDIX A: Melanoma\n\n---
"** This is how you can do your own random sampling**\n\nSince 'skiprows' can take in a list of rows you want to skip, you can make a list of random rows you want to input.   I.e. you can sample your data anyway you like!\n\nRecall how many rows the train set in TalkingData has:"
Let's say you want to pull a random sample of 1 million lines out of the total dataset.  That means that you want a list of `lines - 1 - 1000000` random numbers ranging from 1 to 184903891. \n\nNote: generating such long list also takes a lot of space and  some time.  Be patient and make sure to use del and gc.collect() when done!
"In my previous notebook (https://www.kaggle.com/yuliagm/talkingdata-eda-plus-time-patterns) we found that the data is organized by click time.  Therefore if our random sampling went according to plan, the resulting set should roughly span the full time period and mimick the click pattern. \n\nWe see from above that first and last click span the 4 day period.\n\nLet's try a chart to see if the pattern looks consistent:"
Looks all-right!  \n\nNow you can analyze your own subsample and run models on it.
"#  Be humble, sit down and check out the visualizations\n\n#### Last Update - 15 Sept 2017 \n- Added pairplot for feature interaction using **Holoviews** in second round of feature engineering\n- **Multiple Interactive histograms using Plotly** to check different distance features between pickup-drop\n- Correlation matrix heatmap using **Seaborn** to check the correlated variables\n- Histogram of test and validation data using **Seaborn**\n"
#### Importing packages for analysis
#### Importing packages for analysis
"### Reading and checking the head of training data and data from OSRM fastest route dataset\nTraining data is given as primary dataset for this competition and is available from the beginning of the competition. After almost 2 weeks from the start of this competition, oscarleo uploaded this dataset in Kaggle datasets. This dataset is generated from Open Source Routing Machine. This is similar to Gmaps but this is open source, so a user can make any number of queries using this engine ( Unlike Gmaps, where per day limit of free requests is 2000. Though I have compared the results of OSRM with that if Gmaps, Gmaps results considerably different than this one, almost all the results for time duration are different **with the same fraction**. So, while training any regressor, it will help very much in predicting trip duration. I am importing both of these datasets in the start of our analysis. \n"
"### Lets visualize the trip duration given using log-scale distplot in sns\nWe are asked to predict trip_duration of the test set, so we first check what kind of trips durations are present in the dataset. First I plotted it on a plain scale and not on a log scale, and some of the records have very long trip durations ~100 hours. Such long trips are making all another trip invisible in the histogram on plain scale => We go ahead with the log scale. Another reason of using the log scale for visualizing trip-duration on the log scale is that this competition uses rmsle matrix so it would make sense to visualize the target variable in log scale only."
"**Findings** - It is clear with the above histogram and kernel density plot that the trip-durations are like Gaussian and few trips have very large duration, like ~350000 seconds which is 100 hours (which is weird, as long as it isn't an intercity taxi ride from NYC to SF or Alaska), while most of the trips are e^4 = 1 minute to e^8 ~ 60 minutes. and probably are taken inside Manhattan or in new york only. Let's check the lat-long distributions are then used them to have a heat map kind of view of given lat-longs."
"**Findings** - It is clear with the above histogram and kernel density plot that the trip-durations are like Gaussian and few trips have very large duration, like ~350000 seconds which is 100 hours (which is weird, as long as it isn't an intercity taxi ride from NYC to SF or Alaska), while most of the trips are e^4 = 1 minute to e^8 ~ 60 minutes. and probably are taken inside Manhattan or in new york only. Let's check the lat-long distributions are then used them to have a heat map kind of view of given lat-longs."
"**Findings** - From the plot above it is clear that pick and drop latitude are centered around 40 to 41, and longitude are situated around -74 ton-73. We are not getting any histogram kind of plots when we are plotting lat-long as the distplot function of sns is getting affected by outliers, trips which are very far from each other like lat 32 to lat 44, are taking very long time, and have affected this plot such that it is coming off as a spike. Let's remove those large duration trip by using a cap on lat-long and visualize the distributions of latitude and longitude given to us."
"**Findings** - From the plot above it is clear that pick and drop latitude are centered around 40 to 41, and longitude are situated around -74 ton-73. We are not getting any histogram kind of plots when we are plotting lat-long as the distplot function of sns is getting affected by outliers, trips which are very far from each other like lat 32 to lat 44, are taking very long time, and have affected this plot such that it is coming off as a spike. Let's remove those large duration trip by using a cap on lat-long and visualize the distributions of latitude and longitude given to us."
"**Findings** - We put the following caps on lat-long -\n- latitude should be between 40.6 to 40.9\n- Longitude should be between -74.05 to -73.70 \n\nWe get that the distribution spikes becomes as distribution in distplot (distplot is a histogram plot in seaborn package), we can see that most of the trips are getting concentrated between these lat-long only. Let's plot them on an empty image and check what kind of a city map we are getting as we can't use gmaps and folium on kaggle kernel for visualizations. \n**Update** - As at that time we didn't have folium in kaggle python docker, Now we have it and I have included beautiful folium visualizations in this kernel."
"**Findings** - We put the following caps on lat-long -\n- latitude should be between 40.6 to 40.9\n- Longitude should be between -74.05 to -73.70 \n\nWe get that the distribution spikes becomes as distribution in distplot (distplot is a histogram plot in seaborn package), we can see that most of the trips are getting concentrated between these lat-long only. Let's plot them on an empty image and check what kind of a city map we are getting as we can't use gmaps and folium on kaggle kernel for visualizations. \n**Update** - As at that time we didn't have folium in kaggle python docker, Now we have it and I have included beautiful folium visualizations in this kernel."
"## Heatmap of coordinates\n### Let's do basic image processing here \nWe have taken an empty image and make it a color it black so that we can see colors where the lat-longs are falling. To visualize we need to consider each point of this image as a point represented by lat-long, to achieve that we will bring the lat-long to image coordinate range and then take a summary of lat-long and their count, assign a different color for different count range. Running next cell will result in beautiful visualization shown below.\n\nThis is the very low-level implementation of making plots. As in the early stage of competition people using R was making plots on leaflet and we, Python users were missing gmaps and folium in kaggle kernel's python docker, I requested them to get the folium installed but it took some time and I wasn't aware of the ""datashader"" package that time.So, I decided to use the knowledge I gained in last kaggle competition I participated in (Nature Conservancy Fisheries Monitoring) and made such plots using basic image processing. "
"## Heatmap of coordinates\n### Let's do basic image processing here \nWe have taken an empty image and make it a color it black so that we can see colors where the lat-longs are falling. To visualize we need to consider each point of this image as a point represented by lat-long, to achieve that we will bring the lat-long to image coordinate range and then take a summary of lat-long and their count, assign a different color for different count range. Running next cell will result in beautiful visualization shown below.\n\nThis is the very low-level implementation of making plots. As in the early stage of competition people using R was making plots on leaflet and we, Python users were missing gmaps and folium in kaggle kernel's python docker, I requested them to get the folium installed but it took some time and I wasn't aware of the ""datashader"" package that time.So, I decided to use the knowledge I gained in last kaggle competition I participated in (Nature Conservancy Fisheries Monitoring) and made such plots using basic image processing. "
"**Findings** - From the heatmap kind of image above -\n- Red points signifies that 1-10 trips in the given data have that point as pickup point\n- Green points signifies that more than 10-50 trips in the given data have that point as pickup point \n- Yellow points signifies that more than 50+ trips in the given data have that point as pickup point\n\nClearly, the whole manhattan is yellow colored and with few green points as well, that shows that in Manhatten most of the trips are getting originated.  This is the basic way in which you can plot large geospatial data in an empty image without being dependent on any package. But if you hate image processing, you can use **datashader**, datashader is a package which is used to show billions of data points on an image, they also use the similar approach with a different color gradient.\nThought I will also show the same plot with a sample data of 1000 trips on pygmaps in next few cell. it will generate an HTML in output and user has to open that HTML in the browser."
"# Features' Exploration (Checking if we have any explainable pattern) -\nEven if we don't visualize these features we can make model and model will predict the trip_duration, then why are we visualizing? - Because - it will give us an explanation of model's output and will give us some pattern which may even guide if we should make multiple models, or one model if we should include that particular variable or there is no sense in including that variable. in short - it will give us new ideas to make a model. \n**Note** - Basically in consulting world, if you can't explain model it won't sell, and without visuals, it's very hard to explain patterns.\n- **1. Let's check the average time taken by two different vendors vs weekday**"
"**Findings** - it's clear that the vendor 1 is taking more time than vendor 2 on all the days of the week, we can also subset data frame based on the month and that will also give us the same results. The difference between the average time taken by vendor 1 is ~250 seconds more than vendor 2. "
"### Violin Plot -\nWhen someone says seaborn, two things come to my mind first greyjoys ( GOT fan I am ;) ) and second violin plots, I know seaborn package because of it's violin plots, so I will explain the distributions of next variable with violin plots. Following are few facts about violin plots -  \n- Violin plot can be made using seaborn package in python and with split\n- Here we are using them to check the distributions, and horizontal lines inside them shows the quartiles\n- Green one is vendor 1 and red one is vendor 2 and trip_duration is plotted on log scale"
"**Findings** -\n- There are trips for both the vendor with zeros passengers and few of these trips have negative time as well, I don't understand how can a taxi trip have negative time, possibly they aren't right data points to train the model, we would remove them before making model\n- Trips with zero passengers can be trips when a taxi is called to a particular location and the customer is charged for getting the taxi there, that is one possible explanation.\n- Distributions are similar for both the vendors, but vendor one has more number of larger trips than vendor two for passenger count 2 and 3\n- There are very less number of trips with passenger count 7, 8 and 9"
## Box-Plots \n**Interpretation**\n- Most popular plots to check the distribution of variables\n- box covers data from second and third quadrant and rest is shorn by bars\n- Dots on the both side of bars shows outliers
"**Findings** -\n- From the boxplot above we can see that 75%ile of avg trip duration on Sunday(0) and Saturday(6) is less than 2000 seconds. i.e. around 33 minutes\n- Time taken by Monday, Tuesday, Wednesday, and Thursday are greater than rest of the days."
## line-plots \n- Simple lineplots can explain how the trip duration is changing with time for different days of week\n- very easy to interpret
"**findings** - \n- Its clear from the above plot that on day 0, that is Sunday and day 6 that is Saturday, the trip duration is very less that all the weekdays at 5 AM to 15 AM time. \n- See this, on  Saturday around midnight, the rides are taking far more than usual time, this is obvious through now verified  using given data"
## Alluvial Plots or parallel coordinates - \nAlluvial(in R) or Parallel coordinates(py) is use to plot the cluster characteristics. \n- passenger count 5 and 6 are mostly travelled with vendor 2 only \n- Cluster 4 has mostly vendor 2 cabs while cluster 5 has both in almost equal amount\nSuch kind of observations about clusters can be seen using parallel coordinates and its offered by pandas package.
"# Extracting same features for Test data - \n- We will extract same features for test dataset and then will train xgboost regressor to see how we are doing with prediction, how much predicting power out extracted features have. If we didn't get satisfactory accuracy we will extract extra features and so on.."
"## Let's checkout holoviews pairplot ..\nSimilar to sns pairplot, Holoviews also offers a pairplot kind of plot. This plot looks better than seaborn's but its very heavy in nature, if we make Holoview = True below. The plot will get the notebook hanged and this notebook will starttaking very long in loading. Feel free to change in Run and checkout the plot."
## Let's check correlation using a heatmap and check how the features are correlated ..
## Let's check correlation using a heatmap and check how the features are correlated ..
"** Findings ** - \n- Variables like pick_month, week_of_year, day_of_year etc are of very less importance and are not at all related to other variables.\n- OSRM variables are very important => correlation is as high as 0.8\n- Strong correlation is observed between different IDVs but we will be using tree based model so No need to remove those variables which are highly correlated. ( thought try removing for Robustness of model)\n-Trip Duration is slight correated with many IDVs"
"# Identifying Duplicate Questions\n\nWelcome to the Quora Question Pairs competition! Here, our goal is to identify which questions asked on [Quora](https://www.quora.com/), a quasi-forum website with over 100 million visitors a month, are duplicates of questions that have already been asked. This could be useful, for example, to instantly provide answers to questions that have already been answered. We are tasked with predicting whether a pair of questions are duplicates or not, and submitting a binary prediction against the logloss metric.\n\nIf you have any questions or want to discuss competitions/hardware/games/anything with other Kagglers, then join the KaggleNoobs Slack channel [here](https://goo.gl/gGWFXe). We also have regular AMAs with top Kagglers there.\n\n**And as always, if this helped you, some upvotes would be very much appreciated - that's where I get my motivation! :D**\n\nLet's dive right into the data!"
"Looks like we are simply given two files this time round, one for the training set and one for the test set. They are relatively small compared to other recent competitions, weighing in at less than 400MB total.\n\nIt's worth noting that there is a lot more testing data than training data. This could be a sign that some of the test data is dummy data designed to deter hand-labelling, and not included in the calculations, like we recently saw in the [DSTL competition](https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/leaderboard).\n\nLet's open up one of the datasets.\n\n## Training set"
"We are given a minimal number of data fields here, consisting of:\n\n**`id`:** Looks like a simple rowID    \n**`qid{1, 2}`:** The unique ID of each question in the pair    \n**`question{1, 2}`:** The actual textual contents of the questions.    \n**`is_duplicate`:** The **label** that we are trying to predict - whether the two questions are duplicates of each other."
"In terms of questions, everything looks as I would expect here. Most questions only appear a few times, with very few questions appearing several times (and a few questions appearing many times). One question appears more than 160 times, but this is an outlier.\n\nWe can see that we have a 37% positive class in this dataset. Since we are using the [LogLoss](https://www.kaggle.com/wiki/LogarithmicLoss) metric, and LogLoss looks at the actual predicts as opposed to the order of predictions, we should be able to get a decent score by creating a submission predicting the mean value of the label.\n\n## Test Submission"
"Nothing out of the ordinary here. We are once again given rowIDs and the textual data of the two questions. It is worth noting that we are not given question IDs here however for the two questions in the pair.\n\nIt is also worth pointing out that the actual number of test rows are likely to be much lower than 2.3 million. According to the [data page](https://www.kaggle.com/c/quora-question-pairs/data), most of the rows in the test set are using auto-generated questions to pad out the dataset, and deter any hand-labelling. This means that the true number of rows that are scored could be very low.\n\nWe can actually see in the head of the test data that some of the questions are obviously auto-generated, as we get delights such as ""How their can I start reading?"" and ""What foods fibre?"". Truly insightful questions.\n\nNow onto the good stuff - the text data!\n## Text analysis\n\nFirst off, some quick histograms to understand what we're looking at. **Most analysis here will be only on the training set, to avoid the auto-generated questions**"
"We can see that most questions have anywhere from 15 to 150 characters in them. It seems that the test distribution is a little different from the train one, but not too much so (I can't tell if it is just the larger data reducing noise, but it also seems like the distribution is a lot smoother in the test set).\n\nOne thing that catches my eye is the steep cut-off at 150 characters for the training set, for most questions, while the test set slowly decreases after 150. Could this be some sort of Quora question size limit?\n\nIt's also worth noting that I've truncated this histogram at 200 characters, and that the max of the distribution is at just under 1200 characters for both sets - although samples with over 200 characters are very rare.\n\nLet's do the same for word count. I'll be using a naive method for splitting words (splitting on spaces instead of using a serious tokenizer), although this should still give us a good idea of the distribution."
"We can see that most questions have anywhere from 15 to 150 characters in them. It seems that the test distribution is a little different from the train one, but not too much so (I can't tell if it is just the larger data reducing noise, but it also seems like the distribution is a lot smoother in the test set).\n\nOne thing that catches my eye is the steep cut-off at 150 characters for the training set, for most questions, while the test set slowly decreases after 150. Could this be some sort of Quora question size limit?\n\nIt's also worth noting that I've truncated this histogram at 200 characters, and that the max of the distribution is at just under 1200 characters for both sets - although samples with over 200 characters are very rare.\n\nLet's do the same for word count. I'll be using a naive method for splitting words (splitting on spaces instead of using a serious tokenizer), although this should still give us a good idea of the distribution."
"We see a similar distribution for word count, with most questions being about 10 words long. It looks to me like the distribution of the training set seems more ""pointy"", while on the test set it is wider. Nevertheless, they are quite similar.\n\nSo what are the most common words? Let's take a look at a word cloud."
"We see a similar distribution for word count, with most questions being about 10 words long. It looks to me like the distribution of the training set seems more ""pointy"", while on the test set it is wider. Nevertheless, they are quite similar.\n\nSo what are the most common words? Let's take a look at a word cloud."
"## Semantic Analysis\n\nNext, I will take a look at usage of different punctuation in questions - this may form a basis for some interesting features later on."
"# Initial Feature Analysis\n\nBefore we create a model, we should take a look at how powerful some features are. I will start off with the word share feature from the benchmark model."
"Here we can see that this feature has quite a lot of predictive power, as it is good at separating the duplicate questions from the non-duplicate ones. Interestingly, it seems very good at identifying questions which are definitely different, but is not so great at finding questions which are definitely duplicates.\n\n## TF-IDF\n\nI'm now going to try to improve this feature, by using something called TF-IDF (term-frequency-inverse-document-frequency). This means that we weigh the terms by how **uncommon** they are, meaning that we care more about rare words existing in both questions than common one. This makes sense, as for example we care more about whether the word ""exercise"" appears in both than the word ""and"" - as uncommon words will be more indicative of the content.\n\nYou may want to look into using sklearn's [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to compute weights if you are implementing this yourself, but as I am too lazy to read the documentation I will write a version in pure python with a few changes which I believe should help the score."
"##  3b. Correlation Matrix & Heatmap\n***\n**Moderate Positively Correlated Features:** \n- projectCount vs evaluation: 0.349333\n- projectCount vs averageMonthlyHours:  0.417211\n- averageMonthlyHours vs evaluation: 0.339742\n\n**Moderate Negatively Correlated Feature:**\n - satisfaction vs turnover:  -0.388375\n\n**Stop and Think:**\n- What features affect our target variable the most (turnover)?\n- What features have strong correlations with each other?\n- Can we do a more in depth examination of these features?\n\n**Summary:**\n\nFrom the heatmap, there is a **positive(+)** correlation between projectCount, averageMonthlyHours, and evaluation. Which could mean that the employees who spent more hours and did more projects were evaluated highly. \n\nFor the **negative(-)** relationships, turnover and satisfaction are highly correlated. I'm assuming that people tend to leave a company more when they are less satisfied. "
"## 3b2. Statistical Test for Correlation\n***\n\n### One-Sample T-Test (Measuring Satisfaction Level)\nA one-sample t-test checks whether a sample mean differs from the population mean. Since satisfaction has the highest correlation with our dependent variable turnover, let's test to see whether the average satisfaction level of employees that had a turnover differs from the those that had no turnover.\n\n**Hypothesis Testing:** Is there significant difference in the **means of satisfaction level** between employees who had a turnover and temployees who had no turnover?\n\n - **Null Hypothesis:** *(H0: pTS = pES)* The null hypothesis would be that there is **no** difference in satisfaction level between employees who did turnover and those who did not..\n\n - **Alternate Hypothesis:** *(HA: pTS != pES)* The alternative hypothesis would be that there **is** a difference in satisfaction level between employees who did turnover and those who did not.."
"### Conducting the T-Test\n***\nLet's conduct a t-test at **95% confidence level** and see if it correctly rejects the null hypothesis that the sample comes from the same distribution as the employee population. To conduct a one sample t-test, we can use the **stats.ttest_1samp()** function:"
"### T-Test Result\n***\nThe test result shows the **test statistic ""t"" is equal to -51.33**. This test statistic tells us how much the sample mean deviates from the null hypothesis. If the t-statistic lies **outside** the quantiles of the t-distribution corresponding to our confidence level and degrees of freedom, we reject the null hypothesis. We can check the quantiles with **stats.t.ppf()**:"
"##  3c. Distribution Plots (Satisfaction - Evaluation - AverageMonthlyHours)\n***\n**Summary:** Let's examine the distribution on some of the employee's features. Here's what I found:\n - **Satisfaction** - There is a huge spike for employees with low satisfaction and high satisfaction.\n - **Evaluation** - There is a bimodal distrubtion of employees for low evaluations (less than 0.6) and high evaluations (more than 0.8)\n - **AverageMonthlyHours** - There is another bimodal distribution of employees with lower and higher average monthly hours (less than 150 hours & more than 250 hours)\n - The evaluation and average monthly hour graphs both share a similar distribution. \n - Employees with lower average monthly hours were evaluated less and vice versa.\n - If you look back at the correlation matrix, the high correlation between evaluation and averageMonthlyHours does support this finding.\n \n**Stop and Think:** \n - Is there a reason for the high spike in low satisfaction of employees?\n - Could employees be grouped in a way with these features?\n - Is there a correlation between evaluation and averageMonthlyHours?"
"##  3d. Salary V.S. Turnover\n***\n**Summary:** This is not unusual. Here's what I found:\n - Majority of employees who left either had **low** or **medium** salary.\n - Barely any employees left with **high** salary\n - Employees with low to average salaries tend to leave the company.\n \n**Stop and Think:** \n - What is the work environment like for low, medium, and high salaries?\n - What made employees with high salaries to leave?"
"##  3f. Turnover V.S. ProjectCount \n***\n**Summary:** This graph is quite interesting as well. Here's what I found:\n - More than half of the employees with **2,6, and 7** projects left the company\n - Majority of the employees who did not leave the company had **3,4, and 5** projects\n - All of the employees with **7** projects left the company\n - There is an increase in employee turnover rate as project count increases\n \n**Stop and Think:** \n - Why are employees leaving at the lower/higher spectrum of project counts?\n - Does this means that employees with project counts 2 or less are not worked hard enough or are not highly valued, thus leaving the company?\n - Do employees with 6+ projects are getting overworked, thus leaving the company?\n\n"
##  3g. Turnover V.S. Evaluation \n***\n**Summary:** \n - There is a biomodal distribution for those that had a turnover. \n - Employees with **low** performance tend to leave the company more\n - Employees with **high** performance tend to leave the company more\n - The **sweet spot** for employees that stayed is within **0.6-0.8** evaluation
##  3g. Turnover V.S. Evaluation \n***\n**Summary:** \n - There is a biomodal distribution for those that had a turnover. \n - Employees with **low** performance tend to leave the company more\n - Employees with **high** performance tend to leave the company more\n - The **sweet spot** for employees that stayed is within **0.6-0.8** evaluation
##  3h. Turnover V.S. AverageMonthlyHours \n***\n**Summary:** \n - Another bi-modal distribution for employees that turnovered \n - Employees who had less hours of work **(~150hours or less)** left the company more\n - Employees who had too many hours of work **(~250 or more)** left the company \n - Employees who left generally were **underworked** or **overworked**.
##  3h. Turnover V.S. AverageMonthlyHours \n***\n**Summary:** \n - Another bi-modal distribution for employees that turnovered \n - Employees who had less hours of work **(~150hours or less)** left the company more\n - Employees who had too many hours of work **(~250 or more)** left the company \n - Employees who left generally were **underworked** or **overworked**.
##  3i. Turnover V.S. Satisfaction \n***\n**Summary:** \n - There is a **tri-modal** distribution for employees that turnovered\n - Employees who had really low satisfaction levels **(0.2 or less)** left the company more\n - Employees who had low satisfaction levels **(0.3~0.5)** left the company more\n - Employees who had really high satisfaction levels **(0.7 or more)** left the company more
##  3i. Turnover V.S. Satisfaction \n***\n**Summary:** \n - There is a **tri-modal** distribution for employees that turnovered\n - Employees who had really low satisfaction levels **(0.2 or less)** left the company more\n - Employees who had low satisfaction levels **(0.3~0.5)** left the company more\n - Employees who had really high satisfaction levels **(0.7 or more)** left the company more
"##  3j. ProjectCount VS AverageMonthlyHours \n***\n\n**Summary:**\n - As project count increased, so did average monthly hours\n - Something weird about the boxplot graph is the difference in averageMonthlyHours between people who had a turnver and did not. \n - Looks like employees who **did not** have a turnover had **consistent** averageMonthlyHours, despite the increase in projects\n - In contrast, employees who **did** have a turnover had an increase in averageMonthlyHours with the increase in projects\n\n**Stop and Think:** \n - What could be the meaning for this? \n - **Why is it that employees who left worked more hours than employees who didn't, even with the same project count?**"
"##  3j. ProjectCount VS AverageMonthlyHours \n***\n\n**Summary:**\n - As project count increased, so did average monthly hours\n - Something weird about the boxplot graph is the difference in averageMonthlyHours between people who had a turnver and did not. \n - Looks like employees who **did not** have a turnover had **consistent** averageMonthlyHours, despite the increase in projects\n - In contrast, employees who **did** have a turnover had an increase in averageMonthlyHours with the increase in projects\n\n**Stop and Think:** \n - What could be the meaning for this? \n - **Why is it that employees who left worked more hours than employees who didn't, even with the same project count?**"
"##  3k. ProjectCount VS Evaluation\n***\n**Summary:** This graph looks very similar to the graph above. What I find strange with this graph is with the turnover group. There is an increase in evaluation for employees who did more projects within the turnover group. But, again for the non-turnover group, employees here had a consistent evaluation score despite the increase in project counts. \n\n**Questions to think about:**\n - **Why is it that employees who left, had on average, a higher evaluation than employees who did not leave, even with an increase in project count? **\n - Shouldn't employees with lower evaluations tend to leave the company more? "
"##  3k. ProjectCount VS Evaluation\n***\n**Summary:** This graph looks very similar to the graph above. What I find strange with this graph is with the turnover group. There is an increase in evaluation for employees who did more projects within the turnover group. But, again for the non-turnover group, employees here had a consistent evaluation score despite the increase in project counts. \n\n**Questions to think about:**\n - **Why is it that employees who left, had on average, a higher evaluation than employees who did not leave, even with an increase in project count? **\n - Shouldn't employees with lower evaluations tend to leave the company more? "
"##  3l. Satisfaction VS Evaluation\n***\n**Summary:** This is by far the most compelling graph. This is what I found:\n - There are **3** distinct clusters for employees who left the company\n \n**Cluster 1 (Hard-working and Sad Employee):** Satisfaction was below 0.2 and evaluations were greater than 0.75. Which could be a good indication that employees who left the company were good workers but felt horrible at their job. \n - **Question:** What could be the reason for feeling so horrible when you are highly evaluated? Could it be working too hard? Could this cluster mean employees who are ""overworked""?\n\n**Cluster 2 (Bad and Sad Employee):** Satisfaction between about 0.35~0.45 and evaluations below ~0.58. This could be seen as employees who were badly evaluated and felt bad at work.\n - **Question:** Could this cluster mean employees who ""under-performed""?\n\n**Cluster 3 (Hard-working and Happy Employee):** Satisfaction between 0.7~1.0 and evaluations were greater than 0.8. Which could mean that employees in this cluster were ""ideal"". They loved their work and were evaluated highly for their performance. \n - **Question:** Could this cluser mean that employees left because they found another job opportunity?"
"##  3l. Satisfaction VS Evaluation\n***\n**Summary:** This is by far the most compelling graph. This is what I found:\n - There are **3** distinct clusters for employees who left the company\n \n**Cluster 1 (Hard-working and Sad Employee):** Satisfaction was below 0.2 and evaluations were greater than 0.75. Which could be a good indication that employees who left the company were good workers but felt horrible at their job. \n - **Question:** What could be the reason for feeling so horrible when you are highly evaluated? Could it be working too hard? Could this cluster mean employees who are ""overworked""?\n\n**Cluster 2 (Bad and Sad Employee):** Satisfaction between about 0.35~0.45 and evaluations below ~0.58. This could be seen as employees who were badly evaluated and felt bad at work.\n - **Question:** Could this cluster mean employees who ""under-performed""?\n\n**Cluster 3 (Hard-working and Happy Employee):** Satisfaction between 0.7~1.0 and evaluations were greater than 0.8. Which could mean that employees in this cluster were ""ideal"". They loved their work and were evaluated highly for their performance. \n - **Question:** Could this cluser mean that employees left because they found another job opportunity?"
##  3m. Turnover V.S. YearsAtCompany \n***\n**Summary:** Let's see if theres a point where employees start leaving the company. Here's what I found:\n - More than half of the employees with **4 and 5** years left the company\n - Employees with **5** years should **highly** be looked into \n \n**Stop and Think:** \n - Why are employees leaving mostly at the **3-5** year range?\n - Who are these employees that left?\n - Are these employees part-time or contractors? 
##  3m. Turnover V.S. YearsAtCompany \n***\n**Summary:** Let's see if theres a point where employees start leaving the company. Here's what I found:\n - More than half of the employees with **4 and 5** years left the company\n - Employees with **5** years should **highly** be looked into \n \n**Stop and Think:** \n - Why are employees leaving mostly at the **3-5** year range?\n - Who are these employees that left?\n - Are these employees part-time or contractors? 
"## 3n. K-Means Clustering of Employee Turnover\n***\n**Cluster 1 (Blue):** Hard-working and Sad Employees\n\n**Cluster 2 (Red):** Bad and Sad Employee \n\n**Cluster 3 (Green):** Hard-working and Happy Employee \n\n**Clustering PROBLEM:**\n    - How do we know that there are ""3"" clusters?\n    - We would need expert domain knowledge to classify the right amount of clusters\n    - Hidden uknown structures could be present"
"## 3n. K-Means Clustering of Employee Turnover\n***\n**Cluster 1 (Blue):** Hard-working and Sad Employees\n\n**Cluster 2 (Red):** Bad and Sad Employee \n\n**Cluster 3 (Green):** Hard-working and Happy Employee \n\n**Clustering PROBLEM:**\n    - How do we know that there are ""3"" clusters?\n    - We would need expert domain knowledge to classify the right amount of clusters\n    - Hidden uknown structures could be present"
"# Feature Importance\n***\n**Summary:**\n\nBy using a decision tree classifier, it could rank the features used for the prediction. The top three features were employee satisfaction, yearsAtCompany, and evaluation. This is helpful in creating our model for logistic regression because it’ll be more interpretable to understand what goes into our model when we utilize less features. \n\n**Top 3 Features:**\n1. Satisfaction\n2. YearsAtCompany\n3. Evaluation\n"
"# Feature Importance\n***\n**Summary:**\n\nBy using a decision tree classifier, it could rank the features used for the prediction. The top three features were employee satisfaction, yearsAtCompany, and evaluation. This is helpful in creating our model for logistic regression because it’ll be more interpretable to understand what goes into our model when we utilize less features. \n\n**Top 3 Features:**\n1. Satisfaction\n2. YearsAtCompany\n3. Evaluation\n"
"# 4a. Modeling the Data: Logistic Regression Analysis\n***\n**NOTE:** This will be an in-depth analysis of using logistic regression as a classifier. I do go over other types of models in the other section below this. **This is more of a use-case example of what can be done and explained to management in a company.**\n\nLogistic Regression commonly deals with the issue of how likely an observation is to belong to each group. This model is commonly used to predict the likelihood of an event occurring. In contrast to linear regression, the output of logistic regression is transformed with a logit function. This makes the output either 0 or 1. This is a useful model to take advantage of for this problem because we are interested in predicting whether an employee will leave (0) or stay (1). \n\nAnother reason for why logistic regression is the preferred model of choice is because of its interpretability. Logistic regression predicts the outcome of the response variable (turnover) through a set of other explanatory variables, also called predictors. In context of this domain, the value of our response variable is categorized into two forms: 0 (zero) or 1 (one). The value of 0 (zero) represents the probability of an employee not leaving the company and the value of 1 (one) represents the probability of an employee leaving the company.\n\n**Logistic Regression models the probability of ‘success’ as: **\n\n \nThe equation above shows the relationship between, the dependent variable (success), denoted as (θ) and independent variables or predictor of event, denoted as xi. Where α is the constant of the equation and, β is the coefficient of the predictor variables\n\n"
"# 4a. Modeling the Data: Logistic Regression Analysis\n***\n**NOTE:** This will be an in-depth analysis of using logistic regression as a classifier. I do go over other types of models in the other section below this. **This is more of a use-case example of what can be done and explained to management in a company.**\n\nLogistic Regression commonly deals with the issue of how likely an observation is to belong to each group. This model is commonly used to predict the likelihood of an event occurring. In contrast to linear regression, the output of logistic regression is transformed with a logit function. This makes the output either 0 or 1. This is a useful model to take advantage of for this problem because we are interested in predicting whether an employee will leave (0) or stay (1). \n\nAnother reason for why logistic regression is the preferred model of choice is because of its interpretability. Logistic regression predicts the outcome of the response variable (turnover) through a set of other explanatory variables, also called predictors. In context of this domain, the value of our response variable is categorized into two forms: 0 (zero) or 1 (one). The value of 0 (zero) represents the probability of an employee not leaving the company and the value of 1 (one) represents the probability of an employee leaving the company.\n\n**Logistic Regression models the probability of ‘success’ as: **\n\n \nThe equation above shows the relationship between, the dependent variable (success), denoted as (θ) and independent variables or predictor of event, denoted as xi. Where α is the constant of the equation and, β is the coefficient of the predictor variables\n\n"
"## Using Logistic Regression Coefficients \n***\nWith the elimination of the other variables, I’ll be using the three most important features to create our model: Satisfaction, Evaluation, and YearsAtCompany.\n\nFollowing overall equation was developed: \n\n**Employee Turnover Score** = Satisfaction*(**-3.769022**) + Evaluation*(**0.207596**) + YearsAtCompany*(**0.170145**) + **0.181896**\n"
## ROC Graph\n***
"# 5. Interpreting the Data\n***\n\n**Summary:** \nWith all of this information, this is what Bob should know about his company and why his employees probably left:\n 1. Employees generally left when they are **underworked** (less than 150hr/month or 6hr/day)\n 2. Employees generally left when they are **overworked** (more than 250hr/month or 10hr/day)\n 3. Employees with either **really high or low evaluations** should be taken into consideration for high turnover rate\n 4. Employees with **low to medium salaries** are the bulk of employee turnover\n 5. Employees that had **2,6, or 7 project count** was at risk of leaving the company\n 6. Employee **satisfaction** is the highest indicator for employee turnover.\n 7. Employee that had **4 and 5 yearsAtCompany** should be taken into consideration for high turnover rate\n 8. Employee **satisfaction**, **yearsAtCompany**, and **evaluation** were the three biggest factors in determining turnover."
"\nGoals\nThis kernel hopes to accomplish many goals, to name a few...\n    \n        Learn/review/explain complex data science topics through write-ups.\n        Do a comprehensive data analysis along with visualizations.\n        Create models that are well equipped to predict housing prices.  \n    \n\n\nImporting Necessary Libraries and datasets"
A Glimpse of the datasets.\nSample Train Dataset
Relationship between missing values and Sale Price:
These plots compare the median SalePrice in the observations where data is missing vs the observations where a value is available. As you can see there is a significant difference in median sale price between where missing value exists and where missing value doesn't exist. We are using median here because mean would not direct us towards a better assumption as there are some outliers present. 
"You can see these values are represented in years as we hoped. However, we generally don't use data for example year in their raw format, instead we try to get information from them. Let's look at the ""YrSold"" plot "
"This plot should raise an eyebrows. As the year increases the price of the houses seems to be decreasing, which in real time is quite unusual. Let's see if there is a relationship between year features and SalePrice"
"This plot should raise an eyebrows. As the year increases the price of the houses seems to be decreasing, which in real time is quite unusual. Let's see if there is a relationship between year features and SalePrice"
"These charts seems more like a real life situation case. The longer the time between the house was built/remodeled and sold, the lower the sale price. This is likely, because the houses will have an older look and might need repairing. "
Let's analyze the discrete variables and see how they are related with the target variable SalePrice. 
"There tend to be some relationships between the variables and the SalePrice, for example some are monotonic like OverallQual, some almost monotonic except for a unique values like OverallCond. We need to be particulary careful for these variables to extract maximum values for a linear model. "
"There tend to be some relationships between the variables and the SalePrice, for example some are monotonic like OverallQual, some almost monotonic except for a unique values like OverallCond. We need to be particulary careful for these variables to extract maximum values for a linear model. "
Continous Variables\n\nLet's find out the continous variables. 
"As you can see these variables are not normally distributed including our target variable ""SalePrice"". In order to maximise performance of linear models, we can use log transformation. We will describe more on this in following parts. We will do the transformation in the feature engineering section. Let's now see how the distribution might look once we do the transformation. "
"As you can see, we get a better spread of data once we use log transformation.\n\nI want to focus our attention on the target variable which is **SalePrice.** We already know that our target variable is not normally distributed. However, lets go into more details. If we want to create any linear model, it is essential that the features are normally distributed. This is one of the assumptions of multiple linear regression. In previous codes we have seen that log transformation can help us to make features more like normally distribute. I will explain more on this later."
"As you can see, we get a better spread of data once we use log transformation.\n\nI want to focus our attention on the target variable which is **SalePrice.** We already know that our target variable is not normally distributed. However, lets go into more details. If we want to create any linear model, it is essential that the features are normally distributed. This is one of the assumptions of multiple linear regression. In previous codes we have seen that log transformation can help us to make features more like normally distribute. I will explain more on this later."
"These **three** charts above can tell us a lot about our target variable.\n* Our target variable, **SalePrice** is not normally distributed.\n* Our target variable is right-skewed. \n* There are multiple outliers in the variable.\n\n\n**P.S.** \n* If you want to find out more about how to customize charts, try [this](https://matplotlib.org/tutorials/intermediate/gridspec.html#sphx-glr-tutorials-intermediate-gridspec-py) link. \n* If you are learning about Q-Q-plots for the first time. checkout [this](https://www.youtube.com/watch?v=smJBsZ4YQZw) video. \n* You can also check out [this](https://www.youtube.com/watch?v=9IcaQwQkE9I) one if you have some extra time. \n\nLet's find out how the sales price is distributed."
"As we look through these scatter plots, I realized that it is time to explain the assumptions of Multiple Linear Regression. Before building a multiple linear regression model, we need to check that these assumptions below are valid.\n\n\n    Assumptions of Regression\n        \n            Linearity ( Correct functional form )\n            Homoscedasticity ( Constant Error Variance )( vs Heteroscedasticity ).\n            Independence of Errors ( vs Autocorrelation )\n            Multivariate Normality ( Normality of Errors )\n            No or little Multicollinearity.\n        \n\n   \nSince we fit a linear model, we assume that the relationship is **linear**, and the errors, or residuals, are pure random fluctuations around the true line. We expect that the variability in the response(dependent) variable doesn't increase as the value of the predictor(independent) increases, which is the assumptions of equal variance, also known as **Homoscedasticity**. We also assume that the observations are independent of one another(**No Multicollinearity**), and a correlation between sequential observations or auto-correlation is not there.\n\nNow, these assumptions are prone to happen altogether. In other words, if we see one of these assumptions in the dataset, it's more likely that we may come across with others mentioned above. Therefore, we can find and fix various assumptions with a few unique techniques.\n\nSo, **How do we check regression assumptions? We fit a regression line and look for the variability of the response data along the regression line.** Let's apply this to each one of them.\n\n**Linearity(Correct functional form):** \nLinear regression needs the relationship between each independent variable and the dependent variable to be linear. The linearity assumption can be tested with scatter plots. The following two examples depict two cases, where no or little linearity is present. "
"Here we are plotting our target variable with two independent variables **GrLivArea** and **MasVnrArea**. It's pretty apparent from the chart that there is a better linear relationship between **SalePrice** and **GrLivArea** than **SalePrice** and **MasVnrArea**. One thing to take note here, there are some outliers in the dataset. It is imperative to check for outliers since linear regression is sensitive to outlier effects. Sometimes we may be trying to fit a linear regression model when the data might not be so linear, or the function may need another degree of freedom to fit the data. In that case, we may need to change our function depending on the data to get the best possible fit. In addition to that, we can also check the residual plot, which tells us how is the error variance across the true line. Let's look at the residual plot for independent variable **GrLivArea** and our target variable **SalePrice **. "
"Here we are plotting our target variable with two independent variables **GrLivArea** and **MasVnrArea**. It's pretty apparent from the chart that there is a better linear relationship between **SalePrice** and **GrLivArea** than **SalePrice** and **MasVnrArea**. One thing to take note here, there are some outliers in the dataset. It is imperative to check for outliers since linear regression is sensitive to outlier effects. Sometimes we may be trying to fit a linear regression model when the data might not be so linear, or the function may need another degree of freedom to fit the data. In that case, we may need to change our function depending on the data to get the best possible fit. In addition to that, we can also check the residual plot, which tells us how is the error variance across the true line. Let's look at the residual plot for independent variable **GrLivArea** and our target variable **SalePrice **. "
"Ideally, if the assumptions are met, the residuals will be randomly scattered around the centerline of zero with no apparent pattern. The residual will look like an unstructured cloud of points centered around zero. However, our residual plot is anything but an unstructured cloud of points. Even though it seems like there is a linear relationship between the response variable and predictor variable, the residual plot looks more like a funnel. The error plot shows that as **GrLivArea** value increases, the variance also increases, which is the characteristics known as **Heteroscedasticity**. Let's break this down. \n\n**Homoscedasticity ( Constant Variance ):** \nThe assumption of Homoscedasticity is crucial to linear regression models. Homoscedasticity describes a situation in which the error term or variance or the ""noise"" or random disturbance in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable. In other words, there is a constant variance present in the response variable as the predictor variable increases. If the ""noise"" is not the same across the values of an independent variable like the residual plot above, we call that **Heteroscedasticity**. As you can tell, it is the opposite of **Homoscedasticity.**\n\n\n\nThis plot above is an excellent example of Homoscedasticity. As you can see, the residual variance is the same as the value of the predictor variable increases. One way to fix this Heteroscedasticity is by using a transformation method like log-transformation or box-cox transformation. We will do that later.\n\n**Multivariate Normality ( Normality of Errors):**\nThe linear regression analysis requires the dependent variable to be multivariate normally distributed. A histogram, box plot, or a Q-Q-Plot can check if the target variable is normally distributed. The goodness of fit test, e.g., the Kolmogorov-Smirnov test can check for normality in the dependent variable. We already know that our target variable does not follow a normal distribution. Let's bring back the three charts to show our target variable."
"Success!! As you can see, the log transformation removes the normality of errors, which solves most of the other errors we talked about above. Let's make a comparison of the pre-transformed and post-transformed state of residual plots. "
"Here, we see that the pre-transformed chart on the left has heteroscedasticity, and the post-transformed chart on the right has Homoscedasticity(almost an equal amount of variance across the zero lines). It looks like a blob of data points and doesn't seem to give away any relationships. That's the sort of relationship we would like to see to avoid some of these assumptions. \n\n**No or Little multicollinearity:** \nMulticollinearity is when there is a strong correlation between independent variables. Linear regression or multilinear regression requires independent variables to have little or no similar features. Multicollinearity can lead to a variety of problems, including:\n* The effect of predictor variables estimated by our regression will depend on what other variables are included in our model. \n* Predictors can have wildly different results depending on the observations in our sample, and small changes in samples can result in very different estimated effects. \n* With very high multicollinearity, the inverse matrix, the computer calculates may not be accurate. \n* We can no longer interpret a coefficient on a variable as the effect on the target of a one-unit increase in that variable holding the other variables constant. The reason behind that is, when predictors are strongly correlated, there is not a scenario in which one variable can change without a conditional change in another variable.\n\nHeatmap is an excellent way to identify whether there is multicollinearity or not. The best way to solve multicollinearity is to use regularization methods like Ridge or Lasso."
Outliers\n\nExtreme values affects the performance of a linear model. Let's find out if we have any in our variables. 
As you can see most of the continous variables seem to contaion outliers. We will get rid of some of these outliers in the feature engineering section. 
"# Modeling the Data\n \nBefore modeling each algorithm, I would like to discuss them for a better understanding. This way I would review what I know and at the same time help out the community. If you already know enough about Linear Regression, you may skip this part and go straight to the part where I fit the model. However, if you take your time to read this and other model description sections and let me know how I am doing, I would genuinely appreciate it. Let's get started. \n\n**Linear Regression**\n\n    We will start with one of the most basic but useful machine learning model, **Linear Regression**. However, do not let the simplicity of this model fool you, as Linear Regression is the base some of the most complex models out there. For the sake of understanding this model, we will use only two features, **SalePrice** and **GrLivArea**. Let's take a sample of the data and graph it."
"As we discussed before, there is a linear relationship between SalePrice and GrLivArea. We want to know/estimate/predict the sale price of a house based on the given area, How do we do that? One naive way is to find the average of all the house prices. Let's find a line with the average of all houses and place it in the scatter plot. Simple enough."
"As we discussed before, there is a linear relationship between SalePrice and GrLivArea. We want to know/estimate/predict the sale price of a house based on the given area, How do we do that? One naive way is to find the average of all the house prices. Let's find a line with the average of all houses and place it in the scatter plot. Simple enough."
You can tell this is not the most efficient way to estimate the price of houses. The average line clearly does not represent all the datapoint and fails to grasp the linear relationship between GrLivArea & SalePrice.  Let use one of the evaluation regression metrics and find out the Mean Squared Error(more on this later) of this line.
Now that we have our predicted y values let's see how the predicted regression line looks in the graph.
Phew!! This looks like something we can work with!! Let's find out the MSE for the regression line as well.
"A much-anticipated decrease in mean squared error(mse), therefore better-predicted model. The way we compare between the two predicted lines is by considering their errors. Let's put both of the model's side by side and compare the errors."
"On the two charts above, the left one is the average line, and the right one is the regression line. Blue dots are observed data points and red lines are error distance from each observed data points to model-predicted line. As you can see, the regression line reduces much of the errors; therefore, performs much better than average line. \n\nNow, we need to introduce a couple of evaluation metrics that will help us compare and contrast models. One of them is mean squared error(MSE) which we used while comparing two models. Some of the other metrics are...\n\n* RMSE (Root Mean Squared Error)\n### $$ \operatorname{RMSE}= \sqrt{\frac{1}{n}\sum_{i=1}^n(\hat{y_i} - y_i)^2} $$\n\nHere\n* $y_i$ = Each observed data point. \n* $\bar{y}$ = Mean of y value.\n* $\hat{y_i}$ = Predicted data point for each $x_i$ depending on i. \n\n\n* MSE(Mean Squared Error)\n### $$\operatorname{MSE}= \frac{1}{n}\sum_{i=1}^n(\hat{y_i} - y_i)^2$$\n\n* MAE (Mean Absolute Error)\n### $$\operatorname{MAE} = \frac{\sum_{i=1}^n|{\bar{y} - y_i}|}{n}$$\n\n* RSE (Relative Squared Error)\n### $$\operatorname{RSE}= \frac{\sum_{i=1}^n(\hat{y_i} - y_i)^2}{\sum_{i=1}^n(\bar{y} - y_i)^2}$$\n\n* RAE (Relative Absolute Error) \n### $$\operatorname{RAE}= \frac{\sum_{i=1}^n |\hat{y_i} - y_i|}{\sum_{i=1}^n |\bar{y} - y_i|}$$\n\n> and \n* $R^2$ (Coefficient of the determination)"
All you need to know when tackling Time Series Data!!\n\n\n\n  Table of Contents\n  Fetch the data1\n  Downcasting2\n  Melting the data3\n  Exploratory Data Analysis4 \n  Feature Engineering5\n  Modelling and Prediction6
# 1. Fetch the data
"Below plot shows how much effect downcasting has had on the memory usage of DataFrames. Clearly, we have been able to reduce `sales` & `prices` to less than 1/4th of their actual memory usage. `calendar` is already a small dataframe."
"# 3. Melting the data\nCurrently, the data is in three dataframes: `sales`, `prices` & `calendar`. The `sales` dataframe contains daily sales data with days(d_1 - d_1969) as columns. The `prices` dataframe contains items' price details and `calendar` contains data about the days d.  \n\n3.1 Convert from wide to long format\nHere's an example of conversion of a wide dataframe to a long dataframe.\n"
"# 4. Exploratory Data Analysis\n4.1 The Dataset\nThe M5 dataset, generously made available by Walmart, involves the unit sales of various products sold in the USA, organized in the form of grouped time series. More specifically, the dataset involves the unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated.  The products are sold across ten stores, located in three States (CA, TX, and WI).\n\nI have drawn an interactive visualization showing the distribution of 3049 items across different aggregation levels."
4.2 Item Prices\nHere I'll be studying about the item prices and their distribution. Please note the prices vary weekly. So to study the distribution of prices I have taken their average.
4.2 Item Prices\nHere I'll be studying about the item prices and their distribution. Please note the prices vary weekly. So to study the distribution of prices I have taken their average.
"Below are some of the observations from the above plot:-\n\n  The distribution of item prices is almost uniform for all the stores across Califoria, Texas and Wisconsin.\n  Item HOBBIES_1_361 priced at around 30.5 dollars is the costliest item being sold at walmarts across California.\n  Item HOUSEHOLD_1_060 priced at around 29.875 dollars is the costliest item being sold at walmarts across Texas.\n  Item HOBBIES_1_361 priced at around 30.48 dollars is the costliest item being sold at TX_1 and TX_3 in Texas. While item HOBBIES_1_255 priced at around 30.5 dollars is the costliest at TX_2\n"
"Below are some of the observations from the above plot:-\n\n  The distribution of item prices is almost uniform for all the stores across Califoria, Texas and Wisconsin.\n  Item HOBBIES_1_361 priced at around 30.5 dollars is the costliest item being sold at walmarts across California.\n  Item HOUSEHOLD_1_060 priced at around 29.875 dollars is the costliest item being sold at walmarts across Texas.\n  Item HOBBIES_1_361 priced at around 30.48 dollars is the costliest item being sold at TX_1 and TX_3 in Texas. While item HOBBIES_1_255 priced at around 30.5 dollars is the costliest at TX_2\n"
"As can be seen from the plot above, food category items are quite cheap as compared with hobbies and household items. Hobbies and household items have almost the same price range."
4.3 Items Sold\nLet's study the sales accross all the stores.
"Below are some of the observations from the above plot:-\n\n  California: CA_3 has sold the most number of items while, CA_4 has sold the least number of items.\n  Texas: TX_2 and **TX_3** have sold the maximum number of items. TX_1 has sold the least number of items.\n  Wisconsin: WI_2 has sold the maximum number of items while, WI_3 has sold the least number of items.\n  USA: CA_3 has sold the most number of items while, CA_4 has sold the least number of items.\n\n\n**Let's study number of items sold over time across all the stores.**"
"Below are some of the observations from the above plot:-\n\n  California: CA_3 has sold the most number of items while, CA_4 has sold the least number of items.\n  Texas: TX_2 and **TX_3** have sold the maximum number of items. TX_1 has sold the least number of items.\n  Wisconsin: WI_2 has sold the maximum number of items while, WI_3 has sold the least number of items.\n  USA: CA_3 has sold the most number of items while, CA_4 has sold the least number of items.\n\n\n**Let's study number of items sold over time across all the stores.**"
"4.4 State wise Analysis\n  California1\n  Texas2\n  Wisconsin3\n\nIn this section, I will be studying the sales and revenue of all the stores individually across all the three states: California, Texas & Wisconsin. I have plotted total three plots for each store: CA_1, CA_2, CA_3, CA_4, TX_1, TX_2, TX_3, WI_1, WI_2 & WI_3. Details about the plots are as follows:-\n- First plot shows the daily sales of a store. I have plotted the values separately for SNAP days. Also, SNAP promotes food purchase, I have plotted food sales as well to check if it really affects the food sales.\n- Second plot shows the daily revenue of a store with separate plotting for SNAP days.\n- Third is a heatmap to show daily sales. It's plotted in such a way that it becomes easier to see day wise values.\n\n\nWhat is SNAP?\nThe United States federal government provides a nutrition assistance benefit called the Supplement Nutrition Assistance Program (SNAP).  SNAP provides low income families and individuals with an Electronic Benefits Transfer debit card to purchase food products.  In many states, the monetary benefits are dispersed to people across 10 days of the month and on each of these days 1/10 of the people will receive the benefit on their card.  More information about the SNAP program can be found [here.](https://www.fns.usda.gov/snap/supplemental-nutrition-assistance-program)\n\n\n\nFor the heatmaps, the data is till 16th week of 2016 and datetime.weekofyear of function is returning 1,2 & 3 january of 2016 in 53rd week. Plotly's heatmap is connecting the data gap between the 16th and 53rd week. Still figuring out on how to remove this gap.\n"
**Plotting feature importances**
"**Make the submission**\n\nIf you remember for EDA, feature engineering and training I had melted the provided data from wide format to long format. Now, I have the predictions in long format but the format to be evaluated for the competition is in long format. Therefore, I'll convert it into wide format using `pivot` function in pandas. Below is an image explaining the pivot function.\n"
Churn is a one of the biggest problem in  the telecom industry. Research has shown that the average monthly churn rate among the top 4 wireless carriers in the US is 1.9% - 2%. 
**Let us read the data file in the python notebook**
1. **Gender Distribution** - About half of the customers in our data set are male while the other half are female
2. **% Senior Citizens** - There are only 16% of the customers who are senior citizens. Thus most of our customers in the data are younger people.\n
2. **% Senior Citizens** - There are only 16% of the customers who are senior citizens. Thus most of our customers in the data are younger people.\n
"3. **Partner and dependent status**  - About 50% of the customers have a partner, while only 30% of the total customers have dependents. "
"3. **Partner and dependent status**  - About 50% of the customers have a partner, while only 30% of the total customers have dependents. "
"**What would be interesting is to look at the % of customers, who have partners, also have dependents. We will explore this next. **"
"Interestingly,  among the customers who have a partner, only about half of them also have a dependent, while other half do not have any independents. \nAdditionally, as expected, among the customers who do not have any partner, a majority (80%) of them do not have any dependents ."
"I also looked at any differences between the % of customers with/without dependents and partners by gender. There is no difference in their distribution by gender. Additionally, there is no difference in senior citizen status by gender."
"**1. Tenure:**  After looking at the below histogram we can see that a lot of customers have been with the telecom company for just a month, while quite a many are there for about 72 months. This could be potentially because different customers have different contracts. Thus based on the contract they are into it could be more/less easier for the customers to stay/leave the telecom company."
"**2. Contracts:** To understand the above graph, lets first look at the # of customers by different contracts. "
"**2. Contracts:** To understand the above graph, lets first look at the # of customers by different contracts. "
As we can see from this graph most of the customers are in the month to month contract. While there are equal number of customers in the 1 year and 2 year contracts.\n\n
Below we will understand the tenure of customers based on their contract type.
"Interestingly most of the monthly contracts last for 1-2 months, while the 2 year contracts tend to last for about 70 months. This shows that the customers taking a longer contract are more loyal to the company and tend to stay with it for a longer period of time. \n\nThis is also what we saw in the earlier chart on correlation with the churn rate. "
We will observe that the total charges increases as the monthly bill for a customer increases.
"### E.) Finally, let's take a look at out predictor variable (Churn) and understand its interaction with other important variables as was found out in the correlation plot. "
1. Lets first look at the churn rate in our data
"In our data, 74% of the customers do not churn. Clearly the data is skewed as we would expect a large majority of the customers to not churn. This is important to keep in mind for our modelling as skeweness could lead to a lot of false negatives. We will see in the modelling section on how to avoid skewness in the data."
"**i.) Churn vs Tenure**: As we can see form the below plot, the customers who do not churn, they tend to stay for a longer tenure with the telecom company. "
"**ii.) Churn by Contract Type**: Similar to what we saw in the correlation plot, the customers who have a month to month contract have a very high churn rate."
"**ii.) Churn by Contract Type**: Similar to what we saw in the correlation plot, the customers who have a month to month contract have a very high churn rate."
**iii.) Churn by Seniority**: Senior Citizens have almost double the churn rate than younger population.
**iii.) Churn by Seniority**: Senior Citizens have almost double the churn rate than younger population.
**iv.) Churn by Monthly Charges**: Higher % of customers churn when the monthly charges are high.
**iv.) Churn by Monthly Charges**: Higher % of customers churn when the monthly charges are high.
**v.) Churn by Total Charges**: It seems that there is higer churn when the total charges are lower.
**v.) Churn by Total Charges**: It seems that there is higer churn when the total charges are lower.
"## After going through the above EDA we will develop some predictive models and compare them.\n\nWe will develop Logistic Regression, Random Forest, SVM, ADA Boost and XG Boost"
"Wth SVM I was able to increase the accuracy to upto 82%. However, we need to take a deeper look at the true positive and true negative rates, including the Area Under the Curve (AUC) for a better prediction. I will explore this soon. Stay Tuned!"
**4. ADA Boost**
"### Gender, age and country\n"
"Ouch ! The gender gap is huge! Unfortunately, this is common in the tech industry. Statistics show that** women hold only 25% of computing jobs**, which is already low but what we're having here is worse. 16.71% is too low, **there's 5 times as many male respondents as female respondents.**"
"Instances with 0, 5, 100 years old don't make much sens. Removing those instances here (we'll keep them later on as the age doesn't affect the other properties) would yield more significant results."
The age median is about 30 years old and most participants are between 25 and 37 years old.
The age median is about 30 years old and most participants are between 25 and 37 years old.
"Seems like most Kagglers are either Americans or Indians. More precisely,"
### Formal education and Major\n
"Nearly half of the kagglers who took this survey are Master's graduates, impressive.   \nWhat's more, 80.34% of respondents hold at least a bachelor degree. "
### How did you start learning Data Science ? \n 
"The most popular way to start learning Data Science / Machine Learning is Online Courses. In fact, I was expecting those results because anytime I ask someone *'Please where can I learn X efficiently'* (where X is related to Machine Learning), I end up with a MOOC recommendation.   \nKaggle has a very small proportion but that's natural, people usually come to kaggle to complete their learning once they've got the basics.\n\nPersonally, I started learning data science both at university and with online courses at the same time. Then, couple of months later, I discovered Kaggle and its kernels quickly became my favourite learning tool. So I perfectly identify with the results to this question ! "
"The most popular way to start learning Data Science / Machine Learning is Online Courses. In fact, I was expecting those results because anytime I ask someone *'Please where can I learn X efficiently'* (where X is related to Machine Learning), I end up with a MOOC recommendation.   \nKaggle has a very small proportion but that's natural, people usually come to kaggle to complete their learning once they've got the basics.\n\nPersonally, I started learning data science both at university and with online courses at the same time. Then, couple of months later, I discovered Kaggle and its kernels quickly became my favourite learning tool. So I perfectly identify with the results to this question ! "
### Employment status\n
"An interesting question was asked to kagglers in this survey : 'Are you actively looking to switch careers to data science?'.\nOnly 3012 respondents answered this question. My guess is that the other respondents are either already working as data scientists, in which case Yes/No would have no sens, or are still students."
"Without surprise, most of working kagglers who aren't yet working as data scientists would love to switch careers !\nThat being said, some of them don't which raises the question of the motive of using Kaggle for people who wouldn't want to work in Data Science.    \nThis question was included in the survey and it's one of the very few questions that only came as a free form question, i.e choices were not given.     \nIn the following, we're going to see which words were the most often used by non-switchers who answered this question. The following words *['data', 'science', 'mining', 'big', 'bigdata', 'machine', 'learning']* are deliberately plotted in shades of green.    \nLet's see what we've got ! \n> EDIT : Credits to @Chris Crawford who inspired me with his very nice goose-shaped WordCloud that you can check [here](http://www.kaggle.com/crawford/analyzing-the-analyzers)."
"Without surprise, most of working kagglers who aren't yet working as data scientists would love to switch careers !\nThat being said, some of them don't which raises the question of the motive of using Kaggle for people who wouldn't want to work in Data Science.    \nThis question was included in the survey and it's one of the very few questions that only came as a free form question, i.e choices were not given.     \nIn the following, we're going to see which words were the most often used by non-switchers who answered this question. The following words *['data', 'science', 'mining', 'big', 'bigdata', 'machine', 'learning']* are deliberately plotted in shades of green.    \nLet's see what we've got ! \n> EDIT : Credits to @Chris Crawford who inspired me with his very nice goose-shaped WordCloud that you can check [here](http://www.kaggle.com/crawford/analyzing-the-analyzers)."
"Without suprise, the chosen key words in green are among the most used in the answers that were given. Let's see how all of the words are linked : \n* **Curiosity and interest (and interesting) ** were frequently used, it suggests that some answers were 'Interest towards machine learning' for example.\n* **hobby and fun** were also frequently used, it suggests that some people practice data science and participate in competitions as a hobby, to have fun, not because they want to work in that field.\n* **challenge, project and competition** were popular too, and yeah kaggle is a pretty great platform for fierce competitors looking for challenging subjects :)"
### Industry and Job title \n
"As I said earlier, there are some cases where both plots (proportion and raw number of coders) can give valuable informations : this is one of those case and I kept both of them here on purpose, here's what we notice : \n\n**The plot on the left** shows that even if there are more Python users than R users in this survey as we saw earliers, we notice on this plot that there are industries where R is still dominant or as competitive as Python.\n\n* R fares as good as / better than Python in the following industries : Government, Insurance, Non-profit, Pharmaceutical, Retail and Marketing.\n* Python outdoes R, by a large margin, in the tech industry which is also the industry with the most respondents in this survey (which makes sense since Data Scientist is a tech job).\n\n**The plot on the right** shows that when it comes to industries, it's R who seems to be more versatile ! When we see the proportions, we notice that it's way more balanced for R than it is for Python."
"As I said earlier, there are some cases where both plots (proportion and raw number of coders) can give valuable informations : this is one of those case and I kept both of them here on purpose, here's what we notice : \n\n**The plot on the left** shows that even if there are more Python users than R users in this survey as we saw earliers, we notice on this plot that there are industries where R is still dominant or as competitive as Python.\n\n* R fares as good as / better than Python in the following industries : Government, Insurance, Non-profit, Pharmaceutical, Retail and Marketing.\n* Python outdoes R, by a large margin, in the tech industry which is also the industry with the most respondents in this survey (which makes sense since Data Scientist is a tech job).\n\n**The plot on the right** shows that when it comes to industries, it's R who seems to be more versatile ! When we see the proportions, we notice that it's way more balanced for R than it is for Python."
"* We notice that most of R coders and Python coders are data scientists, well the opposite would have been very surprising !     \n* A great proportion of Python coders (a cumulative 25%) are either Software Engineers or Machine Learning Engineers while only 5% of R coders occupy those two roles ! Seems that really few R practitioners are considered ML masters.\n* 30% of R coders are Statisticians, Data Analysts or Business Analysts while only 11% of Python coders have one these 3 job titles.\n\n**Except for the Data Scientist role (and Student / Researcher) it seems that the roles where we have the highest % of R coders are the roles where the % of Python coders is the lowest and vice versa !**\n"
"Ouf ! This confrontation between R and Python is really intense, let's take a quick break from it to get our break back by asking ourselves the following question : *How dependant is the job titile of the major of a worker*\n\nUp until now, we've been analyzing the dataset through visualizations and significant plots. I would like to introduce to beginners how we evaluate dependacy between two categorical variables in statistics. Ladies and gentlemen, let me introduce to you the one and only **chi2 test of independence**!\n\nI'd recommend reading this [this](http://stattrek.com/chi-square-test/independence.aspx?Tutorial=AP) for people who never heard about this test, it's a really simple explanation followed by an example of its use.\n\nLet's apply it here, first comes the contingency table : "
"**To be entirely accurate and honest, we shouldn't be applying the chi-2 test here because one of the hypothesis is that all values should be higher than 5. Here we notice for example that we have 0 computer scientist that had Psychology as a Major.**       \nThat being said, my goal here is to show you guys how to conduct such a test during your future analysis so we'll let that go, no offense to our beloved rigourous statisticians :)      \nSo now the chi-2 test : "
### Main function and percentage of time for specific tasks  \n
"It seems that most R users tend to perform tasks related to **business /data analytics** part. Let's recall what we found earliers : More than 70% of R coders use data visualization (ML Methods) and more than 20% of them are either business analysts or data analysts (Role). Thus, it makes sense to see here that most of R users' role at work is to analyze data for business purpose.                                                                                     \nOn the other hand, Python users are doing a bit of everything and we find pretty high percentages for three tasks : Business analytisc, building ML prototypes and build ML services that improve the product.\n\nSo what's the volume of work for each task (gathering data, visualizing it ...) in a DS job ? \n"
"It seems that most R users tend to perform tasks related to **business /data analytics** part. Let's recall what we found earliers : More than 70% of R coders use data visualization (ML Methods) and more than 20% of them are either business analysts or data analysts (Role). Thus, it makes sense to see here that most of R users' role at work is to analyze data for business purpose.                                                                                     \nOn the other hand, Python users are doing a bit of everything and we find pretty high percentages for three tasks : Business analytisc, building ML prototypes and build ML services that improve the product.\n\nSo what's the volume of work for each task (gathering data, visualizing it ...) in a DS job ? \n"
"Both type of coders spend most of their time gathering data (38% for Python users, 40% for R) and building models (19% for Python, 18% for R).                            \nGenerally, both type of coders seem to invest the same time for nearly all tasks. For me, this means that using R or Python is more about a preference than an obligation towards some specific usage.     \n\nThat being said, **the biggest difference observed comes for putting work into production (12% for Python users, 7% for R)**. I remember the first time I asked my manager during my internship why do we use Python rather than R and he simply replied *'We always want to put our models into production and doing that using R can really be a pain in the ass'*.         \nI've personnaly never used R production-wise so I can't relate, but I guess that these statistics support my manager's words ! \n\n> EDIT : I actually recalled that there was a specific question in the survey for workers which  was *At work, how often do the models you build get put into production?* and decided to delve deeper into this aspect.       \nPossible answers were frequencies so I tried to check the % of each frequency for Python (resp. R) users and compare between the percentages for both communities."
### Experience as code-writers \n
"Here's what we observe :\n1. The most voted duration of coding, for both R and Python coders, is 3-5 years.\n2. For Python coders, the second most voted duration is 1-2 years while for R coders, the second most voted duration is actually 'More than 10 years' !\n\nWe conclude that R coders tend to be more experienced as they've been writing code for a longer period of time whil Python started to be more widely used during the 5 last years !"
"The distribution shows that most salaries lie between 50k and 140k USD. According to Glassdoor, the average annual salary for a data scientist is 128k$ so it's coherent with what we've just got here.   \n\nSeaborn's *'distplot'* fits a univariate distribution using kernel density estimation [KDE](http://en.wikipedia.org/wiki/Kernel_density_estimation). We notice with the bins that most kagglers have an income between 70k and 130k and that the fitted distribution is skewed right which means there's much more outliers towards the right (unusually high incomes) than towards the left.\n\nHere's a more sophisticated plot for the distribution of the annual income."
### Salary VS Gender\n
### Salary VS Gender\n
"It seems that the salary gap between the two genders isn't too big but is still in favour of men.       \nThe average for male kagglers is a bit higher than the average for female kagglers.         \nThat being said, there's no woman with an income of 400k or higher while there are some outliers in the men part."
### Salary VS Formal Education\n
"Let's recall what the boxes mean in seaborn's boxplot, the documentation says : *The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, except for points that are determined to be “outliers”*.    \n\nThe median follows a reasonable trend :** the higher the education, the higher the median annual income** except for doctoral education that is shadowed by professional degrees. That being said, the PhD box contain outliers + its whiskers are more extended than those of the Professional degree box so all in all, doctors are the best paid kagglers.  \n\nThe median for people who attended college but hold no degree is higher than the median for Bachelors and Masters holders BUT the first quartile (Q1) of first community is way smaller than the first quartile of the other two communities.   \nSo looking at the turquoise blue box and comparing it with the light pink and mustard yellow boxes, we notice that the majority of people with a professional degree are between Q1 and the median while the majority of Bachelors and Masters holders are between the median and Q3."
### Salary VS Job Title\n
"People labeled as **Machine Learning Engineers or Data Scientst have an annual average income higher than Data Analysts, Business Analysts, Statisticians or Programmers. **    \nOne should be aware of the job's title when looking for work because the salaries seem to be really different even if many resepondents  identify as data scientists ! "
"That's really good scores considering that : \n* Logistic regression is the easiest and most simple classifier to build and understand\n* We only selected 18 demographic features and one hot encoded them\n\n**So with just basic preprocessing and a basic model, we get 68% precision and 60% recall !**         \nAccuracy (#correct outputs / #total outputs) is not so bad too but that's expected since the dataset is a bit imbalanced.\nTo learn more about precision, recall and why those two measures are more accurate than accuracy (see what I did there?), check the following [link](http://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/).\n\nLet's recall that logistic regression outputs probabilities. A treshold (0.5 by default) is used to binarize the output, i.e if probability>0.5, predict positive class, else predict negative class.     \nAs the dataset is imbalanced, maybe reducing the threshold can yield better result because it's less probable to have a very high income. That's why we plot the [ROC curve](http://www.quora.com/Whats-ROC-curve) which basically helps to observe the trade-off between precision and recall when reducing/increasing the threshold, so here we go !"
"The AUC score isn't very high, that suggest that modifying the threshold wouldn't bring a big improvement to the model.   \n\nThat's it for the ML model part! I hope this can help beginners in DS to gain more experience and informations and I would encourage you to copy the same preprocessing and use more sophisticated models like SVM or Tree based models (Random Forest / Gradient boosting)."
"First thing first, let's see which countries has the most learners, check their age and the gender distribution amongst them."
"We notice that there's a little progress compared to the general proportions (16.71% Female and 81.88% Male). This means that there are more and more women involved and interested in learning Data Science, that's good news."
"We notice that some of the highest proportions of learners are found in Nigeria, Egypt, Kenia and Indonesia. Those are countries where data science is at its very start and it finding as much learners as practitioners make sens. In the other hand, only 26% of american kagglers are still in the learning phase.\n\nWhat I found more surprising was the proportion of learners in China (47.34%). I would have guessed that the proportion of learners in China is similar to the one of the US but that's not true at all according to the survey responses.\n\nLet's see what's the formal education of our learners."
"Most learners hold a bachelor degree (earlier, for all respondents, the most frequent education was Master level). That's primarly because amongst learners we find students who are yet to finish their studies.\n\nLet's now see what platforms are these learners using in their quest to becoming data scientists."
### Platforms used for learning \n
"Interesting!    \nKaggle and Online courses seem to be the favorite platforms for data science learners. \nAt first,** I was a bit surprised when I saw that College/University comes as the 7th most used platform** but then I remembered that : \n- learners include professionals who are looking to career switch\n- learners' median age is 26.\n\nLet's check the frequency of use for younger learners (younger than 23) : "
"Interesting!    \nKaggle and Online courses seem to be the favorite platforms for data science learners. \nAt first,** I was a bit surprised when I saw that College/University comes as the 7th most used platform** but then I remembered that : \n- learners include professionals who are looking to career switch\n- learners' median age is 26.\n\nLet's check the frequency of use for younger learners (younger than 23) : "
"Hum, there's some progress (5th instead of 7th) but nothing flagrant. Let's check young kagglers leaving in the US : "
"Hum, there's some progress (5th instead of 7th) but nothing flagrant. Let's check young kagglers leaving in the US : "
"**Aha ! College / University becomes the second most used platform and it isn't anymore outnumbered by Kaggle.  **\n\nThe thing is in several countries, universities didn't have and still don't have specific data science training. Often, programs are either focused on mathematics or on computer science so a student must complete his training alone. In the US, more and more universities (MIT, Columbia, Stanford...) offer specific data science masters or at least machine learning courses for students who are interested.\n\nThis is why generally, in most countries, students feel like they learn about data science a lot more outside their college.\n\nLet's now see how useful are these same platforms for learners."
"**Aha ! College / University becomes the second most used platform and it isn't anymore outnumbered by Kaggle.  **\n\nThe thing is in several countries, universities didn't have and still don't have specific data science training. Often, programs are either focused on mathematics or on computer science so a student must complete his training alone. In the US, more and more universities (MIT, Columbia, Stanford...) offer specific data science masters or at least machine learning courses for students who are interested.\n\nThis is why generally, in most countries, students feel like they learn about data science a lot more outside their college.\n\nLet's now see how useful are these same platforms for learners."
"We've seen that Kaggle and Coursera were the most used platforms for DS learning. We notice with this plot that they're also the platforms that are considered to be the most useful amongst learners and they're far ahead of the third most useful platform (Projects).     \nWe also notice that the relative usefulness of a platform is strongly related to the number of learners using that same platform. Indeed, the most used platforms are the most useful according to the last 2 plots."
Let's see which online platform is the most popular amongst data science learners.
"Coursera has a clear edge over Udacity, edX and DataCamp. This must have something to do with Andrew NG's famous course who helped introducing Machine Learning to a huge amount of people.\n\nLet's now see what the learners are looking forward to learn next year ! "
### Method and Tool to learn next year\n
"At first, I drew the two plots separately but when I saw the results I was like no way, I'm going to put one next to the other.\nThe deep learning hype is real ! Methods Kagglers want to learn most are Deep Learning (I consider Neural nets to be included in Deep Learning) by a really big margin. And when we review the tools, well the results are quite the same since people want to master TensorFlow which is the most used tool for deep learning !\n\nLet's now see how much time kagglers learners spend on DS learning on those platforms and for how many years have they been learning DS/ML."
### Time invested on DS training \n
"Most learners spend between 2 and 10 hours a week learning data science, which is kind of the amount you would spend if your learning consists of one Online Course at a time.  \nThat being said, nearly 1000 learners (from a total of 5494) spend more than 11 hours a week. Those are probably students enrolled in data science program or learners fully dedicated to DS learning which is not the case for workers looking for a career switch who naturally have less time at their disposal to invest in DS learning.\n\nLet's see for how many years kagglers have been learning data science."
"Most learners spend between 2 and 10 hours a week learning data science, which is kind of the amount you would spend if your learning consists of one Online Course at a time.  \nThat being said, nearly 1000 learners (from a total of 5494) spend more than 11 hours a week. Those are probably students enrolled in data science program or learners fully dedicated to DS learning which is not the case for workers looking for a career switch who naturally have less time at their disposal to invest in DS learning.\n\nLet's see for how many years kagglers have been learning data science."
"84.1% of learners started their DS training at most 2 years ago when only 3.5% of started more than 5 years ago !        \nThis shows the effect of the hype around Data Science over the last few years and also its exponential growth.\n\nWhat comes next focuses on how learners perceive the professional aspect of Data Science, starting with ranking skills according to necessity and ranking proofs of DS knowledge."
### Skills and proofs of knowledge for landing a DS job \n
"For Data Science learners, there's no doubt, mastering Python is the most important skill for a job in Data Science ! (we could add that in our Python VS R duel).\nMost learners think that a degree is more a nice asset to have than a necessary one but I'm not so sure about that. \n\nWhat about ways you can prove your knowledge of ML/DS? \n"
"For Data Science learners, there's no doubt, mastering Python is the most important skill for a job in Data Science ! (we could add that in our Python VS R duel).\nMost learners think that a degree is more a nice asset to have than a necessary one but I'm not so sure about that. \n\nWhat about ways you can prove your knowledge of ML/DS? \n"
"Learners were asked *What's the most important way you can prove your knowledge of ML/DS?*. \n\nMaster's degree and PhD aren't well ranked, that's expected since most learners consider a that a degree is not necessary to land a job in DS. That being said, I wouldn't agree with the fact than MOOCs are more important that a Master's degree and I don't think that companies value Online Courses certificates more than a Master in Mathematics / Computer science or any related field.\n\nPrior work experience in ML comes in first position and I have agree with that. Hands-on experience is always valuable for recruiters, especially when it comes to coding stuff.\n"
### Learners' job hunt \n
"So the top-3 resources according to learners are :\n1. Companies' job listing pages,\n2. Tech-Specific job boards (Stack Overflow recruitment platform for example)\n3. General job boards (LinkedIn)\n\nNow to the time spent looking for a job."
"So the top-3 resources according to learners are :\n1. Companies' job listing pages,\n2. Tech-Specific job boards (Stack Overflow recruitment platform for example)\n3. General job boards (LinkedIn)\n\nNow to the time spent looking for a job."
"* 40.2% of learners aren't actually looking for a job. Let's not forget that some are still enrolled in College/University and others (workers who answered 'yes' for career switch) may be looking to switch position within the company they work for.   \n* 34.5% spend an hour or two a week looking for a job, which means that there's no urge to look for a job. It may be students looking for internships or employed people checking from time to time if an exciting opportunity is out there somewhere.\n* A little more than 10% are actively working for a job, spending at least 6 hours per week job hunting."
"* 40.2% of learners aren't actually looking for a job. Let's not forget that some are still enrolled in College/University and others (workers who answered 'yes' for career switch) may be looking to switch position within the company they work for.   \n* 34.5% spend an hour or two a week looking for a job, which means that there's no urge to look for a job. It may be students looking for internships or employed people checking from time to time if an exciting opportunity is out there somewhere.\n* A little more than 10% are actively working for a job, spending at least 6 hours per week job hunting."
"**In this plot, the factors are ordered according to the criteria 'Most Important'.**\n\n1. DS learners are constantly striving for development ! The most important factor during their job hunt, by far, is whether the job would hand them opportunities of professional development.   \n2. The second most important factor is the office environment they would be working in.    \n3. The third one is the programming languages and frameworks they'd be working with. That shows that data scientist aren't really open to work with whatever technology the company is using, they have strong preferences that are crucial for chosing one job over another.  \n4. The salary comes fourth and is close to other factors so it doesn't seem to be too problematic for aspiring data scientists, maybe that's because they're already quite assured that the salary would be great !"
### Visualize decision-trees
### Visualize decision-trees with graphviz
### Visualize decision-trees
### Visualize decision-trees with graphviz
"We can combine all the list of words (stopwords, frequent words and rare words) and create a single list to remove them at once.\n\n## Stemming\n\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (From [Wikipedia](https://en.wikipedia.org/wiki/Stemming))\n\nFor example, if there are two words in the corpus `walks` and `walking`, then stemming will stem the suffix to make them `walk`. But say in another example, we have two words `console` and `consoling`, the stemmer will remove the suffix and make them `consol` which is not a proper english word.\n\nThere are several type of stemming algorithms available and one of the famous one is porter stemmer which is widely used. We can use nltk package for the same."
"We can see that words like `private` and `propose` have their `e` at the end chopped off due to stemming. This is not intented. What can we do fort hat? We can use Lemmatization in such cases.\n\nAlso this porter stemmer is for English language. If we are working with other languages, we can use snowball stemmer. The supported languages for snowball stemmer are"
**Importing the libraries**
## Some dogs\n\n> The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world.
## Some dogs\n\n> The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world.
"# Insights\n\nCheck this posts:\n\n>[Quick data explanation and EDA](https://www.kaggle.com/witold1/quick-data-explanation-and-eda) by @witold1\n\n> [New Insights](https://www.kaggle.com/c/generative-dog-images/discussion/97863#latest-564673)\n\n- There are pictures with more than one dog (even with  3  dogs);\n- There are pictures with the dog (-s) and person (people);\n- There are pictures with more than one person (even with  4  people);\n- There are pictures where dogs occupy less than  1/5  of the picture;\n- There are pictures with text (magazine covers, from dog shows, memes and pictures with text);\n- Even wild predators included, e.g. African wild dog or Dingo, but not wolves.\n\n**Examples**\n\n\n\n\n\n\n"
"This doesn't run because EPOCH = 0, change it and try ;)"
# Best public training\n- 06/29 [RaLSGAN dogs](https://www.kaggle.com/speedwagon/ralsgan-dogs) V9\n- 06/29 this kernel V5\n- some version of this kernel
**Plot Loss per EPOCH**\n> plot_loss()
**Show generated images**\n> show_generated_img()
**Show generated images**\n> show_generated_img()
### Training Loop
"# Outline\n1. Stochastic gradient descent and online learning\n    - 1.1. SGD\n    - 1.2. Online approach to learning\n2. Categorical data processing: Label Encoding, One-Hot Encoding, Hashing trick\n    - 2.1. Label Encoding\n    - 2.2. One-Hot Encoding\n    - 2.3. Hashing trick\n3. Vowpal Wabbit\n    - 3.1. News. Binary classification\n    - 3.2. News. Multiclass classification\n    - 3.3. IMDB reviews\n    - 3.4. Classifying gigabytes of StackOverflow questions\n4. VW and Spooky Author Identification "
"# 1. Stochastic gradient descent and online learning\n##  1.1. Stochastic gradient descent\n\nDespite the fact that gradient descent is one of the first things learned in machine learning and optimization courses, it is hard to overrate one of it's modifications, namely, Stochastic Gradient Descent (SGD).\n\nLets recap that the very idea of gradient descent is to minimize some function by making small steps in the direction of fastest function decreasing. The method was named due to the following fact from calculus: vector $\nabla f = (\frac{\partial f}{\partial x_1}, \ldots \frac{\partial f}{\partial x_n})^T$ of partial derivatives of the function $f(x) = f(x_1, \ldots x_n)$ points to the direction of the fastest function growth. It means that by moving in the opposite direction (antigradient) it is possible to decrease the function value with the fastest rate.\n\n\n\nHere is a snowboarder (me) in Sheregesh, Russian most popular winter resort. I highly recommended it if you like skiing or snowboarding. We place this picture not only for a good view but also for picturing the idea of gradient descent. If you have an aim to ride as fast as possible, you need to choose the way with steepest descent (as long as you stay alive). Calculating antigradient can be seen as evaluating the slope in each particular point."
Lets look at the first document from this collection:
"Now we convert the data into something Vowpal Wabbit can understand, and we throw away words shorter than of 3 symbols. Here we skip many important NLP stages (stemming and lemmatization, for example), however, we will later see that VW solves the problem even without these steps."
"Now we load predictions, compute AUC and plot the ROC curve:"
AUC value we get states that here we've achieved high classification quality.
"The `boosting_type` and `is_unbalance` domains are pretty simple because these are categorical variables. For the hyperparameters that must be integers (`num_leaves`, `min_child_samples`), we use `range(start, stop, [step])` which returns a range of numbers from start to stop spaced by step (or 1 if not specified). `range` always returns integers, which means that if we want evenly spaced values that can be fractions, we need to use `np.linspace(start, stop, [num])`.  This works the same way except the third argument is the number of values (by default 100).\n\nFinally, `np.logspace(start, stop, [num = 100], [base = 10.0])` returns values evenly spaced on a logarithmic scale. According to the [the docs](https://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html) ""In linear space, the sequence starts at $base^{start}$ (base to the power of start) and ends with $base ^{stop}$ "" This is useful for values that differ over several orders of magnitude such as the learning rate."
"### Learning Rate Domain\n\nThe learning rate domain is from 0.005 to 0.5. Using a logarithmic uniform distribution allows us to create a domain where there are as many values from 0.005 to 0.05 as from 0.05 to 0.5. In a linear space, there would be far more values from 0.05 to 0.5 because this represents a larger distance in linear space but in logarithmic space each of these two intervals is the same width because they are multiples of 10 of each other. (Think about going from 1 to 10 and then from 10 to 100. On a logarithmic scale, these intervals are the same size, but on a linear scale the latter is 10 times the size of the former). In other words, a logarithmic uniform distribution lets us sample more evenly from a domain that varies over several orders of magnitude. \n\nIf that's a little confusing, perhaps the graph above makes it clearer. We can also do a sanity check to make sure the spacing is correct by counting the number of values in each interval."
"As an example of a simple domain, the `num_leaves` is a uniform distribution. This means values are evenly spaced on a linear scale."
"# Algorithm for selecting next values\n\nAlthough we don't generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random. \n\nWe will implement these algorithms very shortly, as soon as we cover the final part of hyperparameter tuning."
"The boosting type should be evenly distributed for random search. \n\nAgain, we have to remake this chart in seaborn to have the visualization appear in the rendered notebook (if anyone knows how to address this issue, please tell me in the comments!)"
"Next, for the numeric hyperparameters, we will plot both the sampling distribution (the hyperparameter grid) and the results from random search in a kernel density estimate (KDE) plot. (The grid search results are completely uninteresting). As random search is just drawing random values, we would expect the random search distribution to align with the sampling grid (although it won't be perfectly aligned because of the limited number of searches). \n\nAs an example, below we plot the distribution of learning rates from both the sampling distribution and the random search results. The vertical dashed line indicates the optimal value found from random search."
The following code repeats this plot for all the of the numeric hyperparameters. 
"## Sequence of Search Values\n\nFinally, we can plot the sequence of search values against the iteration for random search. Clearly there will not be any order, but this can let us visualize what happens in a random search!\n\nThe star indicates the best value of the hyperparameter that was found."
"As a final plot, we can show the score versus the value of each hyperparameter. We need to keep in mind that the hyperparameters are not changed one at a time, so if there are relationships between the values and the score, they do not mean that particular hyperparameter is influencing the score. However, we might be able to identify values of hyperparameters that seem more promising. Mostly these plots are for my own interest, to see if there are any trends! "
"We want to avoid placing too much emphasis on any of these relationships because we were not changing one hyperparameter at a time (although we could carry out experiments where we only change one hyperparameter and observes the effects on the score) and so the trends are not due solely to the single hyperparameter we show. If we could plot this in higher dimensions, it might be interesting to see if there are more promising regions of the search space but here we are limited to one dimension (a single hyperparameter versus the score).  If we want to observe the effects of one hyperparameter on the cross validation score, we could alter only that hyperparameter while holding all the others constant. However, the hyperparameters do not act by themselves and there are complex interactions between the model settings."
"**Age, Fare and cabin has missing values.\nwe will see how to fill missing values next.**"
**Visualizations**\n==============
**Visualizations**\n==============
**we can see that Age and Fare are measured on very different scaling. So we need to do feature scaling before predictions.**
"**Looks like Pclass has got highest negative correlation with ""Survived"" followed by Fare, Parch and Age** "
"Missing Value Imputation\n========================\n\n**Its important to fill missing values, because some machine learning algorithms can't accept them eg SVM.**\n\n*But filling missing values with mean/median/mode is also a prediction which may not be 100% accurate, instead you can use models like Decision Trees and Random Forest which handle missing values very well.*"
***Do you have longer names?***
***Whats in the name?***
Important features\n==================
*Gradient Boosting*\n-------------------
## 5.1 Distribution of AMT_CREDIT
## 5.2 Distribution of AMT_INCOME_TOTAL
## 5.2 Distribution of AMT_INCOME_TOTAL
## 5.3 Distribution of AMT_GOODS_PRICE
## 5.3 Distribution of AMT_GOODS_PRICE
## 5.4 Who accompanied client when applying for the  application
## 5.4 Who accompanied client when applying for the  application
## 5.5 Data is balanced or imbalanced
## 5.14.1 Income sources of Applicant's in terms of loan is repayed or not in %
## 5.14.2 Family Status of Applicant's in terms of loan is repayed or not in %
## 5.14.2 Family Status of Applicant's in terms of loan is repayed or not in %
## 5.14.3 Occupation of Applicant's in terms of loan is repayed or not in %
## 5.14.3 Occupation of Applicant's in terms of loan is repayed or not in %
## 5.14.4 Education of Applicant's in terms of loan is repayed or not in %
## 5.14.4 Education of Applicant's in terms of loan is repayed or not in %
## 5.14.5 For which types of house higher applicant's applied for loan in terms of loan is repayed or not in %
## 5.14.5 For which types of house higher applicant's applied for loan in terms of loan is repayed or not in %
## 5.14.6 Types of Organizations in terms of loan is repayed or not in %
## 5.14.6 Types of Organizations in terms of loan is repayed or not in %
## 5.14.7 Distribution of Name of type of the Suite in terms of loan is repayed or not in %
## 5.14.7 Distribution of Name of type of the Suite in terms of loan is repayed or not in %
# 5.15 Exploartion of previous application data
## 5.15.2 On which day highest number of clients applied in prevoies application
"* What a coincedence, Approximately 15 % clients applied in each 5 days a week i.e, Tuesday, Wednesday, Monday, Friday and Thrusday. "
## 5.15.3 Purpose of cash loan in previous application
* **Main purpose of the cash loan was  :**\n  * XAP - 55 %\n  * XNA - 41 %
## 5.15.7 Who accompanied client when applying for the previous application
"* **Who accompanied client when applying for the previous application :**\n  * Unaccompanied : Approx. 60 % times\n  * Family : Approx. 25 % times\n  * Spouse, Partner : Approx. 8 %\n  * Childrens : Approx. 4 %"
## 5.15.9 What kind of goods did the client apply for in the previous application
"## 5.15.10 Was the previous application for CASH, POS, CAR, …"
## 5.15.12 Top channels  through which they acquired the client on the previous application
* **Top channels  through which they acquired the client on the previous application :**\n  * Credidit and cash offices : 43 % times\n  * Country_wide : 30 % times\n  * Stone : 13 % times
## 5.15.13 Top industry of the seller
## 5.15.14 Grouped interest rate into small medium and high of the previous application
## 5.15.14 Grouped interest rate into small medium and high of the previous application
## 5.15.15 Top Detailed product combination of the previous application
# 6. Pearson Correlation of features
# 7. Feature Importance using Random forest
"The confusion matrix shows `5999 + 1897 = 7896 correct predictions` and `1408 + 465 = 1873 incorrect predictions`.\n\n\nIn this case, we have\n\n\n- `True Positives` (Actual Positive:1 and Predict Positive:1) - 5999\n\n\n- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 1897\n\n\n- `False Positives` (Actual Negative:0 but Predict Positive:1) - 1408 `(Type I error)`\n\n\n- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 465 `(Type II error)`"
# **16. Classification metrices** \n\n[Table of Contents](#0.1)
"# **18. ROC - AUC** \n\n[Table of Contents](#0.1)\n\n\n\n### ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of `TP to (TP + FN)`.\n\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of `FP to (FP + TN)`.\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n"
ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.
___\n* _How many orders we have for each status?_\n___
"By the time this dataset was created, the highest amount of orders went from delivered ones. Only 3% of all orders came from the other status."
"So now we can purpose a complete analysis on orders amount of brazilian e-commerce during the period of the dataset. For that let's plot three graphs using a `GridSpec` with the aim answear the following questions:\n\n    1. Is there any growing trend on brazilian e-commerce?\n    2. On what day of week brazilians customers tend to do online purchasing?\n    3. What time brazilians customers tend do buy (Dawn, Morning, Afternoon or Night)?"
"By the chart above we can conclude:\n\n* E-commerce on Brazil really has a growing trend along the time. We can see some seasonality with peaks at specific months, but in general we can see clear that customers are more prone to buy things online than before.\n* Monday are the prefered day for brazilian's customers and they tend to buy more at afternoons.\n\n_Obs: we have a sharp decrease between August 2018 and September 2018 and maybe the origin of that is related to noise on data. For further comparison between 2017 and 2018, let's just consider orders between January and August in both years_"
___\n* _E-commerce: a comparison between 2017 and 2018_\n___
\n3.2 E-Commerce Around Brazil\n\nGo to TOC
___\n* _How customers are distributed in Brazil? (a 30k orders sample from 2018 in a map)_\n___
___\n* _**HeatMaps:** a good view to see where are the core of brazilian e-commerce customers_\n___
"By the map we showed above, we have already the insight that the southeast of Brazil has the highest number of orders given through e-commerce. So, let's see it in a HeatMap!"
"**Nice!** Another good view is to use the folium plugin _[HeatMapWithTime](https://nbviewer.jupyter.org/github/python-visualization/folium/blob/master/examples/HeatMapWithTime.ipynb)_ to see the evolution of e-commerce orders among time.\n\nFor [limitations](https://github.com/python-visualization/folium/issues/859) purpose (i.e. jupyter and Chrome limitations for total number of points shown at HeatMapWithTime, we will show the evolution of orders from January 2018 to July 2018)\n\nAlso, it's possible that the plugin HeatMapWithTime doesn't work properly from a [issue](https://github.com/python-visualization/folium/issues/1221) fixed on version 0.11 (it's seems that the version of the kernel is 0.10). It it is the case for you, just updating the version of folium library would fix it."
"**Nice!** Another good view is to use the folium plugin _[HeatMapWithTime](https://nbviewer.jupyter.org/github/python-visualization/folium/blob/master/examples/HeatMapWithTime.ipynb)_ to see the evolution of e-commerce orders among time.\n\nFor [limitations](https://github.com/python-visualization/folium/issues/859) purpose (i.e. jupyter and Chrome limitations for total number of points shown at HeatMapWithTime, we will show the evolution of orders from January 2018 to July 2018)\n\nAlso, it's possible that the plugin HeatMapWithTime doesn't work properly from a [issue](https://github.com/python-visualization/folium/issues/1221) fixed on version 0.11 (it's seems that the version of the kernel is 0.10). It it is the case for you, just updating the version of folium library would fix it."
\n3.3 E-Commerce Impact on Economy\n\nGo to TOC
"One of the datasets provided have informations about order's payment. To see how payments can take influence on e-commerce, we can build a mini-dashboard with main concepts: `payments type` and `payments installments`. The idea is to present enough information to clarify how e-commerce buyers usually prefer to pay orders."
"In fact, we can see by the line chart that payments made by credit card really took marjority place on brazilian e-commerce. Besides that, since 201803 it's possible to see a little decrease on this type of payment. By the other side, payments made by debit card is showing a growing trend since 201805, wich is a good opportunity for investor to improve services for payments like this.\n\nOn the bar chart above, we can see how brazilian customers prefer to pay the orders: mostly of them pay once into 1 installment and it's worth to point out the quantity of payments done by 10 installments."
"For training a sentimental analysis model, we must need the label to apply in a supervisioned Machine Learning approach. The dataset we doesn't have a clearly label saying wich comment is positive or negative. For doing that, probably the best approach is to look at individual comments and label it handly with 1 (positive comment) and 0 (negative comment) but, thinking in a fast implementation, we will use the `review_score` column to label our data into those two classes. Let's take a look."
"In this approach, let's consider that every comment with scores 1, 2 and 3 are negative comments. In the other hand, comments with score 4 and 5 will be considered as positive. Again, probably this is not the best way to train a sentimental analysis model, but for fastness, we will do this assumption and see if we can extract value from it."
"In this approach, let's consider that every comment with scores 1, 2 and 3 are negative comments. In the other hand, comments with score 4 and 5 will be considered as positive. Again, probably this is not the best way to train a sentimental analysis model, but for fastness, we will do this assumption and see if we can extract value from it."
___\n* _What's the main n-grams presentes in corpus on positive and negative classes?_\n___
"By the end, let's plot a WordCloud for positive and negative words on our dataset."
\n7. Conclusion\n\nGo to TOC
## Q1 & Q2. Age/Gender Distribution
You can see that more than half of the total is under 30.
You can see that more than half of the total is under 30.
"Obviously, I felt that there was a lot of influx of women, but I can see that there are still few."
## Q4 & Q6. Degree / Experience\n 
"I thought there would be the most bachelors, but there are the most masters."
"I thought there would be the most bachelors, but there are the most masters."
"About 75% have less than 5 years of experience and 25% have more than 5 years.\n\nThis time, let's look at the two questions at once."
"About 75% have less than 5 years of experience and 25% have more than 5 years.\n\nThis time, let's look at the two questions at once."
The most common is a 1-2 year bachelor's degree and a 3-5 year graduate.
## Q14. Visualization Library 
"1. The most basic library, matplotlib, is used the most.\n2. Plotly is the most used for interactive visualization.\n3. It is unfortunate that the number of geovisualization libraries is insufficient. \n    - Folium is still less used than Geoplotlib.\n4. There are more d3 users than I think. Is there any reason to use d3 even though it is ML unfriendly?"
"- UK, USA, Germany"
### Please be sure to leave a reference when using code or taking ideas in my visualizations.
"## **Feature Importance**\n\n- Decision Trees models which are based on ensembles (eg. Extra Trees and Random Forest) can be used to rank the importance of the different features. Knowing which features our model is giving most importance can be of vital importance to understand how our model is making it’s predictions (therefore making it more explainable). At the same time, we can get rid of the features which do not bring any benefit to our model."
"- Now we know which features are most important in the Random Forest model, we can train our model just using these features. \n\n- I have implemented this in the kernel - [Random Forest Classifier + Feature Importance : Section 15 - Build the Random Forest model on selected features](https://www.kaggle.com/prashant111/random-forest-classifier-feature-importance). It resulted in improved accuracy."
" \n# INTRODUCTION\n* **Deep learning:** One of the machine learning technique that learns features directly from data. \n* **Why deep learning:** When the amounth of data is increased, machine learning techniques are insufficient in terms of performance and deep learning gives better performance like accuracy.\n\n* **What is amounth of big:** It is hard to answer but intuitively 1 million sample is enough to say ""big amounth of data""\n* **Usage fields of deep learning:** Speech recognition, image classification, natural language procession (nlp) or recommendation systems\n* **What is difference of deep learning from machine learning:** \n    * Machine learning covers deep learning. \n    * Features are given machine learning manually.\n    * On the other hand, deep learning learns features directly from data.\n\n\nLets look at our data."
" \n# Overview the Data Set\n* We will use ""sign language digits data set"" for this tutorial.\n* In this data there are 2062 sign language digits images.\n* As you know digits are from 0 to 9. Therefore there are 10 unique sign.\n* At the beginning of tutorial we will use only sign 0 and 1 for simplicity. \n* In data, sign zero is between indexes 204 and 408. Number of zero sign is 205.\n* Also sign one is between indexes 822 and 1027. Number of one sign is 206. Therefore, we will use 205 samples from each classes(labels).\n* Note: Actually 205 sample is very very very little for deep learning. But this is tutorial so it does not matter so much. \n* Lets prepare our X and Y arrays. X is image array (zero and one signs) and Y is label array (0 and 1)."
" \n# Overview the Data Set\n* We will use ""sign language digits data set"" for this tutorial.\n* In this data there are 2062 sign language digits images.\n* As you know digits are from 0 to 9. Therefore there are 10 unique sign.\n* At the beginning of tutorial we will use only sign 0 and 1 for simplicity. \n* In data, sign zero is between indexes 204 and 408. Number of zero sign is 205.\n* Also sign one is between indexes 822 and 1027. Number of one sign is 206. Therefore, we will use 205 samples from each classes(labels).\n* Note: Actually 205 sample is very very very little for deep learning. But this is tutorial so it does not matter so much. \n* Lets prepare our X and Y arrays. X is image array (zero and one signs) and Y is label array (0 and 1)."
"* In order to create image array, I concatenate zero sign and one sign arrays\n* Then I create label array 0 for zero sign images and 1 for one sign images."
* Up to this point we learn \n    * Initializing parameters (implemented)\n    * Finding cost with forward propagation and cost function (implemented)\n    * Updating(learning) parameters (weight and bias). Now lets implement it.
"* Woow, I get tired :) Up to this point we learn our parameters. It means we fit the data. \n* In order to predict we have parameters. Therefore, lets predict.\n* In prediction step we have x_test as a input and while using it, we make forward prediction."
 \n## Create Model\n* Lets put them all together
\nUp to this point we create 2 layer neural network and learn how to implement\n* Size of layers and initializing parameters weights and bias\n* Forward propagation\n* Loss function and Cost function\n* Backward propagation\n* Update Parameters\n* Prediction with learnt parameters weight and bias\n* Create Model\n\n Now lets learn how to implement L layer neural network with keras.
"# Introduction\n\nA whole host of kernels have been written on the quintessential Iris dataset covering all sorts of topics under the sun. They include implementing different Machine learning models to Exploratory data analysis as well as dimensionality transformation techniques. \n\nThis notebook however will take a somewhat different approach. I will focus solely on one aspect of the dataset, visualising the decision boundary. A decision boundary in a nutshell, is a surface that separates data points belonging to different class lables. Although apparently simple at first glance, there is quite a lot of useful information to be gleaned from visualising a decision boundary, information that will give you an intuitive grasp of learning models. \n\nLet's go."
"# 1. Decision Boundary of Two Classes\n\n1. Before we start on the Iris dataset, as a starter I would like to generate some custom data points so we can have a feel for how the decision boundaries would look like on a two-class dataset (since the Iris set is a three-class set). To do this, we can call Sklearn's very convenient internal datasets by invoking -   **sklearn.datasets** to create datasets in the shapes of circles (make_circles()), crescents (make_moon( ) ) and blobs (make_blobs( ) ) as follows:"
"**Plotting the Decision Surface**\n\nThe best site to obtain Python code for plotting decision surfaces can be found on the Sklearn website. There are multiple excellent examples on the site and here are some links that you may find useful:\n\n 1. [Classifier Comparison][1] \n\n 2. [Plot the Decision Boundaries of a Voting Classifier][2]\n\n  [1]: http://scikit-learn.org/0.15/auto_examples/plot_classifier_comparison.html\n  [2]: http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html\n\nWhen plotting a decision surface, the general layout of the Python code is as follows:\n\n - Define an area with which to plot our decision surface and boundaries. We will use a very handy Numpy method **meshgrid** for this task\n - Extract either the class probabilities by invoking the attribute ""predict_proba"" or the distances between boundaries via the attribute ""decision_function"""
"**Takeaway from the Plots**\n\nAs we can see, our tree-based models are able to capture non-linear boundaries as evinced by the jagged edges and kinks in the decision boundaries to take into account the moon and circle shapes of the custom data-points. Furthermore, we can see that these tree-based models have been able to segregate the data points sufficiently based on a visual inspection of the Decision boundaries where there is clear demarcation between the red and the blue points. "
"## Interactive Plotly visualisations\n\n### 2.1 Overfitting effects on the Decision Boundary\n\nIn the following section, we will use the interactive capabilities of Plotly to visualise the effects of overfitting ones model and how the decision boundary changes due to this. We will plot two Random Forest classifiers, the first being one with reasonable parameters (max_depth = 4) while the second is clearly overfitting (max_depth = 50). \n\nInvoking the Plotly code is very similar to that of Matplotlib when generating the decision surface. We will need a Numpy meshgrid to form the basis of our surface plots as well as the **.predict** method from the learning model which to populate our surface with data."
*PLEASE CLICK AND DRAG THE ABOVE. THESE PLOTS ARE INTERACTIVE. DOUBLE-CLICK IF YOU WANT TO GET BACK TO THE ORIGINAL VIEW.*
Following on from this we plot our Plotly charts in the same vein as follows:
*PLEASE CLICK AND DRAG THE ABOVE. THESE PLOTS ARE INTERACTIVE. DOUBLE-CLICK IF YOU WANT TO GET BACK TO THE ORIGINAL VIEW.*
"### 2.3 Probabilities via Decision surfaces\n\nSo far sub-sections 2.1 and 2.2 have been been about visualising the actual data point separations between the three iris classes (or what the model thinks they are ). However, what if we want to visualise the probability of a data point being in either of the three classes?\n\nHandily enough for us, we can plot the class probabilities via Sklearn. Instead of invoking **.predict** in our plots, we use the **.predict_proba** attribute instead. The thing to note when calling the latter instead of the former is that the resulting output is now a matrix of three columns (instead of a vector). These three columns correspond to the three classes in the iris dataset. Therefore when plotting the decision surface for the class probabilities, the standard way of plotting is as follows:"
*PLEASE CLICK AND DRAG THE ABOVE. THESE PLOTS ARE INTERACTIVE. DOUBLE-CLICK IF YOU WANT TO GET BACK TO THE ORIGINAL VIEW.*
"We see that the isup_grade 0 and 1 i.e no cancer, has the most number of values and that's what expected in case of most medical datasets , the target class will always be underrepresented and that's also the most important challenge when performing machine learning tasks on Medical DATA\n\nNow let's Look at how much data is provided by which data-provider"
"I learned about adding the numerical values i.e count and percentage on the countplot of seaborn from Rohit's kernels . I thought I knew EDA very well but indeed I was wrong . You can learn more about ax.patches here : https://medium.com/@dey.mallika/transform-your-graphs-with-seaborn-ea4fa8e606a6\nAfter learning go ahead and play around the parameters of ax.test() and ask any queries in the comment section\nOne tip for Better visualization : always go through the documentation of the function you are using to visualize and play with all the parameters it takes , you will surprise yourself with suprisingly new things"
From this graph it is also clear that the data will be baised towards non-cancer examples
"# Image EDA\nNow we can finally move on to Image EDA . BUT since we are complete beginners, let's first+ understand the format of image that is provided to us and all the image related jargons that we will be using further\n\n## Q1) What is .tff format and Why it is used?\n\nTagged Image File Format (TIFF) is a variable-resolution bitmapped image format developed by Aldus (now part of Adobe) in 1986. TIFF is very common for transporting color or gray-scale images into page layout applications, but is less suited to delivering web content.\n\nReasons for Usage:\n* IFF files are large and of very high quality. Baseline TIFF images are highly portable; most graphics, desktop publishing, and word processing applications understand them.\n* The TIFF specification is readily extensible, though this comes at the price of some of its portability. Many applications incorporate their own extensions, but a number of application-independent extensions are recognized by most programs.\n* Four types of baseline TIFF images are available: bilevel (black and white), gray scale, palette (i.e., indexed), and RGB (i.e., true color). RGB images may store up to 16.7 million colors. Palette and gray-scale images are limited to 256 colors or shades. A common extension of TIFF also allows for CMYK images.\n* TIFF files may or may not be compressed. A number of methods may be used to compress TIFF files, including the Huffman and LZW algorithms. Even compressed, TIFF files are usually much larger than similar GIF or JPEG files.\n* Because the files are so large and because there are so many possible variations of each TIFF file type, few web browsers can display them without plug-ins.\n\n## Q2) What are image levels?\nIn some image formats the image data has a fixed amount of possible intensities. For instance an image may be defined as uint8 (unsigned integer 8-bit) which means that each pixel can have a value (intensity) between 0-255, and each intensity is a whole number (integer) in that range. So that gives 256 possible intensity levels. Another way to interpret this would be layers. An RGB (red green blue) type image uses three layers to define colour (a single layer would define a large-scale image, some image types contain more than 3 layers). For each pixel there are 3 intensity levels, 1 for each colour, are defined and together (using a kind of mixing of the colours) they define the colour of that pixels. Similarly for a grayscale there can be two levels i.e black and white\n\n## Q3) What is Down-sampling and Up-sampling in Image processing?\nDownsampling and upsampling are two fundamental and widely used image operations, with\napplications in image display, compression, and progressive transmission. Downsampling is\nthe reduction in spatial resolution while keeping the same two-dimensional (2D) representation. It is typically used to reduce the storage and/or transmission requirements of images.\nUpsampling is the increasing of the spatial resolution while keeping the 2D representation\nof an image. It is typically used for zooming in on a small region of an image, and for\neliminating the pixelation effect that arises when a low-resolution image is displayed on a\nrelatively large frame\n\nNow that we know all this we are good to go.\n\n\n I will be using openslide to display images as I learned it in this competition from a very informative kernel:\nhttps://www.kaggle.com/wouterbulten/getting-started-with-the-panda-dataset\nThe benefit of OpenSlide is that we can load arbitrary regions of the slide, without loading the whole image in memory. Want to interactively view a slide? We have added an interactive viewer to this notebook in the last section.\n\nYou can read more about the OpenSlide python bindings in the documentation: https://openslide.org/api/python/"
Now that we have our function let's visualize
"## Overlaying masks on the slides\nNow That we have learned how to visualize masks and display them side by side , Let's Overlay them on one another\n\nAs the masks have the same dimension as the slides, we can overlay the masks on the tissue to directly see which areas are cancerous. This overlay can help you identifying the different growth patterns. To do this, we load both the mask and the biopsy and merge them using PIL."
"## Overlaying masks on the slides\nNow That we have learned how to visualize masks and display them side by side , Let's Overlay them on one another\n\nAs the masks have the same dimension as the slides, we can overlay the masks on the tissue to directly see which areas are cancerous. This overlay can help you identifying the different growth patterns. To do this, we load both the mask and the biopsy and merge them using PIL."
We will look at only 6 examples for better visuals
# Which Classifier is Should I Choose? \n\nThis is one of the most import questions to ask when approaching a machine learning problem. I find it easier to just test them all at once. Here's 10 of your favorite Scikit-Learn algorithms applied to the leaf data. 
## Data Preparation\n
# 2. Importing the necessary libraries📗 
# 3. Reading the train.csv 📚
## Distribution Age over Week
## FVC - The forced vital capacity
### FVC vs Percent
FVC seems to related Percent linearly. Makes sense as both terms are proportional.
### FVC vs Age
Males have higher FVC than females irrespective of age
### FVC vs Weeks
### Pick one patient for FVC vs Weeks
### Pick one patient for FVC vs Weeks
Person never smoked has FVC lower than smoker. Some Ex-smoker have very high FVC.
### Pick one patient for FVC vs Weeks
## Age Distribution of Unique Patients
### Distribution of Age vs Gender In Patient Dataframe
## Gender Distribution
## Heatmap for train.csv
Please compare with the previous visualization information. And we may compare to Pandas Profiling below.
"# Missing Values\n\nWe have small portion of missing values for age and sex I think there is no harm if we impute them with the most frequent ones, meanwhile body parts missing on both datasets, we better be set 'unknown' for missing values for this one..."
"## Checking Variables Before Imputing\n\nJust wanted to check variable distribution before we impute the missing ones. Looks like our assumptions were ok, we can continue with imputing..."
"## Checking Variables Before Imputing\n\nJust wanted to check variable distribution before we impute the missing ones. Looks like our assumptions were ok, we can continue with imputing..."
# Imputing Missing Data\n\nLet's fill the missing values with appropriate methods.
"# Body Part Ratio by Gender and Target\n\nLooks like some body parts are more likely to be malignant, head/neck comes first with followed by oral/genital and upper extremity. Scanned body part locations are similar in order between males and females with small differences on distribution."
# A General Look With Sunburst Chart\n\nSunburst chart is pretty cool looking fella I'd say. It also giving lots of basic information to us. Let's see...\n\n- Only 2% of our targets are malignant\n- On malignant images males are dominant with 62% \n- Gender wise benign images are more balance 52-48% male female ratio\n- Malignant image scan locations differs based on the patients gender:\n    - Meanwhile the torso is most common location in males it's almost half of the scans meanwhile in females it's 39%\n    - Lower extremity is more common with female scans than males 18% males vs 26% females\n    - Again upper extremity malignant scans is common with females than males (23- 17%)\n- Benign image scan locations more similar between male and female patients.
# A General Look With Sunburst Chart\n\nSunburst chart is pretty cool looking fella I'd say. It also giving lots of basic information to us. Let's see...\n\n- Only 2% of our targets are malignant\n- On malignant images males are dominant with 62% \n- Gender wise benign images are more balance 52-48% male female ratio\n- Malignant image scan locations differs based on the patients gender:\n    - Meanwhile the torso is most common location in males it's almost half of the scans meanwhile in females it's 39%\n    - Lower extremity is more common with female scans than males 18% males vs 26% females\n    - Again upper extremity malignant scans is common with females than males (23- 17%)\n- Benign image scan locations more similar between male and female patients.
"# Age and Scan Result Relations\n\nAge looks pretty decent factor on scan result. Getting malignant scan result with elderly age is more possible than young patients. There is spike for both genders after age of 85, if we look distribution of ages there isn't much of 80+ patients and it can be the reason of this spike but we can safely say it's more likely to be malignant scan after age of 60. We see small bump on age 15-20 for females, again it depends on the scan numbers but still, poor souls..."
"# Age and Scan Result Relations\n\nAge looks pretty decent factor on scan result. Getting malignant scan result with elderly age is more possible than young patients. There is spike for both genders after age of 85, if we look distribution of ages there isn't much of 80+ patients and it can be the reason of this spike but we can safely say it's more likely to be malignant scan after age of 60. We see small bump on age 15-20 for females, again it depends on the scan numbers but still, poor souls..."
"# Age Round Two\n\nWanted to double check age distributions after our previous observations. Age seems evenly distributed on both train and test datasets, we can see small bumps at age 75+ and around 40, these worth investigating.\n\nWe can see again older people are more likely to get malignant scan results. One last thing about age distributions, we see more female patients in younger ages this trend changes with the older patients..."
"# Age Round Two\n\nWanted to double check age distributions after our previous observations. Age seems evenly distributed on both train and test datasets, we can see small bumps at age 75+ and around 40, these worth investigating.\n\nWe can see again older people are more likely to get malignant scan results. One last thing about age distributions, we see more female patients in younger ages this trend changes with the older patients..."
"# Unique Patients and Their Scan Images\n\nIt looks like we have multiple scan images per patient, actual unique patient counts are much lower than images on both datasets. We can get more information about patients age like when he had his first scan and his last scan. We can get interesting insights like:\n\n- Most of the malignant results are found around first 20 scans. Of course there can be control scans after the diagnosis...\n- Scan numbers are similar in first 100 scans but we have 200+ scan images for **one particular patient** in dataset, it's pretty interesting since we don't have this case in our training data. We should be careful about this and it can effect our model.\n- Most of the malignant cases are under 20 images but in general we can say it's more likely to be malignant result if there are more scan images..."
# Diagnosis Distribution\n\nThis part we can't use in our model but it's giving us some insights about this disease so we can inspect that too. You can see the details below:
# Loading Image Meta Features\n\nThis is the part where we get basic info directly from images themselves.
"# Image Sizes\n\nWe can see some kind of relation between size and target, but is it meaningful? Too soon to say..."
"## Getting Image Attributes\n\nYou can get these attributes by using the code below, I commented it out here and imported it as a data becasue it's time consuming process."
# Image Colors and Their Effects on Results
"# How are the Image Sizes Affecting Targets in Our Data\n\nWe have important observation here, you can see whole 1920x1080 set in test data which is not present in train data. That can have huge impact on final scores, mind that in your models. You might want to leave out image size related info in your models or regularize your models to smooth that effect. It can cause overfitting because of high correlation between image sizes and target, but these correlation might not be the case in test set (most likely) so keep that in mind."
"# How are the Image Sizes Affecting Targets in Our Data\n\nWe have important observation here, you can see whole 1920x1080 set in test data which is not present in train data. That can have huge impact on final scores, mind that in your models. You might want to leave out image size related info in your models or regularize your models to smooth that effect. It can cause overfitting because of high correlation between image sizes and target, but these correlation might not be the case in test set (most likely) so keep that in mind."
# The Mysterious Images\n\nThe name 'Mystery' comes from Chris Deotte from the comments down below and I decided to investigate them further. In last part we found out a new set of images with the resolution of 1920x1080 and they aren't present in train data at all. So we can assume these images weren't selected randomly for this competition. Down below I'm going to compare them with the rest of data.\n\n* It looks like without the 1920x1080 set mean colors are much more similar between train and test.\n* Again image size distribution gets closer between train and test without the mystery set.\n\n\nGonna check other features and add them here soon...
"I was curious about if these 1920x1080 images belong to high scan patients including 200+ one but it seems these observations are grouped around 10 scans, so it makes things more interesting..."
"Since we checking this mystery patch of images, let's check other features about them too maybe we can find other factors effecting this test sampling..."
"Since we checking this mystery patch of images, let's check other features about them too maybe we can find other factors effecting this test sampling..."
Looks like our 1920x1080 set images consisting little bit younger patients than the rest. Interesting...
# Correlations Between Features
# Modelling Based on Tabular Meta Features
"# Meta Feature Importances\n\nImage size seems pretty important on our model, but don't forget this can be misleading for final scoring. Don't forget about missing image sizes in test set and size correlation with targets in train data!"
# First Step: Creating Meta Submission
"Curiosity Bonus: Top 20 Countries in 2021 Ranking Evolution\n\nLet's look at the top 10 countries in 2021 and their evolution:\n\n* 🎩 For the gentlemen, there have been many more responders from Nigeria and Pakistan, ending up in the top 10 in 2021 from rankings below 20 in 2017.\n* 💃 For the ladies, there has been a huge surge in respondents from Egipt, Indonesia and Nigeria, these countries ending up in the top 10 most responders in 2021.\n\nIt's beautiful to see that more and more people join our community from more diverse backgrounds and countries."
"## 1.4 Education\n\n* The most interesting trend we see is a decrease in the percentage of respondents with Doctoral and Masters Degrees to the detriment of people that have a Bachelor as the highest educational degree so far.\n* This is because, as the pool of young people increased in 2020 and 2021, the number of students that are still in their studies (and not yet finished) increased as well."
"## 1.4 Education\n\n* The most interesting trend we see is a decrease in the percentage of respondents with Doctoral and Masters Degrees to the detriment of people that have a Bachelor as the highest educational degree so far.\n* This is because, as the pool of young people increased in 2020 and 2021, the number of students that are still in their studies (and not yet finished) increased as well."
"\n  👀 So far, we know that we're looking at a 20:80 ratio between 💃ladies and 🎩gents. We know that Data Science is getting much more traction among the youth in 2021 and more and more people are drawn towards the Kaggle platform, especially students. We know that, although the vast majority of respondents are from India and the US, people from Nigeria, Pakistan, Egypt and Indonesia are increasing in numbers fast, so it's possible that we will see more diversity in the years to come.\n\n\n# 2. Getting up close and personal\n\n## 2.1 Occupation\n\nFor both genders, the majority of respondents are either Students or have a profession within Data Science. However, some interesting differences happen within 3rd most common position:\n\n* 🎩 3rd most common position for gents is Software Engineer.\n* 💃 3rd most common position for ladies is Data Analyst.\n\nThroughout the years, there are no significant changes between these rankings."
"Most used programming languages within Data Science\n\nBoth genders look the same in terms of the distribution of languages and what they use more often in their projects.\n\nA few trends to mention:\n\n* C & C++ have been increasing a few percentage points, meaning that there are a few more respondents that use these 2 languages than in previous years.\n* R has been decreasing in popularity, as we can also see in respondents' recommendations. Still, 💃women tend to use it more (23.4% in 2021) than 🎩men so (19.8% in 2021).\n\n\n  👀 Psst! Hover your mouse over the streamgraphs to see the legend and more information!\n\n\n"
"## 2.3 Environment & Surroundings\n\nThe Environment\n\nRegarding the Data Science setup and what the genders use most often within their Data Science environment, we can see that:\n\n* Visual Studio increased in popularity significantly in 2021, after a slight dip in 2020. 🎩Gents use it more often (49% in 2021) than 💃ladies do (39% in 2021).\n* After many years in which Jupyter has been having the majority of the usage (half of the respondents said they use Jupyter frequently), it has lost its popularity suddenly in 2021, with only ~20% of users still saying that they use it on a regular basis."
"## 2.3 Environment & Surroundings\n\nThe Environment\n\nRegarding the Data Science setup and what the genders use most often within their Data Science environment, we can see that:\n\n* Visual Studio increased in popularity significantly in 2021, after a slight dip in 2020. 🎩Gents use it more often (49% in 2021) than 💃ladies do (39% in 2021).\n* After many years in which Jupyter has been having the majority of the usage (half of the respondents said they use Jupyter frequently), it has lost its popularity suddenly in 2021, with only ~20% of users still saying that they use it on a regular basis."
"The Surroundings\n\nRegarding the preferences for hosted notebooks, the trends are as follows:\n\n* Kaggle and Colab have been increasing in popularity each year, being the most preferred hosted notebooks out of all available choices in 2021.\n* There is still a big pool of people (around 30% in 2021 for both genders) that still never use any hosted notebook on a regular basis. I assume these are the people that compete frequently in Kaggle competitions (and win) but don't like to post notebooks for these competitions as well. I would also assume that these people use high GPU/TPU power and have local very powerful computers/workstations. I would love to have a chat with them and understand how could I develop these hosted notebooks in order to help them in their Data Science work.\n\nCode Ocean, IBM Watson Studio, Amazon Sagemaker Studio & EMR, Google Cloud Notebooks & Datalab, Databricks Collaborative Notebooks have less than 8% usage between the respondents, so they were not shown within the graphs."
"The Surroundings\n\nRegarding the preferences for hosted notebooks, the trends are as follows:\n\n* Kaggle and Colab have been increasing in popularity each year, being the most preferred hosted notebooks out of all available choices in 2021.\n* There is still a big pool of people (around 30% in 2021 for both genders) that still never use any hosted notebook on a regular basis. I assume these are the people that compete frequently in Kaggle competitions (and win) but don't like to post notebooks for these competitions as well. I would also assume that these people use high GPU/TPU power and have local very powerful computers/workstations. I would love to have a chat with them and understand how could I develop these hosted notebooks in order to help them in their Data Science work.\n\nCode Ocean, IBM Watson Studio, Amazon Sagemaker Studio & EMR, Google Cloud Notebooks & Datalab, Databricks Collaborative Notebooks have less than 8% usage between the respondents, so they were not shown within the graphs."
"## 2.4 Special Sauces\n\nI. First Special Sauce: The Hardware\n\nThere is a smaller percentage of users for Workstations and Cloud Platforms and an increase in Personal Laptop's popularity. \n\nI believe this is the case because, as the overall percentage of respondents in 2021 consists of many more young adults (aged 18 to 24), they do not yet have the necessary budget and experience to work with heavy-duty equipment, like specialized workstations and Cloud Platforms with online GPU or TPU."
"\n  👀 We now know that the majority of respondents are Students and Data Scientists, although 💃ladies do prefer a Data Analyst job, vs the 🎩gents who would rather go for Data Engineer. We also know that they love and use Python the most, while starting to lose interest in R. Jupyter Notebooks are losing interest vs Visual Studio, which has been skyrocketing in 2021. In terms of special sauces, Personal Laptops are most used, as the pool of respondents is younger, therefore have less experience & resources. An interesting trend is the rise of TPU and the apparent fall of GPU, seen in both men and women, although 🎩gents use much more special hardware than 💃ladies do.\n\n\n# 3. Music at ""the Wireless""\n\n> 📻 Fun fact - ""the Wireless"" started being popular in the 1920s and it soon became an interesting point of attraction in households, especially for the youth. The elder and more ""conservative"" people (that lived most of their life in the second half of the 1800s) were reluctant to use it, saying ""it's just a passing phase"". If you haven't guessed it, ""the wireless"" was to be and it's now named ... the radio. At that time, you had to have it installed by a professional and you would usually hear some news, live presidential/royal speeches and music (most popular genres at the time were blues, jazz, swing, regtime etc.).\n\n\n\nNote: Inspiration from the TV series ""Downton Abbey"".\n\n## 3.1 ""Jazzy"" Visualizations\n\n* Matplotlib, Seaborn and Plotly remain unchallenged in their top 3 ranking for the most used visualization libraries. Something to mention is that for all libraries (besides GGplot, where women tend to have bigger usage than men), men seem to be much more interested in the visualization packages.\n* Matplotlib increased a few percentages in 2021 vs 2018, however Seaborn received the most attention, gaining 15% popularity points for 🎩gents and 17% popularity points for 💃ladies.\n* All other visualization libraries seem to either remain stagnant or even decrease in usage. Why is that? Is it a lack of knowledge from the respondents/users, or just a lack of overall interest? I am thinking this might be the case because D3.js, Geoplotlib, Shiny, Leaflet etc. are very specific libraries for specific problems and not for day-to-day usage."
"## 3.2 Machine Learning on the ""Blues""\n\nI. The Algorithms\n\n* If you shift through years, there is an apparent decrease in specialized respondents (that have more than 2 years of experience in doing ML). This links very well with the fact that we know that there are more youngsters on the platform, that are using their personal computers in order to navigate and learn Machine Learning. Nevertheless, these youngsters will increase the percentage of experienced coders, usage of specialized hardware and overall ""heavy-duty"" tools and libraries in the years to come. Just wait for it! 🧐\n* 🎩Gents tend to have more numbers within the 3+ years of ML Experience than 💃ladies do. This is linked to the fact that more men come from an Engineering background (which requires coding aptitudes) vs. women that come more from Data/Business Analytics (which requires less or even no coding).\n* The most used algorithms for both genders are Trees Ensembles and Random Forests, Gradient Boosting Algorithms and CNNs (Convolutional Neural Networks). This matches quite well the overall scheme of competitions we have been having on Kaggle, which tends to be either tabular or requires Computer Vision (of course, there have been many more other types, forecasting and NLP to name a few - however most competitions do require in most of the cases at least one of the above-mentioned algorithms)."
"II. The Frameworks\n\n* Scikit-learn, Tensorflow and Keras are the most used ML frameworks for both genders. However, 💃ladies do tend to have less interest within them than 🎩men do, as it shows within the smaller numbers of usage for each framework.\n* Most frameworks have a steady trend of usage, with no apparent substantial increase or decrease in popularity. I believe that the most ""apparent"" improvement in the last 4 years was for PyTorch, which increased 8% popularity points for 🎩gents and 6% popularity points for 💃ladies in 2021 vs 2018.\n\nNote: As the number of respondents that answered they used Other Frameworks or None at all was very small (less than 3% for both genders), I excluded them from the graphs."
"## 3.3 Deep Learning ""Swing""\n\n* In regards to Computer Vision, Image Classification (e.g.: ResNet, EffNet etc.) is the most used method for both genders. However, the second most used method differs for the 2: for 🎩gents is Object Detection (e.g.: RetinaNet), while for the 💃Ladies is Image Segmentation (e.g.: U-Net), although the 2 are very close in terms of popularity from one to another.\n* In terms of NLP (Natural Language Processing), we can see a very strong increase in popularity for Transformer Language Models (e.g.: BERT) for both genders. It even surpassed in popularity of the EncoderDecorderModels (e.g.: seq2seq), which were much more popular in 2019."
"II. The Size of the Business\n\n*  Around 20% of respondents (same pattern in both genders) say that they are employed in very small companies (between 1 and 49 employees). This trend remained quite constant from 2019 through 2021.\n*  Another 20%-25% of respondents are at the other end of the spectrum, saying that they work within large and very large companies of more than 1,000 employees (shown for both ladies and gents).\n*  There was a smaller percentage of respondents that answered this question (be aware, not in absolute numbers - but in percentage on year and gender) in 2020 vs 2019. In 2021 it stabilized a little, but I am wondering if the decrease in 2020 was marked by some elderly more experienced people on the platform that preferred to keep these pieces of information private."
"## 4.2 Estate's Profitability\n\nI. The Salary\n\n>  📌 Note: The salaries were adjusted on respondent's country, meaning that the pay (in US dollars) was balanced to be showing the power of purchase within the place of residence, rather than just the overall brute amount. The adjustment was done as I did last year, meaning that I have converted the salary to ""number of McMeal units a person can buy in their country"". The more units, the bigger the purchasing power. Unfortunately, I could not adjust the pay on years too as I could not find data for all 4 years - hence there might be some inflation bias within the numbers.\n\n*  🎩 For gentlemen, the most prominent change between 2018 and 2021 is that there are many more respondents that have lower salaries. This is also evident in the general mean of the amount, which increased in 2019 but dropped at the lowest in 2021.\n*  💃 This trend is the exact same for ladies too. In general, there are many more respondents with lower income in 2021 vs 2018. The average pay dropped suddenly for women in 2020 vs 2019 and remained at the same value in 2021. \n*  I strongly believe this decrease in overall pay is not due to general lower income within the Data Science/ Machine Learning Industry. These industries actually have some of the biggest pays around the world. I would rather think this trend shows because there are many more young people that took the survey, hence they naturally have lower income vs somebody with 5+ years of experience. I am confident we will see a surge in higher pays in the following years."
"## 4.2 Estate's Profitability\n\nI. The Salary\n\n>  📌 Note: The salaries were adjusted on respondent's country, meaning that the pay (in US dollars) was balanced to be showing the power of purchase within the place of residence, rather than just the overall brute amount. The adjustment was done as I did last year, meaning that I have converted the salary to ""number of McMeal units a person can buy in their country"". The more units, the bigger the purchasing power. Unfortunately, I could not adjust the pay on years too as I could not find data for all 4 years - hence there might be some inflation bias within the numbers.\n\n*  🎩 For gentlemen, the most prominent change between 2018 and 2021 is that there are many more respondents that have lower salaries. This is also evident in the general mean of the amount, which increased in 2019 but dropped at the lowest in 2021.\n*  💃 This trend is the exact same for ladies too. In general, there are many more respondents with lower income in 2021 vs 2018. The average pay dropped suddenly for women in 2020 vs 2019 and remained at the same value in 2021. \n*  I strongly believe this decrease in overall pay is not due to general lower income within the Data Science/ Machine Learning Industry. These industries actually have some of the biggest pays around the world. I would rather think this trend shows because there are many more young people that took the survey, hence they naturally have lower income vs somebody with 5+ years of experience. I am confident we will see a surge in higher pays in the following years."
"Curiosity Bonus: Salaries & Roles\n\nI know pay is a taboo subject, but I could not help myself to also look within the top 6 most frequent Roles and what is the average salary for each of them. I have come up with an extremely informative graph:\n\n* For 🎩gents, the highest paying jobs are Project Manager (1st place) and Data Scientist (2nd place). Coming next, the 3rd best-paying salary is for Software Engineering - and we know men love this job, as it is also the 3rd most frequent role found within our respondents.\n* For 💃ladies, the first 2 most high-paying roles are Project Manager (1st place) and Data Scientist (2nd place) as well! What I find the most interesting is that for them, on the 3rd place on the list of the best paying jobs is Business Analyst (very close to Data Analyst I might add), which are also extremely popular roles for women (as we saw in the chart in Chapter 2.1).\n* What I find interesting is that each gender has in the top 4 best paying jobs roles that they are really good at. For women there is Data Science, Data Analysis and Business Analysis, while for men is Data Science, Software Engineering and Research."
"II. The (Machine Learning) Investment\n\nAre companies interested in ML? How much are they willing to pay for it?\n\nSomething that I found rather odd is that there is some sort of decrease in Machine Learning interest from companies, at least from the respondent's perspective. I would have actually expected an increase in companies that have started using ML or have been implementing it for a while. However, not only the percentage of interest decrease but also the overall number of respondents that answered this question decreased too. I would put a part of this decrease on the lack of administrative knowledge from respondents (if they don't know for sure they might choose to hold back from answering)."
" The investing activity is quite correlated to the interest in Machine Learning. For both genders, we can see that ~30% of respondents say that their company either doesn't invest in ML at all, or it invests a very small amount of money (less than 100 dollars yearly). On the other end of the spectrum, very few respondents (less than 5% for 💃ladies and less than 🎩10% for men) say that their company invests more than 100K dollars in ML.\n\n Again, I am reluctant to put this behavior on the lack of interest or lack of administrative knowledge. As we know from previous graphs, there were quite a few people that said they work in big companies, however the percentages don't match with the investment - meaning that large companies don't seem to necessarily invest a lot in Machine Learning."
" The investing activity is quite correlated to the interest in Machine Learning. For both genders, we can see that ~30% of respondents say that their company either doesn't invest in ML at all, or it invests a very small amount of money (less than 100 dollars yearly). On the other end of the spectrum, very few respondents (less than 5% for 💃ladies and less than 🎩10% for men) say that their company invests more than 100K dollars in ML.\n\n Again, I am reluctant to put this behavior on the lack of interest or lack of administrative knowledge. As we know from previous graphs, there were quite a few people that said they work in big companies, however the percentages don't match with the investment - meaning that large companies don't seem to necessarily invest a lot in Machine Learning."
"## 4.3 Manpower within the Estate\n\nA first note here would be that I have explored the activities that make up an important role for a respondent and there are no major changes throughout the years. The most frequent activities remain ""building and running Machine Learning services"", while the least frequent activity is ""performing research to advance the state of the art of Machine Learning"".\n\nIn regards to the number of people that make up the Data Science teams we can observe the following:\n\n* For both 🎩men and 💃women the team size correlates with the size of the company. There are around 35% of respondents that affirm that the DS team within the company is quite small (less than 4 people).\n* On the opposite side there are ~16% (💃ladies) to almost 20% (🎩gents) that state they are working in teams equal or greater than 10 members.\n* There is no significant shift within the percentages of categories throughout the years."
"## 4.3 Manpower within the Estate\n\nA first note here would be that I have explored the activities that make up an important role for a respondent and there are no major changes throughout the years. The most frequent activities remain ""building and running Machine Learning services"", while the least frequent activity is ""performing research to advance the state of the art of Machine Learning"".\n\nIn regards to the number of people that make up the Data Science teams we can observe the following:\n\n* For both 🎩men and 💃women the team size correlates with the size of the company. There are around 35% of respondents that affirm that the DS team within the company is quite small (less than 4 people).\n* On the opposite side there are ~16% (💃ladies) to almost 20% (🎩gents) that state they are working in teams equal or greater than 10 members.\n* There is no significant shift within the percentages of categories throughout the years."
"\n  👀 🎩Gents and 💃ladies are found the most within the Academics and Technology industries. The company and team size are split almost 50-50 (between the respondents that answered these questions), meaning that ~half of them are part of small companies and teams, while the other half are part of larger companies and Data Science teams size. These companies don't seem to pay greater attention to ML nor invest more within this discipline in 2021, however the respondent's bias and administrative knowledge might influence this answer. Lastly, there has been a decrease in pay due to an increase in young (fresh) respondents, with some of the best paying jobs (for both genders) being Project Manager and Data Scientist.\n\n\n# 5. Getting Social - Sharing is Caring\n\n## 5.1 Acquiring Knowledge\n\nThe evolution of learning platforms has been as follows:\n\n* Kaggle Learn had the biggest increase in popularity since 2018, having 34% of the 🎩gents and 30% of the 💃ladies saying that they use Kaggle Learn on a regular basis. Another platform that has increased in popularity has been Udemy, but at a slower pace than Kaggle Learn.\n* Besides these two, all the other learning platforms have been decreasing in usage in the last 4 years."
"\n  👀 🎩Gents and 💃ladies are found the most within the Academics and Technology industries. The company and team size are split almost 50-50 (between the respondents that answered these questions), meaning that ~half of them are part of small companies and teams, while the other half are part of larger companies and Data Science teams size. These companies don't seem to pay greater attention to ML nor invest more within this discipline in 2021, however the respondent's bias and administrative knowledge might influence this answer. Lastly, there has been a decrease in pay due to an increase in young (fresh) respondents, with some of the best paying jobs (for both genders) being Project Manager and Data Scientist.\n\n\n# 5. Getting Social - Sharing is Caring\n\n## 5.1 Acquiring Knowledge\n\nThe evolution of learning platforms has been as follows:\n\n* Kaggle Learn had the biggest increase in popularity since 2018, having 34% of the 🎩gents and 30% of the 💃ladies saying that they use Kaggle Learn on a regular basis. Another platform that has increased in popularity has been Udemy, but at a slower pace than Kaggle Learn.\n* Besides these two, all the other learning platforms have been decreasing in usage in the last 4 years."
"## 5.2 Sharing and Contribution\n\n* GitHub, Kaggle and Colab are the most popular spaces out of all to share Data Science work (for both genders). All three mentioned have been increasing in usage in 2021 vs 2020.\n* Worth mentioning is that there are fewer women (in percentage points, not in absolute numbers) that answered this question versus men."
"## 5.3 Where the gossip is at\n\n* Kaggle and Youtube remain in 2021 (as in the past years too) the most used platforms for gathering information, news and interesting events about Data Science. I personally would have expected Twitter to be very used as well, but it has only 15% usage for gents and ~10% for ladies.\n* For both 💃ladies and 🎩gents there has been an overall decrease within almost all media sources, due to a decrease in the percentage of respondents that answered this question. However, Youtube and Newsletters are the only two platforms that had a slight increase in popularity in 2021 compared to the last years."
"\n  👀 Kaggle has the general lead in all 3 social categories - learning, sharing DS work and media source - and it increased in all categories in 2021 vs the last years. GitHub and Colab are the other two very popular sharing platforms, while as for media sources we can affirm that Youtube and Personal Blogs remain in lead.\n\n\n# 6. Ending the Party in Style\n\n\n\nA dataset is very volatile. In my time as a Data Analyst, and then Data Scientist, there is one thing I can say I have discovered and I live by: looking at a dataset is not a straightforward recipe. A dataset is like a living, breathing organism.\n\nSometimes it confirms some conclusions that you maybe might have already known, or let's say had a hunch, an intuition on them. Sometimes it does the exact opposite, giving the reader some insights they would have never expected. Sometimes it even shocks you. And other times it doesn't really say anything. Nothing new, nothing you already knew, nothing at all. But in these ""nothing"" insights there are still some conclusions to be taken, even actions to be made.\n\nThe following are some of my own, personal insights and findings on this beautiful dataset. However, I would encourage you to read and draw you own conclusions, even challenge mine. A healthy debate brings us more forward than always agreeing with one another - right? :)\n\n\n  \n    \n  \n    \n  \n      💃 Ladies\n    As we have seen so far throughout the analysis, there are no major differences between ladies and gents in terms of preferences towards tools. I will be honest, I have expected to see many more dissimilarities than I am seeing now, however ladies and gents do seem to have in common much more than one would anticipate.\n    The percentage of women that responded to the survey has maintained through time at around 20%. Most ladies are from USA, India and more recently from Nigeria, Egypt and Indonesia. They work as Data Scientists, Data Analysts or Business Analysts in companies oriented towards Tech and Medical industries, or they can be found studying/teaching within Academia. \n    Regarding the tools and skills, ladies are very fond of Python and are starting to rapidly lose interest in R. They do use less special hardware and have fewer years of experience in coding due to the nature of their role/job, which doesn't involve mandatory coding practice.\n  \n\n\n\n\n\n  \n    \n  \n    \n  \n      🎩 Gentlemen\n    Gentlemen are very similar to ladies in terms of preferences and used tools. However, they do have a few areas where they stand out, having different aspects than the ladies. \n    Around 80% of men are responding to the survey each year, with this number maintaining throughout all 5 analyzed years. Usually, they have rezidency within India, USA, Japan, with a big surge in people from Nigeria and Pakistan. Most of the gents are Data Scientists or Software Engineers working within the Tech and Finance industries, or they are teachers/students within Academia. \n    Python is and remains king in the programming languages arena, and they do seem to be liking to work more with specialized hardware, such as GPU and TPU. Gents do tend to have a bit more seniority within the machine learning and coding arena, as their role within companies requires more necessary knowledge on this part.\n  \n\n\n\n\n\n  \n    \n  \n    \n  \n      🕒 Time\n    Many of the shifts observed within the evolutions were affected by the increase in youth (people aged 18 to 24 years old). Hence, the decrease we have observed in Doctorate/ Masters respondents, in overall coding experience and in income were naturally influenced by this aspect.\n\n    For programming languages we see more activity within C/C++, while R is losing popularity each year. Visual Studio is also enjoying an increase in usage, vs Jupyter who had a dip in 2021. TPU is growing more and more popular each year, and it seems that it does so to the detriment of GPU, which is slowly decreasing. Another algorithm that had a steep increase is the Transformer (e.g.: BERT).\n    Regarding the companies and their structure, the respondents seem to be split quite in the middle: around half saying they work in small companies and teams, while the other stated that they work in larger companies and teams (with no apparent change through time). However, I was surprised that the interest and investment in ML don't tend to increase throughout the years and it doesn't correlate with the company size - meaning that larger companies don't seem to necessarily invest more in ML.\n  \n\n\n\n# 7. Giving Thanks\n\n* Font Generator for ParkLane (Great Gatsby Font). (n.d.). Font Generator. Retrieved November 19, 2021, from https://www.font-generator.com/fonts/ParkLane/?size=58&color=000000&bg=none\n* Sources for graph images. (n.d.). Pinterest Board. Retrieved November 19, 2021, from https://ro.pinterest.com/andrada_teodora/1920/\n* Templates for the D3 Graphs. (n.d.). The D3.Js Graph Gallery. Retrieved November 22, 2021, from https://www.d3-graph-gallery.com/index.html\n* Special loving thanks: to my amazing partner, who has been extremely supportive and eager to help in this analysis. This notebook would have never looked like it does now without him, and for his patience and kindness, I am forever grateful.\n\n\n## My Specs\n\n> My W&B Dashboard is growing:\n\n\n\n\n* 🖥 Z8 G4: Workstation\n* 💾 2 CPUs & 96GB Memory\n* 🎮 2x NVIDIA A6000\n* 💻 Zbook Studio G7 on the go"
"We can see that `Fare` contains mainly values of around `0` to `30`, but there's a few really big ones. This is very common with fields contain monetary values, and it can cause problems for our model, because once that column is multiplied by a coefficient later, the few rows with really big values will dominate the result.\n\nYou can see the issue most clearly visually by looking at a histogram, which shows a long tail to the right (and don't forget: if you're not entirely sure what a histogram is, Google ""[histogram tutorial](https://www.google.com/search?q=histogram+tutorial&oq=histogram+tutorial)"" and do a bit of reading before continuing on):"
"To fix this, the most common approach is to take the logarithm, which squishes the big numbers and makes the distribution more reasonable. Note, however, that there are zeros in the `Fare` column, and `log(0)` is infinite -- to fix this, we'll simply add `1` to all values first:"
The histogram now shows a more even distribution of values without the long tail:
"It looks from the `describe()` output like `Pclass` contains just 3 values, which we can confirm by looking at the [Data Dictionary](https://www.kaggle.com/competitions/titanic/data) (which you should always study carefully for any project!) -- "
"To fix this, we should pass every prediction through the *sigmoid function*, which has a minimum at zero and maximum at one, and is defined as follows:"
"PyTorch already defines that function for us, so we can modify `calc_preds` to use it:"
"> We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. "
# 3. Read in Data 
"\n\nGoal: for each building and meter pair, visualize where target is missing and where target is zero VS time.\n\n\n\n"
"- Vertical blue lines may be suspicious\n\nLegend:\n* X axis: hours elapsed since Jan 1st 2016, for each of the 4 meter types\n* Y axis: building_id\n* Brown: meter reading available with non-zero value\n* Light blue: meter reading available with zero value\n* White: missing meter reading"
> ### Some plots
"The sum, facetted for each energy aspect, shows some aberrant values."
"The sum, facetted for each energy aspect, shows some aberrant values."
"Looking at the max value for each day, and for each energy aspect, shows that only a single building (for day and energy aspect) is causing the aberrant peaks"
The max values of electricity are caused by only 6 buildings.
> ### Chilledwater
The max values of electricity are caused by only 10 buildings.
> ### Steam
The max values of electricity are caused by only 4 buildings.
> ### Hotwate
"The max values of electricity are caused by only 7 buildings. Practically, two of them"
"## 6.3 Examine Missing Values\n\n\n\n\n    Taking only the buildings that consume more than the others, could be seen that there are a lot of measure scale errors.\n    The error could be:\n\n    The meter is not configured correctly. E.g., a bad voltage or current primary to secondary ratio.\n    The software has not the units configured correctly. E.g., MJ/kg for steam.\n    The software has not the decimal digits configured correctly.\n    Using a power variable instead of an energy one.\n    The measure could be done with an unique meter, or the sum of several of them.\n\n    Some changes over time, values go to zero or the scale is changed, indicates that some buildings have more than one meter. One error in one meter and the overall measure is garbage.\n\n    This notebook has only analised the outliers that influence the maximum consumption in a daily basis. This is only the tip of the iceberg. A sound analysis should be done to detect and correct these outliers.\n\n    A solution to avoid scale errors is to normalize the values from 0 to 1, for each building and for each energy aspect.\n\n\n"
"# 1. EDA\n\nAltogether, we are given 7 files.\n\n>Tailoring education to a student's ability level is one of the many valuable things an AI tutor can do. Your challenge in this competition is a version of that overall task; you will predict whether students are able to answer their next questions correctly. You'll be provided with the same sorts of information a complete education app would have: that student's historic performance, the performance of other students on the same question, metadata about the question itself, and more.\n\n>This is a time-series code competition, you will receive test set data and make predictions with Kaggle's time-series API. Please be sure to review the Time-series API Details section closely.\n\nSo we should realize that example_test.csv really is just an example. The submission happens via the API."
"As the train dataset is huge, I am gladly using the pickle that Rohan Rao prepared in this kernel: https://www.kaggle.com/rohanrao/tutorial-on-reading-large-datasets/ (Thanks Rohan!). I actually do this at work all the time, and in this case it reduces the time to load the dataset (with the data types specified in the file description) from close to 9 minutes to about 16 seconds.\n\nAs we can see, we have over 101 million rows the the train set.\n"
"timestamp: (int64) the time in milliseconds between this user interaction and the first event completion from that user. As you can see, most interactions are from users that were not active very long on the platform yet."
"Do we have the full history of all user_id's? Yes, if we filter train on timestamp==0, we get a time 0 for all users."
"# The target: answered_correctly\nAnswered_correctly is our target, and we have to predict to probability for an answer to be correct. Without looking at the lecture interactions (-1), we see about 1/3 of the questions was answered incorrectly."
"I also want to find out if there is a relationship between timestamp and answered_correctly. To find out I have made 5 bins of timestamp. As you can see, the only noticable thing is that users who have registered relatively recently perform a little worse than users who are active longer."
"I also want to find out if there is a relationship between timestamp and answered_correctly. To find out I have made 5 bins of timestamp. As you can see, the only noticable thing is that users who have registered relatively recently perform a little worse than users who are active longer."
"Let's also check out what the distribution of answered_correctly looks like if we groupby the (10,000 unique) task_container_id's."
"Let's also check out what the distribution of answered_correctly looks like if we groupby the (10,000 unique) task_container_id's."
"Below I am plotting the number of answers per user_id against the percentage of questions answered correctly (sample of 200). As some users have answered huge amounts of questions, I have taken out the outliers (user_ids with 1000+ questions answered). As you can see, the trend is upward but there is also a lot of variation among users that have answered few questions."
"Does it help if the 'prior_question_had_explanation'? Yes, as you can see the percent answered correctly is about 17% higher when there was an explanation. Although it is probably better to treat not having an explanation as a disadvantage as there was an explanation before the vast majority of questions.\n\nIn addition, it is also interesting to see that the percent answered correctly for the missing values is closer to True than to False."
"prior_question_elapsed_time: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n\nAt first glance, this does not seem very interesting regarding our target. For both wrong and correct answers, the mean is about 25 seconds."
"However, as the feature works with regards to the CV (see Baseline model), I also wanted to find out if there is a trend. Below, I have taken a sample of 200 rows. As you can see, there is s slightly downward trend."
"# 1.2 Exploring Questions\n\nMetadata for the questions posed to users.\n\n* question_id: foreign key for the train/test content_id column, when the content type is question (0).\n* bundle_id: code for which questions are served together.\n* correct_answer: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n* part: the relevant section of the TOEIC test.\n* tags: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.\n"
"As you can see, the differences are significant!"
"However, we should also realize that the tag with the worst percent_correct only has about 250,000 answers. This a low number compared to the tags with most answers."
"However, we should also realize that the tag with the worst percent_correct only has about 250,000 answers. This a low number compared to the tags with most answers."
"What are the so-called ""Parts""? When following the link provided in the data description we find out that this relates to a test.\n\n> The TOEIC L&R uses an optically-scanned answer sheet. There are 200 questions to answer in two hours in Listening (approximately 45 minutes, 100 questions) and Reading (75 minutes, 100 questions). \n\nThe listening section consists of Part 1-4 (Listening Section (approx. 45 minutes, 100 questions)).\n\nThe reading section consists of Part 5-7 (Reading Section (75 minutes, 100 questions))."
"Below, I am displaying the count and percent correct by part. As you can see, Part 5 has a lot more question_id's and is also the most difficult."
"# 1.3 Exploring Lectures\n\nMetadata for the lectures watched by users as they progress in their education.\n* lecture_id: foreign key for the train/test content_id column, when the content type is lecture (1).\n* part: top level category code for the lecture.\n* tag: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n* type_of: brief description of the core purpose of the lecture\n"
Let's have a look at the type_of.
"Since there are not that many lectures, I want to check if it helps if a user watches lectures at all. As you can see, it helps indeed!"
"Since there are not that many lectures, I want to check if it helps if a user watches lectures at all. As you can see, it helps indeed!"
"Batches (task_container_id) may also contain lectures, and I want to find out if there are any batches with high numbers of lectures."
"Is there a correlation between the percent_lecture and the percent_correct? No, I don't really see it. If anything, the percent_correct actually seems to go down slightly."
"The last thing that I want to check is if having a lecture in a batch helps. As you can see, it does not. Batches without lectures have about 8% more correct answers than batches with lectures."
"Before we create a Random Forest or Gradient Boosting Machine, we'll first need to learn how to create a *decision tree*, from which both of these models are built.\n\nAnd to create a decision tree, we'll first need to create a *binary split*, since that's what a decision tree is built from.\n\nA binary split is where all rows are placed into one of two groups, based on whether they're above or below some threshold of some column. For example, we could split the rows of our dataset into males and females, by using the threshold `0.5` and the column `Sex` (since the values in the column are `0` for `female` and `1` for `male`). We can use a plot to see how that would split up our data -- we'll use the [Seaborn](https://seaborn.pydata.org/) library, which is a layer on top of [matplotlib](https://matplotlib.org/) that makes some useful charts easier to create, and more aesthetically pleasing by default:"
"Here we see that (on the left) if we split the data into males and females, we'd have groups that have very different survival rates: >70% for females, and <20% for males. We can also see (on the right) that the split would be reasonably even, with over 300 passengers (out of around 900) in each group.\n\nWe could create a very simple ""model"" which simply says that all females survive, and no males do. To do so, we better first split our data into a training and validation set, to see how accurate this approach turns out to be:"
"Alternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work -- here's an example of how we could look at `LogFare`:"
"The [boxenplot](https://seaborn.pydata.org/generated/seaborn.boxenplot.html) above shows quantiles of `LogFare` for each group of `Survived==0` and `Survived==1`. It shows that the average `LogFare` for passengers that didn't survive is around `2.5`, and for those that did it's around `3.2`. So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\n\nLet's create a simple model based on this observation:"
"One particularly nice feature of random forests is they can tell us which independent variables were the most important in the model, using `feature_importances_`:"
"We can see that `Sex` is by far the most important predictor, with `Pclass` a distant second, and `LogFare` and `Age` behind that. In datasets with many columns, I generally recommend creating a feature importance plot as soon as possible, in order to find which columns are worth studying more closely. (Note also that we didn't really need to take the `log()` of `Fare`, since random forests only care about order, and `log()` doesn't change the order -- we only did it to make our graphs earlier easier to read.)\n\nFor details about deriving and understanding feature importances, and the many other important diagnostic tools provided by random forests, take a look at [chapter 8](https://github.com/fastai/fastbook/blob/master/08_collab.ipynb) of [our book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)."
"### Color coding\n\nThrough this challenge in order to visualize the tasks color coding are adopted, here we show what color corresponds to what number. (code taken from [here](https://www.kaggle.com/nagiss/abstraction-and-reasoning-view-all-data))"
"## Task training/db3e9e38, continue the pattern\n\nLet's try to design a cellular automata for task `training/db3e9e38`. \n         \n  "
"### 1.2 Exploratory Data analysis with dabl\n\ndabl provides a high-level interface that summarizes several common high-level plots. For low dimensional datasets, all features are shown; for high dimensional datasets, only the most informative features for the given task are shown"
* **Initial Model Building with dabl**\nWe can find an initial model for our data. The SimpleClassifier implements the familiar scikit-learn API of fit and predict.
"### 2.2 Bar Chart\n\nAlternatively, you can also plot a barchart to show the missing values\n"
### 2.3 Heatmap\n\n[`missingno.heatmap`](https://github.com/ResidentMario/missingno#heatmap) measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:
### 2.3 Heatmap\n\n[`missingno.heatmap`](https://github.com/ResidentMario/missingno#heatmap) measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:
\n3. Emot \n\n\n[Emot](https://github.com/NeelShah18/emot) is an Emoji and Emoticons detection package for Python. It can come in real handy when we have to preprocess our text data to get rid of the emoticons.
Let's compare the PPS matrix to the basic correlation matrix
"These were some of the interesting and  useful python libraries for data science, that I have come across recently. In case you know about others which can be added to the list, do mention them in the comments below. "
### Loading Metadata
Let's load the metadata of the dateset. 'title' and 'journal' attributes may be useful later when we cluster the articles to see what kinds of articles cluster together.
"### Next let's create a function that will process the text data for us. \nFor this purpose, we will be using the spacy library. This function will convert text to lower case, remove punctuation, and find and remove stopwords. For the parser, we will use en_core_sci_lg. This is a model for processing biomedical, scientific or clinical text."
Applying the text-processing function on the **body_text**. 
So that step took a while! Let's take a look at what our data looks like when compressed into 2 dimensions. 
"This looks pretty bland. t-SNE was able to reduce the dimensionality of the texts, but now clustering is required. \nLet's use the clusters found by k-means as labels. This will help visually separate different concentrations of topics."
"This looks pretty bland. t-SNE was able to reduce the dimensionality of the texts, but now clustering is required. \nLet's use the clusters found by k-means as labels. This will help visually separate different concentrations of topics."
"The labeled plot gives better insight into how the papers are grouped. Interestingly, both k-means and t-SNE can find independent clusters even though they were run independently. This shows that structure within the literature can be observed and measured to some extent. \n\nNow there are other cases where the colored labels are spread out on the plot. This is a result of t-SNE and k-means finding different connections in the higher dimensional data. The topics of these papers often intersect so it was hard to cleanly separate them. "
## Setup
## Widgets
"# COVID Global Forecast: SIR model + ML regressions\n\nIn the context of the global COVID-19 pandemic, Kaggle has launched several challenges in order to provide useful insights that may answer some of the open scientific questions about the virus. This is the case of the [COVID19 Global Forecasting](https://www.kaggle.com/c/covid19-global-forecasting-week-1), in which participants are encouraged to fit worldwide data in order to predict the pandemic evolution, hopefully helping to determine which factors impact the transmission behavior of COVID-19.\n\n**TABLE OF CONTENTS**\n\n1. [Exploratory data analysis (EDA)](#section1)\n\n    1.1. [COVID-19 global tendency excluding China](#section11)\n    \n    1.2. [COVID-19 tendency in China](#section12)\n    \n    1.3. [Italy, Spain, UK and Singapore](#section13)\n    \n2. [SIR model](#section2)\n\n    2.1. [Implementing the SIR model](#section21)\n    \n    2.2. [Fit SIR parameters to real data](#section22)\n    \n3. [Data enrichment](#section3)\n\n    3.1. [Join data, filter dates and clean missings](#section31)\n    \n    3.2. [Compute lags and trends](#section32)\n    \n    3.3. [Add country details](#section33)\n    \n4. [Predictions for the early stages of the transmission](#section4)\n\n    4.1. [Linear Regression for one country](#section41)\n    \n    4.2. [Linear Regression for all countries (method 1)](#section42)\n    \n    4.3. [Linear Regression for all countries (method 2)](#section43)\n    \n    4.4. [Linear regression with lags](#section44)\n    \n5. [Predictions for the late stages of the transmission](#section5)\n\n    5.1. [Logistic curve fit](#section51)\n    \n    5.2. [Logistic curve fit for all countries](#section52)\n    \n    5.3. [ARIMA](#section53)\n    \n6. [Statement of the author](#section6)\n\n**Disclaimer 1**: this notebook is being updated frequently with the objective of improving predictions by using new models.\n\n**Disclaimer 2**: the training dataset is also updated on a daily basis in order to include the most recent cases. In order to be up to date and prevent data leaking and other potential problems, daily updates on ""filtered dates"" will be applied.\n\n**Disclaimer 3**: the COVID Global Forecasting competition is updated week by week (with a new competition). I'll move the notebook from previous weeks to the new one, so that it only appears in the most recent competition. "
"# 1. Exploratory data analysis (EDA) \n\nFirst of all, let's take a look on the data structure:"
"The dataset covers 163 countries and almost 2 full months from 2020, which is enough data to get some clues about the pandemic. Let's see a few plots of the worldwide tendency to see if we can extract some insights:"
"**Observations**: The global curve shows a rich fine structure, but these numbers are strongly affected by the vector zero country, China. Given that COVID-19 started there, during the initial expansion of the virus there was no reliable information about the real infected cases. In fact,  the criteria to consider infection cases was modified around 2020-02-11, which strongly perturbed the curve as you can see from the figure. "
"## 1.1. COVID-19 global tendency excluding China \n\nSince details of the initial breakthrough strongly interfere with the results, it's recomended to analyze China independently. Let's first see the results without China: "
"**Observations**: In this case the general behavior looks cleaner, and in fact the curve resembles a typical epidemiology model like [SIR](http://mat.uab.cat/matmat/PDFv2013/v2013n03.pdf). SIR models present a large increasing in the number of infections that, once it reaches the maximum of the contagion, decreases with a lower slope. For comparison, a SIR simulation from section [2. SIR model](#section2):\n\n![__results___28_0.png](attachment:__results___28_0.png)"
"## 1.2. COVID-19 tendency in China \n\nSince China was the initial infected country, the COVID-19 behavior is different from the rest of the world. The medical system was not prepared for the pandemic, in fact no one was aware of the virus until several cases were reported. Moreover, China government took strong contention measures in a considerable short period of time and, while the virus is widely spread, they have been able to control the increasing of the infections. "
"**Observations**:\n\n* **Smoothness**. Both plots are less smooth than theoretical simulations or the curve from the rest of the world cumulative\n* **Infected criteria**. The moment in which the criteria to consider an infected case was changed is direclty spotted\n* **Irregularities**. There are some iregularities. I should check the literature in depth to look for evidences, but the reasons may be that both the resources spent to monitor the epidemy and the security measures to stop ot have been changing over time\n* **Plateaux**. It looks like the curve has reached a plateaux, which would imply that China is on their maximum of contagion "
"## 1.3. Italy, Spain, UK and Singapore \n\nBoth Italy and Spain are experiencing the larger increase in COVID-19 positives in Europe. At the same time, UK is a unique case given that it's one of the most important countries in Europe but recently has left the European Union, which has create an effective barrier to human mobility from other countries. The fourth country we will study in this section is Singapore, since it's an asiatic island, is closer to China and its  socio-economic conditions is different from the other three countries."
As a fraction of the total population of each country:
As a fraction of the total population of each country:
"In order to compare the 4 countries, it's also interesting to see the evolution of the infections from the first confirmed case:"
"In order to compare the 4 countries, it's also interesting to see the evolution of the infections from the first confirmed case:"
"**Observations**:\n* **Italy**. With almost 120.000 confirmed cases, Italy shows one of the most alarming scenarios of COVID-19. The infections curve is very steep, and more than 2% of population has been infected\n* **Spain**. Spain has the same number of cumulative infected cases than Italy, near 120.000. However, Spain's total population is lower (around 42 millions) and hence the percentage of population that has been infected rises up to 3%.\n* **United Kingdom**. Despite not being very far from them, the UK shows less cases. This may be due to the number of tests performed, but it's  soon to know for sure. The number of cases is around 40.000, this is, a 0.6 of the total population.\n* **Singapore**. Singapore is relatively isolated given that is an island, and the number of international travels is lower than for the other 3 countries. The number of cases is still very low (>1000), despite the general tendency is to increase. However, the infections started faster in the beginning, but the slope of the infections curve hasn't increased very much in the past weeks. A 0.2% of the population was infected"
"Results obtained for N=world population, only one initial infected case, $\beta=0.3$, $\gamma=0.5$ and a leap pass $h_s = 0.1$ are shown below:"
"**Observations**: \n* The number of infected cases increases for a certain time period, and then eventually decreases given that individuals recover/decease from the disease\n* The susceptible fraction of population decreases as the virus is transmited, to eventually drop to the absorbent state 0\n* The oposite happens for the recovered/deceased case\n\nNotice that different initial conditions and parameter values will lead to other scenarios, feel free to play with these numbers to study the system."
"## 2.2. Fit SIR parameters to real data \n\nThe SIR model is purely theoretical, and we are interested into a real approximation of the COVID-19 expansion in order to extract insights and understand the transmission of the virus. Hence, we need to extract the $\beta$ and $\gamma$ paramaters for each case if we hope to be able to predict the evolution of the system."
"**I'm not happy with the fit of parameters** and I want to work more on this, since I'm not properly reproducing the curves. I'll keep working on this for curiosity, but in the meanwhile I'll develop a data-centric approach to the prediction."
"## 4.1. Linear Regression for one country \n\nSince we are interested into predicting the future time evolution of the pandemic, our first approach consists on a simple Linear Regression. However, remind that **the evolution is** not linear but **exponential** (only in the beginning of the infection), so that a preliminar log transformation is needed. \n\nVisual comparison of both cases for Spain and with data from last 10 days informed, starting on March 1st:"
"As you see, the log transformation results in a fancy straight-like line, which is awesome for Linear Regression. However, let me clarify two important points:\n\n* This ""roughly **exponential behavior**"" **is only true for the initial infection stages of the pandemic** (the initial increasing of infections on the SIR model), but that's exactly the point where most countries are at the moment.\n\n* Why do I only extract the last 10 days of data? For three reasons:\n    1. In order to capture exactly the very short term component of the evolution\n    2. To prevent the effects of certain variables that have been impacting the transmition speed (quarantine vs free circulation)\n    3. To prevent differences on criteria when confirming cases (remember that weird slope on the China plot?)"
## 5.1. Logistic curve fit 
* **Italy**
* **Spain**
* **Italy**
### How many Survived??
"It is evident that not many passengers survived the accident. \n\nOut of 891 passengers in training set, only around 350 survived i.e Only **38.4%** of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn't.\n\nWe will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port Of Embarcation, Age,etc.\n\nFirst let us understand the different types of features."
### Observations:\n1)The Toddlers(age<5) were saved in large numbers(The Women and Child First Policy).\n\n2)The oldest Passenger was saved(80 years).\n\n3)Maximum number of deaths were in the age group of 30-40.
The Women and Child first policy thus holds true irrespective of the class.
### Chances for Survival by Port Of Embarkation
The chances for survival for Port C is highest around 0.55 while it is lowest for S.
The chances for survival for Port C is highest around 0.55 while it is lowest for S.
"### Observations:\n1)Maximum passenegers boarded from S. Majority of them being from Pclass3.\n\n2)The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers.\n\n3)The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around **81%** didn't survive. \n\n4)Port Q had almost 95% of the passengers were from Pclass3."
"### Observations:\n1)Maximum passenegers boarded from S. Majority of them being from Pclass3.\n\n2)The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers.\n\n3)The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around **81%** didn't survive. \n\n4)Port Q had almost 95% of the passengers were from Pclass3."
"### Observations:\n\n1)The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Pclass.\n\n2)Port S looks to be very unlucky for Pclass3 Passenegers as the survival rate for both men and women is very low.**(Money Matters)**\n\n3)Port Q looks looks to be unlukiest for Men, as almost all were from Pclass 3.\n"
The crosstab again shows that larger families were in Pclass3.
### Observations:\n\nHere too the results are quite similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.\n\nThe chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when somebody has >4 parents on the ship.
The lowest fare is **0.0**. Wow!! a free luxorious ride. 
"There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces. As this is also continous, we can convert into discrete values by using binning."
## Correlation Between The Features
"### Interpreting The Heatmap\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\n**POSITIVE CORRELATION:** If an **increase in feature A leads to increase in feature B, then they are positively correlated**. A value **1 means perfect positive correlation**.\n\n**NEGATIVE CORRELATION:** If an **increase in feature A leads to decrease in feature B, then they are negatively correlated**. A value **-1 means perfect negative correlation**.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as **MultiColinearity** as both of them contains almost the same information.\n\nSo do you think we should use both of them as **one of them is redundant**. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between **SibSp and Parch i.e 0.41**. So we can carry on with all features."
"True that..the survival rate decreases as the age increases irrespective of the Pclass.\n\n## Family_Size and Alone\nAt this point, we can create a new feature called ""Family_size"" and ""Alone"" and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not."
"**Family_Size=0 means that the passeneger is alone.** Clearly, if you are alone or family_size=0,then chances for survival is very low. For family size > 4,the chances decrease too. This also looks to be an important feature for the model. Lets examine this further."
"**Family_Size=0 means that the passeneger is alone.** Clearly, if you are alone or family_size=0,then chances for survival is very low. For family size > 4,the chances decrease too. This also looks to be an important feature for the model. Lets examine this further."
"It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family.\n\n## Fare_Range\n\nSince fare is also a continous feature, we need to convert it into ordinal value. For this we will use **pandas.qcut**.\n\nSo what **qcut** does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges."
"### Dropping UnNeeded Features\n\n**Name**--> We don't need name feature as it cannot be converted into any categorical value.\n\n**Age**--> We have the Age_band feature, so no need of this.\n\n**Ticket**--> It is any random string that cannot be categorised.\n\n**Fare**--> We have the Fare_cat feature, so unneeded\n\n**Cabin**--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n\n**Fare_Range**--> We have the fare_cat feature.\n\n**PassengerId**--> Cannot be categorised."
"Now the above correlation plot, we can see some positively related features. Some of them being **SibSp andd Family_Size** and **Parch and Family_Size** and some negative ones like **Alone and Family_Size.**"
Now the accuracy for the KNN model changes as we change the values for **n_neighbours** attribute. The default value is **5**. Lets check the accuracies over various values of n_neighbours.
### Gaussian Naive Bayes
"The classification accuracy can be sometimes misleading due to imbalance. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong.\n\n## Confusion Matrix\n\nIt gives the number of correct and incorrect classifications made by the classifier."
"### Interpreting Confusion Matrix\n\nThe left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong prredictions made. Lets consider the first plot for rbf-SVM:\n\n1)The no. of correct predictions are **491(for dead) + 247(for survived)** with the mean CV accuracy being **(491+247)/891 = 82.8%** which we did get earlier.\n\n2)**Errors**-->  Wrongly Classified 58 dead people as survived and 95 survived as dead. Thus it has made more mistakes by predicting dead as survived.\n\nBy looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived."
### Confusion Matrix for the Best Model
## Feature Importance
## Feature Importance
"We can see the important features for various classifiers like RandomForests, AdaBoost,etc.\n\n#### Observations:\n\n1)Some of the common important features are Initial,Fare_cat,Pclass,Family_Size.\n\n2)The Sex feature doesn't seem to give any importance, which is shocking as we had seen earlier that Sex combined with Pclass was giving a very good differentiating factor. Sex looks to be important only in RandomForests.\n\nHowever, we can see the feature Initial, which is at the top in many classifiers.We had already seen the positive correlation between Sex and Initial, so they both refer to the gender.\n\n3)Similarly the Pclass and Fare_cat refer to the status of the passengers and Family_Size with Alone,Parch and SibSp."
"## **House Prices: EDA to ML (Beginner)**  \n\n**This is my first Kaggle for the House Prices competition.**  \n**It includes the following approaches and techniques:**\n\n* EDA with Pandas and Seaborn\n* Find features with strong correlation to target\n* Data Wrangling, convert categorical to numerical\n* apply the basic Regression models of sklearn \n* use gridsearchCV to find the best parameters for each model\n* compare the performance of the Regressors and choose best one"
"![](https://www.reno.gov/Home/ShowImage?id=7739&t=635620964226970000)\n\n**Competition Description from Kaggle**  \nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n**Data description**  \nThis is a detailed description of the 79 features and their entries, quite important for this competition.  \nYou can download the txt file here: [**download**](https://www.kaggle.com/c/5407/download/data_description.txt)"
**Imports**
"**Settings and switches**\n\n**Here one can choose settings for optimal performance and runtime.**  \n**For example, nr_cv sets the number of cross validations used in GridsearchCV, and**  \n**min_val_corr is the minimum value for the correlation coefficient to the target (only features with larger correlation will be used).** "
### The target variable : Distribution of SalePrice
"As we see, the target variable SalePrice is not normally distributed.  \nThis can reduce the performance of the ML regression models because some assume normal distribution,   \nsee [sklearn info on preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html)  \nTherfore we make a log transformation, the resulting distribution looks much better.  "
"As we see, the target variable SalePrice is not normally distributed.  \nThis can reduce the performance of the ML regression models because some assume normal distribution,   \nsee [sklearn info on preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html)  \nTherfore we make a log transformation, the resulting distribution looks much better.  "
### Numerical and Categorical features
### Plots of relation to target for all numerical features
"**Conclusion from EDA on numerical columns:**\n\nWe see that for some features like 'OverallQual' there is a strong linear correlation (0.79) to the target.  \nFor other features like 'MSSubClass' the correlation is very weak.  \nFor this kernel I decided to use only those features for prediction that have a correlation larger than a threshold value to SalePrice.  \nThis threshold value can be choosen in the global settings : min_val_corr  \n\nWith the default threshold for min_val_corr = 0.4, these features are dropped in Part 2, Data Wrangling:  \n'Id', 'MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF',  'LowQualFinSF',  'BsmtFullBath', 'BsmtHalfBath', 'HalfBath',   \n'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'\n\nWe also see that the entries for some of the numerical columns are in fact categorical values.  \nFor example, the numbers for 'OverallQual' and 'MSSubClass' represent a certain group for that feature ( see data description txt)"
### Relation to SalePrice for all categorical features
"**Conclusion from EDA on categorical columns:**\n\nFor many of the categorical there is no strong relation to the target.  \nHowever, for some fetaures it is easy to find a strong relation.  \nFrom the figures above these are : 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType'\nAlso for the categorical features, I use only those that show a strong relation to SalePrice. \nSo the other columns are dropped when creating the ML dataframes in Part 2 :  \n 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1',  'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', \n'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition' \n "
### Checking correlation to SalePrice for the new numerical columns
"There are few columns with quite large correlation to SalePrice (NbHd_num, ExtQ_num, BsQ_num, KiQ_num).  \nThese will probably be useful for optimal performance of the Regressors in part 3."
# Step 4: Explore Data #\n\nLet's take a moment to look at some of the images in the dataset.
You can display a single batch of images from a dataset with another of our helper functions. The next cell will turn the dataset into an iterator of batches of 20 images.
# Step 6: Training #\n\n## Learning Rate Schedule ##\n\nWe'll train this network with a special learning rate schedule.
"## Fit Model ##\n\nAnd now we're ready to train the model. After defining a few parameters, we're good to go!"
"# Step 7: Evaluate Predictions #\n\nBefore making your final predictions on the test set, it's a good idea to evaluate your model's predictions on the validation set. This can help you diagnose problems in training or suggest ways your model could be improved. We'll look at two common ways of validation: plotting the **confusion matrix** and **visual validation**."
## Confusion Matrix ##\n\nA [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) shows the actual class of an image tabulated against its predicted class. It is one of the best tools you have for evaluating the performance of a classifier.\n\nThe following cell does some processing on the validation data and then creates the matrix with the `confusion_matrix` function included in [`scikit-learn`](https://scikit-learn.org/stable/index.html).
"1. In this problem we have to use 30 different columns and we have to predict the Stage of Breast Cancer M (Malignant)  and B (Bengin)\n 2. This analysis has been done using Basic Machine Learning Algorithm with detailed explanation\n 3. This is good for beginners like as me Lets start.\n \n4.Attribute Information:\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign)\n\n-3-32.Ten real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\n\nb) texture (standard deviation of gray-scale values)\n\nc) perimeter\n\nd) area\n\ne) smoothness (local variation in radius lengths)\n\nf) compactness (perimeter^2 / area - 1.0)\n\ng). concavity (severity of concave portions of the contour)\n\nh). concave points (number of concave portions of the contour)\n\ni). symmetry\n\nj). fractal dimension (""coastline approximation"" - 1)\n\n5  here 3- 32 are divided into three parts first is Mean (3-13),  Stranded Error(13-23) and  Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension) \n\n 6. Here Mean means the means of the all cells,  standard Error of all cell and worst means the worst  cell "
**Import data **
## Data Analysis a little feature selection
"*observation*\n\n - the radius, parameter and area  are highly correlated as expected from their relation*\n    so from these we will use anyone of them *\n - *compactness_mean, concavity_mean and concavepoint_mean are highly correlated so we will use compactness_mean from here *\n - so selected Parameter for use is perimeter_mean, texture_mean, compactness_mean, symmetry_mean*"
#### Visualize frequency distribution of `RainTomorrow` variable
"#### Interpretation\n\n- The above univariate plot confirms our findings that -\n\n   - The `No` variable have 110316 entries, and\n   \n   - The `Yes` variable have 31877 entries.\n\n"
We can plot the bars horizontally as follows :
"### Findings of Univariate Analysis \n\n\n-	The number of unique values in `RainTomorrow` variable is 2.\n\n-	The two unique values are `No` and `Yes`.\n\n-	Out of the total number of `RainTomorrow` values, `No` appears 77.58% times and `Yes` appears 22.42% times.\n\n-	The univariate plot confirms our findings that –\n\n     -  The `No` variable have 110316 entries, and         \n     \n     - The `Yes` variable have 31877 entries.\n"
"On closer inspection, we can see that the `Rainfall`, `Evaporation`, `WindSpeed9am` and `WindSpeed3pm` columns may contain outliers.\n\n\nI will draw boxplots to visualise outliers in the above variables. "
The above boxplots confirm that there are lot of outliers in these variables.
"### Check the distribution of variables\n\n\n- Now, I will plot the histograms to check distributions to find out if they are normal or skewed. \n\n- If the variable follows normal distribution, then I will do `Extreme Value Analysis` otherwise if they are skewed, I will find IQR (Interquantile range)."
"We can see that all the four variables are skewed. So, I will use interquantile range to find outliers."
### Heat Map 
"#### Interpretation\n\n\n\nFrom the above correlation heat map, we can conclude that :-\n\n- `MinTemp` and `MaxTemp` variables are highly positively correlated (correlation coefficient = 0.74).\n\n- `MinTemp` and `Temp3pm` variables are also highly positively correlated (correlation coefficient = 0.71).\n\n- `MinTemp` and `Temp9am` variables are strongly positively correlated (correlation coefficient = 0.90).\n\n- `MaxTemp` and `Temp9am` variables are strongly positively correlated (correlation coefficient = 0.89).\n\n- `MaxTemp` and `Temp3pm` variables are also strongly positively correlated (correlation coefficient = 0.98).\n\n- `WindGustSpeed` and `WindSpeed3pm` variables are highly positively correlated (correlation coefficient = 0.69).\n\n- `Pressure9am` and `Pressure3pm` variables are strongly positively correlated (correlation coefficient = 0.96).\n\n- `Temp9am` and `Temp3pm` variables are strongly positively correlated (correlation coefficient = 0.86).\n"
"Now, I will draw pairplot to depict relationship between these variables."
"#### Interpretation\n\n\n- I have defined a variable `num_var` which consists of `MinTemp`, `MaxTemp`, `Temp9am`, `Temp3pm`, `WindGustSpeed`, `WindSpeed3pm`, `Pressure9am` and `Pressure3pm` variables.\n\n- The above pair plot shows relationship between these variables."
"The confusion matrix shows `20892 + 3285 = 24177 correct predictions` and `3087 + 1175 = 4262 incorrect predictions`.\n\n\nIn this case, we have\n\n\n- `True Positives` (Actual Positive:1 and Predict Positive:1) - 20892\n\n\n- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 3285\n\n\n- `False Positives` (Actual Negative:0 but Predict Positive:1) - 1175 `(Type I error)`\n\n\n- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 3087 `(Type II error)`"
## 16. Classification Metrices 
"## 18. ROC - AUC \n\n\n\n### ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of `TP to (TP + FN).`\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of `FP to (FP + TN).`\n\n\n\n\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n\n\n\n"
ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.
# 1. Packages\n\nYou can find the packages below what I used.
# 2. Importing Data
"This feature is highly correlated with sales but first, you are supposed to sum the sales feature to find relationship. Transactions means how many people came to the store or how many invoices created in a day.\n\nSales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n\nThat's why, transactions will be one of the relevant features in the model. In the following sections, we will generate new features by using transactions."
"There is a stable pattern in Transaction. All months are similar except December from 2013 to 2017 by boxplot. In addition, we've just seen same pattern for each store in previous plot. Store sales had always increased at the end of the year."
"There is a stable pattern in Transaction. All months are similar except December from 2013 to 2017 by boxplot. In addition, we've just seen same pattern for each store in previous plot. Store sales had always increased at the end of the year."
**Let's take a look at transactions by using monthly average sales!**\n\n We've just learned a pattern what increases sales. It was the end of the year. We can see that transactions increase in spring and decrease after spring.
**Let's take a look at transactions by using monthly average sales!**\n\n We've just learned a pattern what increases sales. It was the end of the year. We can see that transactions increase in spring and decrease after spring.
"When we look at their relationship, we can see that there is a highly correlation between total sales and transactions also. "
"When we look at their relationship, we can see that there is a highly correlation between total sales and transactions also. "
"The days of week is very important for shopping. It shows us a great pattern. Stores make more transactions at weekends. Almost, the patterns are same from 2013 to 2017 and Saturday is the most important day for shopping."
"The days of week is very important for shopping. It shows us a great pattern. Stores make more transactions at weekends. Almost, the patterns are same from 2013 to 2017 and Saturday is the most important day for shopping."
"# 4. Oil Price\n\n\nThe economy is one of the biggest problem for the governments and people. It affects all of things in a good or bad way. In our case, Ecuador is an oil-dependent country. Changing oil prices in Ecuador will cause a variance in the model. I researched Ecuador's economy to be able to understand much better and I found an article from IMF. You are supposed to read it if you want to make better models by using oil data.\n\n- https://www.imf.org/en/News/Articles/2019/03/20/NA032119-Ecuador-New-Economic-Plan-Explained\n\n\n\n\n\n\n\nThere are some missing data points in the daily oil data as you can see below. You can treat the data by using various imputation methods. However, I chose a simple solution for that. Linear Interpolation is suitable for this time serie. You can see the trend and predict missing data points, when you look at a time serie plot of oil price."
"# 4. Oil Price\n\n\nThe economy is one of the biggest problem for the governments and people. It affects all of things in a good or bad way. In our case, Ecuador is an oil-dependent country. Changing oil prices in Ecuador will cause a variance in the model. I researched Ecuador's economy to be able to understand much better and I found an article from IMF. You are supposed to read it if you want to make better models by using oil data.\n\n- https://www.imf.org/en/News/Articles/2019/03/20/NA032119-Ecuador-New-Economic-Plan-Explained\n\n\n\n\n\n\n\nThere are some missing data points in the daily oil data as you can see below. You can treat the data by using various imputation methods. However, I chose a simple solution for that. Linear Interpolation is suitable for this time serie. You can see the trend and predict missing data points, when you look at a time serie plot of oil price."
"**I just said, ""Ecuador is a oil-dependent country"" but is it true? Can we really see that from the data by looking at?**\n\nFirst of all, let's look at the correlations for sales and transactions. The correlation values are not strong but the sign of sales is negative. Maybe, we can catch a clue. Logically, if daily oil price is high, we expect that the Ecuador's economy is bad and it means the price of product increases and sales decreases. There is a negative relationship here.  "
"**I just said, ""Ecuador is a oil-dependent country"" but is it true? Can we really see that from the data by looking at?**\n\nFirst of all, let's look at the correlations for sales and transactions. The correlation values are not strong but the sign of sales is negative. Maybe, we can catch a clue. Logically, if daily oil price is high, we expect that the Ecuador's economy is bad and it means the price of product increases and sales decreases. There is a negative relationship here.  "
"You should never decide what you will do by looking at a graph or result! You are supposed to change your view and define new hypotheses.\n\nWe would have been wrong if we had looked at some simple outputs just like above and we had said that there is no relationship with oil prices and let's not use oil price data.\n\nAll right! We are aware of analyzing deeply now. Let's draw a scatter plot but let's pay attention for product families this time. All of the plots almost contains same pattern. When daily oil price is under about 70, there are more sales in the data. There are 2 cluster here. They are over 70 and under 70. It seems pretty understandable actually. \n\nWe are in a good way I think. What do you think? Just now, we couldn't see a pattern for daily oil price, but now we extracted a new pattern from it."
"You should never decide what you will do by looking at a graph or result! You are supposed to change your view and define new hypotheses.\n\nWe would have been wrong if we had looked at some simple outputs just like above and we had said that there is no relationship with oil prices and let's not use oil price data.\n\nAll right! We are aware of analyzing deeply now. Let's draw a scatter plot but let's pay attention for product families this time. All of the plots almost contains same pattern. When daily oil price is under about 70, there are more sales in the data. There are 2 cluster here. They are over 70 and under 70. It seems pretty understandable actually. \n\nWe are in a good way I think. What do you think? Just now, we couldn't see a pattern for daily oil price, but now we extracted a new pattern from it."
"# 5. Sales\n\nOur main objective is, predicting store sales for each product family. For this reason, sales column should be examined more seriously. We need to learn everthing such as seasonality, trends, anomalies, similarities with other time series and so on."
"Most of the stores are similar to each other, when we examine them with correlation matrix. Some stores, such as 20, 21, 22, and 52 may be a little different."
There is a graph that shows us daily total sales below.
There is a graph that shows us daily total sales below.
"I realized some unnecessary rows in the data while I was looking at the time serie of the stores one by one. If you select the stores from above, some of them have no sales at the beginning of 2013. You can see them, if you look at the those stores 20, 21, 22, 29, 36, 42, 52 and 53. I decided to remove those rows before the stores opened. In the following codes, we will get rid of them."
"As you can see below, these examples are too rare and also the sales are low. I'm open your suggestions for these families. I won't do anything for now but, you would like to improve your model you can focus on that.\n\nBut still, I want to use that knowledge whether it is simple and I will create a new feature. It shows that the product family is active or not."
"We can catch the trends, seasonality and anomalies for families."
"We can catch the trends, seasonality and anomalies for families."
"We are working with the stores. Well, there are plenty of products in the stores and we need to know which product family sells much more? Let's make a barplot to see that.\n\nThe graph shows us GROCERY I and BEVERAGES are the top selling families."
"We are working with the stores. Well, there are plenty of products in the stores and we need to know which product family sells much more? Let's make a barplot to see that.\n\nThe graph shows us GROCERY I and BEVERAGES are the top selling families."
#### Does onpromotion column cause a data leakage problem?\n\nIt is really a good question. The Data Leakage is one of the biggest problem when we will fit a model. There is a great discussion from [Nesterenko Marina](https://www.kaggle.com/nesterenkomarina) [@nesterenkomarina](https://www.kaggle.com/nesterenkomarina). You should look at it before fitting a model.\n\n- https://www.kaggle.com/c/store-sales-time-series-forecasting/discussion/277067
How different can stores be from each other? I couldn't find a major pattern among the stores actually. But I only looked at a single plot. There may be some latent patterns. 
"# 6. Holidays and Events\n\nWhat a mess! Probably, you are confused due to the holidays and events data. It contains a lot of information inside but, don't worry. You just need to take a breathe and think! It is a meta-data so you have to split it logically and make the data useful.\n\nWhat are our problems?\n- Some national holidays have been transferred.\n- There might be a few holidays in one day. When we merged all of data, number of rows might increase. We don't want duplicates.\n- What is the scope of holidays? It can be regional or national or local. You need to split them by the scope.\n- Work day issue\n- Some specific events\n- Creating new features etc.\n\n\nEnd of the section, they won't be a problem anymore!"
"# 9. ACF & PACF for each family\n\nThe lag features means,shifting a time serie forward one step or more than one. So, a lag feature can use in the model to improve it. However, how many lag features should be inside the model? For understanding that, we can use ACF and PACF. The PACF is very useful to decide which features should select.\n\nIn our problem, we have multiple time series and each time series have different pattern of course. You know that those time series consists of store-product family combinations and we have 54 stores and 33 product families. We can't examine all of them one by one. For this reason, I will look at average sales for each product but it will be store independent.\n\nIn addition, the test data contains 15 days for each family. We should be careful when selecting lag features. We can't create new lag features from 1 lag to 15 lag. It must be starting 16. "
"I decided to chose these lags 16, 20, 30, 45, 365, 730 from PACF. I don't know that they will help me to improve the model but especially, 365th and 730th lags may be helpful. If you compare 2016 and 2017 years for sales, you can see that they are highly correlated."
"I decided to chose these lags 16, 20, 30, 45, 365, 730 from PACF. I don't know that they will help me to improve the model but especially, 365th and 730th lags may be helpful. If you compare 2016 and 2017 years for sales, you can see that they are highly correlated."
# 10. Simple Moving Average
---\n\n# *Section 1 - Data Exploration*\n\nThe first step is to import needed libraries.
"To begin our analysis, lets take our first look at the dataset. To save some precious time on our Exploratory Data Analysis process, we are going to use 2 libraries: **""pandas_profiling""** and **""autoviz""**."
"## PPS (Predictive Power Score)\n\nYou may have heard about correlation matrices. Basically, correlation matrices are able to identify linear relationships between variables. Because relationships in our data may sometimes be non-linear (most of the times, actually), we can use a PPS (Predictive Power Score) matrix, to figure out **non-linear relations** between columns.\n\nIf you want to understand why PPS is important, I recommend you to read this medium article: https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598\n\nAlso, take a look at the Python PPS implementation used in this notebook: https://github.com/8080labs/ppscore"
"Looking at this PPS matrix, we can see that the best univariate predictor of the **Survived** variable is the column **Ticket**, with 0.19 pps, followed by **Sex**, with 0.13 pps. That makes sense because women were prioritized during the rescue, and ticket is closely related to **Pclass**. The best univariate predictor of the **Parch** variable is the column **Cabin**, with 0.37 pps, and so on."
"---\n\n# *Section 2 - Supervised Learning: Classification*\n\nNow that we have some nice context about the data we are working with, let's dive into the modeling part.\n\nFirst of all, we import the libraries we're going to use."
"## Feature Engineering \n\nTo help us get a better performance, we can create new features based on the original features of our dataset.\n\nSome new features created here were based on the great ideas shown on this brilliant notebook from **Gunes Evitan**: https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial#2.-Feature-Engineering"
"### Feature Importances for Tree-Based Models, top 10 values:"
"## Predictions\n\nNow that we have tried different preprocessing and modeling techniques, resulting in a final best pipeline, let's use it to predict the test data provided by kaggle.\n\n**Remember:** All transformations that were done in the training dataset must be done in the test set."
"### Visualizing Agglomerative Clustering results using the PCA reduced Dataset\n\nTo check out the clusters that were created by the Agglomerative Clustering algorithm, we can use the PCA 2-dimensional dataset to visualize points and it's respective segmentations."
"## 2.3 - DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nThis is another useful algorithm for clustering tasks. As it's name says, it bases itself on the density of the points distributed in the feature space to create clusters. Basically, DBSCAN categorizes data points into 3 types: **Noise** points, **Border** points and **Core** points. It's main parameters are **""n_samples""** and **""eps""**.\n\n![DBSCAN.png](attachment:DBSCAN.png)\n\nLet's think about an example where we defined ""eps"" value as 1.5, and ""n_samples"" as 4. Imagine that you could draw circles (like the ones showed in the image above) around each data point represented in the feature space. The **radius** of these circles is defined by the parameter **""eps""** (1.5, in this example). \n\nNow, to simulate how the algorithm works: first, take an arbitrary point and look at it. If there are at least **""n_samples"" (in this example, 4) points** inside it's ""1.5 eps"" radius drawn circle, this point is considered as **""Core""**, and DBSCAN assigns it to cluster 0. \n\nThen, for each point inside this Core point's drawn circle, do the same task: if there are at least ""n_samples"" points inside their own circles, they're also categorized as ""Core"" points, and also assigned to cluster 0.\n\n![dbscan_2.png](attachment:dbscan_2.png)\n\nIn a case where a data point has **less than ""n_samples""** points inside it's circle (2, for example), the algorithm checks if at least one of these points are considered ""Core"" ones. If so, this data point is classified as **""Border""**. \n\nIf the condition above is false (meaning there are less than ""n_samples"" points inside this point's circle, **and none of them are core ones**), it is classified as **""Noise""** (it can be interpreted as an outlier, that doesn't belong to any cluster). This process continues untill all points are analyzed and clusterized. \n\nThe main benefits of using DBSCAN, when compared to the other algorithms we have seen so far, are: you don't need to previously set the number of clusters to be created; it will be automatically found, depending (mainly) on the values set for ""eps"" and ""n_samples"". Also, it can correctly detect more complex cluster shapes and points that doesn't belong to any clusters (outliers).\n\nBenefits aside, the algorithm also has it's own disadvantadges. It fails to clusterize points when the variance inside each cluster is too different. Aditionally, just like Agglomerative Clustering, it doesn't allow predictions on new data."
## Introduction \n\nHello! This is my very first Kernel. It is meant to give a grasp of a problem of speech representation. I'd also like to take a look on a features specific to this dataset. \n\nContent:\n* [1. Visualization of the recordings - input features](#visualization)\n   * [1.1. Wave and spectrogram](#waveandspectrogram)\n   * [1.2. MFCC](#mfcc)\n   * [1.3. Sprectrogram in 3d](#3d)\n   * [1.4. Silence removal](#resampl)\n   * [1.5. Resampling - dimensionality reductions](#silenceremoval)\n   * [1.6. Features extraction steps](#featuresextractionsteps)\n* [2. Dataset investigation](#investigations)\n   * [2.1. Number of files](#numberoffiles)\n   * [2.2. Mean spectrograms and fft](#meanspectrogramsandfft)\n   * [2.3. Deeper into recordings](#deeper)\n   * [2.4. Length of recordings](#len)\n   * [2.5. Note on Gaussian Mixtures modeling](#gmms)\n   * [2.6. Frequency components across the words](#components)\n   * [2.7. Anomaly detection](#anomaly)\n* [3. Where to look for the inspiration](#wheretostart)\n\nAll we need is here:
"\n# 1. Visualization \n \n\nThere are two theories of a human hearing - place ( https://en.wikipedia.org/wiki/Place_theory_(hearing) (frequency-based) and temporal (https://en.wikipedia.org/wiki/Temporal_theory_(hearing) )\nIn speech recognition, I see two main tendencies - to input [spectrogram](https://en.wikipedia.org/wiki/Spectrogram) (frequencies), and more sophisticated features MFCC - Mel-Frequency Cepstral Coefficients, PLP. You rarely work with raw, temporal data.\n\nLet's visualize some recordings!\n\n## 1.1. Wave and spectrogram:\n \n\nChoose and read some file:"
"Frequencies are in range (0, 8000) according to [Nyquist theorem](https://en.wikipedia.org/wiki/Nyquist_rate).\n\nLet's plot it:"
"If we use spectrogram as an input features for NN, we have to remember to normalize features. (We need to normalize over all the dataset, here's example just for one, which doesn't give good *mean* and *std*!)"
"## 1.3. Spectrogram in 3d\n \n\nBy the way, times change, and the tools change. Have you ever seen spectrogram in 3d?"
(Don't know how to set axis ranges to proper values yet. I'd also like it to be streched like a classic spectrogram above..)
"We can agree that the entire word can be heard. It is impossible to cut all the files manually and do this basing on the simple plot. But you can use for example *webrtcvad* package to have a good *VAD*.\n\nLet's plot it again, together with guessed alignment of* 'y' 'e' 's'* graphems"
"## 1.5. Resampling - dimensionality reduction\n \n\nAnother way to reduce the dimensionality of our data is to resample recordings.\n\nYou can hear that the recording don't sound very natural, because they are sampled with 16k frequency, and we usually hear much more. However, [the most speech related frequencies are presented in smaller band](https://en.wikipedia.org/wiki/Voice_frequency). That's why you can still understand another person talking to the telephone, where GSM signal is sampled to 8000 Hz.\n\nSummarizing, we could resample our dataset to 8k. We will discard some information that shouldn't be important, and we'll reduce size of the data.\n\nWe have to remember that it can be risky, because this is a competition, and sometimes very small difference in performance wins, so we don't want to lost anything. On the other hand, first experiments can be done much faster with smaller training size.\n\nWe'll need to calculate FFT (Fast Fourier Transform). Definition:\n"
"There's a very important fact. Recordings come from very different sources. As far as I can tell, some of them can come from mobile GSM channel.\n\nNevertheless,** it is extremely important to split the dataset in a way that one speaker doesn't occur in both train and test sets.**\nJust take a look and listen to this two examlpes:"
Even better to listen:
There are also recordings with some weird silence (some compression?):\n
"It means, that we have to prevent overfitting to the very specific acoustical environments.\n"
Let's plot mean FFT for every word
"## 2.5. Gaussian Mixtures modeling\n \n\nWe can see that mean FFT looks different for every word. We could model each FFT with a mixture of Gaussian distributions. Some of them however, look almost identical on FFT, like *stop* and *up*... But wait, they are still distinguishable when we look at spectrograms! High frequencies are earlier than low at the beginning of *stop* (probably *s*).\n\nThat's why temporal component is also necessary. There is a [Kaldi](http://kaldi-asr.org/) library, that can model words (or smaller parts of words) with GMMs and model temporal dependencies with [Hidden Markov Models](https://github.com/danijel3/ASRDemos/blob/master/notebooks/HMM_FST.ipynb).\n\nWe could use simple GMMs for words to check what can we model and how hard it is to distinguish the words. We can use [Scikit-learn](http://scikit-learn.org/) for that, however it is not straightforward and lasts very long here, so I abandon this idea for now."
## 2.7. Anomaly detection\n \n\nWe should check if there are any recordings that somehow stand out from the rest. We can lower the dimensionality of the dataset and interactively check for any anomaly.\nWe'll use PCA for dimensionality reduction:
"Notice that there are *yes/e4b02540_nohash_0.wav*, *go/0487ba9b_nohash_0.wav* and more points, that lie far away from the rest. Let's listen to them."
"## INTRODUCTION\n- It’s a Python based scientific computing package targeted at two sets of audiences:\n    - A replacement for NumPy to use the power of GPUs\n    - Deep learning research platform that provides maximum flexibility and speed\n- pros: \n    - Interactively debugging PyTorch. Many users who have used both frameworks would argue that makes pytorch significantly easier to debug and visualize.\n    - Clean support for dynamic graphs\n    - Organizational backing from Facebook\n    - Blend of high level and low level APIs\n- cons:\n    - Much less mature than alternatives\n    - Limited references / resources outside of the official documentation\n- I accept you know neural network basics. If you do not know check my tutorial. Because I will not explain neural network concepts detailed, I only explain how to use pytorch for neural network\n- Neural Network tutorial: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners \n- The most important parts of this tutorial from matrices to ANN. If you learn these parts very well, implementing remaining parts like CNN or RNN will be very easy. \n\n**Content:**\n1. [Basics of Pytorch](#1)\n    - Matrices\n    - Math\n    - Variable\n1. [Linear Regression](#2)\n1. [Logistic Regression](#3)\n1. [Artificial Neural Network (ANN)](#4)\n1. [Concolutional Neural Network (CNN)](#5)\n1. Recurrent Neural Network (RNN)\n    - https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch\n1. Long-Short Term Memory (LSTM)\n    - https://www.kaggle.com/kanncaa1/long-short-term-memory-with-pytorch"
" \n## Basics of Pytorch\n### Matrices\n- In pytorch, matrix(array) is called tensors.\n- 3*3 matrix koy. This is 3x3 tensor.\n- Lets look at array example with numpy that we already know.\n    - We create numpy array with np.numpy() method\n    - Type(): type of the array. In this example it is numpy\n    - np.shape(): shape of the array. Row x Column"
" \n### Linear Regression\n- Detailed linear regression tutorial is in my machine learning tutorial in part ""Regression"". I will not explain it in here detailed.\n- Linear Regression tutorial: https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners\n- y = Ax + B.\n    - A = slope of curve\n    - B = bias (point that intersect y-axis)\n- For example, we have car company. If the car price is low, we sell more car. If the car price is high, we sell less car. This is the fact that we know and we have data set about this fact.\n- The question is that what will be number of car sell if the car price is 100."
- Now this plot is our collected data\n- We have a question that is what will be number of car sell if the car price is 100$\n- In order to solve this question we need to use linear regression.\n- We need to line fit into this data. Aim is fitting line with minimum error.\n- **Steps of Linear Regression**\n    1. create LinearRegression class\n    1. define model from this LinearRegression class\n    1. MSE: Mean squared error\n    1. Optimization (SGD:stochastic gradient descent)\n    1. Backpropagation\n    1. Prediction\n- Lets implement it with Pytorch
- Now this plot is our collected data\n- We have a question that is what will be number of car sell if the car price is 100$\n- In order to solve this question we need to use linear regression.\n- We need to line fit into this data. Aim is fitting line with minimum error.\n- **Steps of Linear Regression**\n    1. create LinearRegression class\n    1. define model from this LinearRegression class\n    1. MSE: Mean squared error\n    1. Optimization (SGD:stochastic gradient descent)\n    1. Backpropagation\n    1. Prediction\n- Lets implement it with Pytorch
"- Number of iteration is 1001.\n- Loss is almost zero that you can see from plot or loss in epoch number 1000.\n- Now we have a trained model.\n- While usign trained model, lets predict car prices."
"- Number of iteration is 1001.\n- Loss is almost zero that you can see from plot or loss in epoch number 1000.\n- Now we have a trained model.\n- While usign trained model, lets predict car prices."
" \n### Logistic Regression\n- Linear regression is not good at classification.\n- We use logistic regression for classification.\n- linear regression + logistic function(softmax) = logistic regression\n- Check my deep learning tutorial. There is detailed explanation of logistic regression. \n    - https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n- **Steps of Logistic Regression**\n    1. Import Libraries\n    1. Prepare Dataset\n        - We use MNIST dataset.\n        - There are 28*28 images and 10 labels from 0 to 9\n        - Data is not normalized so we divide each image to 255 that is basic normalization for images.\n        - In order to split data, we use train_test_split method from sklearn library\n        - Size of train data is 80% and size of test data is 20%.\n        - Create feature and target tensors. At the next parts we create variable from these tensors. As you remember we need to define variable for accumulation of gradients.\n        - batch_size = batch size means is that for example we have data and it includes 1000 sample. We can train 1000 sample in a same time or we can divide it 10 groups which include 100 sample and train 10 groups in order. Batch size is the group size. For example, I choose batch_size = 100, that means in order to train all data only once we have 336 groups. We train each groups(336) that have batch_size(quota) 100. Finally we train 33600 sample one time.\n        - epoch: 1 epoch means training all samples one time.\n        - In our example: we have 33600 sample to train and we decide our batch_size is 100. Also we decide epoch is 29(accuracy achieves almost highest value when epoch is 29). Data is trained 29 times. Question is that how many iteration do I need? Lets calculate: \n            - training data 1 times = training 33600 sample (because data includes 33600 sample) \n            - But we split our data 336 groups(group_size = batch_size = 100) our data \n            - Therefore, 1 epoch(training data only once) takes 336 iteration\n            - We have 29 epoch, so total iterarion is 9744(that is almost 10000 which I used)\n        - TensorDataset(): Data set wrapping tensors. Each sample is retrieved by indexing tensors along the first dimension.\n        - DataLoader(): It combines dataset and sample. It also provides multi process iterators over the dataset.\n        - Visualize one of the images in dataset\n    1. Create Logistic Regression Model\n        - Same with linear regression.\n        - However as you expect, there should be logistic function in model right?\n        - In pytorch, logistic function is in the loss function where we will use at next parts.\n    1. Instantiate Model\n        - input_dim = 28*28 # size of image px*px\n        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n        - create model\n    1. Instantiate Loss \n        - Cross entropy loss\n        - It calculates loss that is not surprise :)\n        - It also has softmax(logistic function) in it.\n    1. Instantiate Optimizer \n        - SGD Optimizer\n    1. Traning the Model\n    1. Prediction\n- As a result, as you can see from plot, while loss decreasing, accuracy(almost 85%) is increasing and our model is learning(training).    "
"# **Introduction**\n\nThis is an intial Explanatory Data Analysis for the [Mercari Price Suggestion Challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge#description) with matplotlib and Plot.ly - a visualization tool that creates beautiful interactive plots and dashboards.  The competition is hosted by Mercari, the biggest Japanese community-powered shopping app with the main objective to predict an accurate price that Mercari should suggest to its sellers, given the item's information. \n\n***Update***: The abundant amount of food from my family's Thanksgiving dinner has really energized me to continue working on this model. I decided to dive deeper into the NLP analysis and found an amazing tutorial by Ahmed BESBES. The framework below is  based on his source code at: https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html. It provides guidance on pre-processing documents and  machine learning techniques (K-means and LDA) to clustering topics.  So that this kernel will be divided into 2 parts: \n\n1. Explanatory Data Analysis \n2. Text Processing  \n    2.1. Pre-processing: Tokenizing and  tf-idf algorithm  \n    2.2. K-means Clustering  \n    2.3. Latent Dirichlet Allocation (LDA)  \n "
"# **Exploratory Data Analysis**\nOn the first look at the data, besides the unique identifier (item_id), there are 7 variables in this model. This notebook will sequentially go through each of them with a brief statistical summary. \n\n1. **Numerical/Continuous Features**\n    1. price: the item's final bidding price. This will be our reponse / independent variable that we need to predict in the test set\n    2. shipping cost     \n \n1. **Categorical Features**: \n    1. shipping cost: A binary indicator, 1 if shipping fee is paid by seller and 0 if it's paid by buyer\n    2. item_condition_id: The condition of the items provided by the seller\n    1. name: The item's name\n    2. brand_name: The item's producer brand name\n    2. category_name: The item's single or multiple categories that are separated by ""\"" \n    3. item_description: A short description on the item that may include removed words, flagged by [rm]"
### Wordcloud of all comments
"In the wordcloud above, we can see the most common words in the comments. These words include ""wikipedia"", ""page"", and ""article"" among other words. More offensive words like ""f**k"" seem to occur less often, indicating that toxic, insulting comments are seen less frequently than non-toxic comments. "
### English vs. Non-English
"We can see that English comments dominate the training data, with a total of 220636 comments written in English and a mere 2913 comments written in languages other than English. There is a heavy imbalance in the language of comments in the training data."
### Bar chart of non-English languages
"We can see that German, Scots, and Danish are the most common non-English languages featuring in the dataset, with more than 100 comments in each language. Spanish, Persian, and Arabic are not far behind. We can thus conclude that Europe and the middle-east are the most represented regions in the dataset."
### Pie chart of non-English languages
"From the pie chart above, we can once again see that German, Danish, and Scots with more than 15% of the pie belonging to each of these three languages."
### World plot of non-English languages
"From the word plot above, we can see that western Europe and the middle-east are the most represented regions in the dataset. Africa, Asia, and eastern Europe are relatively under-represented."
### Non-English European 
"We can see that German and English are the most common European languages to feature in the dataset, although Spanish and Greek are not far behind."
"We can see that German and English are the most common European languages to feature in the dataset, although Spanish and Greek are not far behind."
"This plot shows that middle-eastern languages such as Arabic and Persian are represented more than languages from the Indian subcontinent or south-east Asia, such as Hindi, Vietnamese, and Indonesian. There is not a single comment in Mandarin, Korean, or Japanese!"
"This plot shows that middle-eastern languages such as Arabic and Persian are represented more than languages from the Indian subcontinent or south-east Asia, such as Hindi, Vietnamese, and Indonesian. There is not a single comment in Mandarin, Korean, or Japanese!"
We can see that Africa is not as well represented as the other continents in the dataset. The two most common African languages in the dataset are Afrikaans and Somali.
### Distribution of comment words
"From the plot above, we can see that the distribution of comment words has a strong rightward (positive) skew with maximum probability denisty occuring at around 13 words. As the number of words increases beyond 13, the frequency reduces sharply."
### Average comment words vs. Language
"I have plotted the average comment words in each language above. Certain languages tend to have more words on average than other languages. For example, comments written in Akan, Persian, and Sinhala have more than 300 words on average! This may be due to the small number of samples in these languages and presence of one or two outliers."
### Average comment length vs. Country
"In the world plot above, we can see that certain regions in the world tend to have a higher average comment length comment length than other countries. Persian, Arabic, and Hindi comments all have at least 100 words on average! Most long-comment languages seem to originate in Asia."
"### Negative sentiment\n\nNegative sentiment refers to negative or pessimistic emotions. It is a score between 0 and 1; the greater the score, the more negative the abstract is."
"From the above plot, we can see that negative sentiment has a strong rightward (positive) skew, indicating that negativity is usually on the lower side. This suggests that most comments are not toxic or negative. In fact, the most common negativity value is around 0.04. Virtually no comments have a negativity greater than 0.8."
### Negativity vs. Country
"In the world plot, we can see that the language with highest average negativity is Afrikaans. Also, languages from western Europe and south-east Asia tend to have higher toxicity than Hindi and Russian."
### Negativity vs. Toxicity
"I have plotted the distribution of negativity for toxic and non-toxic comments above. We can clearly see that toxic comments have a significantly greater negative sentiment than toxic comments (on average). The probability density of negativity peaks at around 0 for non-toxic comments, while the negativity for toxic comments are minimum at this point. This suggests that a comment is very likely to be non-toxic if it has a negativity of 0."
"### Positive sentiment\n\nPositive sentiment refers to positive or optimistic emotions. It is a score between 0 and 1; the greater the score, the more positive the abstract is."
"From the above plot, we can see that positive sentiment has a strong rightward (positive) skew, indicating that positivity is usually on the lower side. This suggests that most comments do not express positivity explicitly. In fact, the most common negativity value is around 0.08. Virtually no comments have a positivity greater than 0.8."
### Positivity vs. Country
"In the world plot, we can see that the languages with the highest average positivity are English, Spanish, Portuguese, and Danish."
### Positivity vs. Toxicity
"I have plotted the distribution of positivity for toxic and non-toxic comments above. We can see that both the distributions are very similar, indicating that positivity is not an accurate indicator of toxicity in comments. "
"### Neutrality sentiment\n\nNeutrality sentiment refers to the level of bias or opinion in the text. It is a score between 0 and 1; the greater the score, the more neutral/unbiased the abstract is."
"From the above plot, we can see that the neutrality sentiment distribution has a strong leftward (negative) skew, which is in constrast to the negativity and positivity sentiment distributions. This indicates that the comments tend to be very neutral and unbiased in general. This also suggests that most comments are not highly opinionated and polarizing, meaning that most comments are non-toxic."
### Neutrality vs. Country
"In the world plot, we can see that the languages with the highest neutrality are Persian, Hindi, and Russian. Few western European languages like German and English seem to have a lower average neutrality than most other languages."
### Neutrality vs. Toxicity
"We can see that non-toxic comments tend to have a higher neutrality value than toxic comments on average. The probability density of the non-toxic distribution experiences a sudden jump at 1, and the probability density of the toxic distribution is significantly lower at the same point. This suggests that a comment with neutrality close to 1 is more likely to be non-toxic than toxic."
"### Compound sentiment\n\nCompoundness sentiment refers to the total level of sentiment in the sentence. It is a score between -1 and 1; the greater the score, the more emotional the abstract is."
"From the distribution above, we can see that compound sentiment is evenly distributed across the specturm (from -1 to 1) with very high variance and random peaks throughout the range."
### Average compound sentiment vs. Country
"In the world plot above, we can see that western European countries, south-east Asia, and Turkey have a lower average compound sentiment than most other countries. India, Russia, and Iran are among the countries with the maximum compound sentiment."
### Compound sentiment vs. Toxicity
"We can see that compound sentiment tends to be higher for non-toxic comments as compared to toxic comments. The non-toxic distribution has a leftward (negative) skew, while the toxic distribution has a positive (rightward) skew. This indicates that non-toxic comments tend to have a higher compound sentiment than toxic comments on average."
### Distribution of Flesch reading ease
"The Flesch reading ease distribution has a slight leftward (negative) skew, although the distribution is roughly normal. The most common values of the metric lie between 66 and 66.5."
### Flesch reading ease vs. Country
"In the world plot above, we can see that the Flesch readability ease is maximum in the Russian and Vietnamese languages. These languages have few words per sentence and few syllables per word, indicating that they are ""easier"" to read."
### Flesch reading ease vs. Toxicity
"I have plotted the distribution of Flesch reading ease for toxic and non-toxic comments above. We can see that both the distributions are very similar, indicating that Flesch reading ease is not an accurate indicator of toxicity in comments. "
### Distribution of automated readability
"The automated readability distribution has a slight rightward (positive) skew, although the distribution has a roughly normal shape. The most common value of automated readability in the dataset is approximately 9. Very few comments have a readability ease greater than 60."
### Automated readability vs. Country
"In the world plot above, we can see that automated readability is maximum in Hindi, Arabic, and Somali comments. Whereas, Turkish, English, and south-east Asian comments seem to have relatively lower automated readability value than most countries."
### Automated readability vs. Toxicity
"I have plotted the distribution of automated readability for toxic and non-toxic comments above. We can see that both the distributions are very similar, indicating that automated readability is not an accurate indicator of toxicity in comments. "
### Distribution of Dale-Chall readability
"The Dale-Chall readability distribution has a slight rightward (positive) skew, although the distribution has a roughly normal shape. There are also peaks to the left of the main distribution. The most common value of automated readability in the dataset is approximately 7. Very few comments have a readability ease greater than 20."
### Dale-Chall readability vs. Country
"The Dale-Chall readability score seems to be maximum in middle-eastern and south-east Asian languages. Russian and Arabic, on the other hand, have a lower Dale-Chall readability than most other languages in the dataset."
### Dale-Chall readability
"Dale-Chall readability seems to be higher (on average) for non-toxic comments, indicating that non-toxic comments use more ""sophisticated"" or ""difficult"" language. Toxic comments, on the other hand, are more blunt. We can see this from the fact that the non-toxic distribution peaks at a higher value than the toxic distribution."
### Non-toxic vs. Toxic
"We can see from the above wordclouds, that toxic comments use more insluting or hateful words such as ""f**k"", while the non-toxic comments do not usually use such words."
### Obscene vs. Severe Toxic vs. Threat vs. Insult
"In the above wordclouds, we can see that most of these categories use insulting/hateful language. But, the threat category seems to be slightly different from the remaining categories, as it uses words like ""kill"" and ""die"", indicating that most threats involve threats to kill someone."
### Pie chart of targets
"From the pie chart above, we can see that the most common target is toxic, and the other targets, such as insult and threat are relatively uncommon."
### Bar chart of targets
"From the bar chart above, we can once again see that toxic is the most common target, while threat is the rarest."
### Toxicity vs. Country
"From the above world plot, we can see that Irish and Afrikaans are the countries with maximum average toxicity, while most other countries have a similar average toxicity value."
"### Visualize model predictions\n\nNow, I will visualize the performance of the model on few validation samples."
We can see that the model gets only two out of four answers correct. We need to find a better model to achieve a higher accuracy.
#### Trends:\n \nAmazon's stock price is showing signs of upper trend yearly. \n Amazon's stock price show upper trend signs during January (December Sales tend to give a boost to Amazon's stock price)\nThere is no weekly trend for stock prices. \n
"Technical Analysis: \nIn this section we will go into basic technical concepts when dealing with stock investing. This are simple theories however, we shouldn't solely rely on these concepts to maximize profits as it is the case with patterns related to moving averages. Before going into this concepts, I will like to show how OHLC and Candlesticks are interpreted.\n\nOpen High Low Close (OHLC): \n\n\nCandleSticks: \n\n\nMoving Averages: \n"
"\n## Python Libraries\n* In this section, we import used libraries during this kernel."
"\n## Data Content\n1. **ID number**\n1. **Diagnosis (M = malignant, B = benign)**\n1. **radius (mean of distances from center to points on the perimeter)**\n1. **texture (standard deviation of gray-scale values)**\n1. **perimeter**\n1. **area**\n1. **smoothness (local variation in radius lengths)**\n1. **compactness (perimeter^2 / area - 1.0)**\n1. **concavity (severity of concave portions of the contour)**\n1. **concave points (number of concave portions of the contour)**\n1. **symmetry**\n1. **fractal dimension (""coastline approximation"" - 1)**\n\n* The mean, standard error and ""worst"" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n* All feature values are recoded with four significant digits.\n* Missing attribute values: none\n* Class distribution: 357 benign, 212 malignant"
Before violin and swarm plot we need to normalization or standirdization. Because differences between values of features are very high to observe on plot. I plot features in 3 group and each group includes 10 features to observe better.
"Lets interpret the plot above together. For example, in **texture_mean** feature, median of the *Malignant* and *Benign* looks like separated so it can be good for classification. However, in **fractal_dimension_mean** feature,  median of the *Malignant* and *Benign* does not looks like separated so it does not gives good information for classification."
"In order to compare two features deeper, lets use joint plot. Look at this in joint plot below, it is really correlated.\n Pearsonr value is correlation value and 1 is the highest. Therefore, 0.86 is looks enough to say that they are correlated. \nDo not forget, we are not choosing features yet, we are just looking to have an idea about them."
"What about three or more feauture comparision ? For this purpose we can use pair grid plot. Also it seems very cool :)\nAnd we discover one more thing **radius_worst**, **perimeter_worst** and **area_worst** are correlated as it can be seen pair grid plot. We definetely use these discoveries for feature selection."
"What about three or more feauture comparision ? For this purpose we can use pair grid plot. Also it seems very cool :)\nAnd we discover one more thing **radius_worst**, **perimeter_worst** and **area_worst** are correlated as it can be seen pair grid plot. We definetely use these discoveries for feature selection."
"Up to this point, we make some comments and discoveries on data already. If you like what we did, I am sure swarm plot will open the pub's door :) "
"**What if we want to observe all correlation between features?** Yes, you are right. The answer is heatmap that is old but powerful plot method."
"Well, finaly we are in the pub and lets choose our drinks at feature selection part while using heatmap(correlation matrix)."
"After drop correlated features, as it can be seen in below correlation matrix, there are no more correlated features. Actually, I know and you see there is correlation value 0.9 but lets see together what happen if we do not drop it."
"Well, we choose our features but **did we choose correctly ?** Lets use random forest and find accuracy according to chosen features."
"Well, we choose our features but **did we choose correctly ?** Lets use random forest and find accuracy according to chosen features."
"Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. \nNow lets see other feature selection methods to find better results."
"Best 5 feature to classify is that **area_mean, area_se, texture_mean, concavity_worst and concavity_mean**. So lets se what happens if we use only these best scored 5 feature."
"Accuracy is almost 96% and as it can be seen in confusion matrix, we make few wrong prediction. What we did up to now is that we choose features according to correlation matrix and according to selectkBest method. Although we use 5 features in selectkBest method accuracies look similar.\nNow lets see other feature selection methods to find better results."
"Finally, we find best 11 features that are **texture_mean, area_mean, concavity_mean, texture_se, area_se, concavity_se, symmetry_se, smoothness_worst, concavity_worst, symmetry_worst and fractal_dimension_worst** for best classification. Lets look at best accuracy with plot.\n"
"Lets look at what we did up to this point. Lets accept that guys this data is very easy to classification. However, our first purpose is actually not finding good accuracy. Our purpose is learning how to make **feature selection and understanding data.** Then last make our last feature selection method."
"\n### 5) Tree based feature selection and random forest classification\n\nIn random forest classification method there is a **feature_importances_** attributes that is the feature importances (the higher, the more important the feature). **!!! To use feature_importance method, in training data there should not be correlated features. Random forest choose randomly at each iteration, therefore sequence of feature importance list can change.**\n"
"As you can seen in plot above, after 5 best features importance of features decrease. Therefore we can focus these 5 features. As I sad before, I give importance to understand features and find best of them. "
"\n## Feature Extraction with PCA\n\nWe will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.\n "
"* According to variance ration, 3 component can be chosen.\n* If you have any doubt about PCA, you can check my intuitive way of PCA tutorial."
# Importing Libraries
"## Data Preparation\n* As we are working with four different datasets, so i will be creating a dataframe storing all emotions of the data in dataframe with their paths.\n* We will use this dataframe to extract features for our model training."
First let's plot the count of each emotions in our dataset.
We can also plot waveplots and spectograms for audio signals\n\n* Waveplots - Waveplots let us know the loudness of the audio at a given time.\n* Spectograms - A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. It’s a representation of frequencies changing with respect to time for given audio/music signals.
#### 1. Simple Audio
#### 2. Noise Injection
#### 2. Noise Injection
We can see noise injection is a very good augmentation technique because of which we can assure our training model is not overfitted
#### 3. Stretching
#### 4. Shifting
#### 4. Shifting
#### 5. Pitch
#### 5. Pitch
"- From the above types of augmentation techniques i am using noise, stretching(ie. changing speed) and some pitching."
**Preprocessing**
"There seems to be 2 extreme outliers on the bottom right, really large houses that sold for really cheap. More generally, the author of the dataset recommends removing 'any houses with more than 4000 square feet' from the dataset.  \nReference : https://ww2.amstat.org/publications/jse/v19n3/decock.pdf"
**1* Linear Regression without regularization**
"RMSE on Training set shows up weird here (not when I run it on my computer) for some reason.  \nErrors seem randomly distributed and randomly scattered around the centerline, so there is that at least. It means our model was able to capture most of the explanatory information."
"**2* Linear Regression with Ridge regularization (L2 penalty)**\n\nFrom the *Python Machine Learning* book by Sebastian Raschka :  Regularization is a very useful method to handle collinearity, filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights.  \n\nRidge regression is an L2 penalized model where we simply add the squared sum of the weights to our cost function."
"We're getting a much better RMSE result now that we've added regularization. The very small difference between training and test results indicate that we eliminated most of the overfitting. Visually, the graphs seem to confirm that idea.  \n\nRidge used almost all of the existing features."
"**3* Linear Regression with Lasso regularization (L1 penalty)**\n\nLASSO stands for *Least Absolute Shrinkage and Selection Operator*. It is an alternative regularization method, where we simply replace the square of the weights by the sum of the absolute value of the weights. In contrast to L2 regularization, L1 regularization yields sparse feature vectors : most feature weights will be zero. Sparsity can be useful in practice if we have a high dimensional dataset with many features that are irrelevant.  \n\nWe can suspect that it should be more efficient than Ridge here."
"RMSE results are better both on training and test sets. The most interesting thing is that Lasso used only one third of the available features. Another interesting tidbit : it seems to give big weights to Neighborhood categories, both in positive and negative ways. Intuitively it makes sense, house prices change a whole lot from one neighborhood to another in the same city.  \n\nThe ""MSZoning_C (all)"" feature seems to have a disproportionate impact compared to the others. It is defined as *general zoning classification : commercial*. It seems a bit weird to me that having your house in a mostly commercial zone would be such a terrible thing."
"**4* Linear Regression with ElasticNet regularization (L1 and L2 penalty)**\n\nElasticNet is a compromise between Ridge and Lasso regression. It has a L1 penalty to generate sparsity and a L2 penalty to overcome some of the limitations of Lasso, such as the number of variables (Lasso can't select more features than it has observations, but it's not the case here anyway)."
"The optimal L1 ratio used by ElasticNet here is equal to 1, which means it is exactly equal to the Lasso regressor we used earlier (and had it been equal to 0, it would have been exactly equal to our Ridge regressor). The model didn't need any L2 regularization to overcome any potential L1 shortcoming."
# Importing all the libraries needed
"# Context \n\n\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help"
## Correlation Matrix\n### Its necessary to remove correlated variables to improve your model.One can find correlations using pandas “.corr()” function and can visualize the correlation matrix using plotly express.\n- Lighter shades represents positive correlation\n- Darker shades represents negative correlation
"Here we can see Heart Disease has a high negative correlation with ""MaxHR"" and somewhat negative correlation wiht ""Cholesterol"", where as here positive correatlation with ""Oldpeak"",""FastingBS"" and ""RestingBP"""
"To plot multiple pairwise bivariate distributions in a dataset, you can use the pairplot() function. This shows the relationship for (n, 2) combination of variable in a DataFrame as a matrix of plots and the diagonal plots are the univariate plots."
### Now to check the linearity of the variables it is a good practice to plot distribution graph and look for skewness of features. Kernel density estimate (kde) is a quite useful tool for plotting the shape of a distribution.
### Now to check the linearity of the variables it is a good practice to plot distribution graph and look for skewness of features. Kernel density estimate (kde) is a quite useful tool for plotting the shape of a distribution.
# Outliers\n\nA box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables.The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution.The box plot (a.k.a. box and whisker diagram) is a standardized way of displaying the distribution of data based on the five number summary:\n- Minimum\n- First quartile\n- Median\n- Third quartile\n- Maximum.\n\nIn the simplest box plot the central rectangle spans the first quartile to the third quartile (the interquartile range or IQR).A segment inside the rectangle shows the median and “whiskers” above and below the box show the locations of the minimum and maximum.
"### Robust Scaler\nWhen working with outliers we can use Robust Scaling for scakling our data,\nIt scales features using statistics that are robust to outliers. This method removes the median and scales the data in the range between 1st quartile and 3rd quartile. i.e., in between 25th quantile and 75th quantile range. This range is also called an Interquartile range. \nThe median and the interquartile range are then stored so that it could be used upon future data using the transform method. If outliers are present in the dataset, then the median and the interquartile range provide better results and outperform the sample mean and variance. \nRobustScaler uses the interquartile range so that it is robust to outliers"
"# 3. Handling Categorical Variables\n\nCategorical variables/features are any feature type can be classified into two major types:\n- Nominal\n- Ordinal\n\nNominal variables are variables that have two or more categories which do not have any kind of order associated with them. For example, if gender is classified into two groups, i.e. male and female, it can be considered as a nominal variable.Ordinal variables, on the other hand, have “levels” or categories with a particular order associated with them. For example, an ordinal categorical variable can be a feature with three different levels: low, medium and high. Order is important.\n\nIt is a binary classification problem:\nthe target here is **not skewed** but we use the best metric for this binary classification problem which would be Area Under the ROC Curve (AUC). We can use precision and recall too, but AUC combines these two metrics. Thus, we will be using AUC to evaluate the model that we build on this dataset.\n\nWe have to know that computers do not understand text data and thus, we need to convert these categories to numbers. A simple way of doing that can be to use :\n- Label Encoding\n```python\nfrom sklearn.preprocessing import LabelEncoder\n```\n- One Hot Encoding\n```python\npd.get_dummies()\n```\n\nbut we need to understand where to use which type of label encoding:\n\n**For not Tree based Machine Learning Algorithms the best way to go will be to use One-Hot Encoding**\n- One-Hot-Encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. \n- The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality. In these cases, I typically employ one-hot-encoding followed by PCA for dimensionality reduction. I find that the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes. PCA finds the linear overlap, so will naturally tend to group similar features into the same feature\n\n**For Tree based Machine Learning Algorithms the best way to go is with Label Encoding**\n\n- LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat. Still there are algorithms like decision trees and random forests that can work with categorical variables just fine and LabelEncoder can be used to store values using less disk space."
# dist1 & dist2\nPlotting with logx to better show the distribution. Possibly this could be the distance of the transaction vs. the card owner's home/work address. This is just a guess.
"# C1 - C14\nBecause we are provided many numerical columns, we can create a pairplot to plot feature interactions. I know these plots can be hard to read, but it is helpful for gaining intution about potential feature interactions and if certain features have more variance than others."
## Identity info as a function of time
"## Compare Numeric Features in Train and Test\nSimilar to above but for the transaction data, specific examples that look interesting."
"## Visualization utilities\ndata -> pixels, nothing of much interest for the machine learning practitioner in this section."
# Datasets
# Libraries
# Data
**Target distribution**
"The target is highly balanced, so we luckily don't have to consider techniques like under/over-sampling."
**Continuous features**
"*Notes:*\n* 0-18 year olds were **more** likely to be transported than not.\n* 18-25 year olds were **less** likely to be transported than not.\n* Over 25 year olds were about **equally** likely to be transported than not.\n\n*Insight:*\n* Create a new feature that indicates whether the passanger is a child, adolescent or adult."
"*Notes:*\n* 0-18 year olds were **more** likely to be transported than not.\n* 18-25 year olds were **less** likely to be transported than not.\n* Over 25 year olds were about **equally** likely to be transported than not.\n\n*Insight:*\n* Create a new feature that indicates whether the passanger is a child, adolescent or adult."
"*Notes:*\n* Most people don't spend any money (as we can see on the left).\n* The distribution of spending decays exponentially (as we can see on the right).\n* There are a small number of outliers.\n* People who were transported tended to spend less.\n* RoomService, Spa and VRDeck have different distributions to FoodCourt and ShoppingMall - we can think of this as luxury vs essential amenities. \n\n*Insight:*\n* Create a new feature that tracks the total expenditure across all 5 amenities.\n* Create a binary feature to indicate if the person has not spent anything. (i.e. total expenditure is 0).\n* Take the log transform to reduce skew."
**Categorical features**
*Notes:*\n* VIP does not appear to be a useful feature; the target split is more or less equal. \n* CryoSleep appears the be a very useful feature in contrast.\n\n*Insights:*\n* We might consider dropping the VIP column to prevent overfitting.
Bin age feature into groups. This will be helpful for filling missing values like expenditure according to age.
**Expenditure**
Calculate total expenditure and identify passengers with no expenditure.
**Passenger group**
Extract passenger group and group size from PassengerId.
"We can't really use the Group feature in our models because it has too big of a cardinality (6217) and would explode the number of dimensions with one-hot encoding.\n\nThe Group size on the other hand should be a useful feature. In fact, we can compress the feature further by creating a 'Solo' column that tracks whether someone is travelling on their own or not. The figure on the right shows that group size=1 is less likely to be transported than group size>1."
"We can't really use the Group feature in our models because it has too big of a cardinality (6217) and would explode the number of dimensions with one-hot encoding.\n\nThe Group size on the other hand should be a useful feature. In fact, we can compress the feature further by creating a 'Solo' column that tracks whether someone is travelling on their own or not. The figure on the right shows that group size=1 is less likely to be transported than group size>1."
**Cabin location**
"Extract deck, number and side from cabin feature."
"*This is interesting!* It appears that Cabin_number is grouped into chunks of 300 cabins. This means we can compress this feature into a categorical one, which indicates which chunk each passenger is in.\n\n*Other notes:* The cabin deck 'T' seems to be an outlier (there are only 5 samples)."
"*This is interesting!* It appears that Cabin_number is grouped into chunks of 300 cabins. This means we can compress this feature into a categorical one, which indicates which chunk each passenger is in.\n\n*Other notes:* The cabin deck 'T' seems to be an outlier (there are only 5 samples)."
**Last name**
Calculate family size from last name.
# Missing values
"Missing values make up about 2% of the data, which is a relatively small amount. For the most part, they don't seem to be happening at the same time (except the features made from splitting Cabin and Name), but let's inspect closer."
"*Notes:*\n* Missing values are independent of the target and for the most part are isolated. \n* Even though only 2% of the data is missing, about 25% of all passengers have at least 1 missing value.\n* PassengerId is the only (original) feature to not have any missing values. \n\n\n*Insight:*\n* Since most of the missing values are isolated it makes sense to try to fill these in as opposed to just dropping rows.\n* If there is a relationship between PassengerId and other features we can fill missing values according to this column."
**HomePlanet and CabinDeck**
"*Notes:*\n* Passengers on decks A, B, C or T came from Europa.\n* Passengers on deck G came from Earth.\n* Passengers on decks D, E or F came from multiple planets."
**HomePlanet and Surname**
**Fantastic!** Everyone with the same surname comes from the same home planet.
**HomePlanet and Destination**
"Most people heading towards TRAPPIST-1e came from Earth so it makes sense to guess they came from there. But remember from earlier, no one on deck D came from Earth so we need to filter these out."
\nThe reason we are filling missing surnames is because we will use surnames later to fill missing values of other features. It also means we can improve the accuracy of the family size feature.
The majority (83%) of groups contain only 1 family. So let's fill missing surnames according to the majority surname in that group.
**CabinSide and Group**
**Another rule!** Everyone in the same group is also on the same cabin side. For cabin deck and cabin number there is also a fairly good (but not perfect) correlation with group.
**CabinNumber and CabinDeck**
There is an interesting pattern here. The cabin_number and group_number share a linear relationship on a deck by deck basis. We can therefore extrapolate the missing cabin numbers using linear regression on a deck by deck basis to get an approximate cabin number.
Let's look at the distribution of the predicted probabilities.
It is interesting to see that the models are either very confident or very unconfident but not much in between.
"## CSV to DataFrame\n\nCSV files can be loaded into a dataframe by calling `pd.read_csv` . After loading the training and test files, print a `sample` to see what you're working with."
## Visualizing Data\n\nVisualizing data is crucial for recognizing underlying patterns to exploit in the model. 
"## Considerations\n\nAs we can see here, the pixel array data is stored as a Numpy array, a powerful numeric Python library for handling and manipulating matrix data (among other things). In addition, it is apparent here that the original radiographs have been preprocessed for us as follows:\n\n* The relatively high dynamic range, high bit-depth original images have been rescaled to 8-bit encoding (256 grayscales). For the radiologists out there, this means that the images have been windowed and leveled already. In clinical practice, manipulating the image bit-depth is typically done manually by a radiologist to highlight certain disease processes. To visually assess the quality of the automated bit-depth downscaling and for considerations on potentially improving this baseline, consider consultation with a radiologist physician.\n\n* The relativley large original image matrices (typically acquired at >2000 x 2000) have been resized to the data-science friendly shape of 1024 x 1024. For the purposes of this challenge, the diagnosis of most pneumonia cases can typically be made at this resolution. To visually assess the feasibility of diagnosis at this resolution, and to determine the optimal resolution for pneumonia detection (oftentimes can be done at a resolution *even smaller* than 1024 x 1024), consider consultation with a radiogist physician.\n\n## Visualizing An Example\n\nTo take a look at this first DICOM image, let's use the `pylab.imshow()` method:"
"# Exploring the Data and Labels\n\nAs alluded to above, any given patient may potentially have many boxes if there are several different suspicious areas of pneumonia. To collapse the current CSV file dataframe into a dictionary with unique entries, consider the following method:"
"# Visualizing Boxes\n\nIn order to overlay color boxes on the original grayscale DICOM files, consider using the following  methods (below, the main method `draw()` requires the method `overlay_box()`):"
"As we saw above, patient `00436515-870c-4b36-a041-de91049b9ab4` has pnuemonia so let's take a look at the overlaid bounding boxes:"
"In this notebook, let us try and explore the data given for Zillow prize competition. Before we dive deep into the data, let us know a little more about the competition.\n\n**Zillow:**\n\nZillow is an online real estate database company founded in 2006 - Wikipedia\n\n**Zestimate:**\n\n“Zestimates” are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property. And, by continually improving the median margin of error (from 14% at the onset to 5% today),\n\n**Objective:**\n\nBuilding a model to improve the Zestimate residual error.\n\nThe competition is in two stages. This public competition will go on till Jan 2018 and has $50,000 in prize. Please make sure to read about the [Prize details][1] and [Competition overview][2] since it is quite different in this one.\n\nLet us first import the necessary modules.\n\n\n  [1]: https://www.kaggle.com/c/zillow-prize-1#prizes\n  [2]: https://www.kaggle.com/c/zillow-prize-1#Competition%20Overview"
Let us list the files present in the input folder.
"**Logerror:**\n\nTarget variable for this competition is ""logerror"" field. So let us do some analysis on this field first. "
This looks nice with some outliers at both the ends.! \n\nLet us remove the outliers and then do a histogram plot on the same.
Wow. nice normal distribution on the log error.\n\n**Transaction Date:**\n\nNow let us explore the date field. Let us first check the number of transactions in each month. 
"As we could see from the data page as well\n*The train data has all the transactions before October 15, 2016, plus some of the transactions after October 15, 2016.*\n\nSo we have shorter bars in the last three months. \n\n**Parcel Id:**"
There are so many NaN values in the dataset. So let us first do some exploration on that one. 
Let us explore the latitude and longitude variable to begin with.
Let us explore the latitude and longitude variable to begin with.
"From the data page, *we are provided with a full list of real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016.*\n\nWe have about 90,811 rows in train but we have about 2,985,217 rows in properties file. So let us merge the two files and then carry out our analysis. "
"**Univariate Analysis:**\n\nSince there are so many variables, let us first take the 'float' variables alone and then get the correlation with the target variable to see how they are related."
The correlation of the target variable with the given set of variables are low overall. \n\nThere are few variables at the top of this graph without any correlation values. I guess they have only one unique value and hence no correlation value. Let us confirm the same.
Here as well the distribution is very similar to the previous one. No wonder the correlation between the two variables are also high.\n\n**Bathroom Count:**
"There is an interesting 2.279 value in the bathroom count.\n\nEdit: As MihwaHan pointed in the comments, this is the mean value :)\n\nNow let us check how the log error changes based on this."
"There is an interesting 2.279 value in the bathroom count.\n\nEdit: As MihwaHan pointed in the comments, this is the mean value :)\n\nNow let us check how the log error changes based on this."
**Bedroom count:**
**Bedroom count:**
3.03 is the mean value with which we replaced the Null values.
**YearBuilt:**\n\nLet us explore how the error varies with the yearbuilt variable.
There is a minor incremental trend seen with respect to built year.\n\nNow let us see how the logerror varies with respect to latitude and longitude.
There is a minor incremental trend seen with respect to built year.\n\nNow let us see how the logerror varies with respect to latitude and longitude.
There are no visible pockets as such with respect to latitude or longitude atleast with the naked eye.\n\nLet us take the variables with highest positive correlation and highest negative correlation to see if we can see some visible patterns.
There are no visible pockets as such with respect to latitude or longitude atleast with the naked eye.\n\nLet us take the variables with highest positive correlation and highest negative correlation to see if we can see some visible patterns.
"There are no visible patterns here as well. So this is going to be a hard competition to predict I guess.\n\nJust for fun, we will let the machine form some arbitrary pattern for us :D"
"There are no visible patterns here as well. So this is going to be a hard competition to predict I guess.\n\nJust for fun, we will let the machine form some arbitrary pattern for us :D"
Hurray.! Finally we got some nice pattern in the data :P\n\nWe had an understanding of important variables from the univariate analysis. But this is on a stand alone basis and also we have linearity assumption. Now let us build a non-linear model to get the important variables by building Extra Trees model.
Hurray.! Finally we got some nice pattern in the data :P\n\nWe had an understanding of important variables from the univariate analysis. But this is on a stand alone basis and also we have linearity assumption. Now let us build a non-linear model to get the important variables by building Extra Trees model.
"Seems ""tax amount"" is the most importanct variable followed by ""structure tax value dollar count"" and ""land tax value dollor count""\n\n"
"Seems ""tax amount"" is the most importanct variable followed by ""structure tax value dollar count"" and ""land tax value dollor count""\n\n"
"Using xgboost, the important variables are 'structured tax value dollar count' followed by 'latitude' and 'calculated finished square feet' "
"Let's import some libraries. First, we will need the [statsmodels](http://statsmodels.sourceforge.net/stable/) library, which has many statistical modeling functions, including time series. For R afficionados who had to move to Python, `statsmodels` will definitely look more familiar since it supports model definitions like 'Wage ~ Age + Education'."
"As an example, let's look at real mobile game data. Specifically, we will look into ads watched per hour and in-game currency spend per day:"
"Unfortunately, we cannot make predictions far in the future -- in order to get the value for the next step, we need the previous values to be actually observed. But moving average has another use case - smoothing the original time series to identify trends. Pandas has an implementation available with [`DataFrame.rolling(window).mean()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). The wider the window, the smoother the trend. In the case of very noisy data, which is often encountered in finance, this procedure can help detect common patterns."
Let's smooth by the previous 4 hours.
"Judging by the plots, our model was able to successfully approximate the initial time series, capturing the daily seasonality, overall downwards trend, and even some anomalies. If you look at the model deviations, you can clearly see that the model reacts quite sharply to changes in the structure of the series but then quickly returns the deviation to the normal values, essentially ""forgetting"" the past. This feature of the model allows us to quickly build anomaly detection systems, even for noisy series data, without spending too much time and money on preparing the data and training the model."
"We'll apply the same algorithm for the second series which, as you may recall, has trend and a 30-day seasonality."
White noise chart:
"The process generated by the standard normal distribution is stationary and oscillates around 0 with with deviation of 1. Now, based on this process, we will generate a new one where each subsequent value will depend on the previous one: $x_t = \rho x_{t-1} + e_t$ "
Here is the code to render the plots.
"On the first plot, you can see the same stationary white noise as before. On the second plot with $\rho$ increased to 0.6, wider cycles appeared, but it still appears stationary overall. The third plot deviates even more from the 0 mean but still oscillates about the mean. Finally, with $\rho=1$, we have a random walk process i.e. a non-stationary time series.\n\nThis happens because, after reaching the critical value, the series $x_t = \rho x_{t-1} + e_t$ does not return to its mean value. If we subtract $x_{t-1}$ from both sides, we will get $x_t - x_{t-1} = (\rho - 1) x_{t-1} + e_t$, where the expression on the left is referred to as the first difference. If $\rho=1$, then the first difference gives us stationary white noise $e_t$. This is the main idea behind the [Dickey-Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test) for stationarity of time series (testing the presence of a unit root). If we can get a stationary series from a non-stationary series using the first difference, we call those series integrated of order 1. The null hypothesis of the test is that the time series is non-stationary, which was rejected on the first three plots and finally accepted on the last one. We have to say that the first difference is not always enough to get a stationary series as the process might be integrated of order d, d > 1 (and have multiple unit roots). In such cases, the augmented Dickey-Fuller test is used, which checks multiple lags at once.\n\nWe can fight non-stationarity using different approaches: various order differences, trend and seasonality removal, smoothing, and transformations like Box-Cox or logarithmic."
We can visualize the resulting features.
"Since we now have different scales in our variables, thousands for the lag features and tens for categorical, we need to transform them into same scale for exploring feature importance and, later, regularization. "
Let's look at the averages by hour.
"Finally, let's put all the transformations together in a single function ."
# Objective\nThe objective of this notebook is to detect missing values and then go over some of the methods used for imputing them.\n\n\n\n# Data\n\nThere are two publically available datasets which will be used to explain the concepts:\n\n* 1. [Titanic Dataset](https://www.kaggle.com/c/titanic) for Non Time Series problem\n* 2. [Air Quality Data in India (2015 - 2020)](https://www.kaggle.com/rohanrao/air-quality-data-in-india) for Time Series problem\n\n\n# Loading necessary libraries and datasets
## Reading in the dataset\n* Reading in the Titanic Dataset.
"## Detecting missing data visually using Missingno library\n\n>To graphically analyse the missingness of the data, let's use a library called [Missingno](https://github.com/ResidentMario/missingno) It is a package for graphical analysis of missing values. To use this library, we need to import it as follows: `import missingno as msno`"
">The bar chart above gives a quick graphical overview of the completeness of the dataset. We can see that Age, Cabin and embarked columns have missing values. Next,it would make sense to find out the locations of the missing data."
## Finding reason for missing data using a Heatmap 
The heatmap function shows that there are no strong correlations between missing values of different features. This is good; low correlations further indicate that the data are MAR.
## Finding reason for missing data using Dendrogram \nA dendogram is a tree diagram of missingness. It groups the highly correlated variables together.
">Let's read the above dendrogram from a top-down perspective: \n* Cluster leaves which linked together at a distance of zero fully predict one another's presence—one variable might always be empty when another is filled, or they might always both be filled or both empty, and so on(missingno documentation)\n>\n>![Screenshot%202020-04-25%20at%208.19.56%20AM.png](attachment:Screenshot%202020-04-25%20at%208.19.56%20AM.png)\n>\n>* the missingness of Embarked tends to be more similar to Age than to Cabin and so on.However, in this particluar case, the correlation is high since Embarked column has a very few missing values.\n\nThis dataset doesn't have much missing values but if you use the same methodology on datasets having a lot of missing values, some interesting pattern will definitely emerge."
"# Convolutional Neural Networks (CNN)\n\nContent: \n* [Loading the Data Set](#1)\n* [Normalization, Reshape and Label Encoding ](#2)\n* [Train Test Split](#3)\n* [Convolutional Neural Network](#4)\n    * [What is Convolution Operation?](#5)\n    * [Same Padding](#6)\n    * [Max Pooling](#7)\n    * [Flattening](#8)\n    * [Full Connection](#9)\n* [Implementing with Keras](#10)\n    * [Create Model](#11)\n    * [Define Optimizer](#12)\n    * [Compile Model](#13)\n    * [Epochs and Batch Size](#14)\n    * [Data Augmentation](#15)\n    * [Fit the Model](#16)\n    * [Evaluate the Model](#17)\n* [Deep Learning Tutorial for Beginners](https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners)\n* [Artificial Neural Network with Pytorch](https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers)\n* [Convolutional Neural Network with Pytorch](https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers)\n* [Recurrent Neural Network with Pytorch](https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch)\n* [Conclusion](#18)\n"
\n## Loading the Data Set\n* In this part we load and visualize the data.
"## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. "
"## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *scikit* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load."
"## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *scikit* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load."
"## 3.2 Meet and Great Data\n\nThis is the meet and great step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this tasks, we first import our data. Next we use the info() and sample () function, to get a quick and dirty overview of variable datatypes (i.e. qualitative vs quantitative). Click here for the [Source Data Dictionary](https://www.kaggle.com/c/titanic/data).\n\n1. The *Survived* variable is our outcome or dependent variable. It is a binary nominal datatype of 1 for survived and 0 for did not survive. All other variables are potential predictor or independent variables. **It's important to note, more predictor variables do not make a better model, but the right variables.**\n2. The *PassengerID* and *Ticket* variables are assumed to be random unique identifiers, that have no impact on the outcome variable. Thus, they will be excluded from analysis.\n3. The *Pclass* variable is an ordinal datatype for the ticket class, a proxy for socio-economic status (SES), representing 1 = upper class, 2 = middle class, and 3 = lower class.\n4. The *Name* variable is a nominal datatype. It could be used for [feature engineering](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/) to derive the gender from title, family size from surname, and SES from titles like doctor or master; however these variables already exist. For the first model iteration, this variable will be excluded from analysis. It can be used during subsequent iterations to evaluate if more complexity improves the base model accuracy. \n5. The *Sex* and *embarked* variables are a nominal datatype. They will be converted to dummy variables for mathematical calculations.\n6. The *Age* and *fare* variable are continuous quantitative datatypes.\n7. The *SibSp* represents number of siblings/spouse aboard and *Parch* represents number of parents or children aboard. Both are dicrete quantitative datatypes. This can be used for feature engineering to create a family size or is alone variable.\n8. The *cabin* variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occurred and SES from deck levels. However, since there are many null values, it does not add value and thus is excluded from analysis."
"## 5.1 Model Optimization\n\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~85% accuracy. If this were a college course, that would be a B-grade. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of model optimization. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So, keep that in mind.\n\nFor model optimization, we have a couple options: 1) find a ""better"" algorithm, 2) tune our current algorithm parameters, 3) feature engineer new variables to find new signals in the data, or 4) we can go back to the beginning and determine if we asked the right questions, got the right data, and made the right decisions along the process.\n\n### Data Science 101: Determine a Baseline Accuracy ###\nBefore we decide how-to make our model better, let's determine if ~85% is good enough. To do that, we have to go back to the basics of data science 101. We know this is a binary problem, because there are only two possible outcomes; passengers survived or died. So, think of it like a coin flip. If you have a fair coin and you guessed heads or tail, then you have a 50-50 chance of guessing right. So, let's set 50% as an F grade, because if your model accuracy is any worse than that, then why do I need you when I can just flip a coin?\n\nOkay, so with no information about the dataset, we can always get 50% with a binary problem. But we have information about the dataset, so we should be able to do better. We know that 1,502/2,224 or 67.5% of people died. Therefore, If I just guessed that 100% of people died, then I would be right 67.5% of the time. So, let's set 68% as a D grade, because again, if your model accuracy is any worst that that, then why do I need you, when I can just assume if you were on the Titanic that day you died and have a 68% accuracy.\n\n### Data Science 101: How-to Create Your Own Model ###\nOur accuracy is increasing, but can we do better? Are there any signals in our data? To illustrate this, we're going to build our own decision tree model, because it is the easiest to conceptualize and requires simple addition and multiplication calculations. When creating a decision tree, you want to ask questions that gives you better information about your outcome by segregated the survived/1 from the dead/0. This is part science and part art, so let's just play the 21-question game to show you how it works. If you want to follow along on your own, download the train dataset and import into Excel. Create a pivot table with survival in the columns, count and % of row count in the values, and the features described below in the rows.\n\nRemember, the name of the game is to create subsets using a decision tree model to get survived/1 in one bucket and dead/0 in another bucket. Our rule of thumb will be the majority rules. Meaning, if the majority or 50% or more survived, then everybody in our subgroup survived/1, but if 50% or less survived then if everybody in our subgroup died/0. Also, we will stop if the subgroup is 10% of our total dataset or 9 cases and/or our model accuracy plateaus or decreases. Got it? Let's go!\n\n***Question 1: Were you on the Titanic?*** If Yes, then majority (62%) died. Note our sample survival is different than our population of 68%. Nonetheless, if we assumed everybody died, our sample accuracy is 62%.\n\n***Question 2: Are you male or female?*** Male, majority (81%) died. Female, majority (74%) survived. Giving us an accuracy of 79%.\n\n***Question 3A (going down the female branch with count = 344): Are you in class 1, 2, or 3?*** Class 1, majority (97%) survived and Class 2, majority (92%) survived. Since are dead group is less than 9, we will stop going down this branch. Class 3, is even at a 50-50 split. No new information to improve our model is gained.\n\n***Question 4A (going down the female class 3 branch with count = 144): Did you embark from port C, Q, or S?*** We gain a little information. C and Q, the majority still survived, so no change. Also, the dead subgroup is 9 or less, so we will stop. S, the majority (63%) died. So, we will change females, class 3, embarked S from assuming they survived, to assuming they died. Our model accuracy increases to 81%. \n\n***Question 5A (going down the female class 3 embarked S branch with count = 88):*** So far, it looks like we made good decisions. Adding another level does not seem to gain much more information. This subgroup 55 died and 33 survived, since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. We can play with our features. One I found was fare 0-8, majority survived. It's a small sample size 11-9, but one often used in statistics. We slightly improve our accuracy, but not much to move us past 82%. So, we'll stop here.\n\n***Question 3B (going down the male branch with count = 577):*** Going back to question 2, we know the majority of males died. So, we are looking for a feature that identifies a subgroup that majority survived. Surprisingly, class or even embarked didn't matter, title does and gets us to 82%. Guess and checking other features, none seem to push us past 82%. So, we'll stop here for now.\n\nSo, with very little information, we get to 82% accuracy. We'll give that a grade of a C for average or our baseline. But can we do better? By making a better decision tree, new features, etc. Before we do, let's code what we just wrote above.\n"
"# Credits\nProgramming is all about ""borrowing"" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \n\n* Müller, Andreas C.; Guido, Sarah. Introduction to Machine Learning with Python: A Guide for Data Scientists. O'Reilly Media.\n\n"
"# Titanic Project Example Walk Through \nIn this notebook, I hope to show how a data scientist would go about working through a problem. The goal is to correctly predict if someone survived the Titanic shipwreck. I thought it would be fun to see how well I could do in this competition without deep learning. \n\n*The accompanying video is located here:* https://www.youtube.com/watch?v=I3FBJdiExcg\n\n**Best results : 79.425 % accuracy (Top 12%)**\n\n## Overview \n### 1) Understand the shape of the data (Histograms, box plots, etc.)\n\n### 2) Data Cleaning \n\n### 3) Data Exploration\n\n### 4) Feature Engineering \n\n### 5) Data Preprocessing for Model\n\n### 6) Basic Model Building \n\n### 7) Model Tuning \n\n### 8) Ensemble Modle Building \n\n### 9) Results "
"Here we import the data. For this analysis, we will be exclusively working with the Training set. We will be validating based on data from the training set as well. For our final submissions, we will make predictions based on the test set. "
"Here we import the data. For this analysis, we will be exclusively working with the Training set. We will be validating based on data from the training set as well. For our final submissions, we will make predictions based on the test set. "
"## Project Planning\nWhen starting any project, I like to outline the steps that I plan to take. Below is the rough outline that I created for this project using commented cells. "
# **6. Import libraries** \n\n[Table of Contents](#0.1)\n
### Ignore warnings\n
# **15. Use elbow method to find optimal number of clusters** \n\n[Table of Contents](#0.1)
"- By the above plot, we can see that there is a kink at k=2. \n\n- Hence k=2 can be considered a good number of the cluster to cluster this data.\n\n- But, we have seen that I have achieved a weak classification accuracy of 1% with k=2.\n\n- I will write the required code with k=2 again for convinience."
Loading the libraries
## 1.2. Read data 
Missing values : \n* Insulin = 48.7% - 374\n* SkinThickness = 29.56% - 227\n* BloodPressure = 4.56% - 35\n* BMI = 1.43% - 11\n* Glucose = 0.65% - 5
"OK, all missing values are encoded with NaN value"
**To fill these Nan values the data distribution needs to be understood against the target**. 
A **correlation matrix** is a table showing correlation coefficients between sets of variables. Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj). This allows you to see which pairs have the highest correlation.
Bellow we define a stylized report with Plotly
## 5.5. Scores Tables 
We can complete model performance report with a table contain all results by fold
# 6. Machine Learning 
"That seems reasonable. Let's also check with a **[Receiver Operator Curve (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)**,"
"Another common metric is the **Area Under the Curve**, or **AUC**. This is a convenient way to capture the performance of a model in a single number, although it's not without certain issues. As a rule of thumb, an AUC can be classed as follows,\n\n- 0.90 - 1.00 = excellent\n- 0.80 - 0.90 = good\n- 0.70 - 0.80 = fair\n- 0.60 - 0.70 = poor\n- 0.50 - 0.60 = fail\n\nLet's see what the above ROC gives us,"
"So, it looks like the most important factors in terms of permutation is a thalessemia result of 'reversable defect'. The high importance of 'max heart rate achieved' type makes sense, as this is the immediate, subjective state of the patient at the time of examination (as opposed to, say, age, which is a much more general factor).\n\nLet's take a closer look at the number of major vessles using a **Partial Dependence Plot** (learn more [here](https://www.kaggle.com/dansbecker/partial-plots)). These plots vary a single variable in a single row across a range of values and see what effect it has on the outcome. It does this for several rows and plots the average effect. Let's take a look at the 'num_major_vessels' variable, which was at the top of the permutation importance list,"
"So, we can see that as the number of major blood vessels *increases*, the probability of heart disease *decreases*. That makes sense, as it means more blood can get to the heart.\n\nWhat about the 'age',"
"So, we can see that as the number of major blood vessels *increases*, the probability of heart disease *decreases*. That makes sense, as it means more blood can get to the heart.\n\nWhat about the 'age',"
"That's a bit odd. The higher the age, the lower the chance of heart disease? Althought the blue confidence regions show that this might not be true (the red baseline is within the blue zone).\n\nWhat about the 'st_depression',"
"That's a bit odd. The higher the age, the lower the chance of heart disease? Althought the blue confidence regions show that this might not be true (the red baseline is within the blue zone).\n\nWhat about the 'st_depression',"
"Interestingly, this variable also shows a reduction in probability the higher it goes. What exactly is this? A search on Google brought me to the following description by Anthony L. Komaroff, MD, an internal medicine specialist [5](https://www.sharecare.com/health/circulatory-system-health/what-st-segment-electrocardiogram-ecg) .... *""An electrocardiogram (ECG) measures the heart's electrical activity. The waves that appear on it are labeled P, QRS, and T. Each corresponds to a different part of the heartbeat. The **ST segment** represents the heart's electrical activity immediately after the right and left ventricles have contracted, pumping blood to the lungs and the rest of the body. Following this big effort, ventricular muscle cells relax and get ready for the next contraction. During this period, little or no electricity is flowing, so the ST segment is even with the baseline or sometimes slightly above it. The faster the heart is beating during an ECG, the shorter all of the waves become. **The shape and direction of the ST segment are far more important than its length. Upward or downward shifts can represent decreased blood flow to the heart from a variety of causes, including heart attack, spasms in one or more coronary arteries (Prinzmetal's angina), infection of the lining of the heart (pericarditis) or the heart muscle itself (myocarditis), an excess of potassium in the bloodstream, a heart rhythm problem, or a blood clot in the lungs (pulmonary embolism).""***\n\n    [6](https://www.cvphysiology.com/CAD/CAD012)"
"So, this variable, which is described as 'ST depression induced by exercise relative to rest', seems to suggest the higher the value the higher the probability of heart disease, but the plot above shows the opposite. Perhaps it's not just the depression amount that's important, but the interaction with the slope type? Let's check with a 2D PDP,"
"It looks like a low depression is bad in both cases. Odd.\n\nLet's see what the SHAP values tell us. These work by showing the influence of the values of every variable in a single row, compared to their baseline values (learn more [here](https://www.kaggle.com/dansbecker/shap-values))."
# INTRODUCTION\n\nWe have a data which classified if patients have heart disease or not according to features in it. We will try to use this data to create a model which tries predict if a patient has this disease or not. We will use logistic regression (classification) algorithm.
## Read Data
## Comparing Models
Our models work fine but best of them are KNN and Random Forest with 88.52% of accuracy. Let's look their confusion matrixes.
"## How Autoencoders work - Understanding the math and implementation\n\n### Contents \n\n\n1. Introduction\n\n    1.1 What are Autoencoders ? \n    1.2 How Autoencoders Work ? \n\n2. Implementation and UseCases\n\n    2.1 UseCase 1: Image Reconstruction \n    2.2 UseCase 2: Noise Removal \n    2.3 UseCase 3: Sequence to Sequence Prediction \n\n\n\n\n\n## 1. Introduction\n## 1.1 What are Autoencoders \n\nAutoencoders are a special type of neural network architectures in which the output is same as the input. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input (in other words, model the identity function). These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low-dimensional code that can be used by the decoder to reproduce the original input.\n\nA typical autoencoder architecture comprises of three main components: \n\n- **Encoding Architecture :** The encoder architecture comprises of series of layers with decreasing number of nodes and ultimately reduces to a latent view repersentation.  \n- **Latent View Repersentation :** Latent view repersents the lowest level space in which the inputs are reduced and information is preserved.  \n- **Decoding Architecture :** The decoding architecture is the mirro image of the encoding architecture but in which number of nodes in every layer increases and ultimately outputs the similar (almost) input.  \n\n![](https://i.imgur.com/Rrmaise.png)\n\nA highly fine tuned autoencoder model should be able to reconstruct the same input which was passed in the first layer. In this kernel, I will walk you through the working of autoencoders and their implementation.  Autoencoders are widly used with the image data and some of their use cases are: \n\n- Dimentionality Reduction   \n- Image Compression   \n- Image Denoising   \n- Image Generation    \n- Feature Extraction  \n\n\n\n## 1.2 How Autoencoders work \n\nLets understand the mathematics behind autoencoders. The main idea behind autoencoders is to learn a low level repersenation of a high level dimentional data. Lets try to understand the encoding process with an example.  Consider a data repersentation space (N dimentional space which is used to repersent the data) and consider the data points repersented by two variables : x1 and x2. Data Manifold is the space inside the data repersentation space in which the true data resides. "
"To repersent this data, we are currently using 2 dimensions - X and Y. But it is possible to reduce the dimensions of this space into lower dimensions ie. 1D. If we can define following : \n\n- Reference Point on the line : A  \n- Angle L with a horizontal axis  \n\nthen any other point, say B, on line A can be repersented in terms of Distance ""d"" from A and angle L.  "
"To repersent this data, we are currently using 2 dimensions - X and Y. But it is possible to reduce the dimensions of this space into lower dimensions ie. 1D. If we can define following : \n\n- Reference Point on the line : A  \n- Angle L with a horizontal axis  \n\nthen any other point, say B, on line A can be repersented in terms of Distance ""d"" from A and angle L.  "
"But the key question here is with what logic or rule, point B can be represented in terms of A and angle L. Or in other terms, what is the equation among B, A and L. The answer is straigtforward, there is no fixed equation but a best possible equation is obtained by the unsupervised learning process. In simple terms, the learning process can be defined as a rule / equation which converts B in the form of A and L. Lets understand this process from a autoencoder perspective. \n\nConsider the autoencoder with no hidden layers, the inputs x1 and x2 are encoded to lower repersentation d which is then further projected into x1 and x2. \n\n![](https://i.imgur.com/lfq4eEy.png)\n\n\n**Step1 : Repersent the points in Latent View Space**   \n\nIf the coordinates of point A and B in the data representation space are: \n\n- Point A : (x1A, x2A)  \n- Point B : (x1B, x2B)   \n\nthen their coordinates in the latent view space will be:   \n\n(x1A, x2A) ---> (0, 0)  \n(x1B, x2B) ---> (u1B, u2B)  \n\n- Point A : (0, 0)  \n- Point B : (u1B, u2B)   \n\nWhere u1B and u2B can be represented in the form of distance between the point and the reference point  \n\nu1B = x1B - x1A  \nu2B = x2B - x2A\n\n**Step2 : Represent the points with distance d and angle L **    \n\nNow, u1B and u2B can represented as a combination of distance d and angle L. And if we rotate this by angle L, towards the horizontal axis, L will become 0. ie.  \n\n**=> (d, L)**     \n**=> (d, 0)**   (after rotation)   \n\nThis is the output of the encoding process and repersents our data in low dimensions.  If we recall the fundamental equation of a neural network with weights and bias of every layer, then \n\n**=> (d, 0) = W. (u1B, u2B)**    \n==> (encoding)    \n\nwhere W is the weight matrix of hidden layer.  Since, we know that the decoding process is the mirror image of the encoding process. \n\n**=> (u1B, u2B) = Inverse (W) . (d, 0)**    \n==> (decoding)  \n\nThe reduced form of data (x1, x2) is (d, 0) in the latent view space which is obtained from the encoding architecture. Similarly, the decoding architecture converts back this representation to original form (u1B, u2B) and then (x1, x2). An important point is that Rules / Learning function / encoding-decoding equation will be different for different types of data. For example, consider the following data in 2dimentional space.  \n\n\n## Different Rules for Different data\n\nSame rules cannot be applied to all types of data. For example, in the previous example, we projected a linear data manifold in one dimention and eliminated the angle L. But what if the data manifold cannot be projected properly. For example consider the following data manifold view. "
"But the key question here is with what logic or rule, point B can be represented in terms of A and angle L. Or in other terms, what is the equation among B, A and L. The answer is straigtforward, there is no fixed equation but a best possible equation is obtained by the unsupervised learning process. In simple terms, the learning process can be defined as a rule / equation which converts B in the form of A and L. Lets understand this process from a autoencoder perspective. \n\nConsider the autoencoder with no hidden layers, the inputs x1 and x2 are encoded to lower repersentation d which is then further projected into x1 and x2. \n\n![](https://i.imgur.com/lfq4eEy.png)\n\n\n**Step1 : Repersent the points in Latent View Space**   \n\nIf the coordinates of point A and B in the data representation space are: \n\n- Point A : (x1A, x2A)  \n- Point B : (x1B, x2B)   \n\nthen their coordinates in the latent view space will be:   \n\n(x1A, x2A) ---> (0, 0)  \n(x1B, x2B) ---> (u1B, u2B)  \n\n- Point A : (0, 0)  \n- Point B : (u1B, u2B)   \n\nWhere u1B and u2B can be represented in the form of distance between the point and the reference point  \n\nu1B = x1B - x1A  \nu2B = x2B - x2A\n\n**Step2 : Represent the points with distance d and angle L **    \n\nNow, u1B and u2B can represented as a combination of distance d and angle L. And if we rotate this by angle L, towards the horizontal axis, L will become 0. ie.  \n\n**=> (d, L)**     \n**=> (d, 0)**   (after rotation)   \n\nThis is the output of the encoding process and repersents our data in low dimensions.  If we recall the fundamental equation of a neural network with weights and bias of every layer, then \n\n**=> (d, 0) = W. (u1B, u2B)**    \n==> (encoding)    \n\nwhere W is the weight matrix of hidden layer.  Since, we know that the decoding process is the mirror image of the encoding process. \n\n**=> (u1B, u2B) = Inverse (W) . (d, 0)**    \n==> (decoding)  \n\nThe reduced form of data (x1, x2) is (d, 0) in the latent view space which is obtained from the encoding architecture. Similarly, the decoding architecture converts back this representation to original form (u1B, u2B) and then (x1, x2). An important point is that Rules / Learning function / encoding-decoding equation will be different for different types of data. For example, consider the following data in 2dimentional space.  \n\n\n## Different Rules for Different data\n\nSame rules cannot be applied to all types of data. For example, in the previous example, we projected a linear data manifold in one dimention and eliminated the angle L. But what if the data manifold cannot be projected properly. For example consider the following data manifold view. "
"In this type of data, the key problem will be to obtain the projection of data in single dimention without loosing information. When this type of data is projected in latent space, a lot of information is lost and it is almost impossible to deform and project it to the original shape. No matter how much shifts and rotation are applied, original data cannot be recovered. \n\nSo how does neural networks solves this problem ? The intution is, In the manifold space, deep neural networks has the property to bend the space in order to obtain a linear data fold view. Autoencoder architectures applies this property in their hidden layers which allows them to learn low level representations in the latent view space. \n\nThe following image describes this property: \n\n![](https://i.imgur.com/gKCOdiL.png)\n\nLets implement an autoencoder using keras that first learns the features from an image, and then tries to project the same image as the output.  \n\n## 2. Implementation\n\n## 2.1 UseCase 1 : Image Reconstruction\n\n1. Load the required libraries\n"
"In this type of data, the key problem will be to obtain the projection of data in single dimention without loosing information. When this type of data is projected in latent space, a lot of information is lost and it is almost impossible to deform and project it to the original shape. No matter how much shifts and rotation are applied, original data cannot be recovered. \n\nSo how does neural networks solves this problem ? The intution is, In the manifold space, deep neural networks has the property to bend the space in order to obtain a linear data fold view. Autoencoder architectures applies this property in their hidden layers which allows them to learn low level representations in the latent view space. \n\nThe following image describes this property: \n\n![](https://i.imgur.com/gKCOdiL.png)\n\nLets implement an autoencoder using keras that first learns the features from an image, and then tries to project the same image as the output.  \n\n## 2. Implementation\n\n## 2.1 UseCase 1 : Image Reconstruction\n\n1. Load the required libraries\n"
"### 2. Dataset Prepration \n\nLoad the dataset, separate predictors and target, normalize the inputs."
Lets plot the original and predicted image\n\n**Inputs: Actual Images**
**Predicted : Autoencoder Output**
**Predicted : Autoencoder Output**
"So we can see that an autoencoder trained with 20 epoochs is able to reconstruct the input images very well. Lets look at other use-case of autoencoders - Image denoising or removal of noise from the image.  \n\n## 2.2 UseCase 2 - Image Denoising\n\nAutoencoders are pretty useful, lets look at another application of autoencoders - Image denoising. Many a times input images contain noise in the data, autoencoders can be used to get rid of those images. Lets see it in action. First lets prepare the train_x and val_x data contianing the image pixels. \n\n![](https://www.learnopencv.com/wp-content/uploads/2017/11/denoising-autoencoder-600x299.jpg)"
Before adding noise
After adding noise
After adding noise
"Lets now create the model architecture for the autoencoder. Lets understand what type of network needs to be created for this problem. \n\n**Encoding Architecture:**   \n\nThe encoding architure is composed of 3 Convolutional Layers and 3 Max Pooling Layers stacked one by one. Relu is used as the activation function in the convolution layers and padding is kept as ""same"". Role of max pooling layer is to downsample the image dimentions. This layer applies a max filter to non-overlapping subregions of the initial representation.  \n\n**Decoding Architecture:**   \n\nSimilarly in decoding architecture, the convolution layers will be used having same dimentions (in reverse manner) as the encoding architecture. But instead of 3 maxpooling layers, we will be adding 3 upsampling layers. Again the activation function will be same (relu), and padding in convolution layers will be same as well.  Role of upsampling layer is to upsample the dimentions of a input vector to a higher resolution / dimention. The max pooling operation is non-invertible, however an approximate inverse can be obtained by recording the locations of the maxima within each pooling region. Umsampling layers make use of this property to project the reconstructions from a low dimentional feature space.   \n\n"
Lets obtain the predictions of the model
"In this implementation, I have not traiened this network for longer epoochs, but for better predictions, you can train the network for larger number of epoochs say somewhere in the range of 500 - 1000. \n\n## 2.3 UseCase 3: Sequence to Sequence Prediction using AutoEncoders\n\n\nNext use case is sequence to sequence prediction. In the previous example we input an image which was a basicaly a 2 dimentional data, in this example we will input a sequence data as the input which will be 1 dimentional. Example of sequence data are time series data and text data. This usecase can be applied in machine translation. Unlike CNNs in image example, in this use-case we will use LSTMs. \n\nMost of the code of this section is taken from the following reference shared by Jason Brownie in his blog post. Big Credits to him. \n- Reference : https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n\n#### Autoencoder Architecture  \n\nThe architecuture of this use case will contain an encoder to encode the source sequence and second to decode the encoded source sequence into the target sequence, called the decoder. First lets understand the internal working of LSTMs which will be used in this architecture. \n\n- The Long Short-Term Memory, or LSTM, is a recurrent neural network that is comprised of internal gates.   \n- Unlike other recurrent neural networks, the network’s internal gates allow the model to be trained successfully using backpropagation through time, or BPTT, and avoid the vanishing gradients problem.   \n- We can define the number of LSTM memory units in the LSTM layer, Each unit or cell within the layer has an internal memory / cell state, often abbreviated as “c“, and outputs a hidden state, often abbreviated as “h“.   \n- By using Keras, we can access both output states of the LSTM layer as well as the current states of the LSTM layers.  \n\nLets now create an autoencoder architecutre for learning and producing sequences made up of LSTM layers. There are two components: \n\n- An encoder architecture which takes a sequence as input and returns the current state of LSTM as the output  \n- A decoder architecture which takes the sequence and encoder LSTM states as input and returns the decoded output sequence\n- We are saving and accessing hidden and memory states of LSTM so that we can use them while generating predictions on unseen data. \n\nLets first of all, generate a sequence dataset containing random sequences of fixed lengths. We will create a function to generate random sequences. \n\n- X1 repersents the input sequence containing random numbers  \n- X2 repersents the padded sequence which is used as the seed to reproduce the other elements of the sequence  \n- y repersents the target sequence or the actual sequence \n"
"**Reading a CT Scan**\n-----------------\nThe input folder has three things, one is the sample_images folders which has the sample CT Scans. The `stage1_labels.csv` contains the cancer ground truth for the stage 1 training set images and `stage1_sample_submission.csv` shows the submission format for stage 1. "
"Each 3D CT Scan consists of many slices, whose number depends on the resolution of the scanner and each slice has a Instance Number associated with it which tells the index of the slice from the top. All the dicom files for a CT Scan are inside one folder having the CT Scan's name. Now we will read all the dicom slices for a scan and then stack them with respect to their Instance Number to get the 3D Lung CT Scanned Image."
"## Segmentation of Lungs ##\nAfter reading the CT Scan, the first step in preprocessing is the segmentation of lung structures because it is obvious that the regions of interests lies inside the lungs. It is visible that the lungs are the darker regions in the CT Scans. The bright region inside the lungs are the blood vessels or air. A threshold of 604(-400 HU) is used at all places because it was found in experiments that it works just fine. We segment lung structures from each slice of the CT Scan image and try not to loose the possible region of interests attached to the lung wall. There are some nodules which may be attached to the lung wall.\n\nI will first explain a common method using simple Image Processing and Morphological operations to segment the lungs and then will give references and summaries to good links of papers. "
The `get_segmented_lungs` function segments a 2D slice of the CT Scan. I have outputted the slice after all steps for better visualisation and understanding of the code and applied operations.
"After reading the 3D CT Scans, we will first segment the lungs and then generate the binary mask of nodule regions. This will be done by the `create_nodule_mask` function.  The `draw_circle` function is used to mark the nodule regions in the binary mask. 'cands' are the list of nodule points with the radius given in the `annotation.csv` file of LUNA16 dataset. At the end we save the resized CT Scan with its segmented lungs and binary mask of nodules."
"After preprocessing the dataset, the next thing is to train the model for segmentation. The model is written in keras in the `unet_model` function. It takes a 2D slice as input and returns a 2D slice of the same size as output. There are few things to be kept in mind while training\n\n - We wont use the slices that has no nodule region in the mask for training.\n - Dataset augmentation is very important because are nodules are generally circular or spherical in shape and are of different radius. \n - Since the nodule regions are very less, the dataset is skewed. Thus, we should weight the loss function accordingly.\n - The model may overfit on the training dataset. Thus, Dropout or Spatial Dropout are used to avoid overfitting."
**Import all required libraries**\n===============================
**Load Train and Test data**\n============================
Lets plot 10th label.
Oh its 3 !
"## Convolutional Neural Networks\nIf you want to apply machine learning to image recognition, convolutional neural networks (CNN) is the way to go. It has been sweeping the board in competitions for the last several years, but perhaps its first big success came in the late 90's when [Yann LeCun][1] used it to solve MNIST with 99.5% accuracy. I will show you how it is done in Keras, which is a user-friendly neural network library for python.\n\nMany other notebooks here use a simple fully-connected network (no convolution) to achieve 96-97%, which is a poor result on this dataset. In contrast, what I will show you here is nearly state-of-the-art. In the Kernel (<20 minutes training) we will achieve 99%, but if you train it overnight (or with a GPU) you should reach 99.5. If you then ensemble over several runs, you should get close to the best published accuracy of 99.77% . (Ignore the 100% results on the leaderboard; they were created by learning the test set through repeat submissions)\n\nHere goes:\n\n\n  [1]: http://yann.lecun.com/exdb/lenet/"
"If you don't already have [Keras][1], you can easily install it through conda or pip. It relies on either tensorflow or theano, so you should have these installed first. Keras is already available here in the kernel and on Amazon deep learning AMI.\n\n  [1]: https://keras.io/"
"Each data point consists of 784 values. A fully connected net just treats all these values the same, but a CNN treats it as a 28x28 square. Thes two graphs explain the difference: It's easy to understand why a CNN can get better results."
"We now reshape all data this way. Keras wants an extra dimension in the end, for channels. If this had been RGB images, there would have been 3 channels, but as MNIST is gray scale it only uses one.\n\nThis notebook is written for the tensorflow channel ordering. If you have Keras installed for Theano backend, you might start seeing some error message soon related to channel ordering. This can easily be [solved][1].\n\n\n  [1]: https://keras.io/backend/#set_image_dim_ordering"
"# 1. Introduction + Set-up\n\nMachine learning has a phenomenal range of applications, including in health and diagnostics. This tutorial will explain the complete pipeline from loading data to predicting results, and it will explain how to build an X-ray image classification model from scratch to predict whether an X-ray scan shows presence of pneumonia. This is especially useful during these current times as COVID-19 is known to cause pneumonia.\n\nThis tutorial will explain how to utilize TPUs efficiently, load in image data, build and train a convolution neural network, finetune and regularize the model, and predict results. Data augmentation is not included in the model because X-ray scans are only taken in a specific orientation, and variations such as flips and rotations will not exist in real X-ray images. For a tutorial on image data augmentation, check out this [tutorial](https://www.kaggle.com/amyjang/tensorflow-data-augmentation-efficientnet).\n\nRun the following cell to load the necessary packages. Make sure to change the Accelerator on the right to `TPU`."
"We need a Google Cloud link to our data to load the data using a TPU. While we're at it, we instantiate constant variables. It is generally better practice to use constant variables instead of hard-coding numbers."
Define the method to show the images in the batch.
"As the method takes in numpy arrays as its parameters, call the numpy function on the batches to return the tensor in numpy array form."
"# 8. Visualizing model performance\n\nLet's plot the model accuracy and loss for the training and the validating set. These plots show the accuracy and loss values for the second round of training. Since we initially trained the model with 30 epochs, these would be epochs 31-45. Note that no random seed is specified for this notebook. For your notebook, there might be slight variance."
We see that the accuracy for our model is around 98%. Finetune the model further to see if we can achieve a higher accuracy.
# Table of Contents\n1. [Understanding Data](#sec1)\n    * [1.2 Univariate Analysis](#sec1.2)  \n    * [1.3 Bi-variate Analysis](#sec1.3)  \n2. [Data Preprocessing](#sec2)  \n    * [2.1 Removing redundant features](#sec2.1)\n    * [2.2 Dealing with Outliers](#sec2.2)\n    * [2.3 Filling Missing Values](#sec2.3)\n3. [Feature Engineering](#sec3)\n4. [Modeling](#sec4)\n    * [4.1 Scaling of features](#sec4.1)\n    * [4.2 Ensemble Algorithms](#sec4.2)
\n# [1. Understanding Data](#sec1)
"### Numeric Features\n\nFor numerical features, we are always concerned about the **distribution** of these features, including the **statistical characteristics** of these columns e.g mean, median, mode. Hence  we will usually use **Distribution plot** to visualize their data distribution. **Boxplots** are also commonly used to unearth the statistical characteristics of each feature. More often than not, we use it to look for any outliers that we might need to filter out later on during the preprocessing step."
"Some of the Variables with mostly 1 value as seen from the plots above:\n1. BsmtFinSF2\n2. LowQualFinSF\n3. EnclosedPorch\n4. 3SsnPorch\n5. ScreenPorch\n6. PoolArea\n7. MiscVal  \n\nAll these features are highly skewed, with mostly 0s. Having alot of 0s in the distribution doesnt really add information for predicting Housing Price. Hence, we will remove them during our preprocessing step"
"### Categorical Features\n\nIn the case of categorical features, we will often use countplots to visualize the count of each distinct value within each features. We can see that some categorical features like **Utilities, Condition2** consist of mainly just one value, which does not add any useful information. Thus, we will also remove them later on."
"Univariate Analysis helps us to understand all the features better, on an individual scale. To further deepen our insights and uncover potential pattern in the data, we will also need to find out more about the relationship between all these features with one another, which brings us to our next step in our analysis - Bivariate Analysis"
### Correlation Matrix
**Highly Correlated variables**:\n* GarageYrBlt and YearBuilt\n* TotRmsAbvGrd and GrLivArea\n* 1stFlrSF and TotalBsmtSF\n* GarageArea and GarageCars\n\nFrom the correlation matrix we have identified the above variables which are highly correlated with each other. This finding will guide us in our preprocessing steps later on as we aim to remove highly correlated features to avoid performance loss in our model
"### Scatterplot\nUsing scatterplot can also help us to identify potential linear relationship between Numerical features. Although scatterplot does not provide quantitative evidence on the strength of linear relationship between our features, it is useful in helping us to visualize any sort of relationship that correlation matrix could not calculate. E.g Quadratic, Exponential relationships. "
"\n# [2. Data Processing](#sec2)\n\nNow that we have more or less finished analysing our data and gaining insights through the various analysis and visualization, we will have to leverage on these insights to guide our preprocessing decision, so as to provide a clean and error-free data for our model to train on later on.  "
### SalePrice Distribution
"Distribution is skewed to the right, where the tail on the curve’s right-hand side is longer than the tail on the left-hand side, and the mean is greater than the mode. This situation is also called positive skewness.  \nHaving a skewed target will affect the overall performance of our machine learning model, thus, one way to alleviate will be to using **log transformation** on skewed target, in our case, the *SalePrice* to reduce the skewness of the distribution."
"## 2.1 Data related work across fields\nSurely not all professions are made equally, so I wondered **which fields tend to have more professionals involved in data analysis activities?** The below graph shows the percentage of individuals within each field that work in data-related roles on a daily basis. **Additional details are included in the hover text.**\n\n    Source: Kaggle 2020 DS and ML Survey - Q23. Select any activities that make up an important part of your role at work.\n"
"Unsurprisingly fields like **Data Scientists**, with higher percentages of individuals working with data and contributing towards the bulk of the data professionals. **Data Analystst, Business Analysts, ML Engineers and Data Engineers are a close second** with high percentages of data professionals, however due to the fewer number of respondents in these categories they tend to contribute less towards our sum total."
"## 2.4 Data roles in different fields\n\nOne thing I always wondered with all these different roles, what exactly do they work on, on a day-to-day basis - It can't all be the same regardless of the job title right?\n\nAnother question that has always bothered me(and I'm sure many of you too) - ""**What's the difference between the roles data scientist, data engineer, data analyst and business analyst?**"" Lets see if we can answer this in the following graph of data roles.\n\n\n    Source: Kaggle 2020 DS and ML Survey - Q23. Select any activities that make up an important part of your role at work.\n"
"As for the answer to our previous question:\n- For all the four titles(first 4 bar charts), the **main focus remains analysing data for business decisions**.\n- **Data analysts and Business Analysts seem to be almost identical** with respect to the role distribution and average number of roles per individual.\n- Data engineers seem to be similar to the others, except for their significant **focus on building infrastructure for the data management and analysis**.\n- Finally data scientists also have a higher focus on building ML prototypes in a new areas as well as have a relatively higher number of roles on average. Both these traits make it **very similar to Machine Learning Engineers**(except for the focus on impacting business decisions). \n\nAnother interesting detail is machine learning engineers, true to their name, show higher numbers across the board for all the ML related roles. Even their work in research on state of the art ML methods, is only second to research scientists themselves."
## 2.5 Ages of data professionals\n\nLets see if the age data will produce any interesting findings. \n\n\n    Source: Kaggle 2020 DS and ML Survey - Q1. What is your age (# years)?\n
"Some interesting points to note:\n- 70% of students in the age group 18-21 are **pursuing a Bachelor's degree**.\n- In the **22-24 age group, the focus shifts towards Masters**, having a marked jump from 11% to 45% in this age group. Meanwhile the percentage of Bachelors degree holders drops to below 40% for the first time.\n- In later age, groups the proportion Masters still remains around the 40-60% mark, with Doctoral degrees as a close second, while the numbers for bachelors slowly decreases.\n- On the data professional side of things, we see that most fields have a **majority of their respondents in the 25-29 age group**. \n- The **sole exception to this is Project Managers, which peaks at 35-39**. Most likely because these are positions you would only enter after a fair deal of experience. This also helps explain their relatively higher pays that we noted in the previous section."
"## 2.6 Education required\n\nWith all the disciplines that go into data analysis and machine learning, I always wondered what education qualifications were the norm in these professions and possibly what education I should pursue to thrive in these areas. Let's see how the fields stack up in terms of their educational make-up.\n\n\n    Source: Kaggle 2020 DS and ML Survey - Q4. What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\n"
"The general trend seems to be that more than half of the individuals in each field seem to have either a **Master's or Bachelor's degree** in almost all fields. A notable exception being Research Scientists where more than half the respondents have doctoral degrees and when you include Masters and Professional degree holders this number comprises of 93% of the total professionals in their field.\n\nSome additional points of interest:\n- In most fields **Masters degree seems to be the way to go** with 30-55% of respondents having one.\n- A **bachelors degree** seems to be a close second with numbers around the 20-30% mark.\n- **Professional degrees** seem to be a less common choice with around 4-5% having one. Hovewer they do help to get jobs because professional degree holders have the **second lowest fraction of unemployed individuals**, after Doctoral degree holders."
## 2.7.1 Handling Regional bias in pays\n\nSalaries aren't really comparable by simply converting to USD and then drawing conclusions from this data. A major factor in this is the **cost of living** in different countries - Companies situated in places where costs of living are higher pay their employees more to compensate for this. A simple example would be how 100$ would have **different purchasing power in different places**.\n\nI'll handle this disparity by dividing each respondents salary by their **country's cost of living index** and then converting this value to an equivalent amount in USD. A simple demonstration of how this affects reported salaries is shown below\n\n\n    Source: Countries Dataset 2020 - Numbeo: Cost of living index\n
"We see how a country like India, where the cost of living is much lower than that of the US, has the majority of its individuals towards the lower end of the graph. However on normalising with the cost of living data, we see how a much larger portion of Indians enter the higher pay brackets. In fact the salaries in India are **almost tripled to get their corrected values**.\n\nWhile this won't be a 100% accurate it gives us a much **better basis on which we may compare salaries**. And in this case we would probably be safe to assume that  in general, companies in the US just do pay their data experts more."
"## 2.8 Machine Learning practices\n\nSo.. you want to work in machine learning? Well, lets give you a better idea of the type of ML-related work you might encounter in certain fields. \n\n\n    Sources: Kaggle 2020 DS and ML Survey - \n     Q22. Does your current employer incorporate machine learning methods into their business?\n    \n"
"In case you are unclear on what exactly ""production"" means I'll give you a little background here - Traditionally, when a data scientist would create an ML model they would often do so in a sandbox or development environment. This is **great for experimenting and reiterating quickly**, however in order for the consumers to use it, the model must be deployed so that it **scales well with increased usage volumes and is always available** for use by others. This deployment however, requires a skill-set different than what most data scientists possess and often this **burden falls on the Software Engineers**. \n\nIn order for a ML product to be put into production both teams must work closely and this cooperation is what a lot of companies dealing in ML products strive for. This back-and-forth forms the basis by which they manage to build and maintain well established ML models. (You can read more about this [here](https://stackoverflow.blog/2020/10/12/how-to-put-machine-learning-models-into-production/).)\n\nNow back to the graph - an interesting observation is how **Machine Learning Engineers** ranked second here in terms of well-established ML production practices, this is the exact situation where ML Engineers shine - they are a **mix of both Software Engineers and Data Scientists** whose main role is **to take machine learning models and make them scalable in production**.\n\nThe left-most bar also shows us what proportion of the companies don't utilise ML methods in their daily activities."
"## 3.1 The Skills Gap\n\nA lot of enterprises express that their recruits fresh out of their degree courses often lack certain fundamentals to seamlessly fit into the workplace. This may be attributed to their universities not prioritising certain key skills or perhaps due to the lack of initiative taken by the students in learning by themselves. In [Anaconda's State of Data Science Report 2020](https://www.anaconda.com/state-of-data-science-2020), respondents working in industry backgrounds were asked what were some of the essential skills that their newer recruits lacked. On the other side of things, students were even asked what they chose to study in preparation for their work.\n\n    Source: Anaconda's State of DS Report 2020 - \n     The Skills Gap\n    \n"
"While these numbers are specifically for the field of data science, it does still give us an idea of where there is a disconnect between what the students learn and what is actually expected of them in the workplace. Some of the areas where students seem to be lacking are:\n* **Big data management:** When people talk about big data, one of the first things that come to a lot of peoples' mind is deep learning and how it leverages massive volumes of data to generate powerful models. Rarely does one think of big data management which actually encompasses the entire range of **policies, procedures and technologies that are utilised in the organization and administration of large repositories of data**. Often students will glaze over this part of data management because its one of the more mundane parts of the job.\n* **Deep Learning and Machine Learning:** Based on the specifics of the role, companies also place a lot of importance on the recruit having a good foundation in the **working of machine learning and deep learning algorithms** as well as a hands-on experience of **when to apply** these to specfic problems.\n* **Advanced Mathematics:** While admittedly there are slight variations on the exact content of ""advanced mathematics"", it is a safe bet that if one wants to work with data they should be **well-versed with the basics of algebra, statistics, calculus and trigonometry**, and must be comfortable to delve deeper into these topics should their work require it"
## 3.2 Development environments: IDEs and Hosted notebooks\nLet us see if there are any note-worthy trends in terms of what tools data professionals use during their development work.
"\n    Source: Kaggle 2020 DS and ML Survey - \n     Q9. Which of the following integrated development environments (IDE's) do you use on a regular basis?\n    \n\n\nWhile Jupyter products hold the top spot in this graph, most of the **other IDEs do share a sizable chunk of the audience**. One might even be tempted to experiment with language specific IDEs like PyCharm, RStudio, Spyder, MATLAB, etc, because of the greater amount of customisation and **language specific features** that might aid the development process."
"\n    Source: Kaggle 2020 DS and ML Survey - \n     Q9. Which of the following integrated development environments (IDE's) do you use on a regular basis?\n    \n\n\nWhile Jupyter products hold the top spot in this graph, most of the **other IDEs do share a sizable chunk of the audience**. One might even be tempted to experiment with language specific IDEs like PyCharm, RStudio, Spyder, MATLAB, etc, because of the greater amount of customisation and **language specific features** that might aid the development process."
"\n    Source: Kaggle 2020 DS and ML Survey - \n     Q10. Which of the following hosted notebook products do you use on a regular basis?\n    \n\n\nHosted notebooks provide us access to a hassle free environment in which we may collaborate and share our results with others. For example, even though I dont have R installed on my system, Kaggle will happily let me open up an R notebook and start coding immediately without any setup on my side. \n\nAmong those that do use hosted notebooks **Colab and Kaggle notebooks are the top two choices**. With 28% of individuals responding that they dont any hosted notebooks, we understand that **not all roles require their use**."
## 3.4 What languages should I learn?\n\nContinuing on the topic of experience a very relevant question is what languages do I need to know in order to work as a data professional? Lets try to understand what languages students tend to learn versus whats actually utilised in the industry.\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q7. What programming languages do you use on a regular basis?\n    \n
"Some of the key takeaways are:\n- The students seem to be on a strong start here - learning **Python** is not only a safe bet because of its widespread use in the industry, but its also by far the language **most recommended** to beginners. \n- Having at least a **basic understanding of SQL** and database management would go a long way in your career.\n- While languages like C, C++ and Java are what many of us start out with, students should **learn to quickly adapt** and transfer their knowledge to more commonly used languages."
## 3.5 Visualization Tools\nLet's see if there are any favorites among data professionals when it comes to visualising their data. Also **be sure to check the hover info for additional stats and information** about each of the tools.\n\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q14. What data visualization libraries or tools do you use on a regular basis? \n    \n
"From the above graph my main takeaway is that with visualization greatly aiding analysis its often **useful to have experience with at least one** of these tools under your belt. If your job role hinges on **creating impactful visualisations**, then maybe you might need to upskill yourself and learn to utilise tools with interactivity and higher degrees of customisation that help to convey your message better."
## 3.7 What Machine Learning frameworks should I opt for?\n\nLet's try to see what ML frameworks are commonly used in the industry.\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q16. Which of the following machine learning frameworks do you use on a regular basis? \n    \n
"A couple things to note:\n* **Scikit** is the most common choice among respondents. **Tensorflow and Keras** also seem to be popular choices in the second and third place.\n* **Xgboost vs LightGBM vs CatBoost**: A common question asked on Kaggle - when considering only Data Scientists, we see that **49% use Xgboost** making it the preferred choice over LightGBM at 26% and Catboost at 14%.\n\nWhile exploring the data for this analysis I noticed a rather peculiar behaviour - for some reason **Statisticians adopted Caret to a much greater extent** than any other field. On further analysis I realised that because this was a **group that had much higher usages of R**, an **R ML framework** like Caret had a much higher adoption rate. Let's see how the same chart differs if we instead look at those who only use R.\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q16. Which of the following machine learning frameworks do you use on a regular basis? \n    \n"
"A couple things to note:\n* **Scikit** is the most common choice among respondents. **Tensorflow and Keras** also seem to be popular choices in the second and third place.\n* **Xgboost vs LightGBM vs CatBoost**: A common question asked on Kaggle - when considering only Data Scientists, we see that **49% use Xgboost** making it the preferred choice over LightGBM at 26% and Catboost at 14%.\n\nWhile exploring the data for this analysis I noticed a rather peculiar behaviour - for some reason **Statisticians adopted Caret to a much greater extent** than any other field. On further analysis I realised that because this was a **group that had much higher usages of R**, an **R ML framework** like Caret had a much higher adoption rate. Let's see how the same chart differs if we instead look at those who only use R.\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q16. Which of the following machine learning frameworks do you use on a regular basis? \n    \n"
"Seems like our results have changed when only considering R users. If students prefer R as their language of choice, **Caret** seems to be a good option to look into (Also notice how another R package like **TinyModels suddenly jumped up in the list**)\n\nThe lower percentages in general compared to the previous graph, as well as the presence of ""None"" in second place shows that most **R users value the language for purposes other than machine learning**. \n\n*I'm assuming we're seeing certain python packages popping up in this graph because the respondents had knowledge of them but didn't consider Python as a language they used regularly, in the survey responses.*"
"## 3.8 Learning Platforms\n\nEven with all the amazing resources available online, at times we all need a little guidance in developing a systematic learning plan and thats where learning platforms come to our rescue. Let's take a look at what we can learn from the way that data professionals consume online course content.\n\nWhile exploring the data I noticed how at each age group there were noticable **differences between both groups' learning habits** so I created an interactive piece to help you see just that. *(PRO TIP: After you've selected an age group option, using the up and down arrow keys to go through the options makes the graph a lot more interesting to use.)*\n\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q37. On which platforms have you begun or completed data science courses? \n    \n"
"* A common trend we see is that **data professionals are more likely to leverage these resources** - even when only comparing individuals in the same age group.\n* When considering online platforms we see that **Coursera** has cemented its place in our #1 spot. In the next few spots **Udemy, Kaggle Learning courses and Data Camp** are also popular among the two groups.\n* While **University Courses** do often pop up in the top 5 of our list, we see that **less than 30% of all data professionals have completed a data science course in university**. This goes to show that having **a degree in data science isn't always necessary** to work in this field."
## 3.9 Media and learning\n\nThe field of data science and machine learning is advancing at a rapid rate. Let's look at some of the tools that will help you to continuously learn and keep in touch with the latest in your field.\n\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q39. Who/what are your favorite media sources that report on data science topics?\n    \n
"Using media is a great way to catch up on trends and updates in our field. They provide information in a **more entertaining and informal manner** that one can always pick up at their own convenience, making them a great way to **supplement our more structured learning activities**. On seeing the trend that professionals seem to utilise more of these, perhaps students might want to look into a couple ML/DS related content creators on their favorite platforms.\n\nOther notable sources that didn't quite make the top 5 here are - **email newsletters, podcasts, slack communities, reddit**, etc."
"## 3.10 Obstacles to working with data \nWhile studying in preparation for working with data we tend to live in this bubble of ours, assuming that things will work out since we probably are qualified enough to work in our field of choice. We often neglect actually getting a sense of how the job market is and what are the struggles faced by those actively hunting for work in the same field.\n\nTo get an idea of the trappings that prevent students from landing that dream data role, we look at Ananconda's report where they asked students what was the biggest factor that prevented them from securing their ideal data science job.\n\n\n    Source: Anaconda's State of DS Report 2020 - \n     In your opinion, what is the biggest obstacle to obtaining your ideal data science job? \n    \n"
"I initially expected that a lack of job openings would be a major factor, but turns out that this was hardly the case. Most students claimed that their reason for not securing their ideal job was that they **lacked the experience** for it. Often this will be due to companies asking for ""*x years of experience in...*"" which will turn away most students early in their careers, otherwise it might be experience working in a particular field or with specific tools.  While there may be no direct workaround for this, **seeking internships** in a related area of work would go a long way as initial work experience. Data science **side-projects** are also a great way to showcase your ability, so long as the content is novel and well-thought out."
"## 4.2 Natural Language Processing\n\nNatural language processing(or NLP for short) is the branch of artificial intelligence which is concerned with the interaction between humans and machines through the use of natural language. It allows the machine to make sense of, and derive information that is of value from human language.\n\n\n    Source: Kaggle 2020 DS and ML Survey\n"
"From the above graphs we again see similar trends to what we observed in Computer vision - The bulk of our workforce is still data scientists, there is still a preference for working in smaller companies, and even the exact same countries were observed when looking at the top 10 employers.\n\nLet's now look at the NLP specific behaviours:\n* Understanding **Word Embeddings** is part of building a strong foundation in NLP. Word embeddings is a means of representing words in such a manner that words similar in meaning have similar representations.\n* **Transformer networks** function similar to Recurrent Neural Networks in the sense that they are useful to handle sequential. A key difference that makes this useful in NLP is the fact that they don't require data to be processed in order(so the beginning of the sentence needn't always be processed before the end). This makes parellisation easier, resulting in reduced training times.\n* **Encoder decoder models are used in sequence to sequence operations** like text summarising, question answering. They utilise one encoding network to encode the input sequence and another decoding network to convert this into the desired target.\n* **Contextualised word embeddings** provide additional information in the embedding regarding the context in which it was used. eg. ""ground"" has different meanings in ""ground coffee"" and ""training grounds"" which is determined based on context.\n\n##  What do they do?\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q23. Select any activities that make up an important part of your role at work.\n    \n"
"## 4.3 Machine learning in general\n\nThis section covers the users of the other machine learning algorithms that the survey group under Q17( ""*machine learning algorithms used regularly""* ). These are used for a wide range of applications and hopefully this analysis will help us understand how utilised they are in everyday applications.\n\n\n    Source: Kaggle 2020 DS and ML Survey\n"
"For the first time we see Data Analyst ranked highly in the percentage of ml users across fields, this might mean that their focus is on predictive modelling and analytics, without the use of computer vision or NLP methods.\n\nIn countries we see how France and surprisingly China, have dropped off the top 10 list and are replaced by Spain and Nigeria. As the first African country to break the top 10 employers list, it shows that there is promise for machine learning and data science in Africa.\n\nLet's take a look at some of the methods that might be worth knowing in this area.\n* **Linear and Logistic regression:** As everyone's starting point in machine learning, it's important to gain a good understanding of how linear models work which will act as a solid foundation even when working with more complex algorithms.\n* **Decision Trees and Random Forests:** Decision trees work by making a series of sequential decisions leading to a particular prediction. Random forests combine the outputs of several randomly generated trees which improves the overall performance on unseen data, as opposed to using a single decision tree.\n* **Gradient Boosting machines(xgboost, lightGBM):** While I wont go into the specifics of how each one works, in general gradient boosting involves building a predictive model using an ensemble on weaker prediction models. \n* **Convolutional Neural Networks**: CNNs are a class of deep neural networks that form the backbone of image processing in artificial intelligence. They are a great fit whenever the data has a natural 2D(or even 3D) structure.\n\n##  What do they do?\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q23. Select any activities that make up an important part of your role at work.\n    \n"
"# 5. Conclusion\n\nI understand that throughout the course of this notebook I have bombarded you with a lot of information on working with data. It is easy to feel a little overwhelmed by all the different moving parts involved and think that you might have bit off a bit more than you can chew when you decided to try your hand at data science, especially when you are just starting out. \n\nTo this, I'd like to remind you that noone enters their first job( or even switching to a new role) knowing everything on day one - you will learn as you work and you are allowed to( *and will*) make mistakes. It's just a desire to learn and improve which makes things easier.\n\nThanks for joining me in this journey and I hope you found something of use in this notebook. Stay safe and keep learning!\n"
"# What's new?\nSo I've reached stage at which I will I'll be reworking a few of the older graphs and updating the writeups in previous sections At least, I hope I've finished everything . Since it might not be obvious what was reworked, I'll list it out here so you know where to check out the new content:\n\n* [""**Distribution based on company size**""](#company_size_earnings)Wasn't a big fan of how cluttered it was and how the colours wouldn't intuitively help you find who was paid the most. Section now reworked in D3 with the new normalised pays, along with a new tab(""Overall compensation"") to see where companies are spending money.\n* Table of Contents added.\n* [""**How much Coding Experience?**""](#coding_experience) and [""**How much Machine Learning Experience?**""](#ml_experience) both edited to keep the same theme as other d3 charts. [""**Data roles**""](#roles) now highlights the secondary roles also.\n* [**Workflows of Data Professionals**](#workflows) section was added to highlight where time is spent in the actual work of a data professional.\n* [**The Skills Gap**](#skillsgap)\n\n**~ 05-01-2021 - Completed work on the notebook**"
# Technique 4: Random Row Selection\nThis is nothing but performing a **random sampling** of the data and make best use of it.\nFor this we can build a list of random rows by writing a function 
Suppose we want to fetch a random sample of 10000 lines out of the total dataset. i.e we need to fetch a list of `lines - 1 - 10000` random numbers ranging from 1 to 796373. \n\nNote that while we are generating such a long list also takes a lot of space and  some time. So let us use the Technique 1 mentioned above and make sure to use del and gc.collect() when complete!!!!\n\nNow let us generate list of lines to skip
"# Technique 13: autoreload\nThe autoreload module is there to help with code structure. The module reloads the code before each execution. Once you get locked into a TDD loop and you start refactoring code from the notebook into additional files, this module will reload the code in the additional files."
"\n# Technique 14: Increase default memory limit size\n\nJupyter notebook has a default memory limit size. You can try to increase the memory limit by following the steps:\n\n**1) Generate Config file using command:**\n\n*jupyter notebook --generate-config*\n\n**2) Open jupyter_notebook_config.py file situated inside 'jupyter' folder and edit the following property:**\n\n*NotebookApp.max_buffer_size = your desired value*\n\nRemember to remove the '#' before the property value.\n\n**3) Save and run the jupyter notebook. It should now utilize the set memory value. Also, don't forget to run the notebook from inside the jupyter folder.**\n\nAlternatively, you can simply run the Notebook using below command:\n\n*jupyter notebook --NotebookApp.max_buffer_size=your_value*"
"# Introduction\n\n**I have carefully analyzed a lot of notebooks** and I have been working on this competition for some time. My goal in this notebook, is to bring you the best pieces of all required parts of this challenge.\n\nWe will start by loading data, EDA, Modelling and finally, evaluation and submission file.\n\n**Please upvote the notebook if it helps you.**\n\nEnjoy and be safe!"
# Load Data
"**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows"
**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n
"This new Lyft competition is tasking us, the participants, to predict the motion of external cars, cyclists, pedestrians etc. to assist self-driving cars. This is a step ahead from last year's competition, where we were tasked with detecting three-dimensional objects, like stop signs, to teach AVs how to recognize these. "
"This is apparently the **largest collection of traffic agent motion data.** The files are stored in the .zarr file format with Python, which we can easily load using the Level 5 Kit (l5kit for the pip package). Within our training ZARRs, we have the agents, the masks for agents, frames and scenes (which you might recollect from last year) and traffic light faces.\n\nThe test ZARR however is almost practically the same format, but the only exclusion is that of the data masks. for the agents. "
"**UPDATE: Finally got GPU to work by manually installing everything, adding utility scripts did not help at all. Took a painfully long time to get myself to realize that everything needs to be done in the kernel or things will break.**"
Here we go using some helpful functions to visualize the data.
Here we go using some helpful functions to visualize the data.
"Now, let's get a sense of the configuration data. This will include metadata pertaining to the agents, the total time, the frames-per-scene, the scene time and the frame frequency."
"Now, however it's time for us to look at the scenes and analyze them in depth. Theoretically, we could create a nifty little data-loader to do some heavy lifting for us."
"So, there's a lot of information in this one image. I'll try my best to point everything out, but do notify me if I make any errors. OK, let's get started with dissecting the image:\n+ We have an intersection of four roads over here.\n+ The green blob represents the AV's motion, and we would require to predict the movement of the AV in these traffic conditions as a sample."
"I don't exactly know what other inferences we can make without more detail on this data, so let's try a satellite-format viewing of these images. "
"Yes! This allows for far more detail than a simple plot without detail. I'd haphazard an educated guess, and make the following inferences:\n+ Green still represents the autonomous vehicle (AV), and blue is primarily all the other cars/vehicles/exogenous factors we need to predict for.\n+ My hypothesis is that the blue represents the path the vehicle needs to go through.\n+ If we are able to accurately predict the path the vehicles go through, it will make it easier for an AV to compute its trajectory on the fly."
"Now, how about from the agent perspective? This would be quite interesting to consider, as we're modeling from principally the agent perspective in most public notebooks so far."
"So yes, I probably should save these as a GIF to visualize the agent movements. Let's try a simpler form of this and use the semantic view for the agent dataset."
"Uh it seems the rasterizer renders rather well the satellite and semantic views, and both in conjunction help one to get a good sense of the positioning of each vehicle in relation to the road. You can easily understand the placement and motion of the vehicles and highway layout in satellite by taking a good look at the semantic view too."
"Again, box and stub will also give a good representaton of the data albeit with less low-level detail than the semantic view, seeing as the highways are not into much consideration here. The box view helps to just take a low-level look at the vehicles and their projected path whereas the stub view functions similarly to semantic. We can now proceed to taking a good look at the metadata provided by kkiller and potentially train a good model. The ones to check now will be stub and satellite to check."
"We have a literal wealth of information that we can use here to our benefits, including familiar features like:\n1. x, y,  and z coords\n2. yaw\n3. probabilites of other extraneous factors."
"Here we can extrapolate that the variables **centroid_x** and **centroid_y** have strongly negative correlations, and the strongest correlations are between **extent_z** and **extent_x** more than any other, coming in at 0.4. We can also try using an XGBoost/LightGBM model as kkiller has demonstrated in his brilliant kernel as an alternative approach to the problem."
### centroid_x and centroid_y
It seems like the two centroids have a somewhat strongly negative correlation and seemingly similar variable distributions. It seems that as such there is a negative correlation between both the variables.
"### extent_x, extent_y and extent_z"
"It seems both the distributions of extent X and extent Y are heavily right skewed, as is centroid X. However, I have left out extent Z is order for readability of the plot, let's look at it now.\n\nTry to smooth the data and get:"
"It seems both the distributions of extent X and extent Y are heavily right skewed, as is centroid X. However, I have left out extent Z is order for readability of the plot, let's look at it now.\n\nTry to smooth the data and get:"
"Once again, we have a right-skewed distribution as is the same with all the `extent` variables. "
### yaw
"So yes it seems like this distribution has several ""protrusions"" as I shall call them. We can now move on to exploring the frames data to check how feasible it is for our tabular purposes."
"First of all, we have nine ego rotation columns corresponding to each. So I would want to do a quick check of the correlation of these variables before moving on to some more high-level analyses."
Things to note from this correlation analysis:\n1. The rotation coordinates with `y` and `z` seem to be uncorrelated most of the time\n2. The coordinates which have `x` are correlated strongly with the z-dimensional rotation (could this be indicative of something? I very much think so)
Let's see the pixel distributions for the above plot:
And how much they have changed from the original:
And how much they have changed from the original:
Not much difference here except that we've shaved a lot off the `0.0` peak which is the effect of the cutout augmentatioin.
#### To fill these Nan values the data distribution needs to be understood
### Aiming to impute nan values for the columns in accordance with their distribution
## Plotting after Nan removal 
## Skewness\n\nA ***left-skewed distribution*** has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That’s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA ***right-skewed distribution*** has a long right tail. Right-skewed distributions are also called positive-skew distributions. That’s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\n\n![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2014/02/pearson-mode-skewness.jpg)\n\n\n#### to learn more about skewness\nhttps://www.statisticshowto.datasciencecentral.com/probability-and-statistics/skewed-distribution/
#### Pair plot for clean data
***Pearson's Correlation Coefficient***: helps you find out the relationship between two quantities. It gives you the measure of the strength of association between two variables. The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.\n\nA heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information.
#### Heatmap for unclean data
#### Heatmap for clean data
#### Heatmap for clean data
"## Scaling the data \ndata Z is rescaled such that μ = 0 and 𝛔 = 1, and is done through this formula:\n![](https://cdn-images-1.medium.com/max/800/0*PXGPVYIxyI_IEHP7.)\n\n\n#### to learn more about scaling techniques\nhttps://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc\nhttps://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/"
## Result Visualisation
#### The best result is captured at k = 11 hence 11 is used for the final model
\n\n\n\n3  IMPORTS    ⤒
\n\n\n\n4  SETUP AND HELPER FUNCTIONS    ⤒
"\n\n5.7 CALCULATE RANKS OF IMPORTANT COLUMNS\n\n---\n\nFor the important columns we determine the rankings (from largest to smallest for instance) and store those as new columns. The important columns are as follows (the +/- indicates if the ranking is ascending or descending)\n* **`+`**`ddg`\n* **`+`**`sub_matrix_score`\n* **`-`**`b_factor`\n* **`-`**`b_factor_matrix_score_adjusted`\n\nTo do this we will use the **`scipy.stats`** library, specifically the **`scipy.stats.rankdata`** method."
"\n\n5.8 COMBINE THE RANKS OF THE IMPORTANT COLUMNS\n\n---\n\nThis is weird... because we seemingly ignore the calculations we just performed (prior to ranking) in favour of the non-normalized values. \n\nBasically the combination is the following equation\n\n$$\nf(x,y,z) = (x*y*z)^{\dfrac{1}{3}}\n$$\n\n\n\n\n\n"
"\n\n5.8 COMBINE THE RANKS OF THE IMPORTANT COLUMNS\n\n---\n\nThis is weird... because we seemingly ignore the calculations we just performed (prior to ranking) in favour of the non-normalized values. \n\nBasically the combination is the following equation\n\n$$\nf(x,y,z) = (x*y*z)^{\dfrac{1}{3}}\n$$\n\n\n\n\n\n"
\n\n5.9 CREATE SUBMISSION FILE USING COMBINED NORMALIZED RANK VLAUE\n\n---\n\nStraightforward
"**Load Python modules:** The list of modules grows step by step by adding new functionality that is useful for this project. A module could be defined further down once it is needed, but I prefer to have them all in one place to keep an overview."
**Load input data.** And combine the available features of train and test data sets. *test* of course doesn't have the column that indicates survival.
"OK, let's go through the features one by one to see what we find. Here we will see how the distributions of survivors and non-survivors compare. Personally, I like histograms for a first look at comparing two or more populations in case of scaled features. For categorical features we will use barplots plus standard deviation bars, to better judge the significance."
"Above we are creating a kind of summary dashboard, where we collect relevant visualisations to study the distributions of the individual features. We use the matplotlib *subplot* tool to line up the individual plots in a grid. We use overlapping histograms for ordinal features and barplots for categorical features. The barplots show the fraction of people (per group) who survived. There's a lot going on in this figure, so take your time to look at all the details.\n\n**We learn** the following things from studying the individual features:\n\n- *Age:* The medians are identical. However, it's noticeable that fewer young adults have survived (ages 18 - 30-ish) whereas **children younger than 10-ish had a better survival rate.** Also, there are no obvious outliers that would indicate problematic input data. The highest ages are well consistent with the overall distribution. There is a notable shortage of teenagers compared to the crowd of younger kids. But this could have natural reasons.\n\n- *Pclass:* There's a clear trend that **being a 1st class passenger gives you better chances of survival**. Life just isn't fair.\n\n- *SibSp & Parch:* **Having 1-3 siblings/spouses/parents/children on board (SibSp = 1-2, Parch = 1-3) suggests proportionally better survival numbers than being alone (SibSp + Parch = 0) or having a large family travelling with you.**\n\n- *Embarked:* Well, that does look more interesting than expected.  **Embarking at ""C"" resulted in a higher survival rate than embarking at ""S""**. There might be a correlation with other variables, here though.\n\n- *Fare:* This is case where a linear scaling isn't of much help because there is a smaller number of more extreme numbers. A natural choice in this case is to transform the values logarithmically. For this to work we need to adjust for the zero-fare entries.  The plot tells us that the **survival chances were much lower for the cheaper cabins**. Naively, one would assume that those cheap cabins were mostly located deeper inside the ship, i.e. further away from the life boats."
A little follow up: For *SibSp* we see in the plot that most of the differences are not very significant (overlapping error bars). Another way of checking the actual numbers are through *cross tables*:
"Passengers with more than 3 children+parents on board had low survival chances. However the corresponding number are not very large. For SibSp we have 15 vs 3, 5 vs 0, and 7 vs 0.\n\nRandom outcomes with 2 possibilities (like *heads or tails* when flipping a coin) follow the [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution). We can use a *binomial test* to estimate the probability that 5 non-survivors out of a total 5 passengers with SibSp = 5 happened due to chance assuming the overall 38% survival chance for the entire sample."
"We start with an **overview plot of the feature relations:** Here we show a *correlation matrix* for each numerical variable with all the other numerical variables. We excluded *PassengerID*, which is merely a row index. In the plot, stronger correlations have brighter colours in either red (positive correlation) or blue (negative correlation). The closer to white a colour is the weaker the correlation. "
"*Positive vs negative correlation* needs to be understood in terms of whether an increase in one feature leads to an increase (positive) or decrease (negative) in the correlated feature. Perfect correlation would have a correlation index of 1; perfect anti-correlation (= negative correlation) would have -1 (obviously each feature is perfectly correlated with itself; leading to the deep red diagonal). The upper right vs lower left triangle that make up this plot contain the same information, since the corresponding cells show the correlation coefficients of the same features. \n\nThe matrix gives us an overview as to which features are particularly interesting for our analysis. Both strongly positive or negative correlations with the *Survived* feature are valuable. Strong correlations between two other features would suggest that only one of them is necessary for our model (and including the other would in fact induce noise and potentially lead to over-fitting).\n\n**We learn:**\n\n- *Pclass* is somewhat correlated with *Fare* (1st class tickets would be more expensive than 3rd class ones)\n- *SibSp* and *Parch* are weakly correlated (large families would have high values for both; solo travellers would have zero for both)\n- *Pclass* already correlates with *Survived* in a noticeable way"
"In addition, we plot a **Pairplot** of the numerical features. This kind of plot is a more detailed visualisation of relationships between variables. It shows scatter plots for the different feature combinations plus a distribution of each feature on the diagonal. Again, the upper right and lower left triangle contain the same information. This kind of plot is vastly more useful for a set of continuous variables, instead of the categorical or integer values we have here. Nonetheless, it is a valuable exploratory tool that has a place in everyone's toolbox.\n\nThis plot is inspired by, and realised much more aesthetically in, the [comprehensive Ensemble Stacking Kernel by Anisotropic](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)  "
"Now we continue to examine these initial indications in more detail. Earlier, we had a look at the *Survived* statistics of the individual features in the overview figure. Here, we want to look at correlations between the predictor features and how they could affect the target *Survived* behaviour.\n\nUsually it's most interesting to start with the strong signals in the correlation plot and to examine them more in detail."
"Now we continue to examine these initial indications in more detail. Earlier, we had a look at the *Survived* statistics of the individual features in the overview figure. Here, we want to look at correlations between the predictor features and how they could affect the target *Survived* behaviour.\n\nUsually it's most interesting to start with the strong signals in the correlation plot and to examine them more in detail."
"**We learn:**\n\n- For females the survival chances appear to be higher between 18 and 40, whereas for men in that age range the odds are flipped. This difference between 18-40 yr olds might be a better feature than *Sex* and *Age* by themselves.\n\n- Boys have proportional better survival chances than men, whereas girls have similar chances as women have. Rather small numbers, though. "
"We study the correlation of *Age* with *Pclass* using a *violin plot*, which is also split between survived (right half) and not survived (left half). Check out the other visualisations in your forked copy."
"*Violin plots* are a modified version of boxplots, where the shape is a ""kernel density estimate"" of the underlying distribution. These estimates are smoothed and therefore extend beyond the actual values (look closely at the dotted zero level). I have also indicated *Age == 10*, which we will use to define children (vs teenagers) in the engineering part below.\n\n**We learn:**\n\n- Age decreases progressively as Pclass decreases from 1st to 3rd\n- Most older passengers are 1st class, but very few children are. This conflates the impact of *Age* and *Pclass* on the survival chances.\n- In 1st class, younger adults had better survival chances than older ones.\n- Most children in 2nd class survived, and the majority in 3rd class did too."
"Also, we will start to use *factorplots*, i.e. groups of *pointplots*, from the *seaborn* plotting package to visualise the categorical relations:"
"**We learn:**\n\n- Both the factorplot and the mosaicplot indicate that almost all females that died were 3rd class passengers.\n- For males being in 1st class gives a survival boost, otherwise the proportions look roughly similar.\n- Except for 3rd class, the survival for *Embarked == Q* is close to 100% split between male and female.\n\nLet's follow up the numbers for *Pclass vs Embarked* with a *pandas crosstab plot*:"
"**We learn:**\n\n- Both the factorplot and the mosaicplot indicate that almost all females that died were 3rd class passengers.\n- For males being in 1st class gives a survival boost, otherwise the proportions look roughly similar.\n- Except for 3rd class, the survival for *Embarked == Q* is close to 100% split between male and female.\n\nLet's follow up the numbers for *Pclass vs Embarked* with a *pandas crosstab plot*:"
"**We learn:**\n\n- a high percentage of those embarked at ""C"" were 1st class passengers.\n- almost everyone who embarked at ""Q"" went to 3rd class (this means that the clear separation in the factorplot for ""Q"" isn't very meaningful, unfortunately).\n\nThe 2nd point is somewhat curious, since we recall from above that the survival chances for ""Q"" were actually slightly better than for ""S"". Not significantly so, of course, but certainly not worse even though ""S"" had a higher percentage of 1st and 2nd class passengers.\n\nIt seems that embarking at ""Q"" improved your chances for survival if you were a 3rd class passenger. Let's investigate that a bit more:"
"**We learn:**\n\n- a high percentage of those embarked at ""C"" were 1st class passengers.\n- almost everyone who embarked at ""Q"" went to 3rd class (this means that the clear separation in the factorplot for ""Q"" isn't very meaningful, unfortunately).\n\nThe 2nd point is somewhat curious, since we recall from above that the survival chances for ""Q"" were actually slightly better than for ""S"". Not significantly so, of course, but certainly not worse even though ""S"" had a higher percentage of 1st and 2nd class passengers.\n\nIt seems that embarking at ""Q"" improved your chances for survival if you were a 3rd class passenger. Let's investigate that a bit more:"
"Ok, now from here it looks more like ""S"" is the interesting port since survival is less probably for that one if you are a 3rd class passenger. Otherwise  there is no significant difference within each class.\n\nThere seems to be some impact here that isn't captured by the passenger class. What about the other strong feature, Sex?"
"Ok, now from here it looks more like ""S"" is the interesting port since survival is less probably for that one if you are a 3rd class passenger. Otherwise  there is no significant difference within each class.\n\nThere seems to be some impact here that isn't captured by the passenger class. What about the other strong feature, Sex?"
"Now this is somewhat expected since it explains the difference between ""S"" and the other ports. Therefore, it seems that between more 1st class passengers embarking at ""C"" and more men at ""S"" there doesn't seem to be much actual influence in the port of embarkation.\n\nHowever, the last plot should also indicate that ..."
"Now this is somewhat expected since it explains the difference between ""S"" and the other ports. Therefore, it seems that between more 1st class passengers embarking at ""C"" and more men at ""S"" there doesn't seem to be much actual influence in the port of embarkation.\n\nHowever, the last plot should also indicate that ..."
... there were more males among the 3rd class passengers. Possibly travelling alone?
... there were more males among the 3rd class passengers. Possibly travelling alone?
"Sort of, yes. This goes some way to explain features like better survival for SibSp = 1-3. But I think that it doesn't cover all the signal in the Parch feature.\n\n**We learn:**\n\n- Different percentages of passenger classes and sexes have embarked from different ports, which is reflected in the lower survival rates for ""S"" (more men, fewer 1st class) compared to ""C"" (more women and 1st class).\n\n- It's hard to say at this stage whether there is any real impact left for the *Embarked* feature once we correct for these connections. We will come back to this in the modelling stage when we will study feature importances and significances (soon)."
"Finally, let's check what's going on between *Age* and *Embarked*:"
"The curious distribution for the ""Q"" survivors somewhat follows the overall trend for 3rd class passengers (which make up the vast majority of ""Q"") but is notably narrower. Not many of the children there survived, but then there were not many children to begin with. Let's come back to this point in discussing the derived features.\n\n**We learn:**\nThere don't seem to be strong differences in *Age* among the *Embarked* categories that would point at an imbalance that goes beyond the influence of *Pclass* and *Sex*. "
Let's study the relation between *Fare* and *Pclass* in more detail:
**We learn:**\n\n- There is a broad distribution between the 1st class passenger fares (rich -> super rich)\n- There's an interesting bimodality in the 2nd class cabins and a long tail in the 3rd class ones. (*TODO: check cumulative fare question*)\n- For each class there is strong evidence that the cheaper cabins were worse for survival. A similar effect can be seen in a *boxplot*:
### *Child*
"The *Pclass == 1* plot looks interesting at first, but there are only 3 children in this group which makes the apparent pattern just random noise. The other two passenger classes are more interesting, especially for the male children. Note, that since we are selecting by *Age*, which has many missing values, a number of children will be in the *Child == False* group. Nonetheless, this seems useful.\n\n**We learn:** Male children appear to have a survival advantage in 2nd and 3rd class. We should include the *Child* feature in our model testing."
### *Cabin\_known*
"As suspected, it is more likely to know the cabin of a passenger who survived. This could be useful."
"As suspected, it is more likely to know the cabin of a passenger who survived. This could be useful."
"However, we see again that a large part of this effect disappears once we control for *Sex* and *Pclass*. \n\n**We learn:** There remains a potential trend for males and for 3rd class passengers but the uncertainties are large. This feature should be tested in the modelling stage."
### *Deck*
"Ok, so what can we tell from the Deck (derived from the Cabin number)? First of all the overall survival statistics is much better than for the full sample, which is what we found above. Beyond that, the best decks for survival were B, D, and E with about 66% chance. C and F are around 60%. A and G at 50%. The only passenger on deck T died, but that's hardly robust statistics.\n\nThe largest number of cases we have is for B vs C. Let's see whether that's significant:"
Just about formally significant (i.e. < 5%). It might be worth our while to include this feature in at least the initial stages of modelling to see how it performs.
"In addition, there is some variation between the 1st class male passengers, but it doesn't look overly significant."
"Based on this plot we define a new feature called *Bad\_ticket* under which we collect all the ticket numbers that start with digits which suggest less than 25% survival (e.g. *4*, *5*, or *A*). We are aware that some of the survival fractions we see above are based on small number statistics (e.g. 2 vs 0 for *8*). It is well possible that some of our ""bad tickets"" are merely statistical fluctuations from the base survival rate of 38%.  The barplot shows mean survival fractions and the associated 95% confidence limits, which are large for the sparse samples.\n\nHowever, the significant difference between e.g. *1* and *3* (based on large enough numbers) suggests that this new feature could still contain some useful information. I think that without external information, which we are avoiding in this notebook, we can't do much better in trying to tie the ticket number to the survival statistics.\n\nOf course, it's not the tickets themselves that are ""bad"" for survival, but the possibility that the ticket numbers might encode certain areas of the ship that would have led to higher or lower survival chances."
"The factorplot suggests that bad tickets are worse for male passengers, and 3rd class passengers. The individual significances are not overwhelming, but the trend itself might be useful."
"The factorplot suggests that bad tickets are worse for male passengers, and 3rd class passengers. The individual significances are not overwhelming, but the trend itself might be useful."
"The last plot doesn't inspire much confidence in a strong correlation between *Deck* and *Bad\_ticket*, but maybe it will be useful otherwise.\n\n**We learn:** *Bad\_ticket* might be a lower order effect that could give us some additional accuracy. We should test it out in the modelling stage."
"Similar to the known Cabin numbers, what about the *passengers for which we know the age*?"
"As we would expect intuitively, it appears that we are more likely to know someones age if the survived the disaster. There's a difference of about 30% vs 40% and it should be significant:"
"Very much so. However, we have seen before that there might be imbalances in the dominating features *Sex* and *Plcass* that create an apparent signal. Is this another of these cases?"
"It actually is. Turns out that we are more likely to know the age of higher class passengers or women, which are the strongest survival predictors we have found, so far. (Of course, the causality might as well go the other way, but that's not really the question here. What we want to find are the best predictors for survival.)\n\n**We learn:** \nThere is a strong impact of *Sex* and *Pclass* on this new feature. This might be enough to explain all the variance in the *Age\_known* variable. We should test the predictive power in our modelling."
### *Family*
"**We learn:**\nAgain, we find that having 1-3 family members works best for survival. This feature is a mix of *SibSp* and *Parch*, which increases the overall numbers we can work with, but might smooth out some more subtle effects."
### *Alone*
Travelling alone appears bad enough to be significant.
Travelling alone appears bad enough to be significant.
"But more men were travelling alone than women did. Especially among the 3rd class passengers. Also this feature should be evaluated in our modelling step, to see if it's still significant in the presence of the *Sex* feature."
### *Large\_Family*
"In the same way, having a large family appears to be not good for survival."
"In the same way, having a large family appears to be not good for survival."
But most large families were travelling in 3rd class. The tentative imbalance between male and female 3rd class probably reflect the observation we made earlier that men were more likely to travel alone.
### *Shared\_ticket*
Sharing a ticket appears to be good for survival.
Sharing a ticket appears to be good for survival.
But again the sharing of tickets is more frequent with females and 1st class passengers. This is consistent with the other statistics that show that women were more likely to travel together with larger families.\n\n**We learn:** Several of these derived parameters are strongly correlated with *Sex* and *Pclass*. Whether there is actual signal in them that a model can use to improve the learning accuracy needs to be investigated.
"Ok, so we have 18 different titles, but many of them only apply to a handful of people. The dominating ones are Mr (581), Miss (210), Mrs (170), and Master (53); with the number referring to the combined data. Here are the age distributions for those:"
"We see that *Master* is capturing the male children/teenagers very well, whereas *Miss* applies to girls as well as younger women up to about 40. *Mrs* does not contain many teenagers, but has a sizeable overlap with *Miss*; especially in the range of 20-30 years old.\n\nNevertheless, *Miss* is more likely to indicate a younger woman. Overall, there is a certain amount of variance and we're not going to be able to pinpoint a certain age based on the title.\n\nTherefore, we will use 2 *Age Groups*, updating to the *Young* variable we defined above. The idea is to address the issue of missing *Age* values by combining the *Age* and *Title* features into a single feature that should still contain some of the signal regarding survival.\n\nFor this, we define everyone under 30 *or* with a title of *Master*, *Miss*, or *Mlle* (Mademoiselle) as *Young*. All the other titles we group into *Not Young*. This is a bit of a generalisation in terms of how *Miss* and *Mrs* overlap, but it might be a useful starting point. All the other rare titles (like *Don* or *Lady*) have average ages that are high enough to count as *Not Young*."
"We see that *Master* is capturing the male children/teenagers very well, whereas *Miss* applies to girls as well as younger women up to about 40. *Mrs* does not contain many teenagers, but has a sizeable overlap with *Miss*; especially in the range of 20-30 years old.\n\nNevertheless, *Miss* is more likely to indicate a younger woman. Overall, there is a certain amount of variance and we're not going to be able to pinpoint a certain age based on the title.\n\nTherefore, we will use 2 *Age Groups*, updating to the *Young* variable we defined above. The idea is to address the issue of missing *Age* values by combining the *Age* and *Title* features into a single feature that should still contain some of the signal regarding survival.\n\nFor this, we define everyone under 30 *or* with a title of *Master*, *Miss*, or *Mlle* (Mademoiselle) as *Young*. All the other titles we group into *Not Young*. This is a bit of a generalisation in terms of how *Miss* and *Mrs* overlap, but it might be a useful starting point. All the other rare titles (like *Don* or *Lady*) have average ages that are high enough to count as *Not Young*."
"Finally, we model a fare category, *Fare_cat*, as an ordinal integer variable based on the logarithmic fare values:"
"Finally, we model a fare category, *Fare_cat*, as an ordinal integer variable based on the logarithmic fare values:"
"Because of the larger number of ""Miss"" vs ""Master"" mostly women are classified as ""Young"".  We also recover the age difference between the ticket classes that was already obvious in earlier plots. Both factors mean that the impact of *Young* has to be studied carefully."
Let's remind ourselves of the distribution of *Fare* with respect to *Pclass*:
"To simplify this broad distribution, we decide to classify the fares into *3 fare categories*: 0-10, 10-100, and above 100. This transformation can be easily achieved using the base 10 logarithm:"
"Let's investigate the *Fare affair* in more detail. First, we make sure that the passengers in each group really had the same *Fare* values:"
"Almost 100% yes. Above, we extract the standard deviation of the *Fares* among the ticket groups. A standard deviation of zero means that there's no difference. Only 2 values stand out. This is a small number that we could ignore, but we are curious, aren't we?"
"It's Mr Osen and Mr Gustafsson on Ticket 7534. Their *Fares* are close enough, though, to include them in the general treatment.\n\nNow, let's think for a moment: Identical fares could mean that the fare for a cabin was shared equally among the passengers, in which case our previous treatment would have been justified. However, it *could* also mean that the listed value is the *cumulative fare per cabin* and it was simply recorded as the same value for each passenger. Intuitively, this doesn't seem so plausible, since you typically record what is paid for a ticket and not for a cabin. But let's investigate this for a moment and check how it would transform the *Fare* distribution. For this, we create a *Fare_eff* feature above, which we derive by dividing *Fare* by the number of people sharing a ticket (*Ticket_group*; which we also newly created)."
Now **that** is interesting. We see that the distributions become significantly narrower and that the tails and bimodality become much weaker (after getting rid of the zero-fare values for both groups). The really expensive *Fares* in *Pclass == 1* are pretty much all gone. Here's how the standard deviations compare:
"And that's quite expensive for a 3rd class ticket. Maybe these two actually shared a ticket / cabin and we have another transcription / data entry error? The ticket numbers are very similar and someone could easily write ""303"" instead of ""304"". Will we ever know? Maybe not. Does it matter much? Probably not.\n\nMore importantly, there is a reasonable argument to be made for this new *Fare_eff* feature to represent the actual fare better than the original feature. For once, it splits much cleaner between the *Pclasses*:"
"So well, in fact that defining new fare categories seems almost redundant because *Pclass* already captures most of this signal. Nonetheless, we'll try; because we are optimistic people at heart. We use the dashed lines in the plot above for an (empirical) division into 3 classes, which separate the cheaper *Fare_eff* of a *Pclass* group from the more expensive ones of the next one. The new feature is called *Fare_eff_cat* and behaves as follows:"
For a final overview before the modelling stage we have another look at the correlation matrix between all old and new features:
"We designed a number of new features, and unsurprisingly several of those are correlated with the original features we used to create them. For instance *Fare\_cat* and *Fare*. Or *Family* and *SibSp/Parch*. In the modelling step, we will first determine which of the features carry the most signal (*to be done*) and then use them to train a number of different classifiers."
\n\n\n\n0  IMPORTS    ⤒
\n\n\n\n1  BACKGROUND INFORMATION    ⤒\n\n---\n
"\n\n1.1 BASIC COMPETITION INFORMATION\n\n---\n\nPRIMARY TASK DESCRIPTION\n\nIn this competition, you’ll create a model to automatically segment the stomach and intestines on MRI scans. The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment. You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care.\n\nBASIC BACKGROUND INFORMATION\n\nIn 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks. Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, oncologists are able to visualize the daily position of the tumor and intestines, which can vary day to day. \n\nIn these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines. This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate—unless deep learning could help automate the segmentation process. A method to segment the stomach and intestines would make treatments much faster and would allow more patients to get more effective treatment.\n\nCOMPETITION HOST INFORMATION\n\nThe UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015. UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin. The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens.\n\nVISUAL EXPLANATION\n\n\n\nThe tumor above (pink thick line) is close to the stomach (red thick line). High doses of radiation are directed to the tumor while avoiding the stomach. Dose levels are represented by colour. Higher doses are represented by red and lower doses are represented by green.\n\n\n\nMRI is an excellent imaging modality for visualization of soft tissues. This is particularly useful for tumors of the abdomen, such as pancreatic cancer shown below.  The left image shows the patient’s anatomy during exhale, while the image on the right shows the anatomical change during a maximum inspiration breath hold (MIBH). In the MIBH image we can see motion of nearly all the soft tissue, providing us superior ability to align the tumor during our treatment delivery. We are analyzing the clinical impact of using these treatment planning and delivery techniques and our patient’s ability to comply with self-guided breathing maneuvers.[REF]\n\nCOMPETITION IMPACT STATEMENT\n\nCancer takes enough of a toll. If successful, you'll enable radiation oncologists to safely deliver higher doses of radiation to tumors while avoiding the stomach and intestines. This will make cancer patients' daily treatments faster and allow them to get more effective treatment with less side effects and better long-term cancer control."
"\n\n1.3 DATASET OVERVIEW\n\n---\n\nGENERAL INFORMATION\n\nIn this competition we are segmenting organs cells in images. \n\nThe training **annotations are provided as RLE-encoded masks**, and the images are in **16-bit**, **grayscale**, **PNG format**.\n\nEach case in this competition is represented by multiple sets of scan slices\n* Each set is identified by the day the scan took place\n* Some cases are split by time\n    * early days are in train\n    * later days are in test\n* Some cases are split by case\n    * the entirety of the case is in train or test\n\nThe goal of this competition is to be able to generalize to both partially and wholly unseen cases.\n\nNote that, in this case, the test set is entirely unseen.\n* It is roughly 50 cases\n* It contains a varying number of days and slices, (similar to the training set)\n\nFILE INFORMATION\n\n**`train.csv`** \n- IDs and masks for all training objects.\n- **Columns**\n    * **`id`**\n        * unique identifier for object\n    * **`class`**\n        * the predicted class for the object\n    * **`EncodedPixels`**\n        * RLE-encoded pixels for the identified object\n\n\n\n**`sample_submission.csv`**\n- A sample submission file in the correct format\n\n\n\n**`train/`**\n- a folder of case/day folders, each containing slice images for a particular case on a given day.\n\n\n\n\n    ⚠️   NOTE   ⚠️The image filenames include 4 numbers (ex. 276_276_1.63_1.63.png).These four numbers are representative of:slice height (integer in pixels)slice width (integer in pixels)heigh pixel spacing (floating point in mm)width pixel spacing (floating point in mm)The first two defines the resolution of the slide. The last two record the physical size of each pixel.\n\n\n \n\n"
"\n\n4.1 INVESTIGATE THE OCCURENCE SEGMENTATION MAP TYPES\n\n---\n\nIt's quite apparent that not all images have segmentation maps for the various regions (stomach, large-bowel, small-bowel), so we will identify the frequency for which these occur independently... as well as the frequency for which these maps co-occur.\n\nOBSERVATIONS\n\n* There are **38,496** total examples.\n* It can be observed that more than half of the given examples have no annotations present!\n    * There are **21,906** (56.9046%) examples with no annotations/masks/segmentation present\n    * Inversely there are **16,590** (43.0954%) examples with one or more annotations present\n* There are **2,468** (6.41%) examples with **one annotation present**. \n* It can be observed that the vast majority of single mask annotations are **Stomach**!\n    * Of these annotations, **2286** (~92.6%) are **Stomach**\n    * Of these annotations, **123** (~4.98%) are **Large Bowel**\n    * Of these annotations, **59** (~2.39%) are **Small Bowel**\n* There are **10,921** (28.37%) examples with **two annotations present**. \n* It can be observed, in contrast to the single annotation examples, that the majority of annotations do NOT include stomach i.e. **'Large Bowel, Small Bowel'**!\n    * Of these annotations, **7781** (~71.3%) are **'Large Bowel, Small Bowel'**\n    * Of these annotations, **2980** (~27.3%) are **'Large Bowel, Stomach'**\n    * Of these annotations, **160** (~1.47%) are **'Small Bowel, Stomach'**\n* Finally, there are **3,201** (8.32%) examples with **all three annotations present**. \n\n"
"\n\n4.2 INVESTIGATE THE IMAGE SIZES\n\n---\n\nIt's observable that not all images have the same size... however, given that, there is not that much variation between image slice sizes.\n\nOBSERVATIONS\n* Remember, there are **38,496** total examples.\n* Globally, we can see that 3 of the image shapes are **square** while one is **rectangular** and they all fall within a fairly tight distribution of relatively small sizes\n* Of these there are **4** unique sizes:\n    * $234 \times 234$\n        * **Least frequent** image size\n        * **Smallest** image size\n        * Only **144** of the 38,496 occurences are this size (0.37%)\n    * $266 \times 266$\n        * **Most frequent** image size\n        * **Second smallest** image size\n        * **25,920** of the 38,496 occurences are this size (67.33%)\n    * $276 \times 276$\n        * **Second least frequent** image size\n        * **Second largest** image size\n        * **1,200** of the 38,496 occurences are this size (3.12%)\n    * $310 \times 360$\n        * **Second most frequent** image size\n        * **Largest** image size\n        * **11,232** of the 38,496 occurences are this size (29.17%)\n\n\n\n"
"\n\n4.2 INVESTIGATE THE IMAGE SIZES\n\n---\n\nIt's observable that not all images have the same size... however, given that, there is not that much variation between image slice sizes.\n\nOBSERVATIONS\n* Remember, there are **38,496** total examples.\n* Globally, we can see that 3 of the image shapes are **square** while one is **rectangular** and they all fall within a fairly tight distribution of relatively small sizes\n* Of these there are **4** unique sizes:\n    * $234 \times 234$\n        * **Least frequent** image size\n        * **Smallest** image size\n        * Only **144** of the 38,496 occurences are this size (0.37%)\n    * $266 \times 266$\n        * **Most frequent** image size\n        * **Second smallest** image size\n        * **25,920** of the 38,496 occurences are this size (67.33%)\n    * $276 \times 276$\n        * **Second least frequent** image size\n        * **Second largest** image size\n        * **1,200** of the 38,496 occurences are this size (3.12%)\n    * $310 \times 360$\n        * **Second most frequent** image size\n        * **Largest** image size\n        * **11,232** of the 38,496 occurences are this size (29.17%)\n\n\n\n"
"\n\n4.3 INVESTIGATE THE PIXEL SPACING\n\n---\n\nIt's observable that not all images have the same pixel spacing... however, given that, there is not that much variation between pixel spacing.\n\nOBSERVATIONS\n* Remember, there are **38,496** total examples.\n* Globally, we can see that all of the pixel spacings are **square** and that the vast majority are $1.50mm \times 1.50mm$\n* There are only **2** unique sets of pixel spacings:\n    * $1.50mm \times 1.50mm$\n        * **Most frequent** pixel spacing\n        * **Smallest** pixel spacing (barely)\n        * **37,296** of the 38,496 occurences are this size (96.88%)\n    * $1.63mm \times 1.63mm$\n        * **Least frequent** image size\n        * **Largest** pixel spacing (barely)\n        * **1,200** of the 38,496 occurences are this size (3.12%)"
"\n\n4.3 INVESTIGATE THE PIXEL SPACING\n\n---\n\nIt's observable that not all images have the same pixel spacing... however, given that, there is not that much variation between pixel spacing.\n\nOBSERVATIONS\n* Remember, there are **38,496** total examples.\n* Globally, we can see that all of the pixel spacings are **square** and that the vast majority are $1.50mm \times 1.50mm$\n* There are only **2** unique sets of pixel spacings:\n    * $1.50mm \times 1.50mm$\n        * **Most frequent** pixel spacing\n        * **Smallest** pixel spacing (barely)\n        * **37,296** of the 38,496 occurences are this size (96.88%)\n    * $1.63mm \times 1.63mm$\n        * **Least frequent** image size\n        * **Largest** pixel spacing (barely)\n        * **1,200** of the 38,496 occurences are this size (3.12%)"
"\n\n4.4 INVESTIGATE CASE IDS\n\n---\n\nHere's the host description of **`case_id`**\n\n> ""Each case in this competition is represented by multiple sets of scan slices (each set is identified by the day the scan took place). Some cases are split by time (early days are in train, later days are in test) while some cases are split by case - the entirety of the case is in train or test. The goal of this competition is to be able to generalize to both partially and wholly unseen cases.""\n\nI don't really observe any oddities associated with any particular **`case_id`** values. I would probably attempt to group them when stratifying/creating-folds... however, they don't seem to perpetrate an obvious bias.\n\nWhen we colour by **day**, we can see that all cases are made up (mostly) of groups of **144**, or less frequently, **80**, images from different days."
"\n\n4.5 MASK SIZES/AREAS\n\n---\n\nWe know that every other number in an RLE encoding represents a run of mask... so if we add up all those numbers we get the total number of masked pixels in an image. This is much faster than opening and closing each image.\n\nOBSERVATIONS\n\n* It's observable that the distributions of mask area is mostly normal although it skews slightly to the smaller side...\n* All the distributions are similar although the Stomach distribution has an odd gap between 400-750 pixels.\n* It's interesting to note that, while not common, we do have some VERY large masks (>7500 pixels)\n    * Also, it's kind of funny that the biggest masks are for **small** bowel\n    "
"\n\n4.6 MASK DATASET CREATION, CLASS OVERLAP & MASK HEATMAP\n\n---\n\nIt's important to determine if the the masks overlap one another (**multilabel**) or not (**multiclass**). To do this, we will quickly create a dataset of **`npy`** files. During this creation process we will check for overlap.\n\nOBSERVATIONS\n\n* There is overlap, and while it is not that common, some images exhibit a high degree of overlap.\n* This means that we cannot frame the problem as simple categorical semantic segmentation.\n* We must instead frame the problem as multi-label semantic segmentation\n* This means our mask will take the form --> $W \times H \times 3$\n    * Where the channel dimensions are binary masks for each respective segmentation type\n    * This will allow for the masks to overlap\n\n\n**NOTE ON THE PLOTTED IMAGE BELOW:**\n* In the examined image below we can see a section of the small bowel is completely inside of a larger section of larger bowel.\n* This shows why treating this as multi-label semantic segmentation is so important!"
## **4.1 Load libraries** \n\n[Table of Contents](#0.1)
## **4.2 Read dataset** \n\n[Table of Contents](#0.1)
"# **6. Feature importance with XGBoost** \n\n[Table of Contents](#0.1)\n\n\n- XGBoost provides a way to examine the importance of each feature in the original dataset within the model. \n\n- It involves counting the number of times each feature is split on across all boosting trees in the model. \n\n- Then we visualize the result as a bar graph, with the features ordered according to how many times they appear.\n\n- XGBoost has a **plot_importance()** function that helps us to achieve this task. \n\n- Then we can visualize the features that has been given the highest important score among all the features. \n\n- Thus XGBoost provides us a way to do feature selection.\n\n- We will proceed as follows:-"
"- We can see that the feature `Delicassesn` has been given the highest importance score among all the features. \n\n- Based upon this importance score, we can select the features with highest importance score and discard the redundant ones.\n\n- Thus XGBoost also gives us a way to do feature selection."
Let's plot the audio frames
Let's zoom in on first 1000 frames
Let's zoom in on first 1000 frames
\n### Audio Length\n\nWe shall now analyze the lengths of the audio files in our dataset
\n### Audio Length\n\nWe shall now analyze the lengths of the audio files in our dataset
We observe:\n1. The distribution of audio length across labels is non-uniform and has high variance.\n\nLet's now analyze the frame length distribution in Train and Test.
We observe:\n1. The distribution of audio length across labels is non-uniform and has high variance.\n\nLet's now analyze the frame length distribution in Train and Test.
We observe:\n1. Majority of the audio files are short.\n1. There are four `abnormal` length in the test histogram. Let's analyze them.
#### Some sssential imports
\n#### Configuration
"These statements are contradictory, and the label shows that.\n\nLet's look at the distribution of languages in the training set."
## Preparing Data for Input
"\n\n Weights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. Kaggle competitions require fast-paced model development and evaluation. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.\n\n> ⏳ Lots of components = Lots of places to go wrong = Lots of time spent debugging\n\nW&B can be useful for Kaggle competition with it's lightweight and interoperable tools:\n\n* Quickly track experiments,\n* Version and iterate on datasets, \n* Evaluate model performance,\n* Reproduce models,\n* Visualize results and spot regressions,\n* Share findings with colleagues.\n\nTo learn more about Weights and Biases check out this kernel."
# Training Configuration ⚙️
# Training Function
# Validation Function
"**Note:**  Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will ""assume"" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
"**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future."
"**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future."
" Scaling and Distributing \n\nIn this phase of our kernel, we will first scale the columns comprise of Time and Amount . Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n\n What is a sub-Sample?\nIn this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n\n Why do we create a sub-Sample?\nIn the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe  will cause the following issues:\n\nOverfitting: Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. \nWrong Correlations: Although we don't know what the ""V"" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. \n\n\nSummary:  \n\n Scaled amount  and  scaled time  are the columns with scaled values. \n There are 492 cases  of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe. \nWe concat the 492 cases of fraud and non fraud, creating a new sub-sample. \n"
"##  Equally Distributing and Correlating: \n\nNow that we have our dataframe correctly balanced, we can go further with our analysis and data preprocessing."
" Correlation Matrices \nCorrelation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample)  in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n\n### Summary and Explanation: \n\nNegative Correlations: V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.  \n  Positive Correlations:  V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction. \n BoxPlots:   We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions. \n\n\n\n**Note: ** We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe."
"Unfortunately matplotlib (the most common library for plotting in Python) doesn't come with a way to visualize a function, so we'll write something to do this ourselves:"
Here's what our function looks like:
Let's use it to simulate some measurements evenly distributed over time:
"Now let's see what happens if we *underfit* or *overfit* these predictions. To do that, we'll create a function that fits a polynomial of some degree (e.g. a line is degree 1, quadratic is degree 2, cubic is degree 3, etc). The details of how this function works don't matter too much so feel free to skip over it if you like!  (PS: if you're not sure about the jargon around polynomials, here's a [great video](https://www.youtube.com/watch?v=ffLLmV4mZwU) which teaches you what you'll need to know.)"
"Now let's see what happens if we *underfit* or *overfit* these predictions. To do that, we'll create a function that fits a polynomial of some degree (e.g. a line is degree 1, quadratic is degree 2, cubic is degree 3, etc). The details of how this function works don't matter too much so feel free to skip over it if you like!  (PS: if you're not sure about the jargon around polynomials, here's a [great video](https://www.youtube.com/watch?v=ffLLmV4mZwU) which teaches you what you'll need to know.)"
"So, what happens if we fit a line (a ""degree 1 polynomial"") to our measurements?"
"Now we'll look at a few examples of correlations, using this function (the details of the function don't matter too much):"
"OK, let's check out the correlation between income and house value:"
# The State of Affairs Today - 2019 \n##  Top 10 Countries with the majority of respondents \nLet's begin by analyzing the 2019 survey's dataset to get a big picture. We shall begin by importing the dataset and the necessary libraries for the analysis.\n\n
"> ** 📌 Points to note :** \n> * The majority of the respondents(both male and female) are from India followed closely by the U.S. In fact, these countries together make up more than 50% of the entire population.\n> * If a country or territory received less than 50 respondents,[they have been grouped into a group named ""Other""](https://www.kaggle.com/c/kaggle-survey-2019/data) for anonymity.\n\n\n## Comparison of number of respondents w.r.t the previous surveys"
"> ** 📌 Points to note :** \n> * The majority of the respondents(both male and female) are from India followed closely by the U.S. In fact, these countries together make up more than 50% of the entire population.\n> * If a country or territory received less than 50 respondents,[they have been grouped into a group named ""Other""](https://www.kaggle.com/c/kaggle-survey-2019/data) for anonymity.\n\n\n## Comparison of number of respondents w.r.t the previous surveys"
"> **📌 Points to note :**\n> * Whereas the number of responses in 2018 was considerably higher than in 2017, the year 2019 has seen a decline. The people need to be sensitized about the importance of surveys. The data obtained from surveys like these can help to understand the Data Science Scenario around the world better.\n> * To encourage more people to take part and hence, complete surveys, specific innovative steps can be taken. People usually do not like lengthy surveys, especially which expects them to write detailed answers.\n\nLet's now focus our analysis on the six key areas of Gender, Country, Age, Education, Professional Experience and Salary."
"# 1. Gender\n![](https://cdn-images-1.medium.com/max/800/1*LUQEhTYkzHRxwaRir6eZBg.png)\n*Photo Credits : [rudall30](https://www.vectorstock.com/royalty-free-vectors/vectors-by_rudall30)*\n> Barack Obama once said that where women are full participants in a country's politics or economy, societies are more likely to succeed. This is indeed true. Let's see if the same is affirmed by the data.\n\n## 1.1 The Great Gender Divide\nThe following plot analyses the gender distribution in the 2019 survey."
"> **📌 Key Points :**\n* There is a staggering difference between men and women respondents in the survey. Around 82% of the respondents are men while only 16% are women.\n* Clearly, the above results indicate that the gender imbalance that affects the tech sector extends to data science and AI, as well.\n\n## 1.2 Gender Distribution over the years\nLet's see how gender distribution varies for the previous two surveys."
"> **📌 Key Points :**\n* There is a staggering difference between men and women respondents in the survey. Around 82% of the respondents are men while only 16% are women.\n* Clearly, the above results indicate that the gender imbalance that affects the tech sector extends to data science and AI, as well.\n\n## 1.2 Gender Distribution over the years\nLet's see how gender distribution varies for the previous two surveys."
"\n> **📌 Points to note :**\n> * Well, the pattern almost remains the same. Women respondents have been consistently low over the years. With this low participation, it gets really for our ideas and suggestions to be incorporated in the industry. \n**Common girls!! we need more participation**\n> * The challenge of getting and keeping women in tech and data science goes well beyond educational exposure and will require some concrete steps from the establishments. Gender shouldn't limit accomplishments and opportunities for anyone."
## 2.2 Top 20 Countries of Female Respondents\nLet us now dive further into the data to determine the top twenty countries of the female respondents. Let's see which countries made the cut.
"> **📌 Points to note :**\n* Maximum female respondents are from India followed by U.S. This pattern has been observed in the overall population also. \n* It is heartening to see that few females from Turkey, Nigeria, and Pakistan have also responded to the survey, albeit their percentage is very small.\n\n## 2.3 The Indian & the U.S Female Respondents over the years\nSince India and U.S have the maximum percentage of respondents, let's analyse and find out if the scenario was the same in the last two years also."
"> **📌 Points to note :**\n* Maximum female respondents are from India followed by U.S. This pattern has been observed in the overall population also. \n* It is heartening to see that few females from Turkey, Nigeria, and Pakistan have also responded to the survey, albeit their percentage is very small.\n\n## 2.3 The Indian & the U.S Female Respondents over the years\nSince India and U.S have the maximum percentage of respondents, let's analyse and find out if the scenario was the same in the last two years also."
"> ** 📌 Points to note :**\n* The number of female respondents in the U.S was considerably higher than in India for the years 2017 and 2018. However, the year 2019 saw a growth in Indian female respondents and their percentage outpaced the U.S females.\n* This is a good sign for India as a country since the percentage of participation was more than last year. On the underhand, steps could be taken to analyze why the participation of U.S women declined, who were leading from the past two years.\n\n## 2.4 Daunting obstacles remain in Africa\nAs we saw in section 2,1, Africa needs special attention. Let's look at the participants from the African subcontinent."
"> ** 📌 Points to note :**\n* The number of female respondents in the U.S was considerably higher than in India for the years 2017 and 2018. However, the year 2019 saw a growth in Indian female respondents and their percentage outpaced the U.S females.\n* This is a good sign for India as a country since the percentage of participation was more than last year. On the underhand, steps could be taken to analyze why the participation of U.S women declined, who were leading from the past two years.\n\n## 2.4 Daunting obstacles remain in Africa\nAs we saw in section 2,1, Africa needs special attention. Let's look at the participants from the African subcontinent."
"> ** 📌 Points to note :**\n* The number of African females who responded to the survey in 2019 has increased as compared to the previous years. Around 150 females responded to the survey in 2019 whereas, for the previous years, the numbers were even less than 100. However, there is a lot of progress to be still made.\n\n## 2.5 Algerian females make an appearance\nIt will be also interesting to find out the reason for the increased participation from the African subcontinent.Let's dig in further for more details."
"> ** 📌 Points to note :**\n* The number of African females who responded to the survey in 2019 has increased as compared to the previous years. Around 150 females responded to the survey in 2019 whereas, for the previous years, the numbers were even less than 100. However, there is a lot of progress to be still made.\n\n## 2.5 Algerian females make an appearance\nIt will be also interesting to find out the reason for the increased participation from the African subcontinent.Let's dig in further for more details."
"> ** 📌 Points to note :**\n* Interestingly, female respondents from Algeria also took the survey for the first time in 2019.\n* There has been a sharp spike in the Nigerian female respondents as compared to the previous two years.\n* Both the above factors have contributed to the better performance of African females in the 2019 survey."
# 3. Age Distribution\n![](https://cdn-images-1.medium.com/max/800/1*hmOJeowjdAZudATFTHIoLw.png)    \n*PC : www.freepik.com  *  \n    \nAge is an important attribute of any demographic analysis. It'll be interesting to see the age distribution of all the female respondents.\n    \n## 3.1 The Young Brigade dominates\nLet's look at the age distribution of the female respondents in 2019.\n
"> ** 📌 Points to note :**\n* The majority of the female respondents are in the (25 -29) age group followed closely by (22–24) age group. Thus we can say that most of the women lie between the 20 to 30 age bracket. \n* The (20–30) age group can comprise of both students(undergraduate and postgraduate) and professionals. \n* Interestingly, females greater than 70 years of age have also responded. Well, as it is said - Age is only a number !!.\n\n## 3.2 Age distribution pattern over the years\nWas the age distribution pattern same for the last two years too? let's analyze and find out.   "
"> ** 📌 Points to note :**\n* The majority of the female respondents are in the (25 -29) age group followed closely by (22–24) age group. Thus we can say that most of the women lie between the 20 to 30 age bracket. \n* The (20–30) age group can comprise of both students(undergraduate and postgraduate) and professionals. \n* Interestingly, females greater than 70 years of age have also responded. Well, as it is said - Age is only a number !!.\n\n## 3.2 Age distribution pattern over the years\nWas the age distribution pattern same for the last two years too? let's analyze and find out.   "
"> ** 📌 Points to note :**\n* No notable change in the pattern in 2018 was observed.\n* in 2017 however, there were more responses from the (22–24) age group as compared to other age brackets.\n* Overall, the 20–30 group dominates.\n\n ## 3.3 Country wise Age distribution\nWe know that the young generation dominates the current Data Science landscape. Let's also see how are they distributed geographically."
# 4. Education\n![](https://cdn-images-1.medium.com/max/800/1*JEEn6yzozl_ye9-oEUIMxQ.png)\n*PC: www.freepik.com*\n    \nIt has been rightly said that educated females form the backbone of society. Let's see the qualification status of the female respondents in 2019.\n## 4.1 Educational qualifications of the female respondents in 2019
"> **📌 Points to note :**\n* The education status of the females is impressive with the majority (~46%)having a Master's degree followed closely by a Bachelor's degree(27%). There are also 16% PhDs who answered the survey. \n* The analysis also reveals that there is a certain proportion who have had no formal education past high school. In spite of this, they took the survey which in itself is a commendable thing. They should be encouraged to complete their education either full time or through part-time courses.\n\n## 4.2 Educational qualifications of the female respondents, country wise"
"> **📌 Points to note :**\n* The education status of the females is impressive with the majority (~46%)having a Master's degree followed closely by a Bachelor's degree(27%). There are also 16% PhDs who answered the survey. \n* The analysis also reveals that there is a certain proportion who have had no formal education past high school. In spite of this, they took the survey which in itself is a commendable thing. They should be encouraged to complete their education either full time or through part-time courses.\n\n## 4.2 Educational qualifications of the female respondents, country wise"
"> **📌 Points to note :**\n* The U.S has maximum number of women with a Master's and Doctoral degrees followed closely by India. However, It should be kept in mind that a lot of women in India and other countries, generally move to the U.S for their Masters and PhDs.\n* India also tops the list with the maximum number of Bachelor degrees. This is pretty obvious since the majority of women respondents are students in their 20s.\n* There is a general predominance of Master's over other degrees, among all the countries except for Japan which has a higher incidence of a Professional degree."
# 5. Professional Experience\n![](https://cdn-images-1.medium.com/max/800/1*Ncg-xZhA-ZOM9InqStVm6w.jpeg)\n*PC: www.freepik.com *\n    \nWe already know that most of the female respondents comprising mainly of a young population having mostly students. Let's now see the various professional roles that females occupy in the industry.\n\n\n## 5.1 Female respondents' roles over the years
"> **📌 Points to note :**\n* Data Scientist seems to be the principal role for the female respondents since 2017 followed by a Data Analyst. Other roles like Developers, researchers, and project managers can also be seen in the population.\n\n## 5.2 Top 20 roles for female respondents in 2019"
"> **📌 Points to note :**\n* Data Scientist seems to be the principal role for the female respondents since 2017 followed by a Data Analyst. Other roles like Developers, researchers, and project managers can also be seen in the population.\n\n## 5.2 Top 20 roles for female respondents in 2019"
"> **📌 Points to note :**\n* If we exclude students from the result, Data Scientists(~19.5%) form the chunk of the population who took the survey. This is closely followed by women in the Data Analyst role(~11%). \n* Interestingly, there are also women who are not employed but have responded to the survey. These women could not be working by choice or may be looking for jobs. We could connect to these women to understand if they are willing to work and could assist them in the same.\n\n## 5.3 Female respondents' Current Roles country wise in 2019\nLet's combine some of the roles to create broader groups. For instance, Data Engineer and DBA/Database Engineer can be clubbed together. Similarly, Data Analyst and Business Analyst can also be merged. Let's then see the distribution geographically.    "
"> **📌 Points to note :**\n* If we exclude students from the result, Data Scientists(~19.5%) form the chunk of the population who took the survey. This is closely followed by women in the Data Analyst role(~11%). \n* Interestingly, there are also women who are not employed but have responded to the survey. These women could not be working by choice or may be looking for jobs. We could connect to these women to understand if they are willing to work and could assist them in the same.\n\n## 5.3 Female respondents' Current Roles country wise in 2019\nLet's combine some of the roles to create broader groups. For instance, Data Engineer and DBA/Database Engineer can be clubbed together. Similarly, Data Analyst and Business Analyst can also be merged. Let's then see the distribution geographically.    "
"> **📌 Points to note :**\n* Again, leaving out the students, U.S has the maximum number of Data Scientists who took part in the survey, followed by India.\n* The U.S also has the maximum number of Data Analysts participating in the survey, again followed by India.\n* India has the number of Software Engineers participating in the survey.\n* The percentage of unemployed females respondents(~<2%) is also high in India.\n\n## 5.4 Percentage of Female Data Scientists over the years "
"# 6. Salary\n![](https://cdn-images-1.medium.com/max/800/1*8qpREVGhO0noT07tqE0C2g.png)\n*PC: www.freepik.com*\n    \nEven though some [researches](http://www.timothy-judge.com/Judge,%20Piccolo,%20Podsakoff,%20et%20al.%20(JVB%202010).pdf) say otherwise, salary is a big motivational factor in retaining and acquiring new talent. Let's see how well our ladies are paid in the Data Science space.\n\n## 6.1 Salary Range of Female respondents in 2019\nWe shall first analyze the general trend of the salary of the female respondents in 2019. For the responses, which were blank I have included them in the 'Didnot Disclose' category."
"> **📌 Points to note :**\n* The majority of female respondents did not wish to disclose their annual salary. Of the remaining, (~10%) have an annual salary of fewer than 1000 dollars. This makes sense since a major chunk of that population is students who may currently not be having permanent jobs. \n* The next common salary range is (1k–2k) dollars and (10k-15k) dollars. It appears that the salary range of females in 2019 is highly varied.\n* There is also a tiny percentage of females who make more than 200k and 300k dollars a year.\n\n## 6.2 Comparison of Salaries of Female respondents in 2018 and 2019\n\nLet's see if this pattern of salaries is exclusive to 2019 only or is it a recurring phenomenon. Let's compare it with the 2018 Salary range. For this, we shall do some preprocessing of the data so that we get a common salary range for both the years. \n*Note that I did not include the 2017 salary data since it had more than 10 different currencies.*"
"> **📌 Points to note :**\n* The majority of female respondents did not wish to disclose their annual salary. Of the remaining, (~10%) have an annual salary of fewer than 1000 dollars. This makes sense since a major chunk of that population is students who may currently not be having permanent jobs. \n* The next common salary range is (1k–2k) dollars and (10k-15k) dollars. It appears that the salary range of females in 2019 is highly varied.\n* There is also a tiny percentage of females who make more than 200k and 300k dollars a year.\n\n## 6.2 Comparison of Salaries of Female respondents in 2018 and 2019\n\nLet's see if this pattern of salaries is exclusive to 2019 only or is it a recurring phenomenon. Let's compare it with the 2018 Salary range. For this, we shall do some preprocessing of the data so that we get a common salary range for both the years. \n*Note that I did not include the 2017 salary data since it had more than 10 different currencies.*"
"> **📌 Points to note :**\n* The general pattern amongst the salary distribution appears to be the same in 2018 and 2019.\n* The percentage of females earning less than 10k USD in 2019 is less as compared to last year. \n* For all the other salary ranges, annual compensation in 2019 is marginally better than it was in 2018, which is good.\n* Another important point is that unlike in 2018, 2019 does have some females who earn more than 500k USD.\n\n## 6.3 Comparison of Male and Female salaries in 2019.\nWe saw things were slightly better in 2019 as compared to 2018. Let's now see compare the salaries genderwise.\n"
## 6.5 A look at Salaries of Female Data Scientists worldwide
"> **📌 Points to note :**\n* There are definitely salary differences between the U.S and other countries. Data Scientists in U.S are paid relatively higher than in other countries.\n* The majority of Female Data Scientists in the U.S earn between 100–200k USD, while a lot of Data Scientists in India earn less than 1000 dollars a year. Indian Data Scientists are highly underpaid as compared to other countries.\n* The proportion of Data Scientists earning more than 100k annually is also higher than in her countries."
"Below is our code for wdecay, the code is pretty intuitive to understand and does exactly what we described above. This will be a wrapper around our optimizer."
Now we create our optimizer with simple grouped param intialization and optimizer params as defined above.
"Stochastic Weight Averaging\n\nIntroduction\n\nSnapshot ensembling is a technique where we take weights snapshot while training the same network and then after training create an ensemble of nets with the same architecture but different weights. This allows to improve test performance, and it is a very cheap way too because you just train one model once, just saving weights from time to time.\n\nIn SWA (Stochastic Weight Averaging) the authors propose to use a novel ensembling in the weights space. This method produces an ensemble by combining weights of the same network at different stages of training and then uses this model with combined weights to make predictions. There are 2 benefits from this approach:\n - when combining weights, we still get one model at the end, which speeds up predictions\n - it can be applied to any architecture and data set and shows good result in all of them.\n \nIdea\n![swa](https://miro.medium.com/max/1766/1*_USiR_z8PKaDuIcAs9xomw.png)\n\nIntuition for SWA comes from empirical observation that local minima at the end of each learning rate cycle tend to accumulate at the border of areas on loss surface where loss value is low (points W1, W2 and W3 are at the border of the red area of low loss in the left panel of figure above). \nBy taking the average of several such points, it is possible to achieve a wide, generalizable solution with even lower loss (Wswa in the left panel of the figure above).\n\nHere is how it works. Instead of an ensemble of many models, you only need two models:\n - the first model that stores the running average of model weights (w_swa in the formula). This will be the final model after the end of the training which will be used for predictions.\n - the second model (w in the formula) that will be traversing the weight space, exploring it by using a cyclical learning rate schedule.\n \n![swa2](https://miro.medium.com/max/502/1*Afu2bqxzC6p1BpIRTDWJtg.png)\n \nAt the end of each learning rate cycle, the current weights of the second model will be used to update the weight of the running average model by taking weighted mean between the old running average weights and the new set of weights from the second model (formula provided in the figure on the left). \n By following this approach, you only need to train one model, and store only two models in memory during training. For prediction, you only need the running average model and predicting on it is much faster than using ensemble described above, where you use many models to predict and then average results."
"Ending Notes\n\n- There are many more stable training strategies which I haven't covererd and which one do further research on,\n    - Early Stopping\n    - Training Iterations: Longer Fine-Tuning\n    - Transferring via an Intermediate Task - STILTs Training\n    - Weight initialization and data order\n    - Mixed Precision Training\n    \n- I will be sharing a FineTuning kernel with all of the above idea and results soon.\n\n- More comprehensive repository for learning and implementing Transformers for various tasks can be found [here](https://notebooks.quantumstat.com/), [here](https://huggingface.co/transformers/master/community.html#community-notebooks) and [here](https://huggingface.co/transformers/notebooks.html)  \n\n- I want to acknowledge once more that this kernel has code implementations from the potpourri of best papers out there on Stable and Robust Transformer Fine-Tuning Strategies.\n\n - [REVISITING FEW-SAMPLE BERT FINE-TUNING](https://arxiv.org/pdf/2006.05987.pdf)\n - [ON THE STABILITY OF FINE-TUNING BERT](https://arxiv.org/pdf/2006.04884.pdf)\n - [SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models](https://arxiv.org/pdf/1911.03437.pdf)\n - [Fine-Tuning Pretrained Language Models:Weight Initializations, Data Orders, and Early Stopping](https://arxiv.org/pdf/2002.06305.pdf)\n - [MIXOUT: EFFECTIVE REGULARIZATION TO FINETUNE LARGE-SCALE PRETRAINED LANGUAGE MODELS](https://arxiv.org/pdf/1909.11299.pdf)\n - [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/pdf/1905.05583.pdf)\n - [Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks](https://arxiv.org/pdf/1811.01088.pdf)\n \nThanks & Please Do Upvote!"
# AMEX EDA which makes sense ⭐️⭐️⭐️⭐️⭐️\n\nThis EDA analyzes the data and gives some insight which is useful for designing a machine learning pipeline and selecting a model.
"# The labels\n\nWe start by reading the labels for the training data. There are neither missing values nor duplicated customer_IDs. Of the 458913 customer_IDs, 340000 (74 %) have a label of 0 (good customer, no default) and 119000 (26 %) have a label of 1 (bad customer, default).\n\nWe know that the good customers have been subsampled by a factor of 20; this means that in reality there are 6.8 million good customers. 98 % of the customers are good; 2 % are bad.\n\n**Insight:**\n- The classes are imbalanced. A StratifiedKFold for cross-validation is recommended.\n- Because the classes are imbalanced, accuracy would be a bad metric to evaluate a classifier. The [competition metric](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327464) is a mix of area under the roc curve (auc) and recall."
Now we can count how many rows (credit card statements) there are per customer. We see that 80 % of the customers have 13 statements; the other 20 % of the customers have between 1 and 12 statements.\n\n**Insight:** Our model will have to deal with a variable-sized input per customer (unless we simplify our life and look only at the most recent statement as @inversion suggests [here](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327094) or at the average over all statements).
"Let's find out when these customers got their last statement. The histogram of the last statement dates shows that every train customer got his last statement in March of 2018. The first four Saturdays (March 3, 10, 17, 24) have more statements than an average day.\n\nThe test customers are split in two: half of them got their last statement in April of 2019 and half in October of 2019. As was [discussed here](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327602), the April 2019 data is used for the public leaderboard and the October 2019 data is used for the private leaderboard."
"Let's find out when these customers got their last statement. The histogram of the last statement dates shows that every train customer got his last statement in March of 2018. The first four Saturdays (March 3, 10, 17, 24) have more statements than an average day.\n\nThe test customers are split in two: half of them got their last statement in April of 2019 and half in October of 2019. As was [discussed here](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327602), the April 2019 data is used for the public leaderboard and the October 2019 data is used for the private leaderboard."
"**Insight:** Although the data are a kind of time series, we cannot cross-validate with a TimeSeriesSplit because all training happens in the same month.\n\nFor most customers, the first and last statement is about a year apart. Together with the fact that we typically have 13 statements per customer, this indicates that the customers get one credit card statement every month."
"**Insight:** Although the data are a kind of time series, we cannot cross-validate with a TimeSeriesSplit because all training happens in the same month.\n\nFor most customers, the first and last statement is about a year apart. Together with the fact that we typically have 13 statements per customer, this indicates that the customers get one credit card statement every month."
"If we color every statement (i.e. row of train or test) according to the dataset it belongs (training, public lb, and private lb), we see that every dataset covers thirteen months. Train and test don't overlap, but public and private lb periods overlap."
"If we color every statement (i.e. row of train or test) according to the dataset it belongs (training, public lb, and private lb), we see that every dataset covers thirteen months. Train and test don't overlap, but public and private lb periods overlap."
"Now we'll look at the distribution of missing values over time. B_29 is most interesting. Given the each of the three datasets has almost half a million customers, we see that until May of 2019 fewer than a tenth of the customers have a value for B_29. The other nine tenths are missing. Starting in June of 2019, we have B_29 data for almost every customer. \n\n**Insight:** The distribution of the missing B_29 differs between train and test datasets. Whereas in the training and public leaderboard data >90 % are missing, during the last five months of private leaderboard, we have B_29 data for almost every customer. If we use this feature, we should be prepared for surprises in the private leaderboard. Is it better to drop the feature?"
"Now we'll look at the distribution of missing values over time. B_29 is most interesting. Given the each of the three datasets has almost half a million customers, we see that until May of 2019 fewer than a tenth of the customers have a value for B_29. The other nine tenths are missing. Starting in June of 2019, we have B_29 data for almost every customer. \n\n**Insight:** The distribution of the missing B_29 differs between train and test datasets. Whereas in the training and public leaderboard data >90 % are missing, during the last five months of private leaderboard, we have B_29 data for almost every customer. If we use this feature, we should be prepared for surprises in the private leaderboard. Is it better to drop the feature?"
"# The categorical features\n\nAccording to the [data description](https://www.kaggle.com/competitions/amex-default-prediction/data), there are eleven categorical features. We plot histograms for target=0 and target=1. For the ten features which have missing values, the missing values are represented by the rightmost bar of the histogram.\n"
"# The categorical features\n\nAccording to the [data description](https://www.kaggle.com/competitions/amex-default-prediction/data), there are eleven categorical features. We plot histograms for target=0 and target=1. For the ten features which have missing values, the missing values are represented by the rightmost bar of the histogram.\n"
**Insight:**\n- Every feature has at most eight categories (including a nan category). One-hot encodings are feasible.\n- The distributions for target=0 and target=1 differ. This means that every feature gives some information about the target.\n
# The binary features\n\nTwo features are binary:\n- B_31 is always 0 or 1.\n- D_87 is always 1 or missing.
"**Insight:** If you impute missing values for D_87, don't fall into the trap of imputing the mean - the feature would become useless..."
"# The numerical features\n\nIf we plot histograms of the 175 numerical features, we see that they have all kinds of distributions:"
"**Insight:** Histograms with white space at the left or right end can indicate that the data contain outliers. We will have to deal with these outliers. But are these data really outliers? Maybe they are, but they could as well be legitimate traces of rare events. We do not know...\n"
"Let's look at B_19. All values are between 0 and 1.01. To get a high-resolution histogram, we spread it over eleven diagrams. The histogram show only rectangles of width 0.01, which indicate that the values are uniformly distributed in intervals of width 0.01, but every interval has another probability. This means that B_19 originally had some other range, but was scaled, rounded and got added some uniform noise by applying the following function:\n\n```\ndef anonymize(data):\n    data -= data.min()\n    data /= data.max()\n    data = data.round(2)\n    rng = np.random.default_rng()\n    data += rng.uniform(0, 0.01, len(data))\n    return data\n```\n"
"**Insight:** We don't care about the scaling, we cannot do anything against the rounding, but we should remove the artificial noise, e.g. by applying a function such as `x['B_19'] = x['B_19'].apply(lambda t: np.floor(t*100))`"
 \n# 4. Exploratory Data Analysis
`Potential` tends to fall as you grow old
As the `age` increases the `sprint speed` decreases
`Left Footed Players` vs `Right Footed Players`
`Left Footed Players` vs `Right Footed Players`
`Crossing` vs `Dribbling`
`Crossing` vs `Dribbling`
Relation between `potential` and `age` with respected `value` of players
Relation between `potential` and `age` with respected `value` of players
"**Lowest correlation** among the goalkeeping side with other columns and high among themselves \n\n**High correlation** between `Dribbling`, `Volleys`, `Passing` etc..."
"**Lowest correlation** among the goalkeeping side with other columns and high among themselves \n\n**High correlation** between `Dribbling`, `Volleys`, `Passing` etc..."
"We will have comparisions for `Age`, `Overall`, `Potential`, `Accelaration`, `SprintSpeed`, `Agility` , `Stamina`, `Strength`"
Standard imports for data science work. The LightGBM library is used for the gradient boosting machine.
"* `train_bureau` is the training features built manually using the `bureau` and `bureau_balance` data\n* `train_previous` is the training features built manually using the `previous`, `cash`, `credit`, and `installments` data\n\nWe first will see how many features we built over the manual engineering process. Here we use a couple of set operations to find the columns that are only in the `bureau`, only in the `previous`, and in both dataframes, indicating that there are `original` features from the `application` dataframe. Here we are working with a small subset of the data in order to not overwhelm the kernel. This code has also been run on the full dataset (we will take a look at some of the results)."
"With the background details out of the way, let's get started with Bayesian optimization applied to automated hyperparameter tuning! "
The code below reads in the data and creates a smaller version for training and a set for testing. We can only use the training data __a single time__ when we evaluate the final model. Hyperparameter tuning must be done on the training data using cross validation!
We can visualize the learning rate by drawing 10000 samples from the distribution.
The number of leaves on the other hand is a discrete uniform distribution.
The number of leaves on the other hand is a discrete uniform distribution.
"### Conditional Domain\n\nIn Hyperopt, we can use nested conditional statements to indicate hyperparameters that depend on other hyperparameters. For example, the ""goss"" `boosting_type` cannot use subsampling, so when we set up the `boosting_type` categorical variable, we have to set the subsample to 1.0 while for the other boosting types it's a float between 0.5 and 1.0."
"Below is the code showing the progress of scores versus the iteration. For random search we do not expect to see a pattern, but for Bayesian optimization, we expect to see the scores increasing with the search as more promising hyperparameter values are tried."
"Sure enough, we see that the Bayesian hyperparameter optimization scores increase as the search continues. This shows that more promising values (at least on the cross validation reduced dataset) were tried as the search progressed. Random search does record a better score, but the results do not improve over the course of the search. In this case, it looks like if we were to continue searching with Bayesian optimization, we would eventually reach higher scores on the cross vadidation data. \n\nFor fun, we can make the same plot in Altair."
"Sure enough, we see that the Bayesian hyperparameter optimization scores increase as the search continues. This shows that more promising values (at least on the cross validation reduced dataset) were tried as the search progressed. Random search does record a better score, but the results do not improve over the course of the search. In this case, it looks like if we were to continue searching with Bayesian optimization, we would eventually reach higher scores on the cross vadidation data. \n\nFor fun, we can make the same plot in Altair."
"Same chart, just in a different library for practice! \n\n## Learning Rate Distribution\n\nNext we can start plotting the distributions of hyperparameter values searched. We expect random search to align with the search domain, while the Bayesian hyperparameter optimization should tend to focus on more promising values, wherever those happen to be in the search domain.\n\nThe dashed vertical lines indicate the ""optimal"" value of the hyperparameter."
"Same chart, just in a different library for practice! \n\n## Learning Rate Distribution\n\nNext we can start plotting the distributions of hyperparameter values searched. We expect random search to align with the search domain, while the Bayesian hyperparameter optimization should tend to focus on more promising values, wherever those happen to be in the search domain.\n\nThe dashed vertical lines indicate the ""optimal"" value of the hyperparameter."
"## Distribution of all Numeric Hyperparameters\n\nWe can make the same chart now for all of the hyperparameters. For each setting, we plot the values tried by random search and bayesian optimization, as well as the sampling distirbution."
"## Distribution of all Numeric Hyperparameters\n\nWe can make the same chart now for all of the hyperparameters. For each setting, we plot the values tried by random search and bayesian optimization, as well as the sampling distirbution."
"## Evolution of Search\n\nAn interesting series of plots to make is the evolution of the hyperparameters over the search. This can show us what values the Bayesian optimization tended to focus on. The average cross validation score continued to improve throughout Bayesian optimization, indicating that ""more promising"" values of the hyperparameters were being evaluated and maybe a longer search would prove useful (or there could be a plateau in the validation scores with a longer search)."
The final plot is just a bar chart of the `boosting_type`. 
"The Bayes optimization spent many more iterations using the `dart` boosting type than would be expected from a uniform distribution. We can use information such as this in further hyperparameter tuning. For example, we could use the distributions from Bayesian hyperparameter optimization to make a more focused hyperparameter grid for grid or even random search. \n\n![](http://)For this chart, we can also make it in Altair for the practice."
"### %matplotlib notebook\n\nThe  `%matplotlib inline`  function is used to render the static matplotlib plots within the Jupyter notebook. Try replacing the `inline`  part with  `notebook` to get zoom-able & resize-able plots, easily. Make sure the function is called before importing the matplotlib library."
-   **%matplotlib notebook** vs **%matplotlib inline**\n\n![](https://cdn-images-1.medium.com/max/800/1*IAtw6rydG7o58yy2EyzCRA.png)
Now lets take a look at how the housing price is distributed
"With this information we can see that the prices are skewed right and some outliers lies above ~500,000. We will eventually want to get rid of the them to get a normal distribution of the independent variable (`SalePrice`) for machine learning."
Now lets plot them all:
"Features such as `1stFlrSF`, `TotalBsmtSF`, `LotFrontage`, `GrLiveArea`... seems to share a similar distribution to the one we have with `SalePrice`. Lets see if we can find new clues later."
"Perfect, we now have a list of strongly correlated values but this list is incomplete as we know that correlation is affected by outliers. So we could proceed as follow:\n\n- Plot the numerical features and see which ones have very few or explainable outliers\n- Remove the outliers from these features and see which one can have a good correlation without their outliers\n    \nBtw, correlation by itself does not always explain the relationship between data so ploting them could even lead us to new insights and in the same manner, check that our correlated values have a linear relationship to the `SalePrice`. \n\nFor example, relationships such as curvilinear relationship cannot be guessed just by looking at the correlation value so lets take the features we excluded from our correlation table and plot them to see if they show some kind of pattern."
"We can clearly identify some relationships. Most of them seems to have a linear relationship with the `SalePrice` and if we look closely at the data we can see that a lot of data points are located on `x = 0` which may indicate the absence of such feature in the house.\n\nTake `OpenPorchSF`, I doubt that all houses have a porch (mine doesn't for instance but I don't lose hope that one day... yeah one day...)."
Trying to plot all the numerical features in a seaborn pairplot will take us too much time and will be hard to interpret. We can try to see if some variables are linked between each other and then explain their relation with common sense.
"A lot of features seems to be correlated between each other but some of them such as `YearBuild`/`GarageYrBlt` may just indicate a price inflation over the years. As for `1stFlrSF`/`TotalBsmtSF`, it is normal that the more the 1st floor is large (considering many houses have only 1 floor), the more the total basement will be large.\n\nNow for the ones which are less obvious we can see that:\n- There is a strong negative correlation between `BsmtUnfSF` (Unfinished square feet of basement area) and `BsmtFinSF2` (Type 2 finished square feet). There is a definition of unfinished square feet [here](http://www.homeadvisor.com/r/calculating-square-footage/) but as for a house of ""Type 2"", I can't tell what it really is.\n- `HalfBath`/`2ndFlrSF` is interesting and may indicate that people gives an importance of not having to rush downstairs in case of urgently having to go to the bathroom (I'll consider that when I'll buy myself a house uh...)\n\nThere is of course a lot more to discover but I can't really explain the rest of the features except the most obvious ones."
Let's look at their distribution.
"We can see that features such as `TotalBsmtSF`, `1stFlrSF`, `GrLivArea` have a big spread but I cannot tell what insights this information gives us"
And finally lets look at their distribution
"We can see that some categories are predominant for some features such as `Utilities`, `Heating`, `GarageCond`, `Functional`... These features may not be relevant for our predictive model"
"The best way of\nconfirming that the data contains enough information so that a ML algorithm \ncan make strong predictions, is to try and directly visualize the \ndifferences between fraudulent and genuine transactions. Motivated by this\nprinciple, I visualize these differences in several ways in the plots below."
\n##### 5. 1. Dispersion over time
"The 3D plot below distinguishes best between fraud and non-fraud data\nby using both of the engineered error-based features. Clearly, the\noriginal *step* feature is ineffective in seperating out fraud. Note\nthe striped nature of the genuine data vs time which was aniticipated\nfrom the figure in section 5.1."
back to top
Smoking gun and comprehensive evidence embedded in the dataset of the \ndifference between fraudulent\nand genuine transactions is obtained by examining their respective\ncorrelations in the heatmaps below.
back to top
\n##### 6.1. What are the important features for the ML model?\nThe figure below shows that the new feature *errorBalanceOrig* that we created is the most relevant feature for the model. The features are ordered based on the number of samples affected by splits on those features.
back to top
"As you can see, there is a great motivation to calculate the PDF difference between target = 0 and target = 1 distributions. They are clearly different, so it would make sense to say that, if pdf(target = 1) - pdf(target = 0) > 0, then there is a high probability of the client making a transfer.\n\n#### **2.2. CREATING OUR PDF FUNCTION**\n\nHere is the strategy used in this kernel:\n- Calculate the PDF for each feature;\n- Aggregate each feature values within bins;\n- Use the PDF's difference between target = 0 and target = 1 for each bin as a new feature;\n\nNotice that we could create this feature differently, by, for example, getting the target probability.\n\nBelow we will see the difference between the previously seen PDF, where the graph is very smooth, and the plot of the PDF for each bin we created."
"### **3. UNDERSTANDING OVERFITTING**\n\nThe difference of smoothness on the graph above is caused by the IQR_multiplier and the bin_bandwidth_multiplier parameters.\n\n**IQR_multiplier**\nIQR stands for Inter-Quartile Range and is used to define the number of bins for our distribution. This parameter is proportional to the bin size. A bigger IQR_multiplier will yield a bigger bin size and, therefore, less bins per distribution.\n\n**bin_bandwidth_multiplier**\nThe bandwidth value is used to smooth the graph\n\nLet's see how the PDFs behaves as we play with those parameters"
"### **3. UNDERSTANDING OVERFITTING**\n\nThe difference of smoothness on the graph above is caused by the IQR_multiplier and the bin_bandwidth_multiplier parameters.\n\n**IQR_multiplier**\nIQR stands for Inter-Quartile Range and is used to define the number of bins for our distribution. This parameter is proportional to the bin size. A bigger IQR_multiplier will yield a bigger bin size and, therefore, less bins per distribution.\n\n**bin_bandwidth_multiplier**\nThe bandwidth value is used to smooth the graph\n\nLet's see how the PDFs behaves as we play with those parameters"
"When you describe your data with a high number of bins and a low smoothing parameter, you get really ""noisy"" PDF, because your pool of candidates for each bin is too small and specific. This is the opposite of what we want for a robust solution.\n\nWhen we use this model to predict the target on the test dataset, it will overfit, not because the datasets are way too different, but because you didn't give the model any chance to adapt for small changes.\n\nTo prove that to you, let's check the test dataset distribution\n\n#### **3.1. THE TEST DATASET**\n\nHere we are going to plot the target = 1 and target = 0 for the train and test dataset in the same graph.\n\nTo do that, we will get the submission with score 0.901 shared [here](https://www.kaggle.com/darbin/clustering-blender-of-0-901-solutions) and say that the 20098 highest probabilities (same number of target = 1 in the train_df) are equivalent to target = 1 in the test_df. Everything else will be set as target = 0."
"First, let's plot the smoothed PDFs to see if there is any significant difference"
"There is a small, but still significant difference in the distribution of target = 1. Now, let's see how big is this difference when we don't smooth the PDF."
"There is a small, but still significant difference in the distribution of target = 1. Now, let's see how big is this difference when we don't smooth the PDF."
"As you can see, every point representing a bin has a different PDF for train_df and test_df. Expecting that their distributions will match is equivalent of what is happening in the image below:\n\nYou can find [here](https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42) the article about over/underfitting from where this image was taken.\n![overfitting](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle---Why-your-model-is-overfitting/master/1_SBUK2QEfCP-zvJmKm14wGQ.png) \n\nI hope that these graphs can make it clear why your model overfits when you are too specific about a single variable, instead of a group of variables.\n\nA good question here would be: What happens if I use this feature with smoothed values? Well, the smoothier it is, the lesser the impact on your score, and here is why."
"**This kernel has been a work of more than 10 days If you find my kernel useful and my efforts appreciable, Please Upvote it , it motivates me to write more Quality content**"
# Configuring TPU's\n\nFor this version of Notebook we will be using TPU's as we have to built a BERT Model
## Importing Library's
## Some utilities functions 
# Target Feature\n- Let's see the distribution and if we can identify what is the nature of this feature
"Cool;  \nWe can see that our target is a binary feature and as it is 0 or 1 we can't know what is about.\nAnother interesting thing to note is that isn't so imbalanced:\n- Category 0 with 79.4% \n- Category 1 with 30.6\n\nNow, as we have much of them, let's explore the patterns of other binary features"
## Distribution of ord_5 features
"Cool!\nWe can see that many values in ord_5 have ~2100 entries; \nAlso, the data has many category's with ~100 entries; "
# Creating pipeline to evaluate different models
Nice! We can see that Xgb is the best model. Let's use it with hyperopt
# XGB - HyperOpt Optimization
## HyperOpt Run
# Correlation Between Signal and Open Channels\nLet's look closely at random intervals of signal and open channels to observe how they relate. We notice that they are highly correlated and move up and down together. Therefore we can probabily predict open channels from the one feature signal. The only complication is the synthetic drift that was added. So we will remove it.
# Test Data\nLet's display the test data signal
# Test Data\nLet's display the test data signal
"## Reflection\nFrom this plot we can locate the 5 models in action. And we can recognize the added drift. Batch 1 appears to be 5 subsamples where A, B, C, D, E were created by models 1s, 3, 5, 1s, 1f respectively. Model 1s is the model with maximum 1 channel open with low prob. Model 1f is the model with maximum 1 channel open with high prob. And models 3, 5, 10 are models with maximum 3, 5, 10 channels respectively. We observe slant drift in subsamples A, B, E, G, H, I. We observe parabola draft in batch 3. "
"## Training Data Drift\nWe observe drift whereever the following plot is not a horizontal line. We see drift in batches 2, 7, 8, 9, 10."
"## Test Data Drift\nWe observe drift in test subsamples A, B, E, G, H, I and test batch 3.\n"
"## Test Data Drift\nWe observe drift in test subsamples A, B, E, G, H, I and test batch 3.\n"
# Remove Test Data Drift
"## Refactor Release 2017-Jan-29\n\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\n\n### User comments\n\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\n\n### Porting issues\n\n- Specify plot dimensions, bring legend into plot.\n\n\n### Best practices\n\n- Performing feature correlation analysis early in the project.\n- Using multiple plots instead of overlays for readability."
## Acquire data\n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.
## Analyze by visualizing data\n\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.\n\n### Correlating numerical features\n\nLet us start by understanding correlations between numerical features and our solution goal (Survived).\n\nA histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\n\nNote that x-axis in historgram visualizations represents the count of samples or passengers.\n\n**Observations.**\n\n- Infants (Age <=4) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n- Most passengers are in 15-35 age range.\n\n**Decisions.**\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n- We should consider Age (our assumption classifying #2) in our model training.\n- Complete the Age feature for null values (completing #1).\n- We should band age groups (creating #3).
"### Correlating numerical and ordinal features\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\n**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training."
"### Correlating numerical and ordinal features\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\n**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training."
"### Correlating categorical features\n\nNow we can correlate categorical features with our solution goal.\n\n**Observations.**\n\n- Female passengers had much better survival rate than males. Confirms classifying (#1).\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions.**\n\n- Add Sex feature to model training.\n- Complete and add Embarked feature to model training."
"### Correlating categorical features\n\nNow we can correlate categorical features with our solution goal.\n\n**Observations.**\n\n- Female passengers had much better survival rate than males. Confirms classifying (#1).\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions.**\n\n- Add Sex feature to model training.\n- Complete and add Embarked feature to model training."
"### Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\n**Observations.**\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n- Consider banding Fare feature."
"### Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\n**Observations.**\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n- Consider banding Fare feature."
"## Wrangle data\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\n\n### Correcting by dropping features\n\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent."
"### Completing a numerical continuous feature\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).\n\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2."
Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.
## Gender Split
"The graph clearly shows that there are a lot more male respondents as compared to female. It seems that Ladies were either busy with their coding, **or ladies don't code**...:p. Just Kidding."
## Respondents By Country
"**USA and India**, constitute maximum respondents, about 1/3 of the total. Similarly Chile has the lowest number of respondents. Is this graph sufficient enough to say that majority of Kaggle Users are from India and USA. I don't think so, as the total users on Kaggle are more than 1 million while the number of respondents are only 16k."
Look at that humungous Salary!! Thats **even larger than GDP of many countries**. Another example of bogus response. The minimum salary maybe a case of a student. The median salary shows that Data Scientist enjoy good salary benefits.
### Compensation by Country
### Compensation by Country
"The left graph shows the Top 15 high median salary paying countries. It is good to see that these countries provide salary more than the median salary of the complete dataset. Similarly,the right graph shows median salary of the Top 15 Countries by respondents. The most shocking graph is for **India**. India has the 2nd highest respondents, but still it has the lowest median salary in the graph. Individuals in USA have a salary almost 10% more than their counterparts in India. What may be the reason?? Are IT professionals in India really underpaid?? We will check that later."
### Salary By Gender
The salary for males look to be high as compared to others.
## Age
The respondents are young people with majority of them being in the age bracket if 25-35.
## Profession & Major
"Data Science and Machine Learning is used in almost every industry. This is evident from the left graph,as people from different areas of interest like Physics, Biology, etc are taking it up for better understanding of the data. The right side graph shows the Current Job of the respondents. A major portion of the respondents are Dats Scientists. But as it is survey data, we know that there may be many ambigious responses. Later on we will check are these respondents real datas-scientists or self proclaimed data-scientists."
## Compensation By Job Title
Operations Research Practitioner has the highest median salary followed by Predictive Modeler and Data Scientist. Computer Scientist and Programmers have the lowest compensation.
## Machine Learning
"It is evident that most of the respondents are working with Supervised Learning, and Logistic Regression being the favorite among them."
"It is evident that most of the respondents are working with Supervised Learning, and Logistic Regression being the favorite among them."
"It is evident that the next year is going to see a jump in number of **Deep Learning** practitioners. Deep Learning and neural nets or in short AI is a favorite hot-topic for the next Year. Also in terms of Tools, Python is preferred more over R. Big Data Tools like Spark and Hadoop also have a good share in the coming years."
## Best Platforms to Learn
"My personal Kaggle, is the most sought after source for learning Data Science."
## Hardware Used
"Since majority of the respondents fall in the age category below 25, which is where a majority of students fall under, thus a basic Laptop is the most commonly used machine for work."
## Where Do I get Datasets From??
"With hundreds of Dataset available, Kaggle is the most sought after source for datasets."
## Code Sharing
## Challenges in Data Science
## Challenges in Data Science
"The main challenge in Data Science is **getting the proper Data**. The graph clearly shows that dirty data is the bigget challenge. Now what is dirty data?? Dirty data is a database record that contains errors. Dirty data can be caused by a number of factors including duplicate records, incomplete or outdated data, and the improper parsing of record fields from disparate systems. Luckily Kaggle datasets are pretty clean and standardised.\n\nSome other major challenges are the **Lack of Data Science and machine learning talent, difficulty in getting data and lack of tools**. Thats why Data Science is the sexiest job in 21st century.With the increasing amount of data, this demand will substantially grow."
## Job Satisfaction
"Data Scientists and Machine Learning engineers are the most satisfied people(who won't be happy with so much money), while Programmers have the lowest job satisfaction.\n\n## Job Satisfication By Country"
### Recommended Language For Begineers
Clearly Python is the recommended language for begineers. The reason for this maybe due to its simple english-like syntax and general purpose functionality.
### Necessary or Not??
Clearly Python is a much more necessary skill compared to R.\n\nSpecial Thanks to [Steve Broll](https://www.kaggle.com/stevebroll) for helping in the color scheme.
### Number Of Users By Language
"The number of Python users are definetely more than R users. This may be due to the easy learning curve of Python. However there are more users who know both the languages. These responses might be from established Data Scientists,as they tend to have a knowledge in multiple languages and tools."
"Python coders have a slightly higher median salary as that compared to their R counterparts. However, the people who know both these languages, have a pretty high median salary as compared to both of them.\n\n## Language Used By Professionals"
"As I had mentioned earlier, R beats Python in visuals. Thus people with Job-Titles like Data Analyst, Business Analyst where graphs and visuals play a very prominent role, prefer R over Python. Similarly almost 90% of statisticians use R. Also as stated earlier, Python is better in Machine Learning stuff, thus Machine Learning engineers, Data Scientists and others like DBA or Programmers prefer Python over R. \n\nThus for data visuals--->R else---->Python.\n\n**Note: This graph is not for Language Recommended by professionals, but the tools used by the professionals.**"
## Job Function vs Language
"As I had already mentioned ** R excels in analytics, but Python beats in Machine Learning.** The graph shows that R has influence when it comes to pure analytics, but other ways python wins."
## Tenure vs Language Used
"As we had seen earlier, Python is highly recommended for beginners. Thus the proportion of Python users is more in the initial years of coding. The gap between the languages however reduces over the years, as the coding experience increases."
## Common Tools with Python and R
**SQL** seems to be the most common complementory tool used with both the languages.
So about 26% of the total respondents consider themselves as Data Scientist. What does Sort of mean?? Are they still learning or are they unemployed. For now lets consider them as a No.\n\n## Current Job Titles
"Surprisingly there is **no entry for the Job Title Data Scientist**. There reasons for this could be that the people with CurrentJobTitleSelect as Data Scientist(who might be working as Data Scientist) might have not answered the question: **""Do you currently consider yourself a Data Scientist?""**\n\nThere are many overlapping and common skills between the jobs like Data Analyst,Data Scientist and Machine Learning experts, Statisticians,etc. Thus they too have similar skills and consider themselves as Data Scientists even though they are not labeled the same. Now lets check if the previous assumption was True."
"So out of the total respondents, about **40%** of them are Data Scientists or have skills for the same.\n\n## Country-Wise Split"
The graph is similar to the demographic graph where we had shown number of users by country. The difference is that the numbers have reduced as we have only considered Data Scientists.\n\n## Employment Status
The graph is similar to the demographic graph where we had shown number of users by country. The difference is that the numbers have reduced as we have only considered Data Scientists.\n\n## Employment Status
"About **67%** of the data scientists are employed full-time, while about **11-12%** of them are unemployed but looking for job.\n\n## Previous Job and Salary Change"
"About **67%** of the data scientists are employed full-time, while about **11-12%** of them are unemployed but looking for job.\n\n## Previous Job and Salary Change"
Clearly majority of people switching to Data Science get a salary hike about **6-20% or more**.\n\n## Tools used at Work
Clearly majority of people switching to Data Science get a salary hike about **6-20% or more**.\n\n## Tools used at Work
"Similar observations, Python, R and SQL are the most used tools or languages in Data Science\n\n"
"The average Job Satisfaction level is between **6-7.5** for most of the countries. It is lower in Japan(where people work for about 14 hours) and China. It is higher in come countries like Sweden and Mexico.\n\n## Time Spent on Tasks\n\nA Data Scientist is not always building predictive models, he is also responsible for the data quality, gathering the right data, analytics,etc. Lets see how much time a data scientist spends on these differnt tasks."
"Lets do it stepwise:\n\n  - **TimeGatheringData:** It is undoubtedly the most time consuming part. Getting the data is the most painstaking task in the entire process, which is followed by Data Cleaning(not shown as data not available) which is yet other time consuming process. Thus gathering right data and scrubing the data are the most time consuming process.\n  \n  - **TimeVisualizing:** It is probably the least time consuming process(and probably the most enjoyable one..:p), and it reduces even further if we use Enterprise Tools like Tableau,Qlik,Tibco,etc, which helps in building graphs and dashboards with simple drag and drop features.\n  \n  - **TimeFindingInsights:** It is followed after visualising the data, which involves finding facts and patterns in the data, slicing and dicing it to find insights for business processes.It looks to a bit more time consuming as compared to TimeVisualizing.\n  \n  - **TimeModelBuilding:** It is where the data scientists build predictive models, tune these models,etc. It is the 2nd most time consuming process after TimeDataGathering.\n\n## Importance Of Visualisations"
"Lets do it stepwise:\n\n  - **TimeGatheringData:** It is undoubtedly the most time consuming part. Getting the data is the most painstaking task in the entire process, which is followed by Data Cleaning(not shown as data not available) which is yet other time consuming process. Thus gathering right data and scrubing the data are the most time consuming process.\n  \n  - **TimeVisualizing:** It is probably the least time consuming process(and probably the most enjoyable one..:p), and it reduces even further if we use Enterprise Tools like Tableau,Qlik,Tibco,etc, which helps in building graphs and dashboards with simple drag and drop features.\n  \n  - **TimeFindingInsights:** It is followed after visualising the data, which involves finding facts and patterns in the data, slicing and dicing it to find insights for business processes.It looks to a bit more time consuming as compared to TimeVisualizing.\n  \n  - **TimeModelBuilding:** It is where the data scientists build predictive models, tune these models,etc. It is the 2nd most time consuming process after TimeDataGathering.\n\n## Importance Of Visualisations"
"Visualisations are a very integral part of Data Science Projects, and the above graph also shows the same. Almost all data science projects i.e **99%** of the projects have visualisations in them, doesn't matter how big or small. About **95%** of Data Scientists say that Visualisations skills are nice to have or necessary.Visuals help to understand and comprehend the data faster not only to the professionals but also to target customers, who may not be technically skilled.\n\n## Knowledge Of Algorithms (Maths and Stats)"
"Visualisations are a very integral part of Data Science Projects, and the above graph also shows the same. Almost all data science projects i.e **99%** of the projects have visualisations in them, doesn't matter how big or small. About **95%** of Data Scientists say that Visualisations skills are nice to have or necessary.Visuals help to understand and comprehend the data faster not only to the professionals but also to target customers, who may not be technically skilled.\n\n## Knowledge Of Algorithms (Maths and Stats)"
"Data Scientists have a good knowledge of mathematical concepts like Statistics and Linear Algebra, which are the most important part of Machine Learning algorithms. But is this maths really required, as many standard libraries like scikit,tensorflow,keras etc have all these things already implemented. But the experienced data scientists say that we should have a good understanding of the maths behind the algorithms. About **95%** of the data scientists say the stats is an important asset in Data Science.\n\n## Learning Platform Usefullness"
"Data Scientists have a good knowledge of mathematical concepts like Statistics and Linear Algebra, which are the most important part of Machine Learning algorithms. But is this maths really required, as many standard libraries like scikit,tensorflow,keras etc have all these things already implemented. But the experienced data scientists say that we should have a good understanding of the maths behind the algorithms. About **95%** of the data scientists say the stats is an important asset in Data Science.\n\n## Learning Platform Usefullness"
"The above donut charts shows the opinion of Data Scientists about the various platforms to learn Data Science. The plot looks best for **Projects**,where the percentage for not useful is **0%**.According to my personal opinion too, projects are the best platform or way for learning anything in the IT industry. The other excellent platforms are **Online Courses and Kaggle**. The graphs for other platforms are quite similar to each other.\n\n## What should the Resume have??"
"The above donut charts shows the opinion of Data Scientists about the various platforms to learn Data Science. The plot looks best for **Projects**,where the percentage for not useful is **0%**.According to my personal opinion too, projects are the best platform or way for learning anything in the IT industry. The other excellent platforms are **Online Courses and Kaggle**. The graphs for other platforms are quite similar to each other.\n\n## What should the Resume have??"
"It is evident that Work experience in ML projects and Kaggle competitions reflects the knowledge of Data Science. Also a kaggle rank can be a good thing in one's resume.\n\n# Conclusions\n\nSome brief insights that we gathered from the notebook:\n\n1) Majority of the respondents are from USA followed by India. USA also had the maximum number of data scientists followed by India. Also the median Salary is highest in USA.\n\n2) Majority of the respondents are in the age bracket 20-35, which shows that data science is quite famous in the youngsters.\n\n3) The respondents are not just limited to Computer Science major, but also from majors like Statistics, health sciences,etc showing that Data Science is an interdisciplinary domain.\n\n4) Majority of the respondents are fully employed.\n\n5) Kaggle, Online Courses(Coursera,eDx,etc), Projects and Blogs(KDNuggets,AnalyticsVidya,etc) are the top resources/platforms for learning Data Science.\n\n6) Kaggle has the highest share for data acquisition whereas Github has the highest share for code sharing.\n\n7) Data Scientists have the highest Job Satisfaction level and the second highest median salary (after Operations Research Analyst). On the contrary, Programmers have the least Job Satisfaction level and one of the least median salary also.\n\n8) Data Scientists also get a hike of about 6-20% from their previous jobs.\n\n#### Tips For Budding Data Scientists\n\n1) Learn **Python,R and SQL** as they are the most used languages by the Data Scientists. Python and R will help in analytics and predictive modeling while SQL is best for querying the databases.\n\n2) Learn Machine Learning Techniques like **Logistic Regression, Decision Trees, Support Vector Machines**, etc as they are most commonly used Machine Learning techniques/algorithms.\n\n3) **Deep Learning and Neural Nets** will be the most sought after techniques in the future, thus a good knowledge in them will be very helpful.\n\n4) Develop skills for **Gathering Data** and **Cleaning The Data** as they are the most time consuming processes in the workflow of a data scientist. \n\n5) **Visualisations** are very important in Data Science projects and almost all projects require Visualisations for understanding the data better. So one should learn Data Visualisation as Data Scientists consider it to be a **necessary or nice to have skill.**\n\n6) **Maths and Stats** are very important in Data Science, so we should have good understanding of it for actually understanding how the algorithm works.\n\n7) **Projects** are the best way to learn Data Science according to Data Scientists.So working on projects will help you learn data science better.\n\n8) **Experience with ML Projects in company and Kaggle Competitions** are the best ways to show your working knowledge in Data Science. Working on ML projects in a company gives the experience of working with real world datasets, thereby enhancing the knowledge. Kaggle competitions are also a great medium, as you will be competing with Data Scientists over the world. Also a **Kaggle Rank** can be a good USP in the resume.\n"
### Basic EDA
### Brief Analysis of the data
### Brief Analysis of the data
### Analysis Of Diabetic Cases
### Analysis Of Diabetic Cases
### PairPlots:\n\nLets us see the distribution of the features in the dataset
### PairPlots:\n\nLets us see the distribution of the features in the dataset
"### Observations:\n\n1)The diagonal shows the distribution of the the dataset with the kernel density plots.\n\n2)The scatter-plots shows the relation between each and every attribute or features taken pairwise.\nLooking at the scatter-plots, we can say that no two attributes are able to clearly seperate the two outcome-class instances."
### K-Nearest Neighbours
### In a Nutshell
### Correlation Matrix
### Observations:\n\n1)All the features look to be uncorrelated. So we cannot eliminate any features just by looking at the correlation matrix.
**1. Age and Sex:**
"You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\n\nFor men the probability of survival is very low between the age of 5 and 18, but that isn't true for women. Another thing to note is that infants also have a little bit higher probability of survival.\n\nSince there seem to be **certain ages, which have increased odds of survival** and because I want every feature to be roughly on the same scale, I will create age groups later on."
"**3. Embarked, Pclass  and Sex:**"
"Embarked seems to be correlated with survival, depending on the gender. \n\nWomen on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S. \n\nPclass also seems to be correlated with survival. We will generate another plot of it below."
**4. Pclass:**
"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below."
"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below."
"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive."
"Above you can clearly see that the recall is falling of rapidly at a precision of around 85%.  Because of that you may want to select the precision/recall tradeoff before that - maybe at around 75 %.\n\nYou are now able to choose a threshold, that gives you the best precision/recall tradeoff for your current machine learning problem. If you want for example a precision of 80%, you can easily look at the plots and see that you would need a threshold of around 0.4.  Then you could train a model with exactly that threshold and would get the desired accuracy.\n\n\nAnother way is to plot the precision and recall against each other:"
"## ROC AUC Curve\n\nAnother way to evaluate and compare your binary classifier is provided by the ROC AUC Curve. This curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall."
"Amazing, but let' breakdown on what we can see from this plot. First, we can see that our plot consists of 3 subplots - that is the power of using catplot; with such output, we can easily proceed with comparing distributions among interesting attributes. Y and X axes stay exactly the same for each subplot, Y-axis represents a count of observations and X-axis observations we want to count. However, there are 2 more important elements: column and hue; those 2 differentiate subplots. After we specify the column and determined hue we are able to observe and compare our Y and X axes among specified column as well as color-coded. So, what do we learn from this? The observation that is definitely contrasted the most is that 'Shared room' type Airbnb listing is barely available among 10 most listing-populated neighborhoods. Then, we can see that for these 10 neighborhoods only 2 boroughs are represented: Manhattan and Brooklyn; that was somewhat expected as Manhattan and Brooklyn are one of the most traveled destinations, therefore would have the most listing availability. We can also observe that Bedford-Stuyvesant and Williamsburg are the most popular for Manhattan borough, and Harlem for Brooklyn."
"Good, scatterplot worked just fine to output our latitude and longitude points. However, it would be nice to have a map bellow for fully immersive heatmap in ourcase - let's see what we can do!"
"Good, scatterplot worked just fine to output our latitude and longitude points. However, it would be nice to have a map bellow for fully immersive heatmap in ourcase - let's see what we can do!"
"Fantastic! After scaling our image the best we can, we observe that we end up with a very immersive heatmap. Using latitude and longitude points were able to visualize all NYC listings. Also, we added a color-coded range for each point on the map based on the price of the listing. However, it is important to note that we had to drop some extremely high values as they are treated as outliers for our analysis. "
### Load Libraries
"## GPU use \nSince this is a deep learning model, The use of GPU will accelerate the training. \nThe first models are not so demanding so you can still use CPU training (but it will be slower)."
"## Plot The distribution \nPlot the distribution before and after the Normalization. \nAs you can see, we kept the distribution of the data, but we change its scales."
# Create Sequances \nIn this part we allign the data into input features and labels with techniches whic adapt for Time series processing \nThis is out Time Series \n\n![TS2.JPG](attachment:TS2.JPG)\n\nOr in a more schematic ilustriation \n\n![TS1.JPG](attachment:TS1.JPG)
"## Plot Lags\nLet's Plot our lags \nit is a bit hard to see the small lags (as the Time Series containing few years), but for the longer lags \nsuch as 365, we can see the shift ..."
"## Rolling windows \nFor rolling windows, we will use mean and std (standard deviation)"
Let's Compare one example again to verify that the normalization was done properly
## Multi-Dimensional Sliding Window
## Training \nSome enhancement  we save the best model (based on the lowest validation loss)
## Predict
"# DATA SCIENTIST\n**In this tutorial, I only explain you what you need to be a data scientist neither more nor less.**\n\nData scientist need to have these skills:\n\n1. Basic Tools: Like python, R or SQL. You do not need to know everything. What you only need is to learn how to use **python**\n1. Basic Statistics: Like mean, median or standart deviation. If you know basic statistics, you can use **python** easily. \n1. Data Munging: Working with messy and difficult data. Like a inconsistent date and string formatting. As you guess, **python** helps us.\n1. Data Visualization: Title is actually explanatory. We will visualize the data with **python** like matplot and seaborn libraries.\n1. Machine Learning: You do not need to understand math behind the machine learning technique. You only need is understanding basics of machine learning and learning how to implement it while using **python**.\n\n### As a summary we will learn python to be data scientist !!!\n\n**Content:**\n1. [Introduction to Python:](#1)\n    1. [Matplotlib](#2)\n    1. [Dictionaries ](#3)\n    1. [Pandas](#4)\n    1. [Logic, control flow and filtering](#5)\n    1. [Loop data structures](#6)\n1. [Python Data Science Toolbox:](#7)\n    1. [User defined function](#8)\n    1. [Scope](#9)\n    1. [Nested function](#10)\n    1. [Default and flexible arguments](#11)\n    1. [Lambda function](#12)\n    1. [Anonymous function](#13)\n    1. [Iterators](#14)\n    1. [List comprehension](#15)\n1. [Cleaning Data](#16)\n    1. [Diagnose data for cleaning](#17)\n    1. [Exploratory data analysis](#18)\n    1. [Visual exploratory data analysis](#19)\n    1. [Tidy data](#20)\n    1. [Pivoting data](#21)\n    1. [Concatenating data](#22)\n    1. [Data types](#23)\n    1. [Missing data and testing with assert](#24)\n1. [Pandas Foundation](#25)\n    1. [Review of pandas](#26)\n    1. [Building data frames from scratch](#27)\n    1. [Visual exploratory data analysis](#28)\n    1. [Statistical explatory data analysis](#29)\n    1. [Indexing pandas time series](#30)\n    1. [Resampling pandas time series](#31)\n1. [Manipulating Data Frames with Pandas](#32)\n    1. [Indexing data frames](#33)\n    1. [Slicing data frames](#34)\n    1. [Filtering data frames](#35)\n    1. [Transforming data frames](#36)\n    1. [Index objects and labeled data](#37)\n    1. [Hierarchical indexing](#38)\n    1. [Pivoting data frames](#39)\n    1. [Stacking and unstacking data frames](#40)\n    1. [Melting data frames](#41)\n    1. [Categoricals and groupby](#42)\n1. Data Visualization\n    1. Seaborn: https://www.kaggle.com/kanncaa1/seaborn-for-beginners\n    1. Bokeh 1: https://www.kaggle.com/kanncaa1/interactive-bokeh-tutorial-part-1\n    1. Rare Visualization: https://www.kaggle.com/kanncaa1/rare-visualization-tools\n    1. Plotly: https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners\n1. Machine Learning\n    1. https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners/\n1. Deep Learning\n    1. https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n1. Time Series Prediction\n    1. https://www.kaggle.com/kanncaa1/time-series-prediction-tutorial-with-eda\n1. Statistic\n    1. https://www.kaggle.com/kanncaa1/basic-statistic-tutorial-for-beginners\n1. Deep Learning with Pytorch\n    1. Artificial Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n    1. Convolutional Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n    1. Recurrent Neural Network: https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch"
"**WARNING - UYARI**\n* If you run the code above, if it outputs like in the picture, you need to put the "".csv"" path in pd.read_csv () (as in the picture).\n* Yukarıdaki kod bloğunu run edince sonuç ne veriyorsa read_csv içerisine onu yazmanız gerekli.\n* Mesela, eğer yukarı bulunan kodu çalıştırdığınızda, resimdeki gibi bir output veriyorsa pd.read_csv() içerisine resimdeki "".csv"" yolunu koymanız gerekli (resimde olduğu gibi). Yukarıda kod bloğunu run edince ne çıkıyorsa onu yazmanız lazım mesela aşağıdaki gibi.\n\n* read_csv içerisine yukarıda çıkan .csv dosyaları yazılmalı."
" \n### VISUAL EXPLORATORY DATA ANALYSIS\n* Box plots: visualize basic statistics like outliers, min/max or quantiles"
 \n### TIDY DATA\nWe tidy data with melt().\nDescribing melt is confusing. Therefore lets make example to understand it.\n
# 2. Data Understanding\n\n## 2.1 Import Libraries\nFirst of some preparation. We need to import python libraries containing the necessary functionality we will need. \n\n*Simply run the cell below by selecting it and pressing the play button.*
## 2.2 Setup helper Functions\nThere is no need to understand this code. Just run it to simplify the code later in the tutorial.\n\n*Simply run the cell below by selecting it and pressing the play button.*
## 2.2 Setup helper Functions\nThere is no need to understand this code. Just run it to simplify the code later in the tutorial.\n\n*Simply run the cell below by selecting it and pressing the play button.*
"## 2.3 Load data\nNow that our packages are loaded, let's read in and take a peek at the data.\n\n*Select the cell below and run it by pressing the play button.*"
### 3.3.1 Extract titles from passenger names\nTitles reflect social status and may predict survival probability\n\n*Select the cell below and run it by pressing the play button.*
### 3.3.2 Extract Cabin category information from the Cabin number\n\n*Select the cell below and run it by pressing the play button.*
### 3.3.3 Extract ticket class from ticket number\n\n*Select the cell below and run it by pressing the play button.*
### 3.3.4 Create family size and category for family size\nThe two variables *Parch* and *SibSp* are used to create the famiy size variable\n\n*Select the cell below and run it by pressing the play button.*
### 5.2.1 Automagic\nIt's also possible to automatically select the optimal number of features and visualize this. This is uncommented and can be tried in the competition part of the tutorial.\n\n*Select the cell below and run it by pressing the play button.*
## 5.3 Competition time!\nIt's now time for you to get your hands even dirtier and go at it all by yourself in a `challenge`! \n\n1. Try to the other models in step 4.1 and compare their result\n    * Do this by uncommenting the code and running the cell you want to try\n2. Try adding new features in step 3.4.1\n    * Do this by adding them in to the function in the feature section.\n\n\n**The winner is the one to get the highest scoring model for the validation set**
Here is the code to create the Partial Dependence Plot using the scikit-learn library.
"The y axis is interpreted as **change in the prediction** from what it would be predicted at the baseline or leftmost value.\n\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning ""Man of The Match.""  But extra goals beyond that appear to have little impact on predictions.\n\nHere is another example plot:"
"The y axis is interpreted as **change in the prediction** from what it would be predicted at the baseline or leftmost value.\n\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning ""Man of The Match.""  But extra goals beyond that appear to have little impact on predictions.\n\nHere is another example plot:"
This graph seems too simple to represent reality. But that's because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model's structure.\n\nYou can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model.
This graph seems too simple to represent reality. But that's because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model's structure.\n\nYou can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model.
"This model thinks you are more likely to win *Man of the Match* if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\n\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model.  Though this dataset is small enough that we would be careful in how we interpret any model.\n\n# 2D Partial Dependence Plots\nIf you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify this.  \n\nWe will again use the Decision Tree model for this graph.  It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself."
"This model thinks you are more likely to win *Man of the Match* if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\n\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model.  Though this dataset is small enough that we would be careful in how we interpret any model.\n\n# 2D Partial Dependence Plots\nIf you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify this.  \n\nWe will again use the Decision Tree model for this graph.  It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself."
"This graph shows predictions for any combination of Goals Scored and Distance covered. \n\nFor example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km.  If they score 0 goals, distance covered doesn't matter. Can you see this by tracing through the decision tree with 0 goals?\n\nBut distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too?"
"# Introduction\nThe sinking of Titanic is one of the most notorious shipwrecks in the history. In 1912, during her voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.\n\n\nContent: \n\n1. [Load and Check Data](#1)\n1. [Variable Description](#2)\n    * [Univariate Variable Analysis](#3)\n        * [Categorical Variable](#4)\n        * [Numerical Variable](#5)\n1. [Basic Data Analysis](#6)\n1. [Outlier Detection](#7)\n1. [Missing Value](#8)\n    * [Find Missing Value](#9)\n    * [Fill Missing Value](#10)\n1. [Visualization](#11)\n    * [Correlation Between Sibsp -- Parch -- Age -- Fare -- Survived](#12)\n    * [SibSp -- Survived](#13)\n    * [Parch -- Survived](#14)\n    * [Pclass -- Survived](#15)\n    * [Age -- Survived](#16)\n    * [Pclass -- Survived -- Age](#17)\n    * [Embarked -- Sex -- Pclass -- Survived](#18)\n    * [Embarked -- Sex -- Fare -- Survived](#19)\n    * [Fill Missing: Age Feature](#20)\n1. [Feature Engineering](#21)\n    * [Name -- Title](#22)\n    * [Family Size](#23)\n    * [Embarked](#24)\n    * [Ticket](#25)\n    * [Pclass](#26)\n    * [Sex](#27)\n    * [Drop Passenger ID and Cabin](#28)\n1. [Modeling](#29)\n    * [Train - Test Split](#30)\n    * [Simple Logistic Regression](#31)\n    * [Hyperparameter Tuning -- Grid Search -- Cross Validation](#32) \n    * [Ensemble Modeling](#33)\n    * [Prediction and Submission](#34)"
\n# Load and Check Data
\n## Correlation Between Sibsp -- Parch -- Age -- Fare -- Survived
Fare feature seems to have correlation with survived feature (0.26).
\n## SibSp -- Survived
"* Having a lot of SibSp have less chance to survive.\n* if sibsp == 0 or 1 or 2, passenger has more chance to survive\n* we can consider a new feature describing these categories."
\n## Parch -- Survived
* Sibsp and parch can be used for new feature extraction with th = 3\n* small familes have more chance to survive.\n* there is a std in survival of passenger with parch = 3
\n## Pclass -- Survived
\n## Age -- Survived
\n## Age -- Survived
"* age <= 10 has a high survival rate,\n* oldest passengers (80) survived,\n* large number of 20 years old did not survive,\n* most passengers are in 15-35 age range,\n* use age feature in training\n* use age distribution for missing value of age"
\n## Pclass -- Survived -- Age
* pclass is important feature for model training.
\n## Embarked -- Sex -- Pclass -- Survived
* Female passengers have much better survival rate than males.\n* males have better survşval rate in pclass 3 in C.\n* embarked and sex will be used in training.
\n## Embarked -- Sex -- Fare -- Survived
* Passsengers who pay higher fare have better survival. Fare can be used as categorical for training.
"Sex is not informative for age prediction, age distribution seems to be same."
"1st class passengers are older than 2nd, and 2nd is older than 3rd class. "
"### 1a. The Large Synoptic Survey Telescope\n\n\nPLAsTiCC is intended to simulate sources that vary with time in the night-sky as seen during the first three years of operation of the Large Synoptic Survey Telescope (LSST). \n\nThe LSST (illustrated below) is a telescope with an 8.4 meter primary mirror diameter being built high up in Atacama desert of Chile, on a mountain called Cerro Pachon.\n"
"#### _Figure 1: The LSST Telescope, Credit: LSST/NSF/AURA/Todd Mason Productions Inc_"
"Other variable stars have very different light curve shapes from eclipsing binaries. Indeed, variable stars were the first example of astronomers using light curve shapes for classification. \n\nHere's a hand-drawn illustration of some of the different known types of variable stars from Popular Science Monthly in 1906 (page 179)."
"#### _Figure 10: The Atlas of Variable Stars, Credit: Popular Science Monthly, from 1906_"
"In this notebook, we use a dataset we've shared on Kaggle Datasets: [Articles Sharing and Reading from CI&T Deskdrop](https://www.kaggle.com/gspmoreira/articles-sharing-reading-from-cit-deskdrop).  \nWe will demonstrate how to implement **Collaborative Filtering**, **Content-Based Filtering** and **Hybrid methods** in Python, for the task of providing personalized recommendations to the users."
# Loading data: CI&T Deskdrop dataset
Latent factor models compress user-item matrix into a low-dimensional representation in terms of latent factors. One advantage of using this approach is that instead of having a high dimensional matrix containing abundant number of missing values we will be dealing with a much smaller matrix in lower-dimensional space.  \nA reduced presentation could be utilized for either user-based or item-based neighborhood algorithms that are presented in the previous section. There are several advantages with this paradigm. It handles the sparsity of the original matrix better than memory based ones. Also comparing similarity on the resulting matrix is much more scalable especially in dealing with large sparse datasets.  
"An important decision is the number of factors to factor the user-item matrix. The higher the number of factors, the more precise is the factorization in the original matrix reconstructions. Therefore, if the model is allowed to  memorize too much details of the original matrix, it may not generalize well for data it was not trained on. Reducing the number of factors increases the model generalization."
## Title grouped
"It's interesting... Children's and ladys first, huh?"
## Let's cross our Pclass with the Age_cat \nWe will aggregate than to get the mean of Fare by each category pair\n
- Very interesting. We can see that babies has the highest mean value. 
### Looking the Fare distribuition to survivors and not survivors\n
\nDescription of Fare variable\n- Min: 0\n- Median: 14.45\n- Mean: 32.20\n- Max: 512.32 \n- Std: 49.69\n\nI will create a categorical variable to treat the Fare expend\nI will use the same technique used in Age but now I will use the quantiles to binning\n\n
Interesting. With 1 or 2 siblings/spouses have more chance to survived the disaster
We can see a high standard deviation in the survival with 3 parents/children person's \nAlso that small families (1~2) have more chance to survival than single or big families
Let's look this keys values further
Why this occurs and how to solve this problem in graph? it's a overffiting? 
You are more likly to survive if you are travels with 1 to 3 people and if you have 0 or more than three you have a less chance.
Shows the number of females and males who has number of siblings or spouse that is Parch.
"### Passengers dataset\n\nEvery field of knowledge has **the** dataset that is used for teaching purposes: machine learning has Iris and CIFAR, differential equations - Canadian lynx data, and statistics has the airline passengers dataset between 1949 and 1960, first compiled by Box and Jenkins (you will be hearing those two names again in the next module) in 1976. We will use this dataset to demonstrate in practice what kind of information can be obtained using seasonal decomposition.\n\n"
Let's start with a basic additive decomposition:
Let's start with a basic additive decomposition:
"Trend and seasonality are behaving more or less in line with expectations, but the behavior of the residuals is clearly not consistent over time (average level of oscillations in the middle of the sample is very different than on either end). While there are many possible reasons, one quick explanation is the additive vs multiplicative relationship between the series components - which is something we can examine quickly:"
"Trend and seasonality are behaving more or less in line with expectations, but the behavior of the residuals is clearly not consistent over time (average level of oscillations in the middle of the sample is very different than on either end). While there are many possible reasons, one quick explanation is the additive vs multiplicative relationship between the series components - which is something we can examine quickly:"
"Not much of a qualitative change in trend and seasonality components, but the residuals looks much more stable around a constant level - such phenomenon does not of course imply stationarity by itself, but at least a clear signal in the opposite direction is not there anymore. "
"For a slightly more interesting example (of non-stationary behavior), we can examine the passengers dataset:"
"The stationarity of a series can be checked by examining the distribution of the series: we split the series into 2 contiguous parts and compute the summary statistics like the mean, variance and the autocorrelation. If the stats are quite different, then the series is not likely to be stationary."
"The stationarity of a series can be checked by examining the distribution of the series: we split the series into 2 contiguous parts and compute the summary statistics like the mean, variance and the autocorrelation. If the stats are quite different, then the series is not likely to be stationary."
Compute the summary statistics:
Let's go back to our hammer-and-nail approach and try some transformations to make the series stationary.
"Applying a logarithm does not remove the trend, but it does seem to stabilize the amplitude (periodic variations have comparable magnitude now). How does that translate into ADF results?"
"The null is still not rejected, but p-value has dropped - which indicates the transformations are the right way to go. Next, we can try differentiating to get rid of the trend"
"As expected, differentiation removes the trend (oscillations happen around a fixed level), but variations amplitude is magnified."
"# Bank Marketing DataSet - Intelligent Targeting:\n***\n## Marketing Introduction:\n*The process by which companies create value for customers and build strong customer relationships in order to capture value from customers in return.*\n\n**Kotler and Armstrong (2010).**\n***\n\n**Marketing campaigns** are characterized by  focusing on the customer needs and their overall satisfaction. Nevertheless, there are different variables that determine whether a marketing campaign will be successful or not. There are certain variables that we need to take into consideration when making a marketing campaign. \n\n## The 4 Ps:\n1) Segment of the Population: To which segment of the population is the marketing campaign going to address and why? This aspect of the marketing campaign is extremely important since it will tell to which part of the population should most likely receive the message of the marketing campaign. \n2) Distribution channel to reach the customer's place: Implementing the most effective strategy in order to get the most out of this marketing campaign. What segment of the population should we address? Which instrument should we use to get our message out? (Ex: Telephones, Radio, TV, Social Media Etc.)\n3)  Price: What is the best price to offer to potential clients? (In the case of the bank's marketing campaign this is not necessary since the main interest for the bank is for potential clients to open depost accounts in order to make the operative activities of the bank to keep on running.)\n4)  Promotional Strategy: This is the way the strategy  is going to be implemented and how are potential clients going to be address. This should be the last part of the marketing campaign analysis since there has to be an indepth analysis of previous campaigns (If possible) in order to learn from previous mistakes and to determine how to make the marketing campaign much more effective."
# What is a Term Deposit? \nA **Term deposit** is a deposit that a bank or a financial institurion offers with a fixed rate (often better than just opening deposit account) in which your money will be returned back at a specific maturity time. For more information with regards to Term Deposits please click on this link from Investopedia:  https://www.investopedia.com/terms/t/termdeposit.asp
"# Outline: \n***\nA. **Attribute Descriptions**\nI. *[Bank client data](#bank_client_data)\nII. *[Related with the last contact of the current campaign](#last_contact)\nIII. [Other attributes](#other_attributes) \n\nB. **Structuring the data:** \nI. *[Overall Analysis of the Data](#overall_analysis)\nII. *[Data Structuring and Conversions](#data_structuring) \n\nC. **Exploratory Data Analysis (EDA)**\nI. *[Accepted vs Rejected Term Deposits](#accepted_rejected) \nII. *[Distribution Plots](#distribution_plots) \n\nD. **Different Aspects of the Analysis: **\nI. *[Months of Marketing Activty](#months_activity) \nII. *[Seasonalities](#seasonality) \nIII. *[Number of Calls to the potential client](#number_calls) \nIV. *[Age of the Potential Clients](#age_clients) \nV. [Types of Occupations that leads to more term deposits suscriptions](#occupations) \n\nE. **Correlations that impacted the decision of Potential Clients.**\nI. *[Analysis of our Correlation Matrix](#analysis_correlation) \nII. *[Balance Categories vs Housing Loans](#balance_housing)\nIII. [Negative Relationship between H.Loans and Term Deposits](#negative_relationship) \n\nF. ** Classification Model **\nI. [Introduction](#classification_model) \nII. [Stratified Sampling](#stratified)\nIII. [Classification Models](#models)\nIV. [Confusion Matrix](#confusion)\nV. [Precision and Recall Curve](#precision_recall)\nVI. [Feature Importances Decision Tree C.](#decision) \n\nG. ** Next Campaign Strategy**\nI. [Actions the Bank should Consider](#bank_actions)\n\n# A. Attributes Description: \n\nInput variables:\n# Ai. bank client data:\n\n1 - **age:** (numeric)\n2 - **job:** type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n3 - **marital:** marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n4 - **education:** (categorical: primary, secondary, tertiary and unknown)\n5 - **default:** has credit in default? (categorical: 'no','yes','unknown')\n6 - **housing:** has housing loan? (categorical: 'no','yes','unknown')\n7 - **loan:** has personal loan? (categorical: 'no','yes','unknown')\n8 - **balance:** Balance of the individual.\n# Aii. Related with the last contact of the current campaign:\n\n8 - **contact:** contact communication type (categorical: 'cellular','telephone') \n9 - **month:** last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n10 - **day:** last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n11 - **duration:** last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n# Aiii. other attributes:\n\n12 - **campaign:** number of contacts performed during this campaign and for this client (numeric, includes last contact)\n13 - **pdays:** number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n14 - **previous:** number of contacts performed before this campaign and for this client (numeric)\n15 - **poutcome:** outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\nOutput variable (desired target):\n21 - **y** - has the client subscribed a term deposit? (binary: 'yes','no')"
"\n Exploring the Basics \n\n## Summary:\n***\n\n Mean Age is aproximately 41 years old. (Minimum: 18 years old and Maximum: 95 years old.)\n The mean balance is 1,528. However, the Standard Deviation (std) is a high number so we can understand through this that the balance is heavily distributed across the dataset.\nAs the data information said it will be better to drop the duration column since duration is highly correlated in whether a potential client will buy a term deposit. Also, duration is obtained after the call is made to the potential client so if the target client has never received calls this feature is not that useful. The reason why duration is highly correlated with opening a term deposit  is because the more the bank talks to a target client the higher the probability the target client will open a term deposit since a higher duration means a higher interest (commitment) from the potential client. \n\n\n**Note: There are not that much insights we can gain from the descriptive dataset since most of our descriptive data is located not in the ""numeric"" columns but in the ""categorical columns"".**\n"
"# Which Features Influence the Result of a Term Deposit Suscription?\n## DecisionTreeClassifier:\n\nThe top three most important features for our classifier are **Duration (how long it took the conversation between the sales representative and the potential client), contact (number of contacts to the potential client within the same marketing campaign), month (the month of the year).\n\n\n"
## GradientBoosting Classifier Wins!\nGradient Boosting classifier is the best model to predict whether or not a **potential client** will suscribe to a term deposit or not.  84% accuracy!
"# CNN Model Intro with Fashion MNIST Implementation\n### Brief Introduction\nThis notebook will cover the following two major topics :\n\n#### Understand the basic concepts of CNN model\n#### Implement CNN model in realtime using Fashion MNIST dataset\n\n## Understand the basic concepts of CNN model :\n\nMankind is an awesome natural machine and is capable of looking at multiple images every second and process them without realizing how the processing is done. But same is not with machines. \n\nThe first step in image processing is to understand, how to represent an image so that the machine can read it?\n\nEvery image is an cumulative arrangement of dots (a pixel) arranged in a special order. If you change the order or color of a pixel, the image would change as well. \n\n![](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-07-at-9-15-21-pm.png)\nThree basic components to define a basic convolutional neural network.\n\n### The Convolutional Layer\n### The Pooling layer\n### The Output layer\n\nLet’s see each of them in detail\n\n### The Convolutional Layer :\n\nIn this layer if we have an image of size 6*6. We define a weight matrix which extracts certain features from the images*\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28010254/conv1.png)\nWe have initialized the weight as a 3*3 matrix. This weight shall now run across the image such that all the pixels are covered at least once, to give a convolved output. The value 429 above, is obtained by the adding the values obtained by element wise multiplication of the weight matrix and the highlighted 3*3 part of the input image.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28011851/conv.gif)\nThe 6*6 image is now converted into a 4*4 image.  Think of weight matrix like a paint brush painting a wall. The brush first paints the wall horizontally and then comes down and paints the next row horizontally. Pixel values are used again when the weight matrix moves along the image. This basically enables parameter sharing in a convolutional neural network.\n\nLet’s see how this looks like in a real image.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28132834/convimages.png)\n\n* The weight matrix behaves like a filter in an image, extracting particular information from the original image matrix. \n* A weight combination might be extracting edges, while another one might a particular color, while another one might just blur the unwanted noise.\n* The weights are learnt such that the loss function is minimized and extract features from the original image which help the network in correct prediction.\n* When we use multiple convolutional layers, the initial layer extract more generic features,and as network gets deeper the features get complex.\n\nLet us understand some concepts here before we go further deep\n\n#### What is Stride?\n\nAs shown above above, the filter or the weight matrix we moved across the entire image moving one pixel at a time.If this is a hyperparameter to move weight matrix 1 pixel at a time across image it is called as stride of 1. Let us see for stride of 2 how it looks.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28090227/stride1.gif)\n\nAs you can see the size of image keeps on reducing as we increase the stride value. \n\nPadding the input image with zeros across it solves this problem for us. We can also add more than one layer of zeros around the image in case of higher stride values.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28093553/zero-padding.png)\nWe can see how the initial shape of the image is retained after we padded the image with a zero. This is known as same padding since the output image has the same size as the input. \n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28094927/padding.gif)\nThis is known as same padding (which means that we considered only the valid pixels of the input image). The middle 4*4 pixels would be the same. Here we have retained more information from the borders and have also preserved the size of the image.\n\n#### Having Multiple filters & the Activation Map\n\n* The depth dimension of the weight would be same as the depth dimension of the input image.\n* The weight extends to the entire depth of the input image. \n* Convolution with a single weight matrix would result into a convolved output with a single depth dimension. In case of multiple filters all have same dimensions applied together.\n* The output from the each filter is stacked together forming the depth dimension of the convolved image. \n\nSuppose we have an input image of size 32*32*3. And we apply 10 filters of size 5*5*3 with valid padding. The output would have the dimensions as 28*28*10.\n\nYou can visualize it as –\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28113904/activation-map.png)\nThis activation map is the output of the convolution layer.\n\n### The Pooling Layer\n\nIf images are big in size, we would need to reduce the no.of trainable parameters.For this we need to use pooling layers between convolution layers. Pooling is used for reducing the spatial size of the image and is implemented independently on each depth dimension resulting in no change in image depth. Max pooling is the most popular form of pooling layer.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28022816/maxpool.png)\nHere we have taken stride as 2, while pooling size also as 2. The max operation is applied to each depth dimension of the convolved output. As you can see, the 4*4 convolved output has become 2*2 after the max pooling operation.\n\nLet’s see how max pooling looks on a real image.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28133544/pooling.png)\nIn the above image we have taken a convoluted image and applied max pooling on it which resulted in still retaining the image information that is a car but if we closely observe the dimensions of the image is reduced to half which basically means we can reduce the parameters to a great number.\n\nThere are other forms of pooling like average pooling, L2 norm pooling.\n\n#### Output dimensions\n\nIt is tricky at times to understand the input and output dimensions at the end of each convolution layer. For this we will use three hyperparameters that would control the size of output volume.\n\n1. No of Filter: The depth of the output volume will be equal to the number of filter applied.The depth of the activation map will be equal to the number of filters.\n\n2. Stride – When we have a stride of one we move across and down a single pixel. With higher stride values, we move large number of pixels at a time and hence produce smaller output volumes.\n\n3. Zero padding – This helps us to preserve the size of the input image. If a single zero padding is added, a single stride filter movement would retain the size of the original image.\n\nWe can apply a simple formula to calculate the output dimensions.\n\nThe spatial size of the output image can be calculated as( [W-F+2P]/S)+1. \nwhere, W is the input volume size, \n       F is the size of the filter, \n       P is the number of padding applied \n       S is the number of strides. \n       \nLet us take an example of an input image of size 64*64*3, we apply 10 filters of size 3*3*3, with single stride and no zero padding.\n\nHere W=64, F=3, P=0 and S=1. The output depth will be equal to the number of filters applied i.e. 10.\n\nThe size of the output volume will be ([64-3+0]/1)+1 = 62. Therefore the output volume will be 62*62*10.\n\n### The Output layer\n* With no of layers of convolution and padding, we need the output in the form of a class.\n* To generate the final output we need to apply a fully connected layer to generate an output equal to the number of classes we need.\n* Convolution layers generate 3D activation maps while we just need the output as whether or not an image belongs to a particular class. \n* The Output layer has a loss function like categorical cross-entropy, to compute the error in prediction. Once the forward pass is complete the backpropagation begins to update the weight and biases for error and loss reduction.\n\n### Summary:\n* Pass an input image to the first convolutional layer. The convoluted output is obtained as an activation map. The filters applied in the convolution layer extract relevant features from the input image to pass further.\n* Each filter shall give a different feature to aid the correct class prediction. In case we need to retain the size of the image, we use same padding(zero padding), otherwise valid padding is used since it helps to reduce the number of features.\n* Pooling layers are then added to further reduce the number of parameters\n* Several convolution and pooling layers are added before the prediction is made. Convolutional layer help in extracting features. As we go deeper in the network more specific features are extracted as compared to a shallow network where the features extracted are more generic.\n* The output layer in a CNN as mentioned previously is a fully connected layer, where the input from the other layers is flattened and sent so as the transform the output into the number of classes as desired by the network.\n* The output is then generated through the output layer and is compared to the output layer for error generation. A loss function is defined in the fully connected output layer to compute the mean square loss. The gradient of error is then calculated.\n* The error is then backpropagated to update the filter(weights) and bias values.\n* One training cycle is completed in a single forward and backward pass.\n\n### Implement CNN model in realtime using Fashion MNIST dataset\n\n![](https://pyimagesearch.com/wp-content/uploads/2019/02/fashion_mnist_dataset_sample.png)\nFashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\nThe original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try.  ""If it doesn't work on MNIST, it won't work at all"", they said. ""Well, if it does work on MNIST, it may still fail on others.""\n\nZalando seeks to replace the original MNIST dataset\n\n### Data Description\n\n* Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\n* Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. \n* The training and test data sets have 785 columns. \n* The first column consists of the class labels (see above), and represents the article of clothing. \n* The rest of the columns contain the pixel-values of the associated image.\n\nTo locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\nFor example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below. \n\n### Get the Data\n\nYou can use direct links to download the dataset.\n\n| Name  | Content | Examples | Size | Link | MD5 Checksum|\n| --- | --- |--- | --- |--- |--- |\n| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|\n\nAlternatively, you can clone this GitHub repository; the dataset appears under `data/fashion`. This repo also contains some scripts for benchmark and visualization.\n   \n```bash\ngit clone git@github.com:zalandoresearch/fashion-mnist.git\n```\n\n#### Labels\nEach training and test example is assigned to one of the following labels:\n\n* 0 T-shirt/top \n* 1 Trouser\n* 2 Pullover\n* 3 Dress\n* 4 Coat\n* 5 Sandal\n* 6 Shirt\n* 7 Sneaker\n* 8 Bag\n* 9 Ankle boot \n\n#### TL;DR\n\n* Each row is a separate image\n* Column 1 is the class label.\n* Remaining columns are pixel numbers (784 total).\n* Each value is the darkness of the pixel (1 to 255)\n\n### Acknowledgements\n\n* Original dataset was downloaded from https://github.com/zalandoresearch/fashion-mnist\n\n* Dataset was converted to CSV with this script: https://pjreddie.com/projects/mnist-in-csv/"
**Create dataframes for train and test datasets**
Now let us visualise the some samples after the resize of the data which needs to be ready for train the network .\n
Labels\nEach training and test example is assigned to one of the following labels as shown below:\n\n* 0 T-shirt/top\n* 1 Trouser\n* 2 Pullover\n* 3 Dress\n* 4 Coat\n* 5 Sandal\n* 6 Shirt\n* 7 Sneaker\n* 8 Bag\n* 9 Ankle boot\n\nI think the best way is to visualise the above 10 types of classes to get a feel of what these items look like :) .So let us visualise\n
Labels\nEach training and test example is assigned to one of the following labels as shown below:\n\n* 0 T-shirt/top\n* 1 Trouser\n* 2 Pullover\n* 3 Dress\n* 4 Coat\n* 5 Sandal\n* 6 Shirt\n* 7 Sneaker\n* 8 Bag\n* 9 Ankle boot\n\nI think the best way is to visualise the above 10 types of classes to get a feel of what these items look like :) .So let us visualise\n
As you can observe above the shape of shoe from the sample image\n\n### Create the Convolutional Neural Networks (CNN)\n\n#### Define model\n\n#### Compile model\n\n#### Train model\n\nFirst of all let us define the shape of the image before we define the model. Defined the shape of the image as 3d with rows and columns and 1 for the 3d visualisation\n
Let us plot the Training Accuracy vs Loss to get a better understanding of the model training.
- #### Evaluate /Score the model
\nLet's plot training and validation accuracy as well as loss.
### Classification Report\nWe can summarize the performance of our classifier as follows
"It's apparent that our classifier is underperforming for class 6 in terms of both precision and recall. For class 2, classifier is slightly lacking precision whereas it is slightly lacking recall (i.e. missed) for class 4.\n\nPerhaps we would gain more insight after visualizing the correct and incorrect predictions.\n\nLet us examine the test label and check if it the right classification or not."
### I hope you had a good understanding of CNN Model and its usage in practice using Fashion MNIST dataset.\n\n# Please do share your comments/suggestions and if you like this  kernel appreciate to UPVOTE.\n\nI recently created few useful kernels like below which might be of great interest to you in your data science work .Do visit them and share your thoughts/comments/upvote.
"![image.png](attachment:image.png)\n\n# Jane Street Market Prediction: A simple EDA\n\n> ""*Machine learning (ML) at Jane Street begins, unsurprisingly, with data. We collect and store around 2.3TB of market data every day. Hidden in those petabytes of data are the relationships and statistical regularities which inform the models inside our strategies. But it’s not just awesome models. ML work in a production environment like Jane Street’s involves many interconnected pieces.*"" -- [Jane Street Tech Blog ""*Real world machine learning*""](https://blog.janestreet.com/real-world-machine-learning-part-1/).\n\nThis notebook is a simple exploratory data analysis (EDA) of the files provided for the kaggle [Jane Street Market Prediction](https://www.kaggle.com/c/jane-street-market-prediction) competition. Here we shall...\n\n> ""**Explore the data:** *It’s hard to know what techniques to throw at a problem before we understand what the data looks like, and indeed figure out what data to use. Spending the time to visualize and understand the structure of the problem helps pick the right modeling tools for the job. Plus, pretty plots are catnip to traders and researchers!*""\n\n## Contents\n* [The train.csv file is big](#train_csv)\n* [resp](#resp)\n* [weight](#weight)\n* [Cumulative return](#return)\n* [Time](#time)\n* [The features](#features)\n* [The `features.csv` file](#features_file)\n* [Action](#action)\n* [The first day (""day 0"")](#day_0)\n* [Are there any missing values?](#missing_values)\n* [Is there any missing data: Days 2 and 294](#missing_data)\n* [DABL plots (targets: action and resp)](#DABL)\n* [Permutation Importance using the Random Forest](#permutation)\n* [Is there any correlation between day 100 and day 200?](#Pearson)\n* [The test data](#test_data)\n* [Evaluation](#evaluation)"
\n## The train.csv file is big\n\nThe train.csv is large: 5.77G. Let us see just how many rows it has:
We now have loaded `train.csv` in less than 17 seconds.\n\n\n## resp\n\nThere are a total of 500 days of data in `train.csv` (*i.e.* two years of trading data). Let us take a look at the cumulative values of `resp` over time
"as well as four [time horizons](https://www.investopedia.com/terms/t/timehorizon.asp)\n> ""*The longer the Time Horizon, the more aggressive, or riskier portfolio, an investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt.*"""
"as well as four [time horizons](https://www.investopedia.com/terms/t/timehorizon.asp)\n> ""*The longer the Time Horizon, the more aggressive, or riskier portfolio, an investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt.*"""
"We can see that `resp` (in blue) most closely follows time horizon 4 (`resp_4` is the uppermost curve, in purple). \n\nIn the notebook [""*Jane Street: time horizons and volatilities*""](https://www.kaggle.com/pcarta/jane-street-time-horizons-and-volatilities) written by [pcarta](pcarta), if I understand correctly, by using [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) it is calculated that if the time horizon $(T_j$) for `resp_1` (*i.e.* $T_1$) is 1, then \n* $T_j($ `resp_2` $) ~\approx 1.4 ~T_1$\n* $T_j($ `resp_3` $) ~\approx 3.9 ~T_1$ \n* $T_j($ `resp_4` $) ~\approx 11.1 ~T_1$\n\nwhere $T_1$ could correspond to 5 trading days.\n\nLet us now plot a histogram of all of the `resp` values (here only shown for values between -0.05 and 0.05)"
"We can see that `resp` (in blue) most closely follows time horizon 4 (`resp_4` is the uppermost curve, in purple). \n\nIn the notebook [""*Jane Street: time horizons and volatilities*""](https://www.kaggle.com/pcarta/jane-street-time-horizons-and-volatilities) written by [pcarta](pcarta), if I understand correctly, by using [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) it is calculated that if the time horizon $(T_j$) for `resp_1` (*i.e.* $T_1$) is 1, then \n* $T_j($ `resp_2` $) ~\approx 1.4 ~T_1$\n* $T_j($ `resp_3` $) ~\approx 3.9 ~T_1$ \n* $T_j($ `resp_4` $) ~\approx 11.1 ~T_1$\n\nwhere $T_1$ could correspond to 5 trading days.\n\nLet us now plot a histogram of all of the `resp` values (here only shown for values between -0.05 and 0.05)"
This distribution has very long tails
"Finally, let us fit a [Cauchy distribution](https://en.wikipedia.org/wiki/Cauchy_distribution) to this data"
"Note that a Cauchy distribution can be generated from the ratio of two independent normally distributed random variables with mean zero. The paper by [David E. Harris ""*The Distribution of Returns*""](https://www.scirp.org/pdf/JMF_2017083015172459.pdf) goes into detail regarding the use of a Cauchy distribution to model returns.\n\n\n## weight\n\n> *Each trade has an associated `weight` and `resp`, which together represents a return on the trade.\nTrades with `weight = 0` were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.*"
Let us take a look at a histogram of the non-zero weights
"There appear to be two peaks, one situated at `weight` $\approx$ 0.17, and a lower, broader peak at `weight` $\approx$ 0.34. Could this be indicative of two underlying distributions that we see here, superimposed on each other? Maybe one distribution of weights correspond to selling, and the other to buying?\n\nWe can plot the logarithm of the weights (*Credit*: [""*Target Engineering; CV; ⚡ Multi-Target*""](https://www.kaggle.com/marketneutral/target-engineering-cv-multi-target) by [marketneutral](https://www.kaggle.com/marketneutral))"
"There appear to be two peaks, one situated at `weight` $\approx$ 0.17, and a lower, broader peak at `weight` $\approx$ 0.34. Could this be indicative of two underlying distributions that we see here, superimposed on each other? Maybe one distribution of weights correspond to selling, and the other to buying?\n\nWe can plot the logarithm of the weights (*Credit*: [""*Target Engineering; CV; ⚡ Multi-Target*""](https://www.kaggle.com/marketneutral/target-engineering-cv-multi-target) by [marketneutral](https://www.kaggle.com/marketneutral))"
and we can now try to fit a pair of Gaussian functions to this distribution
and we can now try to fit a pair of Gaussian functions to this distribution
"with limited success; the narrower left hand peak seems to be some other distribution. (Just in case, the $\mu$ of the small Gaussian is located at -1.32, and the large Gaussian at 0.4).\n\n## Cumulative return\n\nLet us take a look at the cumulative daily return over time, which is given by `weight` multiplied by the value of `resp`"
"with limited success; the narrower left hand peak seems to be some other distribution. (Just in case, the $\mu$ of the small Gaussian is located at -1.32, and the large Gaussian at 0.4).\n\n## Cumulative return\n\nLet us take a look at the cumulative daily return over time, which is given by `weight` multiplied by the value of `resp`"
"We can see that the shortest time horizons, `resp_1`, `resp_2` and `resp_3`, representing a more conservative strategy, result in the lowest return.\n\nWe shall now plot a histogram of the `weight` multiplied by the value of `resp` (after removing the 0 weights)"
"We can see that the shortest time horizons, `resp_1`, `resp_2` and `resp_3`, representing a more conservative strategy, result in the lowest return.\n\nWe shall now plot a histogram of the `weight` multiplied by the value of `resp` (after removing the 0 weights)"
"\n## Time\nLet us plot the number of `ts_id` per day. Note: I have taken to drawing a vertical dashed line in my plots because I started to wonder [did Jane Street modify their trading model around day 85?](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930) Thanks to comments on that forum the general consenus seems to be that a change in the market took place around that time (perhaps a mean reverting market changing to a momentum market, or *vice versa*)."
"\n## Time\nLet us plot the number of `ts_id` per day. Note: I have taken to drawing a vertical dashed line in my plots because I started to wonder [did Jane Street modify their trading model around day 85?](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930) Thanks to comments on that forum the general consenus seems to be that a change in the market took place around that time (perhaps a mean reverting market changing to a momentum market, or *vice versa*)."
If we assume a [trading day](https://en.wikipedia.org/wiki/Trading_day) is 6½ hours long (*i.e.* 23400 seconds) then
If we assume a [trading day](https://en.wikipedia.org/wiki/Trading_day) is 6½ hours long (*i.e.* 23400 seconds) then
Here is a histogram of the number of trades per day (it has been [suggested](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930#1125847) that the number of trades per day is an indication of the [volatility](https://www.investopedia.com/terms/v/volatility.asp) that day)
Here is a histogram of the number of trades per day (it has been [suggested](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930#1125847) that the number of trades per day is an indication of the [volatility](https://www.investopedia.com/terms/v/volatility.asp) that day)
"If that is the case, then 'volitile' days, say with more than 9k trades (*i.e.* `ts_id`) per day, are the following "
"Also, `feature_0` is the *only* feature in the `features.csv` file that has no `True` tags."
"It is also very interesting to plot the cumulative `resp` and return (`resp`\*`weight`) for `feature_0 = +1` and `feature_0 = -1` individually (Credit: [""*An observation about feature_0*""](https://www.kaggle.com/c/jane-street-market-prediction/discussion/204963) by [therocket290](https://www.kaggle.com/therocket290))"
"It is also very interesting to plot the cumulative `resp` and return (`resp`\*`weight`) for `feature_0 = +1` and `feature_0 = -1` individually (Credit: [""*An observation about feature_0*""](https://www.kaggle.com/c/jane-street-market-prediction/discussion/204963) by [therocket290](https://www.kaggle.com/therocket290))"
"It can be seen that ""+1"" and the ""-1"" projections describe very different return dynamics.\nIn the notebook [""*Feature 0, beyond feature 0*""](https://www.kaggle.com/nanomathias/feature-0-beyond-feature-0) written by [NanoMathias](https://www.kaggle.com/nanomathias) a [uniform manifold approximation and projection (UMAP)](https://arxiv.org/abs/1802.03426) is performed and shows that `feature_0`  effectively classifies two distributions of features.\nThere have been many suggestions made regarding the nature of this feature on the discussion topic [""*What is ""feature_0"" ?*""](https://www.kaggle.com/c/jane-street-market-prediction/discussion/199462) such as `feature_0` representing the direction of the trade or things like bid/ask, long/short, or call/put.\n\nOne possibility is that `feature_0` represents something similar to the [Lee and Ready 'Tick' model](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1540-6261.1991.tb02683.x) for classifying individual trades as market buy or market sell orders, using intraday trade and quote data.\nA buy initiated trade is labeled as ""1"", and a sell-initiated trade is labeled as ""-1"" (*Source*: § 19.3.1 of [""*Advances in Financial Machine Learning*""](https://www.wiley.com/en-es/Advances+in+Financial+Machine+Learning-p-9781119482109) by Marcos Lopez de Prado)\n\n$$\nb_t = \n\begin{cases} \n  1  & \mbox{if }\Delta p_t > 0\\\n  -1 & \mbox{if }\Delta p_t < 0\\\n  b_{t-1} & \mbox{if }\Delta p_t = 0\n\end{cases}\n$$\n\nwhere $p_t$ is the price of the trade indexed by $t = 1,\ldots , T$, and $b_0$ is arbitrarily set to\n1.\n\nIf we look at the correlation matrix (see below) it can be seen that there is a strong positive correlation between `feature_0` and the **Tag 12** features, a strong negative correlation with the **Tag 13** features. There is also a negative correlation with the **Tag 25** and **Tag 27** features, and a positive correlation with  the **Tag 24** features.\n\nOther than features 37, 38, 39 and 40 all of the above features are `resp` related features (see below) with the strongest correlation being with the `resp_4` features.\n\n### feature_{1...129}\nThere seem to be four general 'types' of features, here is a plot of an example of one of each:"
"It can be seen that ""+1"" and the ""-1"" projections describe very different return dynamics.\nIn the notebook [""*Feature 0, beyond feature 0*""](https://www.kaggle.com/nanomathias/feature-0-beyond-feature-0) written by [NanoMathias](https://www.kaggle.com/nanomathias) a [uniform manifold approximation and projection (UMAP)](https://arxiv.org/abs/1802.03426) is performed and shows that `feature_0`  effectively classifies two distributions of features.\nThere have been many suggestions made regarding the nature of this feature on the discussion topic [""*What is ""feature_0"" ?*""](https://www.kaggle.com/c/jane-street-market-prediction/discussion/199462) such as `feature_0` representing the direction of the trade or things like bid/ask, long/short, or call/put.\n\nOne possibility is that `feature_0` represents something similar to the [Lee and Ready 'Tick' model](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1540-6261.1991.tb02683.x) for classifying individual trades as market buy or market sell orders, using intraday trade and quote data.\nA buy initiated trade is labeled as ""1"", and a sell-initiated trade is labeled as ""-1"" (*Source*: § 19.3.1 of [""*Advances in Financial Machine Learning*""](https://www.wiley.com/en-es/Advances+in+Financial+Machine+Learning-p-9781119482109) by Marcos Lopez de Prado)\n\n$$\nb_t = \n\begin{cases} \n  1  & \mbox{if }\Delta p_t > 0\\\n  -1 & \mbox{if }\Delta p_t < 0\\\n  b_{t-1} & \mbox{if }\Delta p_t = 0\n\end{cases}\n$$\n\nwhere $p_t$ is the price of the trade indexed by $t = 1,\ldots , T$, and $b_0$ is arbitrarily set to\n1.\n\nIf we look at the correlation matrix (see below) it can be seen that there is a strong positive correlation between `feature_0` and the **Tag 12** features, a strong negative correlation with the **Tag 13** features. There is also a negative correlation with the **Tag 25** and **Tag 27** features, and a positive correlation with  the **Tag 24** features.\n\nOther than features 37, 38, 39 and 40 all of the above features are `resp` related features (see below) with the strongest correlation being with the `resp_4` features.\n\n### feature_{1...129}\nThere seem to be four general 'types' of features, here is a plot of an example of one of each:"
"### 'Linear' features\n* 1 \n* 7, 9, 11, 13, 15\n* 17, 19, 21, 23, 25\n* 18,  20,  22,  24, 26\n* 27, 29, 21, 33, 35\n* 28, 30, 32, 34, 36\n* 84, 85, 86, 87, 88\n* 90, 91, 92, 93, 94\n* 96, 97, 98, 99, 100\n* 102 (strong change in gradient), 103, 104, 105, 106\n\nas well as\n41, 46, 47, 48, 49, 50, 51, 53, 54, 69, 89, 95 (strong change in gradient), 101, 107 (strong change in gradient), 108, 110, 111, 113, 114, 115, 116, 117, 118, 119 (strong change in gradient), 120, 122, and 124.\n### Features 41, 42 and 43 (Tag 14)\nThe **Tag 14** set are interesting as they appear to be ""stratified""; only adopting discrete values throughout the day (could these be a value of a [security](https://en.wikipedia.org/wiki/Security_(finance%29)?).\nHere are scatter plots of these three features for days 0, 1 and 3 (Note that I have omitted day 2, which I shall discuss in the *missing data* section below):"
"### 'Linear' features\n* 1 \n* 7, 9, 11, 13, 15\n* 17, 19, 21, 23, 25\n* 18,  20,  22,  24, 26\n* 27, 29, 21, 33, 35\n* 28, 30, 32, 34, 36\n* 84, 85, 86, 87, 88\n* 90, 91, 92, 93, 94\n* 96, 97, 98, 99, 100\n* 102 (strong change in gradient), 103, 104, 105, 106\n\nas well as\n41, 46, 47, 48, 49, 50, 51, 53, 54, 69, 89, 95 (strong change in gradient), 101, 107 (strong change in gradient), 108, 110, 111, 113, 114, 115, 116, 117, 118, 119 (strong change in gradient), 120, 122, and 124.\n### Features 41, 42 and 43 (Tag 14)\nThe **Tag 14** set are interesting as they appear to be ""stratified""; only adopting discrete values throughout the day (could these be a value of a [security](https://en.wikipedia.org/wiki/Security_(finance%29)?).\nHere are scatter plots of these three features for days 0, 1 and 3 (Note that I have omitted day 2, which I shall discuss in the *missing data* section below):"
"These three features also have very interesting lag plots, where we plot the value of the feature at `ts_id` $(n)$ with respect to the next value of the feature, *i.e.*  at `ts_id` $(n+1)$, (here for day 0). Red markers have been placed at (0,0) as a visual aid."
"These three features also have very interesting lag plots, where we plot the value of the feature at `ts_id` $(n)$ with respect to the next value of the feature, *i.e.*  at `ts_id` $(n+1)$, (here for day 0). Red markers have been placed at (0,0) as a visual aid."
"### Tag 18 features: 44 (+ tag 15) and 45 (+ tag 17)\nThese are similar to the Tag 14 features seen above, but are now much more centred around 0"
"### Tag 18 features: 44 (+ tag 15) and 45 (+ tag 17)\nThese are similar to the Tag 14 features seen above, but are now much more centred around 0"
with the following lag plots
with the following lag plots
"### Features 60 to 68 (Tag 22)\nWe have the **Tag 22** set:\n* 60, 61, 62, 63, 64, 65, 66, 67, 68"
"### Features 60 to 68 (Tag 22)\nWe have the **Tag 22** set:\n* 60, 61, 62, 63, 64, 65, 66, 67, 68"
"Indeed `feature_60` and `feature_61` (both having Tags 22 & 12) are virtually coincident. \nThe same goes for `feature_62` and `feature_63` (both having Tags 22 & 13), \n`feature_65` and `feature_66` (both having Tags 22 & 12) and\n`feature_67` and `feature_68` (both having Tags 22 & 13). Let us plot these features as distributions"
"Indeed `feature_60` and `feature_61` (both having Tags 22 & 12) are virtually coincident. \nThe same goes for `feature_62` and `feature_63` (both having Tags 22 & 13), \n`feature_65` and `feature_66` (both having Tags 22 & 12) and\n`feature_67` and `feature_68` (both having Tags 22 & 13). Let us plot these features as distributions"
and in between them is `feature_64`
and in between them is `feature_64`
"which has a big gap for values in the range 0.7 to 1.38. (Incidentally,  $\ln(2) \approx 0.693...$ and $\ln(4) \approx 1.386...$, I do not know if there is any significance to this at all).\n\nThe **Tag 22** features also have a very interesting daily pattern. For example, here are scatter and cumulative plots over three days for feature 64"
"which has a big gap for values in the range 0.7 to 1.38. (Incidentally,  $\ln(2) \approx 0.693...$ and $\ln(4) \approx 1.386...$, I do not know if there is any significance to this at all).\n\nThe **Tag 22** features also have a very interesting daily pattern. For example, here are scatter and cumulative plots over three days for feature 64"
"The global minimum value of `feature_64` is \\( \approx -6.4 \\) and the global maximum value is \\( \approx 8 \\) (not all days reach these limits). It is curious that a trading day on the New York Stock Exchange spans from 9:30 until 16:00. What if the units of `feature_64` were \\( \approx 30 \\) minutes, and `feature_64 = 0` corresponds to 12:00? Just for fun let us make a plot of the *arcsin* function, renaming the *y*-axis as the hours of the day..."
"The global minimum value of `feature_64` is \\( \approx -6.4 \\) and the global maximum value is \\( \approx 8 \\) (not all days reach these limits). It is curious that a trading day on the New York Stock Exchange spans from 9:30 until 16:00. What if the units of `feature_64` were \\( \approx 30 \\) minutes, and `feature_64 = 0` corresponds to 12:00? Just for fun let us make a plot of the *arcsin* function, renaming the *y*-axis as the hours of the day..."
"where, for some reason, the tick time is more frequent at the start and end of the day than in the middle. Also for fun let us plot the hypothetical tick frequency, *i.e.*\n\n$$ \frac{d}{dt} (2 \arcsin(t) +1) = \frac{2}{\sqrt{1-t^2}}$$"
"where, for some reason, the tick time is more frequent at the start and end of the day than in the middle. Also for fun let us plot the hypothetical tick frequency, *i.e.*\n\n$$ \frac{d}{dt} (2 \arcsin(t) +1) = \frac{2}{\sqrt{1-t^2}}$$"
"If this were so, then perhaps the period of missing values seen at the start of the day for some of the  features (see the section below on missing values) is actually similar to the period of missing values seen during the middle of the day? Also perhaps the higher tick frequency at the beginning and end of the day is due to a lot of buying when the day opens, and a lot of selling towards the close of the day so as to have no significant position overnight?\n\nIt was first suggested (if I am not mistaken) by [marketneutral](https://www.kaggle.com/marketneutral) in a [post](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201264#1101507) that the data *may* correspond to equities traded on the [Tokyo Stock Exchange](https://www.jpx.co.jp/english/derivatives/rules/trading-hours/index.html), whose trading hours are from 9:00 until 11:30, a break for lunch, and then from 12:30 until 15:00. This could explain the central discontinuity in the Tag 22 features.\n\nWe shall now also look at `feature 65`:"
"If this were so, then perhaps the period of missing values seen at the start of the day for some of the  features (see the section below on missing values) is actually similar to the period of missing values seen during the middle of the day? Also perhaps the higher tick frequency at the beginning and end of the day is due to a lot of buying when the day opens, and a lot of selling towards the close of the day so as to have no significant position overnight?\n\nIt was first suggested (if I am not mistaken) by [marketneutral](https://www.kaggle.com/marketneutral) in a [post](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201264#1101507) that the data *may* correspond to equities traded on the [Tokyo Stock Exchange](https://www.jpx.co.jp/english/derivatives/rules/trading-hours/index.html), whose trading hours are from 9:00 until 11:30, a break for lunch, and then from 12:30 until 15:00. This could explain the central discontinuity in the Tag 22 features.\n\nWe shall now also look at `feature 65`:"
"For a very interesting look at the Tag 22 features see the notebook [""*Important and Hidden Temporal Data*""](https://www.kaggle.com/lachlansuter/important-and-hidden-temporal-data) written by [Lachlan Suter](https://www.kaggle.com/lachlansuter).\n### 'Noisy' features\n* 3, 4, 5, 6\n* 8, 10, 12, 14, 16\n* 37, 38, 39, 40\n* 72, 73, 74, 75, 76\n* 78, 79, 80, 81, 82\n* 83\n\nHere are cumulative plots of some of these features"
"For a very interesting look at the Tag 22 features see the notebook [""*Important and Hidden Temporal Data*""](https://www.kaggle.com/lachlansuter/important-and-hidden-temporal-data) written by [Lachlan Suter](https://www.kaggle.com/lachlansuter).\n### 'Noisy' features\n* 3, 4, 5, 6\n* 8, 10, 12, 14, 16\n* 37, 38, 39, 40\n* 72, 73, 74, 75, 76\n* 78, 79, 80, 81, 82\n* 83\n\nHere are cumulative plots of some of these features"
"Could these represent offer prices, and those with with **Tag 9** bid prices? That said, we can see that after day 85 the value of `feature_40` actually becomes greater than the value of `feature_39`.\n\n### `feature_51` (Tag 19)\nIn the Topic [""*Weight and feature_51 de-anonymized*""](https://www.kaggle.com/c/jane-street-market-prediction/discussion/202014) by [marketneutral](https://www.kaggle.com/marketneutral) it is suggested that `feature_51` is the (log of) the average daily volume of the stock.\nHere I reproduce the plot of `feature_51` w.r.t. `weight` for non-zero weights:"
"Could these represent offer prices, and those with with **Tag 9** bid prices? That said, we can see that after day 85 the value of `feature_40` actually becomes greater than the value of `feature_39`.\n\n### `feature_51` (Tag 19)\nIn the Topic [""*Weight and feature_51 de-anonymized*""](https://www.kaggle.com/c/jane-street-market-prediction/discussion/202014) by [marketneutral](https://www.kaggle.com/marketneutral) it is suggested that `feature_51` is the (log of) the average daily volume of the stock.\nHere I reproduce the plot of `feature_51` w.r.t. `weight` for non-zero weights:"
### `feature_52` (Tag 19)
### `feature_52` (Tag 19)
having the following lag plot
having the following lag plot
and the following curious relationship with `resp`
and the following curious relationship with `resp`
"### 'Negative' features\nFeatures 73, 75, 76, 77 (noisy), 79, 81(noisy), 82. These are all found in the **Tag 23** section.\n\n### 'Hybrid' features (Tag 21): \n55, 56, 57, 58, 59.\n\nThese start off noisy, with prominent almost discontinuous steps around the 0.2M, 0.5M, and 0.8M trade marks, then go linear. These five features form the ""**Tag 21**"" set:"
"### 'Negative' features\nFeatures 73, 75, 76, 77 (noisy), 79, 81(noisy), 82. These are all found in the **Tag 23** section.\n\n### 'Hybrid' features (Tag 21): \n55, 56, 57, 58, 59.\n\nThese start off noisy, with prominent almost discontinuous steps around the 0.2M, 0.5M, and 0.8M trade marks, then go linear. These five features form the ""**Tag 21**"" set:"
"What if these are associated with the five `resp` values? Perhaps: \n* `feature_55` is related to `resp_1`\n* `feature_56` is related to `resp_4` \n* `feature_57` is related to `resp_2` \n* `feature_58` is related to `resp_3` \n* `feature_59` is related to `resp`\n\nIf that *is* the case then \n* **Tag 0** represents `resp_4` features\n* **Tag 1** represents `resp` features\n* **Tag 2** represents `resp_3` features\n* **Tag 3** represents `resp_2` features\n* **Tag 4** represents `resp_1` features\n\n*i.e.*\n* `resp_1` related features: 7, 8, 17, 18, 27, 28, 55, 72, 78, 84, 90, 96, 102, 108, 114, 120, and 121 (Note: 79.6% of all of the missing data is found within this set of features).\n* `resp_2` related features: 11, 12, 21, 22, 31, 32, 57, 74, 80, 86, 92, 98, 104, 110, 116, 124, and 125 (Note: 15.2% of all of the missing data is found within this set of features).\n* `resp_3` related features: 13, 14, 23, 24, 33, 34, 58, 75, 81, 87, 93, 99, 105, 111, 117, 126, and 127\n* `resp_4` related features: 9, 10, 19, 20, 29, 30, 56, 73, 79, 85, 91, 97, 103, 109, 115, 122, and 123\n* `resp` related features: 15, 16, 25, 26, 35, 36, 59, 76, 82, 88, 94, 100, 106, 112, 118, 128, and 129\n\nLet us take a look at a plot of each of these 17 features for each of the resp (Note: This is an image; right click to view and enlarge)"
"Just for fun let us re-plot the above data, but now in '8-bit' mode; totally illegible, but may perhaps serve as an overall visual aid..."
Let us sum the number of tags for each feature:
we can see that with the above formula overall we are very slightly more proactive (0.4%) than inactive. How does this look daily?
We can see that the daily action is fairly consistent; no obvious weekly/monthly/seasonal changes *etc*.
"we can see that the missing data does not appear to be random, indeed there appear to be two big chunks missing at the start and in the middle of each column. Let us assume that a [trading day](https://en.wikipedia.org/wiki/Trading_day) spans from 9:30 until 16:00. Let us also assume that the trades occur at regular intervals (which is almost certainly *not* the case) then `feature_7` has chunks of missing data from 9:30 until 10:03, and is missing ≈16 minutes from 13:17 until 13:33. `feature_11` has missing data from 9:30 until 9:35, and is missing ≈5½ minutes from 13:17 until 13:22.\n\nNow let us look at the sum of the number of missing data in each column for the whole `train.csv` file:"
"First of all, \n\n* **79.6%** of all the missing data is located in the **Tag 4** group, which represent the `resp_1` features\n* **15.2%** of the missing data is in the **Tag 3** group, which represent the `resp_2` features\n* In total, the features associated with `resp_1` and `resp_2` make up **> 95%** of all the missing data.\n\nWe can see that features 7 and 8 both have exactly the same number of missing values (393135). \n17 and 18, and 27 and 28 all have 395535 missing values each. These are all `resp_1` features.\n\nNext we have features 72, 78, 84, 90, 96, 102, 108, 114 all with 351426 missing values each. These too are all `resp_1` features.\n\nFeatures 21, 22, 31, 32 have 81444 missing values, closely followed by features 11 and 12. These are all `resp_2` features.\n\nThere are more features with even less missing values. I think the interesting thing is not so much the quantity of missing values in so much as it may tell us which features represent similar measures/metrics.\n\nIs day 0 special, or does every day have missing data?"
"First of all, \n\n* **79.6%** of all the missing data is located in the **Tag 4** group, which represent the `resp_1` features\n* **15.2%** of the missing data is in the **Tag 3** group, which represent the `resp_2` features\n* In total, the features associated with `resp_1` and `resp_2` make up **> 95%** of all the missing data.\n\nWe can see that features 7 and 8 both have exactly the same number of missing values (393135). \n17 and 18, and 27 and 28 all have 395535 missing values each. These are all `resp_1` features.\n\nNext we have features 72, 78, 84, 90, 96, 102, 108, 114 all with 351426 missing values each. These too are all `resp_1` features.\n\nFeatures 21, 22, 31, 32 have 81444 missing values, closely followed by features 11 and 12. These are all `resp_2` features.\n\nThere are more features with even less missing values. I think the interesting thing is not so much the quantity of missing values in so much as it may tell us which features represent similar measures/metrics.\n\nIs day 0 special, or does every day have missing data?"
"Indeed we can see that there is missing data *almost* every day, with no discernible pattern (weekly, monthly, *etc*). The exceptions are days **2** and **294**, which we shall look at in the next section.\n\nIn the notebook [""*Jane Street EDA Market Regime*""](https://www.kaggle.com/marketneutral/jane-street-eda-market-regime) written by [marketneutral](https://www.kaggle.com/marketneutral) a plot is made of the number of trades per day, and is strikingly similar to the above plot. In view of this, for curiosity, we shall plot the number of missing values in the features with respect to the number of trades, for each day."
"Indeed we can see that there is missing data *almost* every day, with no discernible pattern (weekly, monthly, *etc*). The exceptions are days **2** and **294**, which we shall look at in the next section.\n\nIn the notebook [""*Jane Street EDA Market Regime*""](https://www.kaggle.com/marketneutral/jane-street-eda-market-regime) written by [marketneutral](https://www.kaggle.com/marketneutral) a plot is made of the number of trades per day, and is strikingly similar to the above plot. In view of this, for curiosity, we shall plot the number of missing values in the features with respect to the number of trades, for each day."
"We can see that on average there are $\approx$ 3 missing feature values per trade, per day, except for two spikes located on days 2 and 294 where there are no missing values at all. (The most missing values are on day 14).\n\nThis raises the question of [what to do with missing data in the unseen test data?](https://www.kaggle.com/c/jane-street-market-prediction/discussion/200691). Whatever one decides to do, in this competition time is of the essence, so we have to do it fast, and [Yirun Zhang](https://www.kaggle.com/gogo827jz) has made an exhaustive study of the time taken in various filling methods in the notebook [""*Optimise Speed of Filling-NaN Function*""](https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function).\n\n\n## Is there any missing data: Days 2 and 294\nIf we produce scatter plots of `feature_64` we see that each day has the same sweeping pattern. However we see that **day 2** has only 231 `ts_id`  which all seem to originate from the very end of the day. Here is a plot of day 1 (in blue), day 2 (in red) and day 3 (blue again). Day 2 has been encircled as a visual aid."
"We can see that on average there are $\approx$ 3 missing feature values per trade, per day, except for two spikes located on days 2 and 294 where there are no missing values at all. (The most missing values are on day 14).\n\nThis raises the question of [what to do with missing data in the unseen test data?](https://www.kaggle.com/c/jane-street-market-prediction/discussion/200691). Whatever one decides to do, in this competition time is of the essence, so we have to do it fast, and [Yirun Zhang](https://www.kaggle.com/gogo827jz) has made an exhaustive study of the time taken in various filling methods in the notebook [""*Optimise Speed of Filling-NaN Function*""](https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function).\n\n\n## Is there any missing data: Days 2 and 294\nIf we produce scatter plots of `feature_64` we see that each day has the same sweeping pattern. However we see that **day 2** has only 231 `ts_id`  which all seem to originate from the very end of the day. Here is a plot of day 1 (in blue), day 2 (in red) and day 3 (blue again). Day 2 has been encircled as a visual aid."
"The same goes for **day 294**, which has only 29 `ts_id`.\nThis would also explain why days 2 and 294 have none of the missing values that we usually find during breakfast and lunch the other days.\nIt is possibly worth treating these two days as outliers and dropping them. \n\n\n\n## DABL plots\n\nLet us run **day 0** through the *data analysis baseline library* [dabl](https://github.com/amueller/dabl). First using the `action` as the target:"
"The same goes for **day 294**, which has only 29 `ts_id`.\nThis would also explain why days 2 and 294 have none of the missing values that we usually find during breakfast and lunch the other days.\nIt is possibly worth treating these two days as outliers and dropping them. \n\n\n\n## DABL plots\n\nLet us run **day 0** through the *data analysis baseline library* [dabl](https://github.com/amueller/dabl). First using the `action` as the target:"
We can see that the classes 0 and 1 for `action` are reasonably well balanced.\n\nNow we shall use `resp` as the target:
We can see that the classes 0 and 1 for `action` are reasonably well balanced.\n\nNow we shall use `resp` as the target:
### Plot of `resp` values with respect to time (`ts_id`) for day 0
### Plot of `resp` values with respect to time (`ts_id`) for day 0
"\n## Very quick Permutation Importance using the Random Forest\nWe shall now perform a simple [permutation importance](https://www.kaggle.com/dansbecker/permutation-importance) calculation, a basic way of seeing which features may be important. We shall perform a regression, with `resp` as the target."
"## Matrix Formulation\n\nIn general we can write above vector as $$ \mathbf{ x_{ij}} = \left( \begin{smallmatrix} \mathbf{x_{i1}} & \mathbf{x_{i2}} &.&.&.& \mathbf{x_{in}} \end{smallmatrix} \right)$$\n\nNow we combine all aviable individual vector into single input matrix of size $(m,n)$ and denoted it by $\mathbf{X}$ input matrix, which consist of all training exaples,\n$$\mathbf{X} = \left( \begin{smallmatrix} x_{11} & x_{12} &.&.&.&.& x_{1n}\\\n                                x_{21} & x_{22} &.&.&.&.& x_{2n}\\\n                                x_{31} & x_{32} &.&.&.&.& x_{3n}\\\n                                .&.&.&. &.&.&.& \\\n                                .&.&.&. &.&.&.& \\\n                                x_{m1} & x_{m2} &.&.&.&.&. x_{mn}\\\n                                \end{smallmatrix} \right)_{(m,n)}$$\n\nWe represent parameter of function and dependent variable in vactor form as  \n$$\theta = \left (\begin{matrix} \theta_0 \\ \theta_1 \\ .\\.\\ \theta_j\\.\\.\\ \theta_n \end {matrix}\right)_{(n+1,1)} \n\mathbf{ y } = \left (\begin{matrix} y_1\\ y_2\\. \\. \\ y_i \\. \\. \\ y_m \end{matrix} \right)_{(m,1)}$$\n\nSo we represent hypothesis function in vectorize form $$\mathbf{ h_\theta{(x)} = X\theta}$$.\n\n"
In above plot we fit regression line into the variables.
###  Check for missing value
There is no missing value in the data sex
### Plots
Thier no correlation among valiables.
Thier no correlation among valiables.
"If we look at the left plot the charges varies from 1120 to 63500, the plot is right skewed. In right plot we will apply natural log, then plot approximately tends to normal. for further analysis we will apply log on target variable charges. "
"If we look at the left plot the charges varies from 1120 to 63500, the plot is right skewed. In right plot we will apply natural log, then plot approximately tends to normal. for further analysis we will apply log on target variable charges. "
"From left plot the insurance charge for male and female is approximatley in same range,it is average around 5000 bucks. In right plot the insurance charge for smokers is much wide range compare to non smokers, the average charges for non smoker is approximately 5000 bucks. For smoker the minimum insurance charge is itself 5000 bucks."
\n\n\n\n3  IMPORTS    ⤒
\n\n\n\n4  SETUP AND HELPER FUNCTIONS    ⤒
\n\n4.0 FUNCTIONS FROM OTHER KAGGLERS!\n\n---\n\n\n\nI want to use the incredible and useful functions built by other Kagglers. Resources are listed below with proper attribution and code is in the cell below.\n\n\n\nAnimation Function(s)\n    Content Description: Visualization of the coordinate data given to us with stabilization to remove jitter (in recent versions)\n    Notebook Link: Animated Data Visualization\n    Author (Profile Link): danielpeshkov\n
\n\n4.1 HELPER FUNCTIONS\n\n---\n\n\n\nDon't worry about these for now. I've hidden them in the notebook viewer to not add complexity. I will explain any functions that are important in-line later.
\n\n5.2 EXAMINE THE `PARTICIPANT_ID` COLUMN\n\n---\n\n\n    Number Participants: 21\n    Average Number of Rows Per Participant: 4498.91\n    Standard Deviation in Counts Per Participant: 490.77\n    Minimum Number of Examples For One Participant: 3338\n    Maximum Number of Examples For One Participant: 4968\n\n\nIt's also worth pointing out that the folders in the train_landmark_files directory are named based on the participant_id for whom the respective isolated sign event parquet files are for.
\n\n5.3 EXAMINE THE `SEQUENCE_ID` COLUMN\n\n---\n\nThere's not much here. This is a unique value assigned to every isolated sequence/event. One sequence corresponds to a single isolated sign that we have to detect and label.\n* Every value is unique for every row\n
\n\n5.4 EXAMINE THE `SIGN` COLUMN\n\n---\n\nThis is the label for each respective event/sequence.\n    \n\n    Number Of Unique Signs: 250\n    Average Number of Rows Per Sign: 377.908\n    Standard Deviation in Counts Per Sign: 19.356537293638034\n    Minimum Number of Examples For One Sign: 299\n    Maximum Number of Examples For One Sign: 415\n\n\nIt's a pretty balanced dataset!
\n\n5.5 INCLUDING SEQUENCE METADATA IN TRAIN DATAFRAME\n\n---\n\nWe are going to identify certain pieces of relevant metadata that we want to scrape from the parquet files and include in our main dataframe\n\n\n\nWe will retrieve the following for each sequence\n\n    start_frame\n    end_frame\n    total_frames\n    face_count\n    pose_count\n    left_hand_count\n    right_hand_count\n    x_min\n    x_max\n    y_min\n    y_max\n    z_min\n    z_max\n\n\n\n\nWhat can we observe about the sequences with this new metadata:\n\n    There are always the same keypoints present\n    For each part of the body ('type') we have the following keypoint counts:\n        Right Hand --> 21 Keypoints\n        Left Hand  --> 21 Keypoints\n        Pose --> 33 Keypoints\n        Face --> 468 Keypoints\n    \n    Sequences can start almost anywhere from frame 0 to frame 484 but the mean is ~30\n    Sequences can end almost anywhere from frame 1 to frame 499 but the mean is ~67\n    Sequences can be different lengths (and are inclusive of their bounds) from a length of 2 to a length of 500. Sequences have a mean length of ~37.5\n
### Import libraries
### Load the data
## Sales data \n\n\n### Sample sales data
"These are sales data from randomly selected stores in the dataset. As expected, the sales data is very erratic, owing to the fact that so many factors affect the sales on a given day. On certain days, the sales quantity is zero, which indicates that a certain product may not be available on that day (as noted by Rob in his kernel)."
### Sample sales snippets
"In the above plots, I simply zoom in to sample snippets in the sales data. As stated earlier, we can clearly see that the sales data is very erratic and volatile. Sometimes, the sales are zero for a few days in a row, and at other times, it remains at its peak value for a few days. Therefore, we need some sort of ""denoising"" techniques to find the underlying trends in the sales data and make forecasts."
The below diagram illustrates these graphs side-by-side. Red graphs represent original sales and green graphs represent denoised sales.
"### Average smoothing\n\nAverage smooting is a relatively simple way to denoise time series data. In this method, we take a ""window"" with a fixed size (like 10). We first place the window at the beginning of the time series (first ten elements) and calculate the mean of that section. We now move the window across the time series in the forward direction by a particular ""stride"", calculate the mean of the new window and repeat the process, until we reach the end of the time series. All the mean values we calculated are then concatenated into a new time series, which forms the denoised sales data."
The below diagram illustrates these graphs side-by-side. Red graphs represent original sales and green graphs represent denoised sales.
"## Stores and states \n\nNow, I will look at the sales data across different stores and states in order to gain some useful insights."
### Rolling Average Price vs. Time for each store
"In the above graph, I have plotted rolling sales across all stores in the dataset. Almost every sales curve has ""linear oscillation"" trend at the macroscopic level. Basically, the sales oscillate like a sine wave about a certain mean value, but this mean value has an upward linear trend. This implies that the sales are oscillating at a higher and higher level every few months.\n\nThis trend is reminiscent of the **business cycle**, where economies have short-term oscillatory fluctuations but grow linearly in the long run. Maybe, such small-scale trends at the level of stores add up to decide trends we see at the macroeconomic level. Below is an illustration of the macroeconomic business cycle:"
 
"The above plot compares the sales distribution for each store in the dataset. The stores in California seem to have the highest variance in sales, which might indicate that some places in California grow significantly faster than others, *i.e.* there is development disparity. On the other hand, the Wisconsin and Texas sales seem to be quite consistent among themselves, without much variance. This indicates that development might be more uniform in these states. Moreover, the California stores also seem to have the highest overall mean sales."
"The above plot compares the sales distribution for each store in the dataset. The stores in California seem to have the highest variance in sales, which might indicate that some places in California grow significantly faster than others, *i.e.* there is development disparity. On the other hand, the Wisconsin and Texas sales seem to be quite consistent among themselves, without much variance. This indicates that development might be more uniform in these states. Moreover, the California stores also seem to have the highest overall mean sales."
"From the above graph, we can see the same trends: California stores have the highest variance and mean sales among all the stores in the dataset."
### Rolling Average Price vs. Time (CA)
"In the above graph, we can see the large disparity in sales among California stores. The sales curves almost never intersect each other. This may indicate that there are certain ""hubs"" of development in California which do not change over time. And other areas always remain behind these ""hubs"". The average sales in descending order are CA_3, CA_1, CA_2, CA_4. The store CA_3 has maximum sales while the store CA_4 has minimum sales. "
### Rolling Average Price vs. Time (WI)
"In the above graph, we can see a very low disparity in sales among Wisconsin stores. The sales curves intersect each other very often. This may indicate that most parts of Wisconsin have a similar ""development curve"" and that there is a greater equity in development across the state. There are no specific ""hotspots"" or ""hubs"" of development. The average sales in descending order are WI_2, WI_3, WI_1. The store WI_2 has maximum sales while the store WI_1 has minimum sales. "
### Rolling Average Price vs. Time (TX)
"In the above graph, we can once again see that a very low disparity in sales among Texas stores. The sales curves intersect each other often, albeit not as often as in Wisconsin. This might once again indicate that most parts of Texas have a similar ""development curve"" and that there is a greater equity in development across the state. The variance here is higher than in Wisconsin though, so there might be ""hubs"" of development in Texas as well, but not as pronounced as in California. The average sales in descending order are TX_2, TX_3, TX_1. The store TX_2 has maximum sales while the store TX_1 has minimum sales."
Below are the sales from three sample data points. I will use these samples to demonstrate the working of the models.
"## Naive approach \n\n\nThe first approach is the very simple **naive approach**. It simply forecasts the next day's sales as the current day's sales. The model can be summarized as follows:\n\n\n\nIn the above equation, *yt+1* is the predicted value for the next day's sales and *yt* is today's sales. The model predicts tomorrow's sales as today's sales. Now let us see how this simple model performs on our miniature dataset. The training data is in blue, validation data in orange, and predictions in green."
## Loss for each model 
"From the above graph, we can see that the two smoothing methods: moving average and exponential smoothing are the best-scoring models. Holt linear is not far behind. The remaining models: naive approach, ARIMA, and Prophet are the worst-scoring models. I believe that the accuracy of ARIMA and Prophet can be boosted significantly by tuning the hyperparameters."
"# TensorFlow deep NN\n#### A high-level tutorial into Deep Learning using MNIST data and TensorFlow library.\nby [@kakauandme](https://twitter.com/KaKaUandME) and [@thekoshkina](https://twitter.com/thekoshkina)\n\nAccuracy: 0.99\n\n**Prerequisites:** fundamental coding skills, a bit of linear algebra, especially matrix operations and perhaps understanding how images are stored in computer memory. To start with machine learning, we suggest [coursera course](https://www.coursera.org/learn/machine-learning) by Andrew Ng.\n\n\nNote: \n\n*Feel free to fork and adjust* CONSTANTS *to tweak network behaviour and explore how it changes algorithm performance and accuracy. Besides **TensorFlow graph** section can also be modified for learning purposes.*\n\n*It is highly recommended printing every variable that isn’t 100% clear for you. Also, [tensorboard](https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html) can be used on a local environment for visualisation and debugging.*\n## Libraries and settings"
"## Data preparation\nTo start, we read provided data. The *train.csv* file contains 42000 rows and 785 columns. Each row represents an image of a handwritten digit and a label with the value of this digit."
"To output one of the images, we reshape this long string of pixels into a 2-dimensional array, which is basically a grayscale image."
"The corresponding labels are numbers between 0 and 9, describing which digit a given image is of."
"After training is done, it's good to check accuracy on data that wasn't used in training."
"When, we're happy with the outcome, we read test data from *test.csv* and predict labels for provided images.\n\nTest data contains only images and labels are missing. Otherwise, the structure is similar to training data.\n\nPredicted labels are stored into CSV file for future submission."
## Imports
## Functions
"Our hypothesis is that the higher the class, the higher the chances of survival. This means that a person travelling in the first class has a higher chance of survival than a person traveling on the second or third class.\n\nTo visualize if there is a relationship between 'Pclass' and 'Survival', let's do a bar plot."
"As we can see, about 60% of the people travelling in the first class survived. In contrast, only approximately 25% of the people travelling in the third class survived. Accordingly, this plot suggests that the class in which people travel affects the chances of survival."
"'Age' is the next variable in the list. Our hypothesis is that children are more prone to survive, while people in its adult life may have a lower rate of survival. Personally, I don't have any special intuition about elders, since they are the most vulnerable. This can play for both sides: either people help elders because they are more vulnerable, or they they are not able to cope with the challenges posed by the wreck of a ship.\n\nLet's call the usual suspect (bar plot) to help us understanding the situation."
"With a little bit of creativity, we can say that the plot has three regions: \n\n1. One region that goes between age 0 and 15; \n2. One between age 15 and 48;\n3. A last one between age 48 and 80. \n\nI know that this division is arguable, especially in what concerns the last two categories. However, the point is that this categories split fits into what we know about the way our society is organized: childrens, adults and elders. For now, let's proceed this way."
"Regarding family size, our hypothesis is that those who travel alone, have a lower survival rate. The idea is that people with family can collaborate and help each other escaping.\n\nLet's see if that makes sense using our [beautiful and only friend](https://youtu.be/LsQtnBu3p7Y), the bar plot."
"As we can see, when 'FamilySize' is between 0 and 3, our hypothesis finds some support. People that are travelling alone have a lower survival rate than people who are travelling with one, two or three people more. \n\nHowever, when FamilySize is between 4 and 10, things start to change. Despite the large variability of the results, the survival rate drops. This may suggest that our hypothesis should be revised when 'FamilySize' is higher than 3. \n\nThis variable seems to be more complex than expected. Accordingly, we will not make any transformation in this variable and we will leave it as a continuous variable to preserve all the information it has."
"The same logic applied to 'Pclass' should work for 'Fare': higher fares, higher survival rate.\n\nSince now we want to establish comparisons across different levels of a categorical variable, we will use a box plot instead of a bar plot."
"The plot suggests that those who survived paid a higher fare. Since we believe this variable is connected with 'Pclass', let's see how they work together."
"The plot suggests that those who survived paid a higher fare. Since we believe this variable is connected with 'Pclass', let's see how they work together."
"Here we have an interesting result. It seems that 'Fare' doesn't make difference, in terms of survival, if you are travelling in second or third class. However, if you are travelling in first class, the higher the fare, the higher the chances of survival. Considering this, it would make sense to create interaction features between 'Fare' and 'Pclass'."
"The hypothesis regarding 'Embarked' is that it doesn't influence the chances of survival. It is hard to imagine a scenario in which people from Southampton, for instance, would such a competitive advantage, that it would make them more apt for survival than people from Queensland. Yes, in [Darwin](https://en.wikipedia.org/wiki/Natural_selection) we believe.\n\nA simple plot can enlighten us."
Ups... It seems that people embarking on C were selected by a superior entity to survive. This is strange and may be hidding some relationship that is not obvious with this plot (e.g. people embarking on C were mostly women). \n\nLet's dive deeper. 
"Box-Cox transformations aim to normalize variables. These transformations are an alternative to the typical transformations, such as square root transformations, log transformations, and inverse transformations. The main advantage of Box-Cox transformations is that they optimally normalize the chosen variable. Thus, they avoid the need to randomly try different transformations and automatize the data transformation process."
#### Polynomials
"Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1."
"ohh,as expected ! There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)"
### Number of characters in tweets
The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.
### Number of words in a tweet
###  Average word length in a tweet
"Now,we will analyze tweets with class 1."
"In both of them,""the"" dominates which is followed by ""a"" in class 0 and ""in"" in class 1."
First let's check tweets indicating real disaster.
"Now,we will move on to class 0."
"Now,we will move on to class 0."
### Common words ?
"# Problem Definition\nFor this tutorial, we will build a model to predict the depth to groundwater of an aquifer located in Petrignano, Italy. The question we want to answer is\n> What is the future depth to groundwater of a well belonging to the aquifier in Petrigrano over the next quarter?\n\n> The wells field of the alluvial plain between Ospedalicchio di Bastia Umbra and Petrignano is fed by three underground aquifers separated by low permeability septa. The aquifer can be considered a water table groundwater and is also fed by the Chiascio river. The groundwater levels are influenced by the following parameters: rainfall, depth to groundwater, temperatures and drainage volumes, level of the Chiascio river.\n\n> Indeed, both rainfall and temperature affect features like level, flow, depth to groundwater and hydrometry some time after it fell down. \n\n# Data Collection \nIn a typical workflow for time series, this would be the time for data collection. In this example, we will skip the data collection step and use data from the [Acea Smart Water Analytics challenge](https://www.kaggle.com/c/acea-water-prediction/). Therefore, this section will be a dataset overview. \n\nAlthough the dataset contains multiple waterbodies, we will only be looking at the Aquifer_Petrignano.csv file.\n\nTime series data usually comes in **tabular** format (e.g. csv files)."
"Since we are working with time series, the most essential features are the time related feature. In this example, we have the column `Date` which  uniquely identifies a day. Ideally, the data is already in chronological order and the time stamps are equidistant in time series. This is already the case in our data: The time interval is one day and the data is already in chronological order. Therefore, we do not have to do this additional data preparation step. \n\n\nThis column is provided in string format. Let's convert it to the `datetime64[ns]` data type."
Features:\n* **Rainfall** indicates the quantity of rain falling (mm)\n* **Temperature** indicates the temperature (°C) \n* **Volume** indicates the volume of water taken from the drinking water treatment plant (m$^3$)\n* **Hydrometry** indicates the groundwater level (m)\n\nTarget:\n* **Depth to Groundwater** indicates the groundwater level (m from the ground floor)\n
"# Data Preprocessing\n\n## Chronological Order and Equidistant Timestamps\nThe data should be in **chronological order** and the **timestamps should be equidistant** in time series. The chronological order can be achieved by sorting the dataframe by the timestamps. Equidisant timestamps indicates constant time intervals. To check this, the difference between each timestamp can be taken. If this is not the case, you can decide on a constant time interval and resample the data (see [Resampling](#Resampling)).\n\nThis is already the case in our data: The time interval is one day and the data is already in chronological order. Therefore, we do not have to do this additional data preparation step. "
"## Handling Missing Values\n\nWe can see that `Depth_to_Groundwater` has missing values.\n\nFurthermore, plotting the time series reveals that there seem to be some **implausible zero values** for `Drainage_Volume`, and `River_Hydrometry`. We will have to clean them by replacing them by `nan` values and filling them afterwards."
Now we have to think about what to do with these missing values. 
Now we have to think about what to do with these missing values. 
"* **Option 1: Fill NaN with Outlier or Zero**\n\n    In this specific example filling the missing value with an outlier value such as -999 is not a good idea. However, many notebooks in this challenge have been using -999. \n    \n* **Option 2: Fill NaN with Mean Value**\n\n    Also in this example, we can see that filling NaNs with the mean value is also not sufficient.\n\n* **Option 3: Fill NaN with Last Value with `.ffill()`**\n\n    Filling NaNs with the last value is already a little bit better in this case.\n\n* **Option 4: Fill NaN with Linearly Interpolated Value with `.interpolate()`**\n\n    Filling NaNs with the interpolated values is the best option in this small examlple but it requires knowledge of the neighouring values.\n"
"## Resampling\n\nResampling can provide additional information on the data. There are two types of resampling:\n* **Upsampling** is when the frequency of samples is increased (e.g. days to hours)\n* **Downsampling** is when the frequency of samples is decreased (e.g. days to weeks)\n\nIn this example, we will do some downsampling with the `.resample()` function."
"In this example, resampling would not be necessary. On the other hand, there is no necessity to look at the daily data. Considering weekly data seems to be sufficient as well. Therefore, we will **downsample the data to a weekly basis**."
"## Stationarity\n\nSome time-series models, such as such as [ARIMA](#ARIMA), assume that the underlying data is stationary. \nStationarity describes that the time-series has\n* constant mean and mean is not time-dependent \n* constant variance and variance is not time-dependent \n* constant covariance and covariance is not time-dependent \n\n> If a time series has a specific (stationary) behavior over a given time interval, then it can be assumed that the time series will behave the same at a later time.\n\nTime series **with trend and/or seasonality are not stationary**. Trend indicates that the mean is not constant over time and seasonality indicates that the variance is not constant over time."
"The check for stationarity can be done via three different approaches:\n1. **visually**: plot time series and check for trends or seasonality\n2. **basic statistics**: split time series and compare the mean and variance of each partition\n3. **statistical test**: Augmented Dickey Fuller test\n\nLet's do the **visual check** first. We can see that all features except `Temperature` have non-constant mean and non-constant variance. Therefore, **none of these seem to be stationary**. However, `Temperature` shows strong seasonality (hot in summer, cold in winter) and therefore it is not stationary either."
"The check for stationarity can be done via three different approaches:\n1. **visually**: plot time series and check for trends or seasonality\n2. **basic statistics**: split time series and compare the mean and variance of each partition\n3. **statistical test**: Augmented Dickey Fuller test\n\nLet's do the **visual check** first. We can see that all features except `Temperature` have non-constant mean and non-constant variance. Therefore, **none of these seem to be stationary**. However, `Temperature` shows strong seasonality (hot in summer, cold in winter) and therefore it is not stationary either."
"Next, we will **check the underlying statistics**. For this we will **split the time series into two sections** and check the mean and the variance. You could do more partitions if you wanted.\n\nWith this method, `Temperature` and `River_Hydrometry` show **somewhat similar (constant) mean and variance** and could be seen as stationary. However, with this method, we are not able to see the seasonality in the `Temperature` feature."
"Let's evaluate the histograms. Since we are looking at the mean and variance, we are expecting that the data conforms to a Gaussian distribution (bell shaped distribution) in case of stationarity."
"**Augmented Dickey-Fuller (ADF) test**  is a type of statistical test called a unit root test.  Unit roots are a cause for non-stationarity.\n\n* **Null Hypothesis (H0)**: Time series has a unit root. (Time series is **not stationary**).\n\n* **Alternate Hypothesis (H1)**: Time series has no unit root (Time series is **stationary**).\n\nIf the **null hypothesis can be rejected**, we can conclude that the **time series is stationary**.\n\nThere are two ways to rejects the null hypothesis:\n\nOn the one hand, the null hypothesis can be rejected if the p-value is below a set significance level. The defaults significance level is 5%\n\n* **p-value > significance level (default: 0.05)**: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n* **p-value <= significance level (default: 0.05)**: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n    \nOn the other hand, the null hypothesis can be rejects if the test statistic is less than the critical value.\n* **ADF statistic > critical value**: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n* **ADF statistic < critical value**: Reject the null hypothesis (H0), the data does not have a unit root and is stationary."
"## Encoding Cyclical Features \nThe new time features are cyclical. For example,the feature `month` cycles between 1 and 12 for every year.\nWhile the difference between each month increments by 1 during the year, between two years the `month` feature jumps from 12 (December) to 1 (January). This results in a -11 difference, which can confuse a lot of models."
"Ideally, we want the underlying data to represent the same difference between two consecutive months, even between December and January. A common remedy for this issue is to encode cyclical features into two dimensions with sine and cosine transformation."
"# Exploratory Data Analysis\n\nLet's begin by plotting the seasonal components of each feature and comparing the minima and maxima. By doing this, we can already gain some insights:\n* The depth to groundwater reaches its maximum around May/June and its minimum around November/December\n* The temperature reaches its maxmium around August and its minimum around January\n* The volume reaches its maximum around June and its minimum around August/September. It takes longer to reach its maximum than to reach its minimum.\n* The hydrometry reaches its maximum around March and its minimum around September\n\n* The volume and hydrometry reach their minimum roughly around the same time\n* The volume and hydrometry reach their minimum when the temperature reaches its maximum\n* Temperature lags begind depth to groundwater by around 2 to 3 months"
We can see that the correlation to the target variables increases if we use the time shifted features in comparison to the original features.
We can see that the correlation to the target variables increases if we use the time shifted features in comparison to the original features.
"## Autocorrelation Analysis\n\n\n    For further details on this topic, see my other notebook: \n    Time Series: Interpreting ACF and PACF\n\n\n\n\nThis EDA step is especially important when using [ARIMA](#ARIMA). The autocorrelation analysis helps to identify the AR and MA parameters for the [ARIMA](#ARIMA) model.\n\nAutocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)\n\n* **Autocorrelation  Function (ACF)**: Correlation between time series with a lagged version of itself. The autocorrelation function starts a lag 0, which is the correlation of the time series with itself and therefore results in a correlation of 1. -> MA parameter is q significant lags\n* **Partial Autocorrelation Function (PACF)**: Additional correlation explained by each successive lagged term -> AR parameter is p significant lags\n\nAutocorrelation helps in detecting seasonality.\n\nAs we can infer from the graph above, the autocorrelation continues to decrease as the lag increases, confirming that there is no linear association between observations separated by larger lags.\n\nFor the AR process, we expect that the ACF plot will gradually decrease and simultaneously the PACF should have a sharp drop after p significant lags. To define a MA process, we expect the opposite from the ACF and PACF plots, meaning that: the ACF should show a sharp drop after a certain q number of lags while PACF should show a geometric or gradual decreasing trend."
"## Autocorrelation Analysis\n\n\n    For further details on this topic, see my other notebook: \n    Time Series: Interpreting ACF and PACF\n\n\n\n\nThis EDA step is especially important when using [ARIMA](#ARIMA). The autocorrelation analysis helps to identify the AR and MA parameters for the [ARIMA](#ARIMA) model.\n\nAutocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)\n\n* **Autocorrelation  Function (ACF)**: Correlation between time series with a lagged version of itself. The autocorrelation function starts a lag 0, which is the correlation of the time series with itself and therefore results in a correlation of 1. -> MA parameter is q significant lags\n* **Partial Autocorrelation Function (PACF)**: Additional correlation explained by each successive lagged term -> AR parameter is p significant lags\n\nAutocorrelation helps in detecting seasonality.\n\nAs we can infer from the graph above, the autocorrelation continues to decrease as the lag increases, confirming that there is no linear association between observations separated by larger lags.\n\nFor the AR process, we expect that the ACF plot will gradually decrease and simultaneously the PACF should have a sharp drop after p significant lags. To define a MA process, we expect the opposite from the ACF and PACF plots, meaning that: the ACF should show a sharp drop after a certain q number of lags while PACF should show a geometric or gradual decreasing trend."
We can see some sinusoidal shape in both ACF and PACF functions. This suggests that both AR and MA processes are present.
-----------------------------------------\n# Data visualization
We can see that the negative/positive ratio is not entirely 50/50 as there are 130k negatives and 90k negatives. The ratio is closer to 60/40 meaning that there are 1.5 times more negative images than positives.\n\n### Plot some images with and without cancer tissue for comparison
"**To see the effects of our augmentation, we can plot one image multiple times.**"
"### Compute image statistics\n**Do not use augmentation here!**\n\nCalculating statistics will give channel averages of [0.702447, 0.546243, 0.696453],\nand std's of [0.238893, 0.282094, 0.216251].\n\nWhile we are calculating statistics, we can check if there are images that have a very low maximum pixel intensity (almost totally black) or very high minimum pixel intensity (almost totally white). These kind of images could be caused by bad exposure or cropping to an empty area. In case of an empty area, the image would not be an outlier but equally valid negative sample.\n\nWe find that there is at least one very dark and 6 very bright images."
### Plot some of the very bright or very dark images
"All the dark and bright images are labeled negative. I think the bright ones are just cropped from a non-stained part or they don't have any tissue (plain glass?) so the labels are correct. The samples don't have tumor tissue present. I am not so sure about the dark image, is it an outlier crop from badly exposed area or just some very large cell part filling the whole image. Anyway, removing only a small amount of outliers from this size data set has little or no effect on the prediction performance.\n\n-----------------------------------------"
"Given this situation, one main question that comes to people's mind is that where can I learn about DS and ML. An answer to this question will be very helpful for people to get started in this field. So in this notebook, let us explore the different options where people learn about DS / ML skills. \n\n**Most of the plots are interactive. So please feel free to hover over the plots, zoom in / out, rotate them as needed**\n\nFirstly, I would like to thank Kaggle for conducting this DS / ML survey again this year and making the data available for people like us to use."
"## Learning Category:\n\nIn this intial phase of the analysis, let us take up question 35 in the survey. The question is:\n\n*What percentage of your machine learning/ data science training falls under each category?*\n\nThe choices given for this question are\n 1. Self-taught \n 2. Online courses like Coursera, Udemy, edX etc\n 3. Work\n 4. University\n 5. Kaggle Competiitons\n 6. Other - Free text field\n \nWe need to give a percentage value for each of these learning categories and the total should sum up to 100%.\n\nOverall there are 23,858 responses in this survey and let us check the number of respondents for each of these training categories (percentage of the category is greater than 0)."
"**Observations:**\n * There are some missing values for this question and after dropping them we have 15,745 responses in total.\n * 'Self taught is the category with most number of respondents having percentage of learning greater than 0.\n * With the recent explosion of MOOC courses, 'Online courses' come in second \n * Learning as part of work is third and traditional way of learning - 'University' is fourth\n * Though Kaggle competitions take the fifth spot, the number of respondents is not much lesser than third and forth place.\n \n \n ### Percentage Contribution of Learning Categories:\n \n Now let us see, how much percentage each of the learning categories contribute to the learning process."
"**Observations:**\n \n * Looking at the median of each of the learning categories, it seems there is no one category that completely dominated the learning process of ML / DS\n * Self-taught seems to have higher percentage of share in the learning process compared to others. \n * Only less than half of the respondents have the percentage share of 'University' as greater than 0 \n \n \n ### Distribution of DS / ML Learning Category at different Countries:\n \n Now let us have a look at how these learning categories are distributed across the top countries. We will take the respondents from top 10 countries and do the analysis."
"**Observations:**\n \n * Looking at the median of each of the learning categories, it seems there is no one category that completely dominated the learning process of ML / DS\n * Self-taught seems to have higher percentage of share in the learning process compared to others. \n * Only less than half of the respondents have the percentage share of 'University' as greater than 0 \n \n \n ### Distribution of DS / ML Learning Category at different Countries:\n \n Now let us have a look at how these learning categories are distributed across the top countries. We will take the respondents from top 10 countries and do the analysis."
"**Observations:**\n * Compared to other top countries, the percentage contribution of universities in USA is much higher. I think this might be because there are multiple universities in US offering courses in DS / ML when compared with other countries.\n * In countries like India, Russia & Japan, the role of universities in Learning DS / ML is much lesser compared to other categories. \n * Also if we look at **Asian countries** in the list (India, China, Russia and Japan), **median percentage of Kaggle contribution** for learning DS / ML is greater than 0 while it is zero for other countries. \n * Contribution from learning at Work is more in Russia, France and Japan\n * In Brazil, the contribution of MOOC courses seem to be more than other learning categories\n * I personally think these plots directly represent the comfortable ways to acquire knowledge at these corresponding regions.\n \n \n ### Distribution of Learning Category By Profession:\n \n In this section, let us look at the how the learning categories vary based on the profession."
"**Observations:**\n * Compared to other top countries, the percentage contribution of universities in USA is much higher. I think this might be because there are multiple universities in US offering courses in DS / ML when compared with other countries.\n * In countries like India, Russia & Japan, the role of universities in Learning DS / ML is much lesser compared to other categories. \n * Also if we look at **Asian countries** in the list (India, China, Russia and Japan), **median percentage of Kaggle contribution** for learning DS / ML is greater than 0 while it is zero for other countries. \n * Contribution from learning at Work is more in Russia, France and Japan\n * In Brazil, the contribution of MOOC courses seem to be more than other learning categories\n * I personally think these plots directly represent the comfortable ways to acquire knowledge at these corresponding regions.\n \n \n ### Distribution of Learning Category By Profession:\n \n In this section, let us look at the how the learning categories vary based on the profession."
"**Observations:**\n \n * As expected, University plays a major role in imparting DS / ML knowledge among students and 'Work' has the least contribution\n * In case of Data Scientist, most of the respondents have mentioned that 'Work' plays a major role in learning the concepts. Self-learning also plays an equally important role.\n * For 'Software Enginner' who are learning DS / ML, 'Self-taught' and 'Online courses' are the ways to acquire knowledge compared to other means.\n * Also respondents with title 'Software Engineer' mentioned that Kaggle competitions share a higher percentage in learning DS / ML compared to other professions (looking at the third quartile)\n \n \n ### Distribution of Learning Category by Degree Attained:\n \n Now let us see how the learning categories vary based on the highest degree attained."
"**Observations:**\n \n * As expected, University plays a major role in imparting DS / ML knowledge among students and 'Work' has the least contribution\n * In case of Data Scientist, most of the respondents have mentioned that 'Work' plays a major role in learning the concepts. Self-learning also plays an equally important role.\n * For 'Software Enginner' who are learning DS / ML, 'Self-taught' and 'Online courses' are the ways to acquire knowledge compared to other means.\n * Also respondents with title 'Software Engineer' mentioned that Kaggle competitions share a higher percentage in learning DS / ML compared to other professions (looking at the third quartile)\n \n \n ### Distribution of Learning Category by Degree Attained:\n \n Now let us see how the learning categories vary based on the highest degree attained."
**Observations:**\n * Respondents having a Masters or Doctoral degree have a higher contribution for learning DS / ML from university compared to other two sections (looking at the third quartile of university)\n * Respondents with Bachelors degree have higher contribution of learning from self taught courses and online courses (looking at the median of different learning categories of Bachelors degree)\n * Learning from Kaggle competitions seem to have a fairly stable contribution across all sections.\n \n### Distribution of Learning Category by Gender
**Observations:**\n * Respondents having a Masters or Doctoral degree have a higher contribution for learning DS / ML from university compared to other two sections (looking at the third quartile of university)\n * Respondents with Bachelors degree have higher contribution of learning from self taught courses and online courses (looking at the median of different learning categories of Bachelors degree)\n * Learning from Kaggle competitions seem to have a fairly stable contribution across all sections.\n \n### Distribution of Learning Category by Gender
"**Observations:**\n\n* Looking at the third quartile of all the categories, 'Self-taught' has a higher contribution for Male while 'University' has the higher contribution for Female. \n\n### Distrbution of Learning Categories by Age:\n \n In this section, let us see how the percentage contribution each of the learning categories change based on age."
"**Observations:**\n\n* Looking at the third quartile of all the categories, 'Self-taught' has a higher contribution for Male while 'University' has the higher contribution for Female. \n\n### Distrbution of Learning Categories by Age:\n \n In this section, let us see how the percentage contribution each of the learning categories change based on age."
"**Observations:**\n * 'Self-taught' category contributes more for respondents aged more than 35 compared to respondents aged less than 35\n * Looking at the distribution of online courses by age, we can see that respondents aged more than 60 seem to have a lower median score compared to other age groups\n * Median contribution of 'Work' as learning category is high for middle aged people compared to younger and older ones\n * Median contribution of 'University' as learning category is high for people less than 30 years of age\n * Contribution of Kaggle as a learning category is fairly consistent across age groups with a slght higher third quartile for people aged less than 21. Looks like Kaggle is quite popular with the younger bunch ;)\n \n\n### Other ML / DS Learning Category - FreeForm Text\n \n There is also a free form text column that contains the responses for the DS / ML learning category apart from the choices given. Let us look at them."
"**Observations:**\n * 'Self-taught' category contributes more for respondents aged more than 35 compared to respondents aged less than 35\n * Looking at the distribution of online courses by age, we can see that respondents aged more than 60 seem to have a lower median score compared to other age groups\n * Median contribution of 'Work' as learning category is high for middle aged people compared to younger and older ones\n * Median contribution of 'University' as learning category is high for people less than 30 years of age\n * Contribution of Kaggle as a learning category is fairly consistent across age groups with a slght higher third quartile for people aged less than 21. Looks like Kaggle is quite popular with the younger bunch ;)\n \n\n### Other ML / DS Learning Category - FreeForm Text\n \n There is also a free form text column that contains the responses for the DS / ML learning category apart from the choices given. Let us look at them."
"**Observations:**\n\n* Apart from the given choices, bootcamps and books seem to be the next popular choices of learning DS / ML.\n \n \n ## Online Course Platforms\n\nAs we could see from the previous analysis, online platforms play a major role in imparting DS / ML education. So now let us focus on the different online plarforms. There are multiple online platforms available from which we can learn DS. Some of them can be seen below."
### Number of Respondents for each online platform\n\nFirst let us look at the number of respondents for each online platform.
"We also have a free form text column for this question, to add any other sources apart from the one mentioned. Let us look at them now."
"We also have a free form text column for this question, to add any other sources apart from the one mentioned. Let us look at them now."
\n### Most Used Online Platform to Learn DS\n\nNext let us look at the online platforms where the people had spent most of their time.
\n### Most Used Online Platform to Learn DS\n\nNext let us look at the online platforms where the people had spent most of their time.
"**Observations:** \n * Coursera tops the list with about 39% of the respondents mentioning that they spent most of their time there.\n * DataCamp and Udemy are neck to neck with each other with about 12% share\n * Udacity and edX are about 8.5% and 8% respectively\n * Though Kaggle Learn is relatively new, it is preferred by about 7% of respondents\n \n \n  ### Most used Online Platform by Country:\n  \n  The world map plots are interactive. Please rotate them to have a better view of the countries you would like to see."
### Online Courses Vs Brick & Mortar\n\nWe have seen that online courses have gained a lot of interest in the previous sections. Now let us see what people perceive about the quality of online courses compared to traditional brick and mortar ones.
"**Observations:**\n * About **53%** of the respondents feel that **online courses are better** than traditional brick and mortar courses\n * About 33% of the respondents are neutral\n * Overall, people seem to be more satisfied with online courses.\n \n \n\n \n "
"## Coursera Data Science Courses Review Dataset\n\nNow that we got an idea about the perception of people about online courses, let us check the perception from some other place. Thankfully we also have a [coursera course review dataset](https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset#reviews_by_course.csv) in Kaggle datasets. So in this section let us use this dataset to make some plots and see if they also give similar results."
"**Observations:**\n * Looks like the ""machine learning"" course in coursera is one of the widely popular courses and it has the highest number of reviews\n * ""python-data"" is the second one with most number of reviews followed by ""data-scientist-tools"""
"**Observations:**\n * Looks like the ""machine learning"" course in coursera is one of the widely popular courses and it has the highest number of reviews\n * ""python-data"" is the second one with most number of reviews followed by ""data-scientist-tools"""
"**Observations:**\n * Machine Learning in coursera is not just the most popular course by count, it is the course with most percentage of ""very positive"" reviews as well. I think this is one of the very popular starting courses for people wanting to learn DS / ML\n * Overall, the positive reviews are higher than the negative reviews for all the courses and is inline with Kaggle survey results as well. \n * Now you know which course to start first ;)"
"## In-person Bootcamps\n\nApart from online courses, one another recent addition that has gained popularity to learn data science is in-person bootcamps. More information about the bootcamps can be seen [here](https://www.cio.com/article/3051124/careers-staffing/10-boot-camps-to-kick-start-your-data-science-career.html) and [here](https://www.switchup.org/rankings/best-data-science-bootcamps).\n\n![BootCamps](https://www.kdnuggets.com/images/nycdsa-data-science-bootcamp.jpg)\n\nIn this section, let us see how people perceive bootcamps compared to traditional brick & mortar courses."
"**Observations:**\n * Seems like a good part of the respondents are not aware of the bootcamps. About 33% of the people took a ""No opinion / I do not know"" stance\n * About 39.5% of the people feel that they are better than traditional institutions\n * About 18.8% of the respondents are neutral on their views"
### Favorite Media Sources
### Favorite Media Sources By Country\n\n  The below plot is interactive. Please rotate them to have a better view of the countries you would like to see.
### Other Media Sources : Free Form Text\n\nWe also have a free form text column for other media sources. So let us look at this column to get an idea of the other favorite ML / DS sources.
"**Observations:**\n * ods.ai seem to be one another popular media source \n * We could also see some social media channels like linkedin, facebook, youtube, quora etc\n * Being from India, I could also see Analytics Vidhya mentioned in few places ( which means it needs some better cleaning ;) )"
"Hello everyone!! Hope everything is fine and you are enjoying things on Kaggle as usual. The rage for competing on Kaggle should never end. \nMachine Learning and Deep Learning have a huge scope in healthcare but applying them in healthcare isn't that simple. The stake is very high. It's more than just a `classification` problem. But if applied very carefully, it can benefit the world in enormous ways. **And as a Machine learning engineer, it's our responsibility to help people as much as we can in all possible ways.**\n\nPneumonia is a very common disease. It can be either: 1) Bacterial pneumonia  2) Viral Pneumonia  3) Mycoplasma pneumonia   and 4) Fungal pneumonia.\nThis dataset consists pneumonia samples belonging to the first two classes.  The dataset consists of only very few samples and that too unbalanced. The aim of this kernel is to develop a robust deep learning model from scratch on this limited amount of data. We all know that deep learning models are data hungry but if you know how things work, you can build good models even with a limited amount of data. "
Reproducibility is a great concern when doing deep learning. There was a good discussion on `KaggleNoobs` slack regarding this. We will set a numer of things in order to make sure that the results are almost reproducible(if not fully). 
### How many samples for each class are there in the dataset?
As you can see the data is highly imbalanced. We have almost with thrice pneumonia cases here as compared to the normal cases. This situation is very normal when it comes to medical data. The data will always be imbalanced. either there will be too many normal cases or there will be too many cases with the disease. \n\nLet's look at how a normal case is different from that of a pneumonia case. We will look at somes samples from our training data itself.
As you can see the data is highly imbalanced. We have almost with thrice pneumonia cases here as compared to the normal cases. This situation is very normal when it comes to medical data. The data will always be imbalanced. either there will be too many normal cases or there will be too many cases with the disease. \n\nLet's look at how a normal case is different from that of a pneumonia case. We will look at somes samples from our training data itself.
"If you look carefully, then there are some cases where you won't be able to differentiate between a normal case and a pneumonia case with the naked eye. There is one case in the above plot, at least for me ,which is too much confusing. If we can build a robust classifier, it would be a great assist to the doctor too."
"# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\n\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\n\n[Stacking Starter][1] : by Faron \n\n\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\n\n\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808"
"# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features."
"**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows"
**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us.
**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us.
"# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!"
"**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling ""Scatter"" as follows:"
Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.
"**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:"
# Second-Level Predictions from the First-level Output
**Correlation Heatmap of the Second Level Training set**
There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.
"Oh. \n\n(1595 in real data, 20 if you're in the Kaggle sample dataset)\n\nWell, that's also going to be a challenge for the convnet to figure out, but we're going to try! Also, there are outside datasources for more lung scans. For example, you can grab data from the LUNA2016 challenge: https://luna16.grand-challenge.org/data/ for another 888 scans.\n\nDo note that, if you do wish to compete, you can only use free datasets that are available to anyone who bothers to look.\n\nI'll have us stick to just the base dataset, again mainly so anyone can poke around this code in the kernel environment.\n\nNow, let's see what an actual slice looks like. If you do not have matplotlib, do *pip install matplotlib*\n\n\n\nWant to learn more about Matplotlib? Check out the [Data Visualization with Python and Matplotlib tutorial][1].\n\n\n  [1]: https://pythonprogramming.net/matplotlib-intro-tutorial/"
"Now, I am not a doctor, but I'm going to claim a mini-victory and say that's our first CT scan slice.\n\nWe have about 200 slices though, I'd feel more comfortable if I saw a few more. Let's look at the first 12, and resize them with opencv. If you do not have opencv, do a *pip install cv2*\n\nWant to learn more about what you can do with Open CV? Check out the [Image analysis and manipulation with OpenCV and Python tutorial][1].\n\nYou will also need numpy here. You probably already have numpy if you installed pandas, but, just in case, numpy is *pip install numpy*\n\n\n  [1]: https://pythonprogramming.net/loading-images-python-opencv-tutorial/"
# Section 2: Processing and viewing our Data #\n\n
"Alright, so we're resizing our images from 512x512 to 150x150. 150 is still going to wind up likely being waaaaaaay to big. That's fine, we can play with that constant more later, we just want to know how to do it.\n\nOkay, so now what? I think we need to address the whole non-uniformity of depth next. To be honest, I don't know of any super smooth way of doing this, but that's fine. I can at least think of A way, and that's all we need.\n\nMy thought is that, what we have is really a big list of slices. What we need is to be able to just take any list of images, whether it's got 200 scans, 150 scans, or 300 scans, and set it to be some fixed number.\nLet's say we want to have 20 scans instead. How can we do this?\nWell, first, we need something that will take our current list of scans, and chunk it into a list of lists of scans.\n\nI couldn't think of anything off the top of my head for this, so I Googled ""how to chunk a list into a list of lists."" This is how real programming is happens.\n\nAs per Ned Batchelder via Link: [http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks][1], we've got ourselves a nice chunker generator. Awesome!\n\nThanks Ned!\n\nOkay, once we've got these chunks of these scans, what are we going to do? Well, we can just average them together. My theory is that a scan is a few millimeters of actual tissue at most. Thus, we can hopefully just average this slice together, and maybe we're now working with a centimeter or so. If there's a growth there, it should still show up on scan.\n\nThis is just a theory, it has to be tested.\n\nAs we continue through this, however, you're hopefully going to see just how many theories we come up with, and how many variables we can tweak and change to possibly get better results.\n\n\n  [1]: http://%20http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks"
"Okay, the Python gods are really not happy with me for that hacky solution. If any of you would like to improve this chunking/averaging code, feel free. Really, any of this code...if you have improvements, share them! This is going to stay pretty messy. But hey, we did it! We figured out a way to make sure our 3 dimensional data can be at any resolution we want or need. Awesome!\n\nThat's actually a decently large hurdle. Are we totally done? ...maybe not. One major issue is these colors and ranges of data. It's unclear to me whether or not a model would appreciate that. Even if we do a grayscale colormap in the imshow, you'll see that some scans are just darker overall than others. This might be problematic and we might need to actually normalize this dataset.\n\nI expect that, with a large enough dataset, this wouldn't be an actual issue, but, with this size of data, it might be of huge importance.\n\nIn effort to not turn this notebook into an actual book, however, we're going to move forward! We can now see our new data by doing:"
"# Section 3: Preprocessing our Data #\n\n\n\nOkay, so we know what we've got, and what we need to do with it.\n\nWe have a few options at this point, we could take the code that we have already and do the processing ""online."" By this, I mean, while training the network, we can actually just loop over our patients, resize the data, then feed it through our neural network. We actually don't have to have all of the data prepared before we go through the network.\n\nIf you can preprocess all of the data into one file, and that one file doesn't exceed your available memory, then training should likely be faster, so you can more easily tweak your neural network and not be processing your data the same way over and over.\n\nIn many more realistic examples in the world, however, your dataset will be so large, that you wouldn't be able to read it all into memory at once anyway, but you could still maintain one big database or something.\n\nBottom line: There are tons of options here. Our dataset is only 1500 (even less if you are following in the Kaggle kernel) patients, and will be, for example, 20 slices of 150x150 image data if we went off the numbers we have now, but this will need to be even smaller for a typical computer most likely. \n\nRegardless, this much data wont be an issue to keep in memory or do whatever the heck we want.\n\nIf at all possible, I prefer to separate out steps in any big process like this, so I am going to go ahead and pre-process the data, so our neural network code is much simpler. Also, there's no good reason to maintain a network in GPU memory while we're wasting time processing the data which can be easily done on a CPU.\n\nNow, I will just make a slight modification to all of the code up to this point, and add some new final lines to preprocess this data and save the array of arrays to a file:"
"# Section 3: Preprocessing our Data #\n\n\n\nOkay, so we know what we've got, and what we need to do with it.\n\nWe have a few options at this point, we could take the code that we have already and do the processing ""online."" By this, I mean, while training the network, we can actually just loop over our patients, resize the data, then feed it through our neural network. We actually don't have to have all of the data prepared before we go through the network.\n\nIf you can preprocess all of the data into one file, and that one file doesn't exceed your available memory, then training should likely be faster, so you can more easily tweak your neural network and not be processing your data the same way over and over.\n\nIn many more realistic examples in the world, however, your dataset will be so large, that you wouldn't be able to read it all into memory at once anyway, but you could still maintain one big database or something.\n\nBottom line: There are tons of options here. Our dataset is only 1500 (even less if you are following in the Kaggle kernel) patients, and will be, for example, 20 slices of 150x150 image data if we went off the numbers we have now, but this will need to be even smaller for a typical computer most likely. \n\nRegardless, this much data wont be an issue to keep in memory or do whatever the heck we want.\n\nIf at all possible, I prefer to separate out steps in any big process like this, so I am going to go ahead and pre-process the data, so our neural network code is much simpler. Also, there's no good reason to maintain a network in GPU memory while we're wasting time processing the data which can be easily done on a CPU.\n\nNow, I will just make a slight modification to all of the code up to this point, and add some new final lines to preprocess this data and save the array of arrays to a file:"
"# Section 4: 3D Convolutional Neural Network #\n\n##Moment-o-truth##\n\n\n\nOkay, we've got preprocessed, normalized, data. Now we're ready to feed it through our 3D convnet and...see what happens!\n\nNow, I am not about to stuff a neural networks tutorial into this one. If you're already familiar with neural networks and TensorFlow, great! If not, as you might guess, I have a tutorial...or tutorials... for you!\n\nTo install the CPU version of TensorFlow, just do *pip install tensorflow*\n\nTo install the GPU version of TensorFlow, you need to get alllll the dependencies and such.\n\n**Installation tutorials:**\n\n[Installing the GPU version of TensorFlow in Ubuntu][1]\n\n[Installing the GPU version of TensorFlow on a Windows machine][2]\n\n**Using TensorFlow and concept tutorials:**\n\n[Introduction to deep learning with neural networks][3]\n\n[Introduction to TensorFlow][4] \n\n[Intro to Convolutional Neural Networks][5]\n\n[Convolutional Neural Network in TensorFlow tutorial][6]\n\nNow, the data we have is actually 3D data, not 2D data that's covered in most convnet tutorials, including mine above. So what changes? EVERYTHING! OMG IT'S THE END OF THE WORLD AS WE KNOW IT!!\n\nIt's not really all too bad. Your convolutional window/padding/strides need to change. Do note that, now, to have a bigger window, your processing penalty increases significantly as we increase in size, obviously much more than with 2D windows.\n\nOkay, let's begin.\n\n\n  [1]: https://pythonprogramming.net/how-to-cuda-gpu-tensorflow-deep-learning-tutorial/\n  [2]: https://www.youtube.com/watch?v=r7-WPbx8VuY\n  [3]: https://pythonprogramming.net/neural-networks-machine-learning-tutorial\n  [4]: https://pythonprogramming.net/tensorflow-introduction-machine-learning-tutorial/\n  [5]: https://pythonprogramming.net/convolutional-neural-network-cnn-machine-learning-tutorial/\n  [6]: https://pythonprogramming.net/cnn-tensorflow-convolutional-nerual-network-machine-learning-tutorial/"
"# Introduction: Manual Feature Engineering\n\nIf you are new to this competition, I highly suggest checking out [this notebook](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/) to get started.\n\nIn this notebook, we will explore making features by hand for the Home Credit Default Risk competition. In an earlier notebook, we used only the `application` data in order to build a model. The best model we made from this data achieved a score on the leaderboard around 0.74. In order to better this score, we will have to include more information from the other dataframes. Here, we will look at using information from the `bureau` and `bureau_balance` data. The definitions of these data files are:\n\n* bureau: information about client's previous loans with other financial institutions reported to Home Credit. Each previous loan has its own row.\n* bureau_balance: monthly information about the previous loans. Each month has its own row.\n\nManual feature engineering can be a tedious process (which is why we use automated feature engineering with featuretools!) and often relies on domain expertise. Since I have limited domain knowledge of loans and what makes a person likely to default, I will instead concentrate of getting as much info as possible into the final training dataframe. The idea is that the model will then pick up on which features are important rather than us having to decide that. Basically, our approach is to make as many features as possible and then give them all to the model to use! Later, we can perform feature reduction using the feature importances from the model or other techniques such as PCA. \n\nThe process of manual feature engineering will involve plenty of Pandas code, a little patience, and a lot of great practice manipulation data. Even though automated feature engineering tools are starting to be made available, feature engineering will still have to be done using plenty of data wrangling for a little while longer. "
"## Example: Counts of a client's previous loans\n\nTo illustrate the general process of manual feature engineering, we will first simply get the count of a client's previous loans at other financial institutions. This requires a number of Pandas operations we will make heavy use of throughout the notebook:\n\n* `groupby`: group a dataframe by a column. In this case we will group by the unique client, the `SK_ID_CURR` column\n* `agg`: perform a calculation on the grouped data such as taking the mean of columns. We can either call the function directly (`grouped_df.mean()`) or use the `agg` function together with a list of transforms (`grouped_df.agg([mean, max, min, sum])`)\n* `merge`: match the aggregated statistics to the appropriate client. We need to merge the original training data with the calculated stats on the `SK_ID_CURR` column which will insert `NaN` in any cell for which the client does not have the corresponding statistic\n\nWe also use the (`rename`) function quite a bit specifying the columns to be renamed as a dictionary. This is useful in order to keep track of the new variables we create.\n\nThis might seem like a lot, which is why we'll eventually write a function to do this process for us. Let's take a look at implementing this by hand first. "
"### Kernel Density Estimate Plots\n\nThe kernel density estimate plot shows the distribution of a single variable (think of it as a smoothed histogram). To see the different in distributions dependent on the value of a categorical variable, we can color the distributions differently according to the category. For example, we can show the kernel density estimate of the `previous_loan_count` colored by whether the `TARGET` = 1 or 0. The resulting KDE will show any significant differences in the distribution of the variable between people who did not repay their loan (`TARGET == 1`) and the people who did (`TARGET == 0`). This can serve as an indicator of whether a variable will be 'relevant' to a machine learning model. \n\nWe will put this plotting functionality in a function to re-use for any variable. "
We can test this function using the `EXT_SOURCE_3` variable which we [found to be one of the most important variables ](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) according to a Random Forest and Gradient Boosting Machine. 
"## Imports\n\nWe are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. "
"## Read in Data \n\nFirst, we can list all the available data files. There are a total of 9 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 6 other files containing additional information about each loan. "
That doesn't look right! The maximum value (besides being positive) is about 1000 years! 
"Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients."
"Well that is extremely interesting! It turns out that the anomalies have a lower rate of default. \n\nHandling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (`np.nan`) and then create a new boolean column indicating whether or not the value was anomalous.\n\n"
"The distribution looks to be much more in line with what we would expect, and we also have created a new column to tell the model that these values were originally anomalous (becuase we will have to fill in the nans with some value, probably the median of the column). The other columns with `DAYS` in the dataframe look to be about what we expect with no obvious outliers. \n\nAs an extremely important note, anything we do to the training data we also have to do to the testing data. Let's make sure to create the new column and fill in the existing column with `np.nan` in the testing data."
"As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often. \n\nLet's start looking at this variable. First, we can make a histogram of the age. We will put the x axis in years to make the plot a little more understandable."
"By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a [kernel density estimation plot](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) colored by the value of the target. A [kernel density estimate plot shows the distribution of a single variable](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn `kdeplot` for this graph."
"By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a [kernel density estimation plot](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) colored by the value of the target. A [kernel density estimate plot shows the distribution of a single variable](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn `kdeplot` for this graph."
"The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket. \n\nTo make this graph, first we `cut` the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category."
"All three `EXT_SOURCE` featureshave negative correlations with the target, indicating that as the value of the `EXT_SOURCE` increases, the client is more likely to repay the loan. We can also see that `DAYS_BIRTH` is positively correlated with `EXT_SOURCE_1` indicating that maybe one of the factors in this score is the client age.\n\nNext we can look at the distribution of each of these features colored by the value of the target. This will let us visualize the effect of this variable on the target."
"`EXT_SOURCE_3` displays the greatest difference between the values of the target. We can clearly see that this feature has some relationship to the likelihood of an applicant to repay a loan. The relationship is not very strong (in fact they are all [considered very weak](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf), but these variables will still be useful for a machine learning model to predict whether or not an applicant will repay a loan on time."
"## Pairs Plot\n\nAs a final exploratory plot, we can make a pairs plot of the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable. The [Pairs Plot](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166) is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.\n\nIf you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)! "
"In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the `EXT_SOURCE_1` and the `DAYS_BIRTH` (or equivalently `YEARS_BIRTH`), indicating that this feature may take into account the age of the client. "
"#### Visualize New Variables\n\nWe should explore these __domain knowledge__ variables visually in a graph. For all of these, we will make the same KDE plot colored by the value of the `TARGET`."
It's hard to say ahead of time if these new features will be useful. The only way to tell for sure is to try them out! 
"> many reference & image from [matplotlib cheatsheet](https://github.com/rougier/matplotlib-cheatsheet)\n\n***This is a notebook which organizes various tips and contents of matplotlib which we browse every day.***\n\nI am a developer who loves visualization.\n\nSo far I've built a kernel to share the tips I've gained from doing a lot of visualizations.\n\n**matplotlib** is the most basic visualization tool, and even if you use it well, sometimes you don't need to use the rest of the visualization tools.\n\n### Table of Contents \n\n\n0. **Setting**\n    - dpi\n    - figsize\n    - title\n1. **Alignments**\n    - subplots, tight_layout\n    - subplot2grid\n    - add_axes\n    - add_gridspec\n2. **Colormap**\n    - diverging\n    - qualitative\n    - sequential\n    - scientific\n3. **Text & Annotate & Patch**\n    - parameter\n    - text example\n    - patches example\n4. **Details & Example** \n    - font weight, color, size, etc\n    - Horizontal and Vertical (barplot)\n    - Border(edge) color and thickness\n    - Main Color & Sub Color\n    - Transparency\n    - Span\n5. **MEME**\n    - xkcd style"
"## 0. Setting \n\nSet the resolution through the **dpi** (Dots per Inch) setting of the figure.\nmatplotlib has a low default resolution itself, so setting this up is a bit more professional.\n\n`plt.rcParams['figure.dpi'] = 200` or `dpi=200`\n\nAnd for every plot set **figsize**.\nThe graph gives a very different feeling depending on the ratio. (I think it's best to try this heuristic multiple times.)\n\nThere are many places in matplotlib where you can write **titles** based on objects such as `plt.title ()`, `ax.set_title ()`, `fig.suptitle()`. If you add a title that takes into account font size and font family, fontweight, position, etc., it will be more readable."
"## 1. Alignments\n\n> The first nine graph plots (3 by 3) are a combination of matplotlib layout and design.\n\n- `subplots`\n- `subplot2grid`\n- `add_axes`\n- `gridspec`, `add_subplot`\n- `inset_axes` \n- `make_axes_locatable`\n\nTwo or more graphs are much more visually and semantically better than just one.\n\nThe easiest way to do this is to place the rectangles of the same shape.\n\nUsually you can start with the initial size with subplots."
"The first of the `plt.subplot()` parameters specifies the number of rows and the second the number of columns.\nThe graph looks a bit frustrating. In this case, you can use `plt.tight_layout()` to solve the frustration."
"The first of the `plt.subplot()` parameters specifies the number of rows and the second the number of columns.\nThe graph looks a bit frustrating. In this case, you can use `plt.tight_layout()` to solve the frustration."
"But should it be the same size depending on the subplot?\nFor example, bar graphs and pie charts are often very different in ratio.\n\nIn this case, the layout should be different.\n\nIn this case, you can easily use the grid system using `plt.subplot2grid`.\nIf you're a ***front-end developer***, it may be easier to understand."
"But should it be the same size depending on the subplot?\nFor example, bar graphs and pie charts are often very different in ratio.\n\nIn this case, the layout should be different.\n\nIn this case, you can easily use the grid system using `plt.subplot2grid`.\nIf you're a ***front-end developer***, it may be easier to understand."
"Alternatively, you can use `plt.add_axes()` to create an ax where you want."
"Alternatively, you can use `plt.add_axes()` to create an ax where you want."
"Another way is to use gridspec. This allows you to use `add_subplot` together, similar to subplots to grid.\n\nThis approach allows you to take advantage of the concept of `list` to use a developer-friendly grid."
"Another way is to use gridspec. This allows you to use `add_subplot` together, similar to subplots to grid.\n\nThis approach allows you to take advantage of the concept of `list` to use a developer-friendly grid."
"Here you can change the color of ax or plt itself, such as facecolor, to make it look more dashboard-like."
"## 3. Text & Annotate & Patch\n\nMany people often end up with just a picture in the graph, but the detail of the graph comes from the description. Just putting text on a specific part can change the feel of the graph.\n\n`ax.text` and `ax.annotate` are almost similar, but each has a different purpose.\n\n- In `ax.text`, The `first two numbers` represent the ratio coordinates in the graph. \n- In `ax.annotate`, `xy` represent the coordinates in the graph. \n\n- `va`, `ha` is a parameter that determines whether the current coordinate is the center of the text or the left / right of the text.\n- `color` stands for color, and you can enter a custom color or rgb value directly.\n- `bbox` sets an element for the box that wraps the text.\n    - Internal color (`facecolor`) and edge color(`edgecolor`) can be set separately.\n    - You can adjust the space by setting `pad`ding like in html.\n    - You can use the `boxstyle` to adjust the end of the rectangle."
"Using a patch with text is more effective.\n\nExcept for path patches, they are provided by default, so you can use them well.\n\n**Arrow is especially effective.**"
"Using a patch with text is more effective.\n\nExcept for path patches, they are provided by default, so you can use them well.\n\n**Arrow is especially effective.**"
"## 4. Details & Examples\n\nIn the plot you can make various settings. \n\n> Of course, adding the text or annotate mentioned above is a good idea.\n\n**You can set the following details:**\n\n- Horizontal and Vertical (barplot)\n- Border(edge) color and thickness\n- Main Color & Sub Color\n- Transparency\n- Span\n\n### Font Weight, Color, Family, Size ...\n\nI usually set the details in font weight and size. It is good because it is easy to see just by setting it to bold. Also, if the size of the text is too big or too small, the proportions are strange.\n\nThe fonts do not vary, but the difference between serifs and sans serifs can make a difference.\n\nThe **Tex** syntax is also applicable, so use that as well.\n\n- **keyword** : `fontsize`, `color`, `fontweight`, `fontfamily`\n\n### Horizontal keyboard & Vertical (barplot)\n\nIn general, when the number of x-axes is large, the **readability** of the barplot is significantly lower.\n\nFor example, seaborn's `countplot` often overlaps the x-axis and often makes it less readable.\n\nThis is due to the fact that in a notebook environment, the width is the length of the monitor.\n\nSo in this case, you can place the graph vertically so that the axes are well read and the graph is easier to read."
"### Main Color & Sub Color\n\nIf you draw a plot of seaborn, it is displayed in various colors by default. You can draw various colorful graphs while changing the palette.\n\nBut basically, the visualization should focus on information, so it's not always nice to have a colorful plot.\n\nRather, think about what data you want to focus on.\nIt is better to make a difference between the main color with **strong color** and the sub color of **achromatic color** system.\n\nAlternatively, it is a good idea to choose the palette according to the application mentioned above.\n\nIn the case of a colormap, you can ***select a palette or pass it to a list***, so it is convenient to pass it to a list when highlighting a specific part."
"### Transparency\n\nI told you to use transparency above, but transparency is a great tool.\n\nScatter plots also contain important points with many **overlapping** points. That's why it's important to know the overlapping data by adjusting transparency.\n\nWhen **comparing** lineplots or barplots, placing two or more plots together using transparency allows you to hold comparison information.\n\nIf you add transparency to the graph, you can complete the graph with refined colors.\n\n- `alpha` : Parameter name of normal transparency setting"
"### Transparency\n\nI told you to use transparency above, but transparency is a great tool.\n\nScatter plots also contain important points with many **overlapping** points. That's why it's important to know the overlapping data by adjusting transparency.\n\nWhen **comparing** lineplots or barplots, placing two or more plots together using transparency allows you to hold comparison information.\n\nIf you add transparency to the graph, you can complete the graph with refined colors.\n\n- `alpha` : Parameter name of normal transparency setting"
"### Span\n\nGood division is enough to give points. Zoning can be added for a variety of reasons, depending on the content or to classify units.\n\n- `axvspan` : vertical span\n- `axhspan` : horizontal span\n\nyou can set color, alpha(transparency), range, etc"
"If you just use span and add text, you can change it as follows.\n(The title bold processing is below.)"
"## MEME : xkcd theme\n\n- [xkcd](https://xkcd.com/) : Webcomic for Geeks\n\n> Depending on the current kaggle and version, the font is broken, but you can draw a graph like this:"
##  Importing Dependencies\n   We shall start by importing all the neccessary libraries. I will explain the exact use of each library later in this notebook.
"#  Dataset Preprocessing\nIn this notebook, I am using **Sentiment-140** from [Kaggle](https://www.kaggle.com/kazanova/sentiment140). It contains a labels data of 1.6 Million Tweets and I find it a good amount of data to train our model."
Here are decoding the labels. We map **0 -> Negative and 1 -> Positive** as directed by the datset desciption. Now that we decoded we shall now analyse the dataset by its distribution. Because it's important that we have almost small amount of examples for given classes.
It's a very good dataset without any skewness. Thank Goodness.\n\nNow let us explore the data we having here... 
**Aaww.. It is clean and tidy now. Now let's see some word cloud visualizations of it.**\n\n### Positive Words
### Negative Words
### Negative Words
## Train and Test Split
"# Model Evaluation\nNow that we have trained the model, we can evaluate its performance. We will some evaluation metrics and techniques to test the model.\n\nLet's start with the Learning Curve of loss and accuracy of the model on each epoch."
"The model will output a prediction score between 0 and 1. We can classify two classes by defining a threshold value for it. In our case, I have set 0.5 as THRESHOLD value, if the score above it. Then it will be classified as **POSITIVE** sentiment."
"*'Very well... It seems that your minimum price is larger than zero. Excellent! You don't have one of those personal traits that would destroy my model! Do you have any picture that you can send me? I don't know... like, you in the beach... or maybe a selfie in the gym?'*"
"*'Ah! I see you that you use seaborn makeup when you're going out... That's so elegant! I also see that you:*\n\n* *Deviate from the normal distribution.*\n* *Have appreciable positive skewness.*\n* *Show peakedness.*\n\n*This is getting interesting! 'SalePrice', could you give me your body measures?'*"
### Relationship with numerical variables
"*Hmmm... It seems that 'SalePrice' and 'GrLivArea' are really old friends, with a linear relationship.*\n\n*And what about 'TotalBsmtSF'?*"
"*Hmmm... It seems that 'SalePrice' and 'GrLivArea' are really old friends, with a linear relationship.*\n\n*And what about 'TotalBsmtSF'?*"
"*'TotalBsmtSF' is also a great friend of 'SalePrice' but this seems a much more emotional relationship! Everything is ok and suddenly, in a strong linear (exponential?) reaction, everything changes. Moreover, it's clear that sometimes 'TotalBsmtSF' closes in itself and gives zero credit to 'SalePrice'.*"
### Relationship with categorical features
"*Like all the pretty girls, 'SalePrice' enjoys 'OverallQual'. Note to self: consider whether McDonald's is suitable for the first date.*"
"*Like all the pretty girls, 'SalePrice' enjoys 'OverallQual'. Note to self: consider whether McDonald's is suitable for the first date.*"
"*Although it's not a strong tendency, I'd say that 'SalePrice' is more prone to spend more money in new stuff than in old relics.*\n\nNote: we don't know if 'SalePrice' is in constant prices. Constant prices try to remove the effect of inflation. If 'SalePrice' is not in constant prices, it should be, so than prices are comparable over the years."
#### Correlation matrix (heatmap style)
"In my opinion, this heatmap is the best way to get a quick overview of our 'plasma soup' and its relationships. (Thank you @seaborn!)\n\nAt first sight, there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, and the second one refers to the 'Garage*X*' variables. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. Heatmaps are great to detect this kind of situations and in problems dominated by feature selection, like ours, they are an essential tool.\n\nAnother thing that got my attention was the 'SalePrice' correlations. We can see our well-known 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hi!', but we can also see many other variables that should be taken into account. That's what we will do next."
#### 'SalePrice' correlation matrix (zoomed heatmap style)
"According to our crystal ball, these are the variables most correlated with 'SalePrice'. My thoughts on this:\n\n* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!\n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').\n* 'FullBath'?? Really? \n* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?\n* Ah... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start feeling that we should do a little bit of time-series analysis to get this right. I'll leave this as a homework for you.\n\nLet's proceed to the scatter plots."
"Get ready for what you're about to see. I must confess that the first time I saw these scatter plots I was totally blown away! So much information in so short space... It's just amazing. Once more, thank you @seaborn! You make me 'move like Jagger'!"
"Although we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\n\nOne of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you're trying to buy a bunker).\n\nThe plot concerning 'SalePrice' and 'YearBuilt' can also make us think. In the bottom of the 'dots cloud', we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the 'dots cloud' (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\n\nOk, enough of Rorschach test for now. Let's move forward to what's missing: missing data!"
"We already know the following scatter plots by heart. However, when we look to things from a new perspective, there's always something to discover. As Alan Kay said, 'a change in perspective is worth 80 IQ points'."
"What has been revealed:\n\n* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\n* The two observations in the top of the plot are those 7.something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them."
The point here is to test 'SalePrice' in a very lean way. We'll do this paying attention to:\n\n* Histogram - Kurtosis and skewness.\n* Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.
"Ok, 'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\n\nBut everything's not lost. A simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of positive skewness, log transformations usually works well. When I discovered this, I felt like an Hogwarts' student discovering a new cool spell.\n\n*Avada kedavra!*"
Done! Let's check what's going on with 'GrLivArea'.
Tastes like skewness... *Avada kedavra!*
"Next, please..."
"Ok, now we are dealing with the big boss. What do we have here?\n\n* Something that, in general, presents skewness.\n* A significant number of observations with value zero (houses without basement).\n* A big problem because the value zero doesn't allow us to do log transformations.\n\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.\n\nI'm not sure if this approach is correct. It just seemed right to me. That's what I call 'high risk engineering'."
"The best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).\n\nStarting by 'SalePrice' and 'GrLivArea'..."
"Older versions of this scatter plot (previous to log transformations), had a conic shape (go back and check 'Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)'). As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n\nNow let's check 'SalePrice' with 'TotalBsmtSF'."
"Older versions of this scatter plot (previous to log transformations), had a conic shape (go back and check 'Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)'). As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n\nNow let's check 'SalePrice' with 'TotalBsmtSF'."
"We can say that, in general, 'SalePrice' exhibit equal levels of variance across the range of 'TotalBsmtSF'. Cool!"
### 2.2. Data Visualization\nlet's take look at our data in the most raw shape.\nI really recommend scatter plot because we can get the idea of our data without any manipulation
It seems that the data suffer from outliers\n\nLet's see for example pregnency distribution
"### 4.3.1. Corrolation\n\nIf we fit highly corrolated data in our model, it results in the overfitting probelm. Thus, for example if there are two highly corrolated features we have to drop the one that has more corrolation with other feature.  "
There is not highly corrolated feature in this data set.
"## **0. Introduction**\n\nI decided to write this kernel because **Titanic: Machine Learning from Disaster** is one of my favorite competitions on Kaggle. This is a beginner level kernel which focuses on **Exploratory Data Analysis** and **Feature Engineering**. A lot of people start Kaggle with this competition and they get lost in extremely long tutorial kernels. This is a short kernel compared to the other ones. I hope this will be a good guide for starters and inspire them with new feature engineering ideas.\n\n**Titanic: Machine Learning from Disaster** is a great competition to apply domain knowledge for feature engineering, so I made a research and learned a lot about Titanic. There are many secrets to be revealed beneath the Titanic dataset. I tried to find out some of those secret factors that had affected the survival of passengers when the Titanic was sinking. I believe there are other features still waiting to be discovered. \n\nThis kernel has **3** main sections; **Exploratory Data Analysis**, **Feature Engineering** and **Model**, and it can achieve top **2%** (**0.83732**) public leaderboard score with a tuned Random Forest Classifier. It takes 60 seconds to run whole notebook. If you have any idea that might improve this kernel, please be sure to comment, or fork and experiment as you like. If you didn't understand any part, feel free to ask."
"* Training set has **891** rows and test set has **418** rows\n* Training set have **12** features and test set have **11** features\n* One extra feature in training set is `Survived` feature, which is the target variable"
"#### **1.2.4 Cabin**\n`Cabin` feature is little bit tricky and it needs further exploration. The large portion of the `Cabin` feature is missing and the feature itself can't be ignored completely because some the cabins might have higher survival rates. It turns out to be the first letter of the `Cabin` values are the decks in which the cabins are located. Those decks were mainly separated for one passenger class, but some of them were used by multiple passenger classes.\n![alt text](https://vignette.wikia.nocookie.net/titanic/images/f/f9/Titanic_side_plan.png/revision/latest?cb=20180322183733)\n* On the Boat Deck there were **6** rooms labeled as **T, U, W, X, Y, Z** but only the **T** cabin is present in the dataset\n* **A**, **B** and **C** decks were only for 1st class passengers\n* **D** and **E** decks were for all classes\n* **F** and **G** decks were for both 2nd and 3rd class passengers\n* From going **A** to **G**, distance to the staircase increases which might be a factor of survival"
"* **100%** of **A**, **B** and **C** decks are 1st class passengers\n* Deck **D** has **87%** 1st class and **13%** 2nd class passengers\n* Deck **E** has **83%** 1st class, **10%** 2nd class and **7%** 3rd class passengers\n* Deck **F** has **62%** 2nd class and **38%** 3rd class passengers\n* **100%** of **G** deck are 3rd class passengers\n* There is one person on the boat deck in **T** cabin and he is a 1st class passenger. **T** cabin passenger has the closest resemblance to **A** deck passengers so he is grouped with **A** deck\n* Passengers labeled as **M** are the missing values in `Cabin` feature. I don't think it is possible to find those passengers' real `Deck` so I decided to use **M** like a deck"
### **1.3 Target Distribution**\n* **38.38%** (342/891) of training set is **Class 1**\n* **61.62%** (549/891) of training set is **Class 0**
### **1.4 Correlations**\nFeatures are highly correlated with each other and dependent to each other. The highest correlation between features is **0.549500** in training set and **0.577147** in test set (between `Fare` and `Pclass`). The other features are also highly correlated. There are **9** correlations in training set and **6** correlations in test set that are higher than **0.1**.
"#### **1.5.1 Continuous Features**\nBoth of the continuous features (`Age` and `Fare`) have good split points and spikes for a decision tree to learn. One potential problem for both features is, the distribution has more spikes and bumps in training set, but it is smoother in test set. Model may not be able to generalize to test set because of this reason.\n\n* Distribution of `Age` feature clearly shows that children younger than 15 has a higher survival rate than any of the other age groups\n* In distribution of `Fare` feature, the survival rate is higher on distribution tails. The distribution also has positive skew because of the extremely large outliers"
#### **1.5.2 Categorical Features**\nEvery categorical feature has at least one class with high mortality rate. Those classes are very helpful to predict whether the passenger is a survivor or victim. Best categorical features are `Pclass` and `Sex` because they have the most homogenous distributions.\n\n* Passengers boarded from **Southampton** has a lower survival rate unlike other ports. More than half of the passengers boarded from **Cherbourg** had survived. This observation could be related to `Pclass` feature\n* `Parch` and `SibSp` features show that passengers with only one family member has a higher survival rate
#### **1.5.2 Categorical Features**\nEvery categorical feature has at least one class with high mortality rate. Those classes are very helpful to predict whether the passenger is a survivor or victim. Best categorical features are `Pclass` and `Sex` because they have the most homogenous distributions.\n\n* Passengers boarded from **Southampton** has a lower survival rate unlike other ports. More than half of the passengers boarded from **Cherbourg** had survived. This observation could be related to `Pclass` feature\n* `Parch` and `SibSp` features show that passengers with only one family member has a higher survival rate
"### **1.6 Conclusion**\nMost of the features are correlated with each other. This relationship can be used to create new features with feature transformation and feature interaction. Target encoding could be very useful as well because of the high correlations with `Survived` feature.\n\nSplit points and spikes are visible in continuous features. They can be captured easily with a decision tree model, but linear models may not be able to spot them.\n\nCategorical features have very distinct distributions with different survival rates. Those features can be one-hot encoded. Some of those features may be combined with each other to make new features.\n\nCreated a new feature called `Deck` and dropped `Cabin` feature at the **Exploratory Data Analysis** part."
"### **2.2 Frequency Encoding**\n`Family_Size` is created by adding `SibSp`, `Parch` and **1**. `SibSp` is the count of siblings and spouse, and `Parch` is the count of parents and children. Those columns are added in order to find the total size of families. Adding **1** at the end, is the current passenger. Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates.\n* Family Size with **1** are labeled as **Alone**\n* Family Size with **2**, **3** and **4** are labeled as **Small**\n* Family Size with **5** and **6** are labeled as **Medium**\n* Family Size with **7**, **8** and **11** are labeled as **Large**"
"There are too many unique `Ticket` values to analyze, so grouping them up by their frequencies makes things easier.\n\n**How is this feature different than `Family_Size`?** Many passengers travelled along with groups. Those groups consist of friends, nannies, maids and etc. They weren't counted as family, but they used the same ticket.\n\n**Why not grouping tickets by their prefixes?** If prefixes in `Ticket` feature has any meaning, then they are already captured in `Pclass` or `Embarked` features because that could be the only logical information which can be derived from the `Ticket` feature.\n\nAccording to the graph below, groups with **2**,**3** and **4** members had a higher survival rate. Passengers who travel alone has the lowest survival rate. After **4** group members, survival rate decreases drastically. This pattern is very similar to `Family_Size` feature but there are minor differences. `Ticket_Frequency` values are not grouped like `Family_Size` because that would basically create the same feature with perfect correlation. This kind of feature wouldn't provide any additional information gain."
### **2.4 Target Encoding**\n`extract_surname` function is used for extracting surnames of passengers from the `Name` feature. `Family` feature is created with the extracted surname. This is necessary for grouping passengers in the same family. 
"`Family_Survival_Rate` is calculated from families in training set since there is no `Survived` feature in test set. A list of family names that are occuring in both training and test set (`non_unique_families`), is created. The survival rate is calculated for families with more than 1 members in that list, and stored in `Family_Survival_Rate` feature.\n\nAn extra binary feature `Family_Survival_Rate_NA` is created for families that are unique to the test set. This feature is also necessary because there is no way to calculate those families' survival rate. This feature implies that family survival rate is not applicable to those passengers because there is no way to retrieve their survival rate.\n\n`Ticket_Survival_Rate` and `Ticket_Survival_Rate_NA` features are also created with the same method. `Ticket_Survival_Rate` and `Family_Survival_Rate` are averaged and become `Survival_Rate`, and `Ticket_Survival_Rate_NA` and `Family_Survival_Rate_NA` are also averaged and become `Survival_Rate_NA`."
### **3.2 Feature Importance**
### **3.3 ROC Curve**
### **3.3 ROC Curve**
### **3.4 Submission**
" Outlier Analysis  \n \n> \n \n  \nWhat is an Outlier?  \n Outlier is an observation that is numerically distant from the rest of the data or in a simple word it is the value which is out of the range.let’s take an example to check what happens to a data set with and data set without outliers.\n\n\n|| | Data without outlier |  | Data with outlier | \n|--||--||--|\n|**Data**| |1,2,3,3,4,5,4 |  |1,2,3,3,4,5,**400** | \n|**Mean**| |3.142 | |**59.714** |  \n|**Median**| |3|  |3|\n|**Standard Deviation**| |1.345185| |**150.057**|\n\n As you can see, data set with outliers has significantly different mean and standard deviation. In the first scenario, we will say that average is 3.14. But with the outlier, average soars to 59.71. This would change the estimate completely.\n\n\n> \n> The above meme makes you better understanding of outlier. \n\n Lets take a real world example. In a company of 50 employees, 45 people having monthly salary of Rs.6,000, 5 senior employees having monthly salary of Rs.100000 each. If you calculate the average monthly salary of employees in the company is Rs.14,500, which will give you the wrong conclusion (majority of employees have lesser than 14.5k salary). But if you take median salary, it is Rs.6000 which is more sense than the average.For this reason median is appropriate measure than mean. Here you can see the effect of outlier.\n    \n   \n Outlier  is a commonly used terminology by analysts and data scientists as it needs close attention else it can result in wildly wrong estimations. Simply speaking, Outlier is an observation that appears far away and diverges from an overall pattern in a sample.\n\nCause for outliers \n\n * Data Entry Errors:- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.  \n * Data Entry Errors:- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.  \n * Measurement Error:- It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty.  \n * Natural Outlier:- When an outlier is not artificial (due to error), it is a natural outlier. Most of real world data belong to this category. \n\nOutlier Detection \n\n Outlier can be of two types: Univariate and Multivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space.  \n\nDifferent outlier detection technique. \n\n 1. Hypothesis Testing  \n 2. Z-score method  \n 3. Robust Z-score \n 4. I.Q.R method  \n 5. Winsorization method(Percentile Capping)  \n 6. DBSCAN Clustering \n 7. Isolation Forest  \n 8. Visualizing the data \n\n1. Hypothesis Testing(grubbs test) \n\n$$\n\begin{array}{l}{\text { Grubbs' test is defined for the hypothesis: }} \\ {\begin{array}{ll}{\text { Ho: }}  {\text { There are no outliers in the data set }} \\ {\mathrm{H}_{\mathrm{1}} :}  {\text { There is exactly one outlier in the data set }}\end{array}}\end{array}\n$$\n$$\n\begin{array}{l}{\text {The Grubbs' test statistic is defined as: }} \\ {\qquad G_{calculated}=\frac{\max \left|X_{i}-\overline{X}\right|}{SD}} \\ {\text { with } \overline{X} \text { and } SD \text { denoting the sample mean and standard deviation, respectively. }} \end{array}\n$$\n$$\nG_{critical}=\frac{(N-1)}{\sqrt{N}} \sqrt{\frac{\left(t_{\alpha /(2 N), N-2}\right)^{2}}{N-2+\left(t_{\alpha /(2 N), N-2}\right)^{2}}}\n$$\n\n\begin{array}{l}{\text { If the calculated value is greater than critical, you can reject the null hypothesis and conclude that one of the values is an outlier }}\end{array}"
"2. Z-score method \n\n Using Z score method,we can find out how many standard deviations value away from the mean.  \n\n\n  Figure in the left shows area under normal curve and how much area that standard deviation covers.  \n * 68% of the data points lie between + or - 1 standard deviation.\n * 95% of the data points lie between + or - 2 standard deviation\n * 99.7% of the data points lie between + or - 3 standard deviation\n\n Z-score formula\n\n\begin{array}{l} {Z score=\frac{ X - Mean}{Standard Deviation}}  \end{array}\n\n If the z score of a data point is more than 3 (because it cover 99.7% of area), it indicates that the data value is quite different from the other values. It is taken as outliers.\n"
"8. Visualizing the data \n\nData visualization is useful for data cleaning, exploring data, detecting outliers and unusual groups, identifying trends and clusters etc. Here the list of data visualization plots to spot the outliers.  \n      \n1. Box and whisker plot (box plot).     \n2. Scatter plot.     \n3. Histogram.  \n4. Distribution Plot.     \n5. QQ plot.  \n"
"What Next?? \n\nAfter detecting the outlier we should remove\treat the outlier because it is a silent killer!! yes.  \n      \n* Outliers badly affect mean and standard deviation of the dataset. These may statistically give erroneous results.     \n* It increases the error variance and reduces the power of statistical tests.     \n* If the outliers are non-randomly distributed, they can decrease normality. \n* Most machine learning algorithms do not work well in the presence of outlier. So it is desirable to detect and remove outliers.     \n* They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions. \n  \nWith all these reasons we must be careful about outlier and treat them before build a statistical/machine learning model. There are some techniques used to deal with outliers. \n  \n1. Deleting observations.     \n2. Transforming values. \n3. Imputation.  \n4. Separately treating \n\nDeleting observations:  \nWe delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers. But deleting the observation is not a good idea when we have small dataset. "
"What Next?? \n\nAfter detecting the outlier we should remove\treat the outlier because it is a silent killer!! yes.  \n      \n* Outliers badly affect mean and standard deviation of the dataset. These may statistically give erroneous results.     \n* It increases the error variance and reduces the power of statistical tests.     \n* If the outliers are non-randomly distributed, they can decrease normality. \n* Most machine learning algorithms do not work well in the presence of outlier. So it is desirable to detect and remove outliers.     \n* They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions. \n  \nWith all these reasons we must be careful about outlier and treat them before build a statistical/machine learning model. There are some techniques used to deal with outliers. \n  \n1. Deleting observations.     \n2. Transforming values. \n3. Imputation.  \n4. Separately treating \n\nDeleting observations:  \nWe delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers. But deleting the observation is not a good idea when we have small dataset. "
"Transforming values: \n\nTransforming variables can also eliminate outliers. These transformed values reduces the variation caused by extreme values. \n   \n \n1. Scalling     \n2. Log transformation \n3. Cube Root Normalization  \n4. Box-Cox transformation \n\n    \n* These techniques convert values in the dataset to smaller values.     \n* If the data has to many extreme values or skewed, this method helps to make your data normal.     \n* But These technique not always give you the best results.  \n* There is no lose of data from these methods.     \n* In all these method boxcox transformation gives the best result. \n "
"*CAVEAT: Sorry but just note this notebook can be a bit slow to load probably due to the Plotly embeddings displaying a large number of points*\n\n#Introduction\n\nThere already exists a plethora of notebooks discussing the merits of dimensionality reduction methods, in particular the Big 3 of PCA (Principal Component Analysis), LDA ( Linear Discriminant Analysis) and TSNE ( T-Distributed Stochastic Neighbour Embedding). Quite a handful of these have compared one to the other but few have gathered all 3 in one go. Therefore this notebook will aim to provide an introductory exposition on these 3 methods as well as to portray their visualisations interactively and hopefully more intuitively via the Plotly visualisation library. The chapters are structuredas follows:\n\n 1. **Principal Component Analysis ( PCA )**  - Unsupervised, linear method\n\n\n 2. **Linear Discriminant Analysis (LDA)** - Supervised, linear method\n\n\n 3. **t-distributed Stochastic Neighbour Embedding (t-SNE)** - Nonlinear, probabilistic method\n\nLets go."
"**Curse of Dimensionality & Dimensionality Reduction**\n\nThe term ""Curse of Dimensionality"" has been oft been thrown about, especially when PCA, LDA and TSNE is thrown into the mix. This phrase refers to how our perfectly good and reliable Machine Learning methods may suddenly perform badly when we are dealing in a very high-dimensional space. But what exactly do all these 3 acronyms do? They are essentially transformation methods used for dimensionality reduction. Therefore, if we are able to project our data from a higher-dimensional space to a lower one while keeping most of the relevant information, that would make life a lot easier for our learning methods."
"Now having calculated both our Individual Explained Variance and Cumulative Explained Variance values, let's use the Plotly visualisation package to produce an interactive chart to showcase this."
*PLEASE CLICK AND MOVE THE SCATTER PLOTS ABOVE. THEY ARE INTERACTIVE. DOUBLE CLICK TO GET BACK TO THE ORIGINAL VIEW*
"**Visualising the MNIST Digit set on its own**\n\nNow just for the fun and curiosity of it, let's plot the actual MNIST digit set to see what the underlying dataset actually represents, rather than being caught up with just looking at 1 and 0's."
"Phew, they are definitely digits all right. So let's proceed onto the main event."
"###Interactive visualisations of PCA representation\n\nWhen it comes to these dimensionality reduction methods, scatter plots are most commonly implemented because they allow for great and convenient visualisations of clustering ( if any existed ) and this will be exactly what we will be doing as we plot the first 2 principal components as follows:"
*PLEASE CLICK AND MOVE THE SCATTER PLOTS ABOVE. THEY ARE INTERACTIVE. DOUBLE CLICK TO GET BACK TO THE ORIGINAL VIEW*
"###K-Means Clustering to identify possible classes\n\nImagine just for a moment that we were not provided with the class labels to this digit set because after all PCA is an unsupervised method. Therefore how would we be able to separate out our data points in the new feature space? We can apply a clustering algorithm on our new PCA projection data and hopefully arrive at distinct clusters which would tell us something about the underlying class separation in the data. \n\nTo start off, we set up a KMeans clustering method with Sklearn's *KMeans* call and use the *fit_predict* method to compute cluster centers and predict cluster indices for the first and second PCA projections (to see if we can observe any appreciable clusters)."
"**Takeaway from the Plot**\n\nVisually, the clusters generated by the KMeans algorithm appear to provide a clearer demarcation amongst clusters as compared to naively adding in class labels into our PCA projections. This should come as no surprise as PCA is meant to be an unsupervised method and therefore not optimised for separating different class labels. This particular task however is accomplished by the very next method that we will talk about."
###Interactive visualisations of LDA representation
*PLEASE CLICK AND MOVE THE SCATTER PLOTS ABOVE. THEY ARE INTERACTIVE. DOUBLE CLICK TO GET BACK TO THE ORIGINAL VIEW*
Having invoked the t-SNE algorithm by simply calling *TSNE()* we fit the digit data to the model and reduce its dimensions with *fit_transform*. Finally let's plot the first two components in the new feature space in a scatter plot
*PLEASE CLICK AND MOVE THE SCATTER PLOTS ABOVE. THEY ARE INTERACTIVE. DOUBLE CLICK TO GET BACK TO THE ORIGINAL VIEW*
"## Risk Assesment:\nThe main aim in this section is to compare the average interest rate for the loan status belonging to each type of loans (Good loan or bad loan) and see if there is any significant difference in the average of interest rate for each of the groups.\n\n## Summary: \n\n  Bad Loans:   Most of the loan statuses belonging to this group pay a interest ranging from 15% - 16%. \nGood Loans: Most of the loan statuses belonging to this group pay interest ranging from 12% - 13%.  \nThere has to be a better assesment of risk since there is not that much of a difference in interest payments from Good Loans and Bad Loans. \n Remember, most loan statuses are Current so there is a risk that at the end of maturity some of these loans might become bad loans. \n\n\n\n\n*Credits to Zhiwen for providing an important aspect of the analysis (Relationship of interest rates and loan condition).*"
"## Condition of Loans and Purpose:\n\nIn this section we will go into depth regarding the reasons for clients to apply for a loan.  Our main aim is to see if there are purposes that contribute to a  ""higher""  risk whether the loan will be repaid or not.\n\n### Summary: \n\n Bad Loans Count:  People that apply for educational and small business purposed tend to have a higher risk of being a bad loan. (% wise) \nMost frequent Purpose:  The reason that clients applied the most for a loan was to consolidate debt. \nLess frequent purpose: Clients applied less for educational purposes for all three income categories.  \nInterest Rates:  In all reasons for application except (medical, small business and credi card), the low income category has a higher interest rate. Something that could possibly explain this is the amount of capital that is needed from other income categories that might explain why the low income categories interest rate for these puposes are lower.  \nBad/Good Ratio: Except for educational purposes (we see a spike in high income this is due to the reasons that only two loans were issued and one was a bad loan which caused this ratio to spike to 50%.), but we can see that in all other purposed the bad good ratio is lower the higher your income category.  \n\n"
## Data Visualization\n* Box and density plots
## Data Transformation\n* Skew correction
## Data Transformation\n* Skew correction
## Data Interaction\n* Correlation
## Data Interaction\n* Scatter plot
## Data Visualization\n* Categorical attributes
## Data Visualization\n* Categorical attributes
##Data Preparation\n* One Hot Encoding of categorical data
"## Evaluation, prediction, and analysis\n* Linear Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* Ridge Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* Ridge Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* LASSO Linear Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* LASSO Linear Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* Elastic Net Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* Elastic Net Regression (Linear algo)"
"## Evaluation, prediction, and analysis\n* KNN (non-linear algo)"
"## Evaluation, prediction, and analysis\n* KNN (non-linear algo)"
"## Evaluation, prediction, and analysis\n* CART (non-linear algo)"
"## Evaluation, prediction, and analysis\n* CART (non-linear algo)"
"## Evaluation, prediction, and analysis\n* SVM (Non-linear algo)"
"## Evaluation, prediction, and analysis\n* SVM (Non-linear algo)"
"## Evaluation, prediction, and analysis\n* Bagged Decision Trees (Bagging)"
"## Evaluation, prediction, and analysis\n* Bagged Decision Trees (Bagging)"
"## Evaluation, prediction, and analysis\n* Random Forest (Bagging)"
"## Evaluation, prediction, and analysis\n* Random Forest (Bagging)"
"## Evaluation, prediction, and analysis\n* Extra Trees (Bagging)"
"## Evaluation, prediction, and analysis\n* Extra Trees (Bagging)"
"## Evaluation, prediction, and analysis\n* AdaBoost (Boosting)"
"## Evaluation, prediction, and analysis\n* AdaBoost (Boosting)"
"## Evaluation, prediction, and analysis\n* Stochastic Gradient Boosting (Boosting)"
"## Evaluation, prediction, and analysis\n* Stochastic Gradient Boosting (Boosting)"
"## Evaluation, prediction, and analysis\n* XGBoost"
"## Evaluation, prediction, and analysis\n* XGBoost"
"## Evaluation, prediction, and analysis\n* MLP (Deep Learning)"
"## Evaluation, prediction, and analysis\n* MLP (Deep Learning)"
## Make Predictions
"Is Spending $$$ for MS in Data Science worth it ?\nA detailed comparative analysis of people with and without university degrees for data science\n\n\n2019 was an important year for me, Not only I got engaged this year but also I completed my higher education degree. After working for several years in the industry I decided to take a short break, go back to academics and pursue higher education. No doubt, it was one of the best learning experiences I had but there was also a huge investment of time and money. Many people often contact me regularly asking about my experience and viewpoint about such degrees. They ask questions like - whether is it worth spending huge chunks of money for such degrees? Well, there is no fixed answer for such questions because every individual have a different viewpoint and their opinion might be biased. The best way to answer this question is to make use of data, analyse the cohorts of people (example - data scientists) who are well settled in the industry, measure and compare if there are any significant differences in their roles, position, responsibilities, and annual compensation. This type of analysis can provide many interesting insights and help in looking at the broader view of the scenario. In this notebook, I decided to take a stab at this scenario and have shared my experience along with the key insights and a detailed analysis of Kaggle's annual data science survey data. \n\n\nSource: Upslash  \n\nThere are one set of people who wants to pursue higher education degrees due to their passion and interest. For these people, it makes sense to get enrol in the relevant university courses and pursue their passion. On the other hand, there is another set of people, who wants to obtain these degrees only to get a specific job title, a specific job role, or a position that get them more money. For this set, university degrees are not the only option, there are many alternatives which can also result in the same outcomes. \n\nThe most obvious example is in the field of Data Science and Analytics. In recent years, university degrees such as ""Masters in Data Science"" or ""Masters in Analytics"" are sought as one of the must-haves to enter into this field. It is not astonishing that Data Scientist is one of the fastest-growing job titles across the globe and the demand for skilled data scientists is increasing. This has given the universities an option to attract students and make immense money. Several universities have started dedicated degree courses specialized in data science and analytics. Those want to become a data scientist or to switch from another profession to data science profession are now strongly considering these university degrees as the only pathway. \n\nBut these university courses are not easy to get in and affordable for everyone. These degrees don’t come for free, tuition fees can be exorbitant and can range anything from USD 30,000 to USD 100,000. And that doesn’t include the actual cost of living. Many consider applying for student loans but they add a huge lump sum to the existing mountain of debts. A common myth is that the earning potential for those with postgraduate qualifications is higher but of course, there is no guarantee that one will get a stable job at the end of it. Additionally, Pursuing a university’s higher degree takes anything from one to three years, depending on different factors. This can seem like a long time, especially when the fellow peers are getting started on their careers, while one is still studying. \n\nThe question of interest here is - ** does one need to get that expensive higher education degree**, **do they create a difference from those who do not have university degrees?**. Some resources online also suggest that one can get the depth of knowledge, variety of skills and learn something new. But again, **is it possible to get the same skills, same profile, or even better compensation without such degrees?**.\n\n\n\nKaggle conducted their Annual Data Science survey and it was full of interesting questions. Participants of this survey were asked different questions about their demographics, profiles, companies, what they use etc. I analysed this data intending to dig deeper into the profiles of people who completed the university degrees to learn data science and those who did not. The focus of the story in this notebook is to identify if the working data scientists with official higher education degrees differ significantly from the other group. The analysis and storyline are segmented according to different factors.    \n\n*Note* - For the analysis, I removed the respondents who were ""students"" and ""not employed"". The two groups were selected based on the respondent's choice if they completed the university degrees to learn data science or not.  \n\nContents\n\n1. Sources of Learning Data Science  \n    - Why People Choose Higher Education Degrees   \n    - The Academic Landscape : Masters in Data Science Degrees    \n2. Proportion of Individuals with University Degrees  \n3. Are there a Significant Differences - With and Without University Degrees ?   \n    3.1 Compensation   \n        - Key Characteristics : Data Scientists earning > USD 150K   \n    3.2 Job Roles   \n    3.3 Job Profiles   \n        - Other Tools : Usage and Comparison   \n4. Identifying Key Traits   \n5. Conclusions   \n6. References   \n\n\n1. Sources of Learning Data Science            \n\nData science skills are straightforward to obtain, they need experience and learning. Nowadays there are many online and offline which teaches them in detail. While some prefer online courses such as Coursera or Udacity, some prefer to go to universities for a year or two-year long dedicated courses. Let's look at what are the most popular sources of learning data science among the respondents of the kaggle survey. In this question, one participant could have chosen multiple choices, hence the x-axis represents ""percentage"" of respondents who selected a particular choice. "
"- About 44% of the respondents selected **""Coursera""** as the primary source of learning data science. Coursera is the popular online learning platform which provides both free courses and paid specializations.   \n- Coursera and its founder [Andrew NG](https://en.wikipedia.org/wiki/Andrew_Ng) have made very significant contributions to the data science revolution. Back in 2012, Andrew NG released the very popular Machine Learning course which became the first choice for many to learn data science. In 2017, **[Deeplearning.ai](https://www.deeplearning.ai/)** was launched and it became very popular data science specialization. These courses and many others from well known academic names on the online platform makes Coursera as the primary choice among the data science enthusiasts. \n- The search results for Coursera data science page says that there are **1054 courses about Data Science** as of Dec 2019.  \n\n\n\n- Kaggle Learn was selected by almost one-fourth of the respondents. Kaggle team launched these courses somewhere around early 2017. They are composed of notebook style materials which not only focusses on teaching the concepts but also the programming part as well. \n- Then there are other sources such as Udemy, Udacity, edX, fast.ai etc. These platforms als provide online materials and courses to learn data science.  \n\n- Then there is a group of individuals who prefer to go to a university to pursue higher education degrees. Among the survey participants, about **one-fifth of the participants** had completed university degrees. Accoding to multiple sources ( [Masters-And-More](https://www.master-and-more.eu/en/7-reasons-why-you-should-choose-a-masters-degree/), [Uniplaces](https://blog.uniplaces.com/discover/8-good-reasons-apply-masters-degree/), [CareerAdditct](https://www.careeraddict.com/masters-degree-benefits), [MyBaggage](https://www.mybaggage.com/blog/why-do-a-masters-degree-the-pros-and-cons/) ) different individuals have many different reasons to choose university courses over online courses. The most common are:\n\n    - Personal Goal\n    - Better Salary\n    - Gain More Knowledge\n    - Better Job Roles\n    - Career Change  \n\n\nDesigning My Own Survey : Why People Choose Higher Education Degrees\n\nMy personal reason for engaging in a higher education degree was majorly driven by passion and interest, it was a personal goal of mine to get another degree after the bachelors. But I was curious to know why other individuals decide to pursue a higher education degree. I created my own survey to know the student's choices this question and shared it in several groups associated with National University of Singapore. I managed to get about 120 responses from different people for this survey. Following is the response distribution of the Question.\n\nLink of the Survey: [Survey](https://docs.google.com/forms/d/19ov_EmBVdwc70tPUoylm8gPNz2bK4j21UAirDcQHB_Q) (Note : Names are removed to maintain the privacy of the individuals)      \nLink of the Responses: [Responses](https://docs.google.com/spreadsheets/d/1lRd2yLNJV6svqllrNDUcZHHYc5X20BswxEhGN89B4i8/edit?usp=sharing)   "
"- About 44% of the respondents selected **""Coursera""** as the primary source of learning data science. Coursera is the popular online learning platform which provides both free courses and paid specializations.   \n- Coursera and its founder [Andrew NG](https://en.wikipedia.org/wiki/Andrew_Ng) have made very significant contributions to the data science revolution. Back in 2012, Andrew NG released the very popular Machine Learning course which became the first choice for many to learn data science. In 2017, **[Deeplearning.ai](https://www.deeplearning.ai/)** was launched and it became very popular data science specialization. These courses and many others from well known academic names on the online platform makes Coursera as the primary choice among the data science enthusiasts. \n- The search results for Coursera data science page says that there are **1054 courses about Data Science** as of Dec 2019.  \n\n\n\n- Kaggle Learn was selected by almost one-fourth of the respondents. Kaggle team launched these courses somewhere around early 2017. They are composed of notebook style materials which not only focusses on teaching the concepts but also the programming part as well. \n- Then there are other sources such as Udemy, Udacity, edX, fast.ai etc. These platforms als provide online materials and courses to learn data science.  \n\n- Then there is a group of individuals who prefer to go to a university to pursue higher education degrees. Among the survey participants, about **one-fifth of the participants** had completed university degrees. Accoding to multiple sources ( [Masters-And-More](https://www.master-and-more.eu/en/7-reasons-why-you-should-choose-a-masters-degree/), [Uniplaces](https://blog.uniplaces.com/discover/8-good-reasons-apply-masters-degree/), [CareerAdditct](https://www.careeraddict.com/masters-degree-benefits), [MyBaggage](https://www.mybaggage.com/blog/why-do-a-masters-degree-the-pros-and-cons/) ) different individuals have many different reasons to choose university courses over online courses. The most common are:\n\n    - Personal Goal\n    - Better Salary\n    - Gain More Knowledge\n    - Better Job Roles\n    - Career Change  \n\n\nDesigning My Own Survey : Why People Choose Higher Education Degrees\n\nMy personal reason for engaging in a higher education degree was majorly driven by passion and interest, it was a personal goal of mine to get another degree after the bachelors. But I was curious to know why other individuals decide to pursue a higher education degree. I created my own survey to know the student's choices this question and shared it in several groups associated with National University of Singapore. I managed to get about 120 responses from different people for this survey. Following is the response distribution of the Question.\n\nLink of the Survey: [Survey](https://docs.google.com/forms/d/19ov_EmBVdwc70tPUoylm8gPNz2bK4j21UAirDcQHB_Q) (Note : Names are removed to maintain the privacy of the individuals)      \nLink of the Responses: [Responses](https://docs.google.com/spreadsheets/d/1lRd2yLNJV6svqllrNDUcZHHYc5X20BswxEhGN89B4i8/edit?usp=sharing)   "
"- Most of the people decided to pursue higher education degrees to gain more knowledge. This was the primary reason for about 70% of the individuals who were part of this survey.  Every two out of five people decided to pursue university degrees to get better salaries or to change their professions. Only about one-fourth of individuals had their own personal goal to go for a university degree. \n- If we just look at the number of individuals who selected ""gain more knowledge"", the same but important question arises again: **""Is it worth spending a huge amount of money to gain knowledge that one can get from free sources?""** Arent' the free online sources good enough to gain more knowledge. Or, Can't these sources provide enough skills and knowledge to get that better salary, better job roles, or provide a pathway for a career change.   \n- The interesting fact to note that most of the online courses on Coursera, Udemy etc. are also from the same universities or the same professors. Many of them are free as well. So if ""gaining more knowledge"" is the only goal then it is worth considering these free courses. \n\n\nThe Academic Landscape : Masters in Data Science Degrees\n\nWhatever be the debate but one point is definitely clear, Universities across the globe do benefit a lot from this increased interest in higher education degrees. Many universities have now started specialized masters degree programs in analytics, data science, business analytics etc. The following chart shows some of the popular master's degree programmes from US universities along with their tuition fee and duration. Some of them are provided online, but the same are also provided on campus.  \n\nSource of Data : https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html"
"- Most of the people decided to pursue higher education degrees to gain more knowledge. This was the primary reason for about 70% of the individuals who were part of this survey.  Every two out of five people decided to pursue university degrees to get better salaries or to change their professions. Only about one-fourth of individuals had their own personal goal to go for a university degree. \n- If we just look at the number of individuals who selected ""gain more knowledge"", the same but important question arises again: **""Is it worth spending a huge amount of money to gain knowledge that one can get from free sources?""** Arent' the free online sources good enough to gain more knowledge. Or, Can't these sources provide enough skills and knowledge to get that better salary, better job roles, or provide a pathway for a career change.   \n- The interesting fact to note that most of the online courses on Coursera, Udemy etc. are also from the same universities or the same professors. Many of them are free as well. So if ""gaining more knowledge"" is the only goal then it is worth considering these free courses. \n\n\nThe Academic Landscape : Masters in Data Science Degrees\n\nWhatever be the debate but one point is definitely clear, Universities across the globe do benefit a lot from this increased interest in higher education degrees. Many universities have now started specialized masters degree programs in analytics, data science, business analytics etc. The following chart shows some of the popular master's degree programmes from US universities along with their tuition fee and duration. Some of them are provided online, but the same are also provided on campus.  \n\nSource of Data : https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html"
"Time and money are the two biggest investments associated with university degrees. The graph shows that the tuition fee for most of these courses is not cheap and the duration can range from anywhere 1 to 3 years depending upon specialization, location, and university type. \n\nThe university courses are of two types: Generic courses and Specializations. Generic courses are typically very comprehensive, they cover all parts of data science but they are not very deep and detailed. These type of courses are good for those who want to get acquainted with main elements of this field. The specializations, on the other hand, aim to cover every possible detail of one particular area. They are generally very deep. For both types of courses, the investment of money and time are always higher as compared to the alternative free ones.  Additionally, these courses are never meant to teach everything and do the spoon-feeding. They are more like the guided paths, and it is mostly the self-effort along that path which makes the students learn. If guided correctly, even through the non-degree courses (such as the ones on coursera or kaggle learn etc), one may also get the same outcomes.\n\nHead of Data Science from Restaurant Technologies, Inc. [shared](https://www.kdnuggets.com/2014/06/masters-degree-become-data-scientist.html), ""No single Masters Program could cover all the disciplines needed in significant depth for one to be an expert in all these areas. Selecting an area or two or three and having depth and expertise in those is common. Many companies do not have just a ""Data Scientist"" but teams comprised of experts from the different disciplines.""   \n\n\n2. Proportion of Individuals with University Degrees   \n\nLet's look at what per cent of individuals completed their university degrees to become a data scientist across different countries. Respondents were asked about their country in one of the questions. "
"Time and money are the two biggest investments associated with university degrees. The graph shows that the tuition fee for most of these courses is not cheap and the duration can range from anywhere 1 to 3 years depending upon specialization, location, and university type. \n\nThe university courses are of two types: Generic courses and Specializations. Generic courses are typically very comprehensive, they cover all parts of data science but they are not very deep and detailed. These type of courses are good for those who want to get acquainted with main elements of this field. The specializations, on the other hand, aim to cover every possible detail of one particular area. They are generally very deep. For both types of courses, the investment of money and time are always higher as compared to the alternative free ones.  Additionally, these courses are never meant to teach everything and do the spoon-feeding. They are more like the guided paths, and it is mostly the self-effort along that path which makes the students learn. If guided correctly, even through the non-degree courses (such as the ones on coursera or kaggle learn etc), one may also get the same outcomes.\n\nHead of Data Science from Restaurant Technologies, Inc. [shared](https://www.kdnuggets.com/2014/06/masters-degree-become-data-scientist.html), ""No single Masters Program could cover all the disciplines needed in significant depth for one to be an expert in all these areas. Selecting an area or two or three and having depth and expertise in those is common. Many companies do not have just a ""Data Scientist"" but teams comprised of experts from the different disciplines.""   \n\n\n2. Proportion of Individuals with University Degrees   \n\nLet's look at what per cent of individuals completed their university degrees to become a data scientist across different countries. Respondents were asked about their country in one of the questions. "
"- In the American continent, the USA and Canada are the two most sought places to pursue higher education degrees. About **one-fourth of the respondents** from these countries have completed their university degrees to becoming a data scientist. In the United States, about 27% of the individuals who were part of this survey completed their university degrees. In Europe, there are about one-fifth of respondents who completed their university degrees while in Asia, the percentage is a bit lower only about 15%.  \n- Countries with the highest proportion of data scientists with university degrees are **'Tunisia', 'Austria', 'New Zealand' and 'Greece'** with **over 40% of the individuals** completing university degrees. On the other hand, countries **'Japan', 'Nigeria', 'Belarus', and 'Algeria'** shows a lower number (less than 10%) of individuals completing university degrees.   \n- Among the genders, female respondents have a higher number for completing university degrees than male respondents. **23% of the female respondents** and 20% of the male respondents completed their university degrees.    \n\n\n3. Are there Significant Differences - With or Without University Degrees ? \n\nAccording to Forbes, Most people with data science job titles don’t have these new degrees. I also looked at the profiles of a few data scientist in my Linkedin Network and observed that not all of them have data science degrees. People tend to take different paths - some have degrees in business, economics, maths etc, while some have specialed data science degrees, and some have no university degree. But all of them are working in good organizations with good job roles. In the next section, let's look at the key insights from the Survey Data Analysis. The focus of the analysis is to compare the two groups -  individuals who completed university degree vs those without for becoming a data scientist and identify key differences (if any).\n\nMainly, We will look at three perspectives: Are there a fairly equal percentage of individuals from two groups:\n\n1. With every compensation bracket.   \n2. For each type of job role or activity.   \n3. For each type of activity, they do daily.   \n\n\n3.1 Compensation  "
"- In the American continent, the USA and Canada are the two most sought places to pursue higher education degrees. About **one-fourth of the respondents** from these countries have completed their university degrees to becoming a data scientist. In the United States, about 27% of the individuals who were part of this survey completed their university degrees. In Europe, there are about one-fifth of respondents who completed their university degrees while in Asia, the percentage is a bit lower only about 15%.  \n- Countries with the highest proportion of data scientists with university degrees are **'Tunisia', 'Austria', 'New Zealand' and 'Greece'** with **over 40% of the individuals** completing university degrees. On the other hand, countries **'Japan', 'Nigeria', 'Belarus', and 'Algeria'** shows a lower number (less than 10%) of individuals completing university degrees.   \n- Among the genders, female respondents have a higher number for completing university degrees than male respondents. **23% of the female respondents** and 20% of the male respondents completed their university degrees.    \n\n\n3. Are there Significant Differences - With or Without University Degrees ? \n\nAccording to Forbes, Most people with data science job titles don’t have these new degrees. I also looked at the profiles of a few data scientist in my Linkedin Network and observed that not all of them have data science degrees. People tend to take different paths - some have degrees in business, economics, maths etc, while some have specialed data science degrees, and some have no university degree. But all of them are working in good organizations with good job roles. In the next section, let's look at the key insights from the Survey Data Analysis. The focus of the analysis is to compare the two groups -  individuals who completed university degree vs those without for becoming a data scientist and identify key differences (if any).\n\nMainly, We will look at three perspectives: Are there a fairly equal percentage of individuals from two groups:\n\n1. With every compensation bracket.   \n2. For each type of job role or activity.   \n3. For each type of activity, they do daily.   \n\n\n3.1 Compensation  "
"The plot shows the percentage of respondents from the United States of America in each compensation bracket. Looking at every bucket, it is clear that there are no significant differences between the two groups. Approximately they differ by a few per cent (less than 5). However, a few sections in this chart are very interesting. \n\n- A common belief about university degrees is that one get higher compensation. The chart shows that there is a large percentage of individuals **without a university degree also earning more than 100K USD**. Even without university degrees, if individuals manage to obtain the right skills and the right direction, one can also grab high compensation. That's where Kaggle Learn or Coursera are the best options. As they provide the pathway to get the appropriate skills required to become a data scientist.\n- There are slight differences when the compensation is less than 125K. For this range, there is a higher percentage of individuals having university degrees. The area is shown in the green section in the chart. Well, this compensation range is the average of most of the companies in USA. This means that university degrees in data science can give an initial boost to the candidates in the compensation.  \n- Very Interesting to note that there are more percentage of respondents without university degrees than those who have who are earning in the range of USD 150K-300K. This area is highlighted in red. This implies that there are definately ways to get higher compensation not necessarily after obtaining a masters in data science degree. The obvious reasons can be the amount of experience, age group, or special talent. It will be interesting to specifically look into this cohort where data scientists earn >150K USD. This is analysed in the next section.\n\n\nKey Characteristics : Data Scientists earning > $150K  \n\nThe following graph shows the key characteristics: age distribution, coding experience (in years) etc. for Data Scientist earning more than 150K USD. "
"The plot shows the percentage of respondents from the United States of America in each compensation bracket. Looking at every bucket, it is clear that there are no significant differences between the two groups. Approximately they differ by a few per cent (less than 5). However, a few sections in this chart are very interesting. \n\n- A common belief about university degrees is that one get higher compensation. The chart shows that there is a large percentage of individuals **without a university degree also earning more than 100K USD**. Even without university degrees, if individuals manage to obtain the right skills and the right direction, one can also grab high compensation. That's where Kaggle Learn or Coursera are the best options. As they provide the pathway to get the appropriate skills required to become a data scientist.\n- There are slight differences when the compensation is less than 125K. For this range, there is a higher percentage of individuals having university degrees. The area is shown in the green section in the chart. Well, this compensation range is the average of most of the companies in USA. This means that university degrees in data science can give an initial boost to the candidates in the compensation.  \n- Very Interesting to note that there are more percentage of respondents without university degrees than those who have who are earning in the range of USD 150K-300K. This area is highlighted in red. This implies that there are definately ways to get higher compensation not necessarily after obtaining a masters in data science degree. The obvious reasons can be the amount of experience, age group, or special talent. It will be interesting to specifically look into this cohort where data scientists earn >150K USD. This is analysed in the next section.\n\n\nKey Characteristics : Data Scientists earning > $150K  \n\nThe following graph shows the key characteristics: age distribution, coding experience (in years) etc. for Data Scientist earning more than 150K USD. "
"- The chart shows that there is a considerable number of individuals in each bracket of the machine learning experience, coding experience, and age. The charts are also does not shows any skewness in a particular bracket. \n- With more number of years in experience for coding and machine learning (greater than 5), We see that relatively a higher percentage of people are there earning more than 150K without university degrees.   \n- More number of individuals who are aged less than 34 years and with a university degree earn greater than 150K. Good to see that even more percentage of young individuals, aged 22-24 also get higher compensation in the USA.  \n- The portion on the right side of the age graph shows that experienced people may not need to get university degrees in data science if they are only looking for better salaries.  \n- These trends are also similar in other countries. According to [many](https://www.mastersportal.com/articles/2608/top-9-countries-with-the-highest-investments-in-university-education.html) links, top countries for higher education university degrees are the USA, Germany, Canadas etc. "
- We see similar insights for these countries as the USA where more percentage of non degree holder respondents are earning higher compensation in many brackets. \n\n\n3.2 Job Roles  \n\nNext we look at the job roles of individuals. Does it make a difference in terms of what kind of activity the data scientists do on the daily basis if they are coming with a university degree as compared to without. 
"The two plots capture two different pieces of information: \n- Plot A shows what percentage of respondents selected a particular activity which people do in their day to day activities. This plot shows which are the most common activities people do in their data science project and the difference between a degree and non-degree holders. \n- Plot B shows, out of all the selected choices by all the respondents, what percentage is a particular responsibility is selected. This plot shows, for a particular group (degree or non-degree holders), which activity has more importance. For example, If a particular 'responsibility' is rarely selected, then the overall percentage will be smaller and If it is selected most of the times, then its overall percentage will also be higher. Meaning that it will be important to the particular group - degree or non-degree holders. \n\nFrom these two plots, we can observe that:\n\n- Plot A shows that a higher percentage of individuals with a university degree are involved in different tasks. The biggest differences are observed for data scientists who do ""Data Analysis or Exploration"", and performing ""Research and developing State of the art models"". Since experimentation is always a big part of any data science project, there exists very less differences in the % of respondents of two groups.     \n- The percentage of non-degree holder respondents is always lesser than the counterpart, however, the differences are not extreme. There is still a significant percentage of people who are involved in similar responsibilities as the university holders. Hence, it will be wrong to say that only university holders do a particular type of activities or have specialized activities.   \n- Plot B shows a higher percentage of people with university degrees selected being involved in **Research work, Data Analysis, and Building/Running Infrastructures**. While a slightly more percentage of people without university degrees selected being involved in Experimentation and Building Machine Learning services.  \n- A fairly equal percentage is observed for people who build Machine Learning service despite their degrees. This particular selection shows that there is no major difference in the kind of work a data scientist will do.   "
"The two plots capture two different pieces of information: \n- Plot A shows what percentage of respondents selected a particular activity which people do in their day to day activities. This plot shows which are the most common activities people do in their data science project and the difference between a degree and non-degree holders. \n- Plot B shows, out of all the selected choices by all the respondents, what percentage is a particular responsibility is selected. This plot shows, for a particular group (degree or non-degree holders), which activity has more importance. For example, If a particular 'responsibility' is rarely selected, then the overall percentage will be smaller and If it is selected most of the times, then its overall percentage will also be higher. Meaning that it will be important to the particular group - degree or non-degree holders. \n\nFrom these two plots, we can observe that:\n\n- Plot A shows that a higher percentage of individuals with a university degree are involved in different tasks. The biggest differences are observed for data scientists who do ""Data Analysis or Exploration"", and performing ""Research and developing State of the art models"". Since experimentation is always a big part of any data science project, there exists very less differences in the % of respondents of two groups.     \n- The percentage of non-degree holder respondents is always lesser than the counterpart, however, the differences are not extreme. There is still a significant percentage of people who are involved in similar responsibilities as the university holders. Hence, it will be wrong to say that only university holders do a particular type of activities or have specialized activities.   \n- Plot B shows a higher percentage of people with university degrees selected being involved in **Research work, Data Analysis, and Building/Running Infrastructures**. While a slightly more percentage of people without university degrees selected being involved in Experimentation and Building Machine Learning services.  \n- A fairly equal percentage is observed for people who build Machine Learning service despite their degrees. This particular selection shows that there is no major difference in the kind of work a data scientist will do.   "
"- Based on the number of job responsibilities, a data scientist can be classified into two categories - Generalist and Specialist. Generalist are the individuals involved in all parts of life cycle of a data science project (ie. the points on the extereme right). Specialists are the individuals who are focussed on hardly 1 or 2 job responsibilities, ie. (points in the left). \n- We see that slightly more percentage of generalists are there having university degrees. They are they people who are involved in every step - doing analysis, experimentation, building services, and also research. In general, the plot shows that more percentage of people who completed university degrees are involved in multiple responsibilities. \n\n\n3.3 Job Profiles - What type of tools/techniques are used ? \n\nAnother important point in the set of common beliefs is that the job profile of those who completed their university degrees is very different from those who did not. Mainly the differences are considered in terms of tools, technologies, techniques used on day to day basis. Let's analyse from the kaggle survey data and identify are there really major differences in what type of tools and techniques are used by people with or without university degrees (or self made data scientists as compared to those who obtained university degrees). "
"- Based on the number of job responsibilities, a data scientist can be classified into two categories - Generalist and Specialist. Generalist are the individuals involved in all parts of life cycle of a data science project (ie. the points on the extereme right). Specialists are the individuals who are focussed on hardly 1 or 2 job responsibilities, ie. (points in the left). \n- We see that slightly more percentage of generalists are there having university degrees. They are they people who are involved in every step - doing analysis, experimentation, building services, and also research. In general, the plot shows that more percentage of people who completed university degrees are involved in multiple responsibilities. \n\n\n3.3 Job Profiles - What type of tools/techniques are used ? \n\nAnother important point in the set of common beliefs is that the job profile of those who completed their university degrees is very different from those who did not. Mainly the differences are considered in terms of tools, technologies, techniques used on day to day basis. Let's analyse from the kaggle survey data and identify are there really major differences in what type of tools and techniques are used by people with or without university degrees (or self made data scientists as compared to those who obtained university degrees). "
"- In terms of machine learning models and techniques, a higher percentage of university degree holders use deep learning approaches. This include **Bayesian approaches, General adversarial networks, and neural networks**. This is likely because of the recent interest and developments in deep learning, universities have also added courses with a lot of focus on deep learning. There are specialized courses on neural networks, deep learning, and even GANs. However, it is worth mentioning here that the cost of the specialized courses is relatively higher than the other courses due to extra credits. If anyone wants to just learn these skills, one can obtain them from resources available on the internet.     \n- A slightly higher percentage of individuals without university degrees use simple linear models such as logistic regression models. This is because in most of the companies, data science projects always start with simple models and they tend to work very well. \n- In terms of specific machine learning models, the two groups do not show any major differences. This is likely because everyone who starts with data science, they at least try to get a taste of different machine learning libraries and techniques. "
"- In terms of machine learning models and techniques, a higher percentage of university degree holders use deep learning approaches. This include **Bayesian approaches, General adversarial networks, and neural networks**. This is likely because of the recent interest and developments in deep learning, universities have also added courses with a lot of focus on deep learning. There are specialized courses on neural networks, deep learning, and even GANs. However, it is worth mentioning here that the cost of the specialized courses is relatively higher than the other courses due to extra credits. If anyone wants to just learn these skills, one can obtain them from resources available on the internet.     \n- A slightly higher percentage of individuals without university degrees use simple linear models such as logistic regression models. This is because in most of the companies, data science projects always start with simple models and they tend to work very well. \n- In terms of specific machine learning models, the two groups do not show any major differences. This is likely because everyone who starts with data science, they at least try to get a taste of different machine learning libraries and techniques. "
"- Most common it is advisable to know and use about three techniques or models, The percentage of such people is higher for degree holders. Similar is the case with several machine learning algorithms, more percentage of people are exposed to more techniques/algorithms when they have university degrees.  "
- We observe that there is more usage of Google Colab and Google Cloud Notebooks among the university degree holders than the self-made data scientists. This aligns with the more acquaintance with deep learning techniques and models of degree holders that we observed in the last section. Mainly the data scientists without any degrees have used kaggle notebooks. These are of course one of the best resources to learn and practice data science. Also completely free to use.   \n- University holders seem to be more R users as a higher percentage is observed for Rstudio. Self-made scientists prefer Python over R as Jupyter has slightly more percentage of people without degrees.  \n- Again we observe that there are no major gaps in the usage patterns of individuals with university degrees and those who became scientists on their own. Kaggle learn has both R and Python free courses which are great. 
"\nOther Tools : Usage and Comparison\n\nThe following graphs summarize the usage patterns of other tools, techniques, databases, platforms, and frameworks used by individuals of both the groups - self made data scientists and the ones with university degrees.  \n\n1. Usage Patterns of data scientists with University Degrees: "
"\nOther Tools : Usage and Comparison\n\nThe following graphs summarize the usage patterns of other tools, techniques, databases, platforms, and frameworks used by individuals of both the groups - self made data scientists and the ones with university degrees.  \n\n1. Usage Patterns of data scientists with University Degrees: "
2. Usage Patterns of different tools and frameworks of self made data scientists: 
2. Usage Patterns of different tools and frameworks of self made data scientists: 
"\n4. Identifying Key Traits \n\nLet's look at what are the key characteristics of individuals who prefer to go to university rather taking the other path. In this particular task, we aim to identify what are the most important signals strongly related to university degree holders. To identify these signals, we will treat this task as a predictive modelling problem. The first step is to prepare dataset which includes creating the train and test sets. Next, a simple learning classifier will be trained on the dataset the features are information about the individuals (demographics, tools used, company info etc.) and the target determines if the individual completed the university degree or not. All of the features and the target are binary. After the model is trained with a decent evaluation metric score, the important features of the model are obtained using Permutation Importance. These features provide some sense about their importance with an order or ranking. "
"The classifier shows a decent accuracy score of about 80%, let's now look at the permutation importance of every variable. "
"The chart shows a few interesting traits of people associated with university degrees: More use of R than Python, More focus on research and deep learning, mainly young individuals (aged 22-30), involved in coding for at least a year, not doing kaggle or MOOCs. \n\n\nConclusions  \n\nThe main focus of this analysis was to understand and evaluate if there are any significant differences in the self-taught scientists and university degree holders. The insights presented in this notebook, convey a broad message about the importance of ""masters in data science degrees"". We can observe that it is not impossible to achieve certain job roles or position and even compensation without any degree in data science/analytics. In Section 3 we saw that there are as many as self-made data scientists (without any degree) earning similar compensation as with the degree holders. The case is not just restricted to the USA but also true in many other countries. This means that organizations value skills and talent over any degree, that's why they pay huge chunks of money for that skill. We also observed that there are no major distinctions in the type of work individuals in this field do on a daily basis, everyone has to spend a good amount of time in data analysis and cleaning, There are almost equal percentage of respondents who build machine learning services, prototypes and also do experimentation. \n\nIt might be wrong to strongly say that it’s not worth it all. There are many positives associated with it as well: One can get edge during job hunt over those who do not have masters degree, one can establish their network with professors, fellow students, and industry partners, one can get exposure to many new areas, and can polish a lot of skills. Academic institutions are responding to a market demand and generating programs, due to it. It is possible that what students learn might be very different from what a data scientist does.\n\nJeremy Howard said the following in a [reddit AMA](https://www.reddit.com/r/Futurology/comments/2p6k20/im_jeremy_howard_enlitic_ceo_kaggle_past/cmty7kf/): \n> There is nothing that you could learn in a master of computer science that you could not learn in an online course, or by reading books. However, if you need the structure or motivation of a formal course, all the social interaction with other students, you may find that useful. I think that online courses are the best way to go if you can, because you can access the world's best teachers, and have a much more interactive learning environment — for example the use of short quizzes scattered throughout each lecture.\n> Specifically, the courses that I would recommend our the machine learning course on Coursera, the introduction to data science course on Coursera, and the neural networks for machine learning course — surprise surprise, also on Coursera! Also, all of the introductory books here are a good choice.\n> The most important thing, of course, is to practice download some open datasets from the web, and try to build models and develop insights, and post them on your blog. And be sure to compete in machine learning competitions — that's the only way to know whether your approaches are working or not.\n\nThe key takeaway is that one can become a data scientist (or any other similar position in this field) with right skills and can also manage to get decent compensation even without going through ""Masters in Data Science (or similar degrees)"". There are many alternatives - Coursera, Kaggle, Udemy, Udacity, eDX, Linkedin Learning, Fast.ai to name a few. These resources are either completely free or are much cheaper than dedicated university degrees. Additionally, one can complete these courses in a much shorter time as compared to the university courses. If there are any other reasons for the desire to pursuing masters, example - personal goals, passion, interest, utilizing the breaks etc, one should consider pursuing university courses.\n\nIf someone is thinking of getting a degree, in data science, analytics, just because they have the time and money to spare, is not wise. If the interest in analytics is driven by the desire for a job that offers better rewards than what they are getting now, then one should look for alternative paths to make that career move. [Meta S Brown from Forbes](https://www.forbes.com/sites/metabrown/2016/07/29/4-reasons-not-to-get-that-masters-in-data-science/#4813020640c0) says, ""At the end, we need to remember that Data Scientist is a Title. Many give themselves or have this title because that's the work they do, not because they have a particular degree"". \n\nIt's just about taking the first step, start studying from online courses, do relevant projects, start making kaggle contributions, and keep improving your resume and profile. In the end, whatever path someone takes, they will most likely produce similar outcomes. The following Sankey depicts this message very well. "
"The chart shows a few interesting traits of people associated with university degrees: More use of R than Python, More focus on research and deep learning, mainly young individuals (aged 22-30), involved in coding for at least a year, not doing kaggle or MOOCs. \n\n\nConclusions  \n\nThe main focus of this analysis was to understand and evaluate if there are any significant differences in the self-taught scientists and university degree holders. The insights presented in this notebook, convey a broad message about the importance of ""masters in data science degrees"". We can observe that it is not impossible to achieve certain job roles or position and even compensation without any degree in data science/analytics. In Section 3 we saw that there are as many as self-made data scientists (without any degree) earning similar compensation as with the degree holders. The case is not just restricted to the USA but also true in many other countries. This means that organizations value skills and talent over any degree, that's why they pay huge chunks of money for that skill. We also observed that there are no major distinctions in the type of work individuals in this field do on a daily basis, everyone has to spend a good amount of time in data analysis and cleaning, There are almost equal percentage of respondents who build machine learning services, prototypes and also do experimentation. \n\nIt might be wrong to strongly say that it’s not worth it all. There are many positives associated with it as well: One can get edge during job hunt over those who do not have masters degree, one can establish their network with professors, fellow students, and industry partners, one can get exposure to many new areas, and can polish a lot of skills. Academic institutions are responding to a market demand and generating programs, due to it. It is possible that what students learn might be very different from what a data scientist does.\n\nJeremy Howard said the following in a [reddit AMA](https://www.reddit.com/r/Futurology/comments/2p6k20/im_jeremy_howard_enlitic_ceo_kaggle_past/cmty7kf/): \n> There is nothing that you could learn in a master of computer science that you could not learn in an online course, or by reading books. However, if you need the structure or motivation of a formal course, all the social interaction with other students, you may find that useful. I think that online courses are the best way to go if you can, because you can access the world's best teachers, and have a much more interactive learning environment — for example the use of short quizzes scattered throughout each lecture.\n> Specifically, the courses that I would recommend our the machine learning course on Coursera, the introduction to data science course on Coursera, and the neural networks for machine learning course — surprise surprise, also on Coursera! Also, all of the introductory books here are a good choice.\n> The most important thing, of course, is to practice download some open datasets from the web, and try to build models and develop insights, and post them on your blog. And be sure to compete in machine learning competitions — that's the only way to know whether your approaches are working or not.\n\nThe key takeaway is that one can become a data scientist (or any other similar position in this field) with right skills and can also manage to get decent compensation even without going through ""Masters in Data Science (or similar degrees)"". There are many alternatives - Coursera, Kaggle, Udemy, Udacity, eDX, Linkedin Learning, Fast.ai to name a few. These resources are either completely free or are much cheaper than dedicated university degrees. Additionally, one can complete these courses in a much shorter time as compared to the university courses. If there are any other reasons for the desire to pursuing masters, example - personal goals, passion, interest, utilizing the breaks etc, one should consider pursuing university courses.\n\nIf someone is thinking of getting a degree, in data science, analytics, just because they have the time and money to spare, is not wise. If the interest in analytics is driven by the desire for a job that offers better rewards than what they are getting now, then one should look for alternative paths to make that career move. [Meta S Brown from Forbes](https://www.forbes.com/sites/metabrown/2016/07/29/4-reasons-not-to-get-that-masters-in-data-science/#4813020640c0) says, ""At the end, we need to remember that Data Scientist is a Title. Many give themselves or have this title because that's the work they do, not because they have a particular degree"". \n\nIt's just about taking the first step, start studying from online courses, do relevant projects, start making kaggle contributions, and keep improving your resume and profile. In the end, whatever path someone takes, they will most likely produce similar outcomes. The following Sankey depicts this message very well. "
"""Is Spending $$$ for MS in Data Science worth it?"" \n\n1. If someone wants to learn data science skills only, then no its not worth it because there are many excellent free / cheaper alternatives on the internet. Kaggle is an excellent place to start.    \n2. If someone wants to work as a data scientist (fresher or career change), then it may or may not be worth it because there is no gurantee that one will get a data science job after the university degree. A non degree holder with right skillset, right experience, and right attitude may get a better job than the one with a degree title.  \n3. If someone wants to earn more, then it is not necessary to have university degree, as one with good experience or good skills can grab very high compensation packages.  \n4. If someone wants to do more research, then it may be worth it given that one chooses a right specialization.  \n4. If someone is passionate to learn more and gain exposure then it is definately worth it. University degree can provide a platform to network, participate in different events, and improve the foundation.  \n\n\nReferences   \n\n1. https://www.forbes.com/sites/metabrown/2016/07/29/4-reasons-not-to-get-that-masters-in-data-science/#6b43eb2140c0\n2. https://www.mybaggage.com/blog/why-do-a-masters-degree-the-pros-and-cons/\n3. https://e2i.com.sg/can-you-thrive-without-a-degree-in-singapore/\n4. https://www.kdnuggets.com/2014/06/masters-degree-become-data-scientist.html\n5. https://medium.com/@LiuhuayingYANG/is-your-income-lower-than-others-3d20f9e579bc\n6. https://www.quora.com/Is-a-masters-in-Data-Science-worth-it\n7. https://towardsdatascience.com/was-it-worth-studying-a-data-science-masters-c469e5b3e020\n8. https://www.cleverism.com/4-reasons-not-to-get-that-masters-in-data-science/\n9. https://www.kaggle.com/general/61082\n"
"Semi Supervised Classification using AutoEncoders\n\n## Introduction\n\nBy definition, machine learning can be defined as a complex process of learning the best possible and most relevant patterns, relationships, or associations from a dataset which can be used to predict the outcomes on unseen data. Broadly, their exists three different machine learning processes: \n\n**1. Supervised Learning** is a process of training a machine learning model on a labelled dataset ie. a dataset in which the target variable is known. In this technique, the model aims to find the relationships among the independent and dependent variable. Examples of supervised learning are classification, regression and forecasting. \n\n**2. Unsupervised Learning** is a process of training a machine learning model on a dataset in which target variable is not known. In this technique, the model aims to find the most relevant patterns in the data or the segments of data. Examples of unsupervised learning are clustering, segmentations, dimensionality reduction etc. \n\n**3. Semi-Supervised Learning** is combination of supervised and unsupervised learning processes in which the unlabelled data is used for training a model as well. In this approach, the properties of unspervised learning are used to learn the best possible representation of data and the properties of supervised learning are used to learn the relationships in the representations which are then used to make predictions. \n\nIn this kernel, I have explained how to perform classification task using semi supervised learning approach. This approach makes use of autoencoders to learn the representation of the data then a simple linear classifier is trained to classify the dataset into respective classes.  \n \n\nFraud Detection using Semi Supervised Learning  \n\nI am using the dataset of [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) by ULB machine learning group. Later, I am also applying the same technique on [Titanic](https://www.kaggle.com/c/titanic) dataset. A number of kagglers have shared different approaches such as dataset balancing, anomaly detection, boosting models, deep learning etc but this approach is different. \n\n### Contents \n\n1. Dataset Preparation  \n2. Visualize Fraud Vs Non Fraud Transactions  \n3. AutoEncoders : Latent Representation Extraction  \n4. Obtain the Latent Representations  \n5. Visualize Latent Representations : Fraud vs Non Fraud  \n6. Simple Linear Classifier  \n7. Applying the same technique on Titanic Dataset\n \n## 1. Dataset Preparation\n\nFirst, we will load all the required libraries and load the dataset using pandas dataframe. \n \n\n"
"The dataset consists of 28 anonymized variables, 1 ""amount"" variable, 1 ""time"" variable and 1 target variable - Class. Let's look at the distribution of target. "
## 2. Visualize Fraud and NonFraud Transactions \n\nLet's visualize the nature of fraud and non-fraud transactions using T-SNE. T-SNE (t-Distributed Stochastic Neighbor Embedding) is a dataset decomposition technique which reduced the dimentions of data and produces only top n components with maximum information.  \n\nEvery dot in the following represents a transaction. Non Fraud transactions are represented as Green while Fraud transactions are represented as Red. The two axis are the components extracted by tsne. 
"From the above graph we can observe that there are many non_fraud transactions which are very close to fraud transactions, thus are difficult to accurately classify from a model. \n\n## 3. AutoEncoders to the rescue \n\n\n**What are Autoencoders?** - Autoencoders are a special type of neural network architectures in which the output is same as the input. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input (in other words, model the identity function). These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low-dimensional code that can be used by the decoder to reproduce the original input. \n\n**More about Autoencoders** - If you want to gain more understanding about autoencoders, you can refer to the following kernel : https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases\n\n![](https://i.imgur.com/Rrmaise.png)\n\nWe will create an autoencoder model in which we only show the model non-fraud cases. The model will try to learn the best representation of non-fraud cases. The same model will be used to generate the representations of fraud cases and we expect them to be different from non-fraud ones. \n\nCreate a network with one input layer and one output layer having identical dimentions ie. the shape of non-fraud cases. We will use keras package. "
## Data visualisation\n\nWe  will start by visualising the Close prices for the two assets we have selected.
"The assets have quite different history, but we could check if they correlate in recent times."
"We can visualize the log return for our two assets. See how the signal now looks more like white noise, with less drift than the time series for prices."
## Correlation between assets\n\nWe hypothesized before that crypto asset returns may exhibit some correlation. Let's check this in more detail now.\n\nWe can check how the correlation between Bitcoin and Ethereum change over time for the 2021 period we selected. 
## Correlation between assets\n\nWe hypothesized before that crypto asset returns may exhibit some correlation. Let's check this in more detail now.\n\nWe can check how the correlation between Bitcoin and Ethereum change over time for the 2021 period we selected. 
"Note the high but variable correlation between the assets. Here we can see that there is some changing dynamics over time, and this would be critical for this time series challenge, that is, how to perform forecasts in a highly non-stationary environment.\n\nA stationary behaviour of a system or a process is characterized by non-changing statistical properties over time such as the mean, variance and autocorrelation. On the other hand, a non-stationary behaviour is characterized by a continuous change of statistical properties over time. Stationarity is important because many useful analytical tools and statistical tests and models rely on it.\n\nWe can also check the correlation between all assets visualizing the correlation matrix. Note how some assets have much higher pairwise correlation than others."
## Some Exploratory Data Analysis With Iris
The above graph shows relationship between the sepal length and width. Now we will check relationship between the petal length and width.
The above graph shows relationship between the sepal length and width. Now we will check relationship between the petal length and width.
As we can see that the Petal Features are giving a better cluster division compared to the Sepal features. This is an indication that the Petals can help in better and accurate Predictions over the Sepal. We will check that later.
### Now let us see how are the length and width are distributed
### Now let us see how the length and width vary according to the species
### Now let us see how the length and width vary according to the species
The violinplot shows density of the length and width in the species. The thinner part denotes that there is less density whereas the fatter part conveys higher density
"Now, when we train any algorithm, the number of features and their correlation plays an important role. If there are features and many of the features are highly correlated, then training an algorithm with all the featues will reduce the accuracy. Thus features selection should be done carefully. This dataset has less featues but still we will see the correlation."
**Observation--->**\n\nThe Sepal Width and Length are not correlated\nThe Petal Width and Length are highly correlated\n\nWe will use all the features for training the algorithm and check the accuracy.\n\nThen we will use 1 Petal Feature and 1 Sepal Feature to check the accuracy of the algorithm as we are using only 2 features that are not correlated. Thus we can have a variance in the dataset which may help in better accuracy. We will check it later.
### Let's check the accuracy for various values of n for K-Nearest nerighbours
Above is the graph showing the accuracy for the KNN models using different values of n. 
"# Part 1: Importing Necessary Libraries and datasets\n***\n\n## 1a. Loading libraries\n\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate. "
## 1b. Loading Datasets\n\n***
"We may be able to solve these two missing values by looking at other independent variables of the two raws. Both passengers paid a fare of $80, are of Pclass 1 and female Sex. Let's see how the **Fare** is distributed among all **Pclass** and **Embarked** feature values"
"Here, in both training set and test set, the average fare closest to $80 are in the C Embarked values where pclass is 1. So, let's fill in the missing values as ""C"" "
## 3a. Gender and Survived\n\n***
This bar plot above shows the distribution of female and male survived. The ***x_label*** represents **Sex** feature while the ***y_label*** represents the % of **passenger survived**. This bar plot shows that ~74% female passenger survived while only ~19% male passenger survived.
This bar plot above shows the distribution of female and male survived. The ***x_label*** represents **Sex** feature while the ***y_label*** represents the % of **passenger survived**. This bar plot shows that ~74% female passenger survived while only ~19% male passenger survived.
"This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive. \n\n**Summary**\n***\n- As we suspected, female passengers have survived at a much better rate than male passengers. \n- It seems about right since females and children were the priority. "
"- It looks like ...\n    - ~ 63% first class passenger survived titanic tragedy, while \n    - ~ 48% second class and \n    - ~ only  24% third class passenger survived. \n\n"
"This KDE plot is pretty self-explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that the lower class passengers have survived more than second-class passengers. It is true since there were a lot more third-class passengers than first and second. \n\n**Summary**\n***\nThe first class passengers had the upper hand during the tragedy. You can probably agree with me more on this, in the next section of visualizations where we look at the distribution of ticket fare and survived column. "
## 3c. Fare and Survived\n\n***
"This plot shows something impressive..\n- The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive. \n- When fare is approximately more than 280 dollars, there is no gray shade which means, either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment. Let's check..."
## 3d. Age and Survived\n\n***
"There is nothing out of the ordinary about this plot, except the very left part of the distribution. This may hint on the posibility that children and infants were the priority. "
"## 3e. Combined Feature Relations\n\n***\nIn this section, we are going to discover more than two feature relations in a single graph. I will try my best to illustrate most of the feature relations. Let's get to it. "
"Facetgrid is a great way to visualize multiple variables and their relationships at once. From the chart in section 3a we have a intuation that female passengers had better prority than males during the tragedy. However, from this facet grid, we can also understand which age range groups survived more than others or were not so lucky"
"Facetgrid is a great way to visualize multiple variables and their relationships at once. From the chart in section 3a we have a intuation that female passengers had better prority than males during the tragedy. However, from this facet grid, we can also understand which age range groups survived more than others or were not so lucky"
"This is another compelling facet grid illustrating four features relationship at once. They are **Embarked, Age, Survived & Sex**. \n* The color illustrates passengers survival status(green represents survived, gray represents not survived)\n* The column represents Sex(left being male, right stands for female)\n* The row represents Embarked(from top to bottom: S, C, Q)\n***\nNow that I have steered out the apparent let's see if we can get some insights that are not so obvious as we look at the data. \n* Most passengers seem to be boarded on Southampton(S).\n* More than 60% of the passengers died boarded on Southampton. \n* More than 60% of the passengers lived boarded on Cherbourg(C).\n* Pretty much every male that boarded on Queenstown(Q) did not survive. \n* There were very few females boarded on Queenstown, however, most of them survived. "
"This is another compelling facet grid illustrating four features relationship at once. They are **Embarked, Age, Survived & Sex**. \n* The color illustrates passengers survival status(green represents survived, gray represents not survived)\n* The column represents Sex(left being male, right stands for female)\n* The row represents Embarked(from top to bottom: S, C, Q)\n***\nNow that I have steered out the apparent let's see if we can get some insights that are not so obvious as we look at the data. \n* Most passengers seem to be boarded on Southampton(S).\n* More than 60% of the passengers died boarded on Southampton. \n* More than 60% of the passengers lived boarded on Cherbourg(C).\n* Pretty much every male that boarded on Queenstown(Q) did not survive. \n* There were very few females boarded on Queenstown, however, most of them survived. "
"This facet grid unveils a couple of interesting insights. Let's find out.\n* The grid above clearly demonstrates the three outliers with Fare of over \$500. At this point, I think we are quite confident that these outliers should be deleted.\n* Most of the passengers were with in the Fare range of \$100. "
"This facet grid unveils a couple of interesting insights. Let's find out.\n* The grid above clearly demonstrates the three outliers with Fare of over \$500. At this point, I think we are quite confident that these outliers should be deleted.\n* Most of the passengers were with in the Fare range of \$100. "
**Passenger who traveled in big groups with parents/children had less survival rate than other passengers.**
**Passenger who traveled in big groups with parents/children had less survival rate than other passengers.**
"**While, passenger who traveled in small groups with sibilings/spouses had better changes of survivint than other passengers.**"
\n**Squaring the correlation feature not only gives on positive correlations but also amplifies the relationships.** 
"#### Positive Correlation Features:\n- Fare and Survived: 0.26\n\n#### Negative Correlation Features:\n- Fare and Pclass: -0.6\n- Sex and Survived: -0.55\n- Pclass and Survived: -0.33\n\n\n**So, Let's analyze these correlations a bit.** We have found some moderately strong relationships between different features. There is a definite positive correlation between Fare and Survived rated. This relationship reveals that the passenger who paid more money for their ticket were more likely to survive. This theory aligns with one other correlation which is the correlation between Fare and Pclass(-0.6). This relationship can be explained by saying that first class passenger(1) paid more for fare then second class passenger(2), similarly second class passenger paid more than the third class passenger(3). This theory can also be supported by mentioning another Pclass correlation with our dependent variable, Survived. The correlation between Pclass and Survived is -0.33. This can also be explained by saying that first class passenger had a better chance of surviving than the second or the third and so on.\n\nHowever, the most significant correlation with our dependent variable is the Sex variable, which is the info on whether the passenger was male or female. This negative correlation with a magnitude of -0.54 which points towards some undeniable insights. Let's do some statistics to see how statistically significant this correlation is. "
H0: male mean is greater or equal to female mean\nH1: male mean is less than female mean. 
"#### Compare P-value with $\alpha$\n> It looks like the p-value is very small compared to our significance level($\alpha$)of 0.05. Our observation sample is statistically significant. Therefore, our null hypothesis is ruled out, and our alternative hypothesis is valid, which is ""**There is a significant difference in the survival rate between the male and female passengers.""**"
Let's take a look at the histogram of the age column. 
"## age_group\nWe can create a new feature by grouping the ""Age"" column"
we have our confusion matrix. How about we give it a little more character. 
AUC & ROC Curve
## Preparing the Titanic dataset ##\n\nFor the Titanic challenge we need to guess wheter the individuals from the *test* dataset had survived or not. But for our current purpose let's also find out what can the data tell us about the shipwreck with the help of a *Classification Tree*. Let's load the data and get an overview.
"Thanks to this overview we can see that our dataset needs some treatment. The class *Survived* is already in binary format so no additional formatting is necessary, but features like *Name*, *Ticket* or *Cabin* need to be adapted for the problem we're trying to solve, and we can also engineer some new features by merging or regrouping existing ones. There's already extended work on this so we're just using one the best approches out there (credit to [Sina][1], [Anisotropic][2] and also [Megan Risdal][3] for the suggestion of the ""Title"" feature).\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n  [2]: https://www.kaggle.com/arthurtok/titanic/introduction-to-ensembling-stacking-in-python\n  [3]: https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic"
"Our dataset is now much cleaner than before, with only numerical values and potentially meaningful features. Let's now explore the relationship between our variables by plotting the Pearson Correlation between all the attributes in our dataset (credit to [Anisotropic][1] for this beautiful plot):\n\n\n  [1]: https://www.kaggle.com/arthurtok/titanic/introduction-to-ensembling-stacking-in-python"
"This heatmap is very useful as an initial observation because you can easily get an idea of the predictive value of each feature. In this case, *Sex* and *Title* show the highest correlations (in absolute terms) with the class (*Survived*): 0.54 and 0.49 respectively. But the absolute correlation between both is also very high (0.86, the highest in our dataset), so they are probably carrying the same information and using the two as inputs for the same model wouldn't be a good idea.  High chances are one of them will be used for the first node in our final decision tree, so let's first explore further these features and compare them."
"**Natural Language Processing or NLP** is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way.\n\n**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include Positive, Neutral, and Negative, Review Ratings and Happy, Sad. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject.\n\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.\n\n![Natural-Language-Processing.png](attachment:Natural-Language-Processing.png)\n\nIn this kernel we are going to focus on text classification and sentiment analysis part. In the next lessons we will study Information retrival, Question answering, etc"
\n# 1. Loading Data 💎\n\nJust load the dataset and global variables for colors and so on.
"As we can see, the classes are imbalanced, so we can consider using some kind of resampling. We will study later. Anyway, it doesn't seem to be necessary."
"As we can see, the `ham` message length tend to be lower than `spam` message length."
"## PreTrained Model : VGG16\n\n\n\nKeras library also provides the pre-trained model in which one can load the saved model weights, and use them for different purposes : transfer learning, image feature extraction, and object detection. We can load the model architecture given in the library, and then add all the weights to the respective layers. \n\nBefore using the pretrained models, lets write a few functions which will be used to make some predictions. First, load some images and preprocess them. "
"Now, we can perform following steps : \n1. import VGG16 architecture from keras.applications  \n2. Add the saved weights to the architecture \n3. Use model to perform predictions "
Let's predict the output on new images and check the outcome.
"So a simple neural network with only 20 rows of training data is able to correctly classify the two images on test set. \n\n### EndNotes \nThanks for viewing this kernel, If you liked it, please upvote. "
pd.plotting.scatter_matrix:\n* green: *normal* and red: *abnormal*\n* c:  color\n* figsize: figure size\n* diagonal: histohram of each features\n* alpha: opacity\n* s: size of marker\n* marker: marker type 
"Okay, as you understand in scatter matrix there are relations between each feature but how many *normal(green)* and *abnormal(red)* classes are there. \n* Searborn library has *countplot()* that counts number of classes\n* Also you can print it with *value_counts()* method\n\n This data looks like balanced. Actually there is no definiton or numeric value of balanced data but this data is balanced enough for us.\n Now lets learn first classification method KNN"
"Okay, as you understand in scatter matrix there are relations between each feature but how many *normal(green)* and *abnormal(red)* classes are there. \n* Searborn library has *countplot()* that counts number of classes\n* Also you can print it with *value_counts()* method\n\n This data looks like balanced. Actually there is no definiton or numeric value of balanced data but this data is balanced enough for us.\n Now lets learn first classification method KNN"
" \n###  K-NEAREST NEIGHBORS (KNN)\n* KNN: Look at the K closest labeled data points\n* Classification method.\n* First we need to train our data. Train = fit\n* fit(): fits the data, train the data.\n* predict(): predicts the data\n If you do not understand what is KNN, look at youtube there are videos like 4-5 minutes. You can understand better with it.\n Lets learn how to implement it with sklearn\n* x: features\n* y: target variables(normal, abnormal)\n* n_neighbors: K. In this example it is 3. it means that Look at the 3 closest labeled data points\n"
"Accuracy is 86% so is it good ? I do not know actually, we will see at the end of tutorial.\n Now the question is why we choose K = 3 or what value we need to choose K. The answer is in model complexity\n\n Model complexity:\n* K has general name. It is called a hyperparameter. For now just know K is hyperparameter and we need to choose it that gives best performace. \n* Literature says if k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n* If k is big, model that is less complex model can lead to underfit. \n* At below, I range K value from 1 to 25(exclude) and find accuracy for each K value. As you can see in plot, when K is 1 it memozize train sets and cannot give good accuracy on test set (overfit). Also if K is 18, model is lead to underfit. Again accuracy is not enough. However look at when K is 18(best performance), accuracy has highest value almost 88%. \n\n"
"### Up to this point what you learn:\n* Supervised learning\n* Exploratory data analysis\n* KNN\n    * How to split data\n    * How to fit, predict data\n    * How to measure medel performance (accuracy)\n    * How to choose hyperparameter (K)\n    \n** What happens if I chance the title KNN and make it some other classification technique like Random Forest?**\n* The answer is **nothing**. What you need to is just watch a video about what is random forest in youtube and implement what you learn in KNN. Because the idea and even most of the codes (only KNeighborsClassifier need to be RandomForestClassifier ) are same. You need to split, fit, predict your data and measue performance and choose hyperparameter of random forest(like max_depth). "
" \n### REGRESSION\n* Supervised learning\n* We will learn linear and logistic regressions\n* This orthopedic patients data is not proper for regression so I only use two features that are *sacral_slope* and *pelvic_incidence* of abnormal \n    * I  consider feature is pelvic_incidence and target is sacral_slope \n    * Lets look at scatter plot so as to understand it better\n    * reshape(-1,1): If you do not use it shape of x or y becaomes (210,) and we cannot use it in sklearn, so we use shape(-1,1) and shape of x or y be (210, 1). "
"Now we have our data to make regression. In regression problems target value is continuously varying variable such as price of house or sacral_slope. Lets fit line into this points.\n\n Linear regression\n* y = ax + b       where  y = target, x = feature and a = parameter of model\n* We choose parameter of model(a) according to minimum error function that is lost function\n* In linear regression we use Ordinary Least Square (OLS) as lost function.\n* OLS: sum all residuals but some positive and negative residuals can cancel each other so we sum of square of residuals. It is called OLS\n* Score: Score uses R^2 method that is ((y_pred - y_mean)^2 )/(y_actual - y_mean)^2"
"Now we have our data to make regression. In regression problems target value is continuously varying variable such as price of house or sacral_slope. Lets fit line into this points.\n\n Linear regression\n* y = ax + b       where  y = target, x = feature and a = parameter of model\n* We choose parameter of model(a) according to minimum error function that is lost function\n* In linear regression we use Ordinary Least Square (OLS) as lost function.\n* OLS: sum all residuals but some positive and negative residuals can cancel each other so we sum of square of residuals. It is called OLS\n* Score: Score uses R^2 method that is ((y_pred - y_mean)^2 )/(y_actual - y_mean)^2"
" \n### CROSS VALIDATION\nAs you know in KNN method we use train test split with random_state that split exactly same at each time. However, if we do not use random_state, data is split differently at each time and according to split accuracy will be different. Therefore, we can conclude that model performance is dependent on train_test_split. For example you split, fit and predict data 5 times and accuracies are 0.89, 0.9, 0.91, 0.92 and 0.93, respectively. Which accuracy do you use? Do you know what accuracy will be at 6th times split, train and predict. The answer is I do not know but if I use cross validation I can find acceptable accuracy.\n Cross Validation (CV)\n* K folds = K fold CV.\n* Look at this image it defines better than me :)\n* When K is increase, computationally cost is increase\n* cross_val_score(reg,x,y,cv=5): use reg(linear regression) with x and y that we define at above and K is 5. It means 5 times(split, train,predict)\n"
" \n### ROC Curve with Logistic Regression \n* logistic regression output is probabilities\n* If probability is higher than 0.5 data is labeled 1(abnormal) else 0(normal)\n* By default logistic regression threshold is 0.5\n* ROC is receiver operationg characteristic. In this curve x axis is false positive rate and y axis is true positive rate\n* If the curve in plot is closer to left-top corner, test is more accurate.\n* Roc curve score is auc that is computation area under the curve from prediction scores\n* We want auc to closer 1\n* fpr = False Positive Rate\n* tpr = True Positive Rate\n* If you want, I made ROC, Random forest and K fold CV in this tutorial. https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv/"
 \n### HYPERPARAMETER TUNING\nAs I mention at KNN there are hyperparameters that are need to be tuned\n* For example: \n    * k at KNN\n    * alpha at Ridge and Lasso\n    * Random forest parameters like max_depth\n    * linear regression parameters(coefficients)\n* Hyperparameter tuning: \n    * try all of combinations of different parameters\n    * fit all of them\n    * measure prediction performance\n    * see how well each performs\n    * finally choose best hyperparameters\n* This process is most difficult part of this tutorial. Because we will write a lot of for loops to iterate all combinations. Just I am kidding sorry for this :) (We actually did it at KNN part)\n* We only need is one line code that is GridSearchCV\n    * grid: K is from 1 to 50(exclude)\n    * GridSearchCV takes knn and grid and makes grid search. It means combination of all hyperparameters. Here it is k.\n
"The new question is that we know how many class data includes, but what if number of class is unknow in data. This is kind of like hyperparameter in KNN or regressions. \n* inertia: how spread out the clusters are distance from each sample\n* lower inertia means more clusters\n* What is the best number of clusters ?\n    *There are low inertia and not too many cluster trade off so we can choose elbow"
 \n### STANDARDIZATION\n* Standardizaton is important for both supervised and unsupervised learning\n* Do not forget standardization as pre-processing\n* As we already have visualized data so you got the idea. Now we can use all features for clustering.\n* We can use pipeline like supervised learning.
 \n### HIERARCHY\n* vertical lines are clusters\n* height on dendogram: distance between merging cluster\n* method= 'single' : closest points of clusters
 \n### T - Distributed Stochastic Neighbor Embedding (T - SNE)\n * learning rate: 50-200 in normal\n * fit_transform: it is both fit and transform. t-sne has only have fit_transform\n * Varieties have same position relative to one another
 \n### T - Distributed Stochastic Neighbor Embedding (T - SNE)\n * learning rate: 50-200 in normal\n * fit_transform: it is both fit and transform. t-sne has only have fit_transform\n * Varieties have same position relative to one another
 \n### PRINCIPLE COMPONENT ANALYSIS (PCA)\n* Fundemental dimension reduction technique\n* first step is decorrelation:\n    * rotates data samples to be aligned with axes\n    * shifts data asmples so they have mean zero\n    * no information lost\n    * fit() : learn how to shift samples\n    * transform(): apply the learned transformation. It can also be applies test data\n* Resulting PCA features are not linearly correlated\n* Principle components: directions of variance
* Second step: intrinsic dimension: number of feature needed to approximate the data essential idea behind dimension reduction\n* PCA identifies intrinsic dimension when samples have any number of features\n* intrinsic dimension = number of PCA feature with significant variance\n* In order to choose intrinsic dimension try all of them and find best accuracy\n* Also check intuitive way of PCA with this example: https://www.kaggle.com/kanncaa1/tutorial-pca-intuition-and-image-completion
"# CONCLUSION\nThis is the end of DATA SCIENCE tutorial. The first part is here:\n  https://www.kaggle.com/kanncaa1/data-sciencetutorial-for-beginners/\n**If you have any question or suggest, I will be happy to hear it.**"
