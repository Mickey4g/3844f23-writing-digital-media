 \n## 4-2-2 Mean Frequency
 \n## 4-2-3 countplot
" \n## 4-2-4 hist\nIf you check histogram for all feature, you will find that most of them are so similar"
 \n## 4-2-6 distplot\n The target in data set is **imbalance**
 \n## 4-2-7 violinplot
### Discrete Features w.r.t Target Variable (stroke) :
"- Because of too many unique data points in the **discrete_features**, it is difficult to gain any type of insight. Thus, we will convert these features into categorical features for visualizations.\n- We scale the data points of these features to a constant value that represents a range of values.(like mean)\n- Here, we divide the data points by a constant value and assign it's quotient value as the representative constant. The scaling constants are decided by looking into the data & intuition. "
- We remove the **stroke** feature from the list of categorical features as it is the target variable and we will treat it separately!
- All the categorical features are **Normally Distributed**.
### Categorical Features w.r.t Target Variable (stroke) :
"- All the graphs near about share the same pattern i.e displaying low number of **stroke** cases and no clear cut reason to point towards.  \n- **Female** population has recorded more cases of **stroke** than **male**.\n- Interestingly, people with **no hypertension** & **no heart disease** have displayed to be more prone to **suffering stroke** than people that have these medical conditions.\n- According to the dataset, people that have been **married** have **suffered stroke** more than those people who have never married.\n- When it comes to **smoking_status**, people that have **never smoked** have topped the numbers with **formerly smoked** people coming at the 2nd position to record **stroke** cases.\n- Not much info can be gained from **Residence_type** & **work_type**, however **Private** workers **suffered stroke** cases more than any other worker."
#### gender vs Discrete Features :
"- For both **male** & **female** population, **age** of those **suffering from stroke** is **60+**.\n- For majority of the **avg_glucose_level** values, both **gender** have recorded significant cases of **stroke**.\n- For **male** population, the lower limit of **bmi** values is slightly higher than the **female**. Overall, both the **gender** overlap the same **bmi** values for cases of **stroke**."
#### hypertension vs Discrete Features :
"- Wierdly, cases of **stroke** found in people having **hypertension** have a high lower limit of the **age 60+** than those who do not suffer from **hypertension**.\n- When it comes to **hypertension & avg_glucose_level**, cases of **stroke** & **no stroke** near about share the same values.\n- Due to **hypertension**, lower limits of **bmi** values are slightly reduced making people prone to **stroke**. "
#### heart_disease vs Discrete Features :
- Graphs of **hypertension** & **heart_disease** against discrete features are very similar with slight differences.\n- They share the same effects on **stroke**.
#### ever_married vs Discrete Features :
"- For **ever_married vs discrete features**, repeated insights can be found.\n- People that have been **married** have displayed cases of **stroke** for near about all the values of **avg_glucose_level**."
#### work_type vs Discrete Features :
"- Irrespective of the **work_type**, **stroke** cases have been found for **age of 60+** except for **children**.\n- Clearly, people that have worked to earn a living have suffered from **stroke**. \n- **Stroke** cases have been found more in people working in a job i.e **Govt_job** & **Private** than those who are **Self-employed**."
#### Elbow Curve to get the right number of Clusters\nA fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.
"### Silhouette Analysis\n\n$$\text{silhouette score}=\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster."
"**Single Linkage:**\n\nIn single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two closest points.\n![](https://www.saedsayad.com/images/Clustering_single.png)"
"**Complete Linkage**\n\nIn complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points. \n![](https://www.saedsayad.com/images/Clustering_complete.png)"
"**Average Linkage:**\n\nIn average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters “r” and “s” to the left is equal to the average length each arrow between connecting the points of one cluster to the other.\n![](https://www.saedsayad.com/images/Clustering_average.png)"
#### Cutting the Dendrogram based on K
"On closer inspection, we can suspect that all the continuous variables may contain outliers.\n\n\nI will draw boxplots to visualise outliers in the above variables. "
The above boxplots confirm that there are lot of outliers in these variables.
"### Check the distribution of variables\n\n\nNow, I will plot the histograms to check distributions to find out if they are normal or skewed. "
We can see that all the 8 continuous variables are skewed. 
Let's check the class inbalance for the rsulted training set.
"And, as well, for the validation set."
"## Validation accuracy and loss\n\nLet's plot the train and validation accuracy and loss, from the train history."
The validation accuracy does not improve after few epochs and the validation loss is increasing after few epochs. This confirms our assumption that the model is overfitted. We will try to improve the model by adding Dropout layers.
"# 8. Declutering \nThis is the most important part of any good data visualization. Remove any noise that is not directly contributing to how people are reading and interpreting data.\n\n## 8.1 Turning background to white\nNow that the background is white, we also need to change some other elements, such as grids, ticks and lines. Here my choices were:\n* Turn all text and chart elements to a light grey\n* Hide any chart grid and axis lines that are not essential. I only kept radial axis grid (the circles), because they are important for readers to understand the magnitude of the values displayed."
"It looks much better now, doesn't it?\n\n## 8.2 Smoothing all lines, and making they grey and thinner\nNow we will transform all lines to grey, reduce their width and also make them less sharp by applying some smoothing. "
"This is SO MUCH BETTER! Because now we can actually ""see"" each line! But as they say, the devil is in the details. Let's keep improving it!\n\n## 8.3 Removing legends, adjusting grid and range\n* Now I thought that there where too much lines for grids (0, 10, 20, ..., 50) cluttering the chart. I don't like it, instead I will create only three lines (25% and 50%). \n* I also don't like that many values for legend, they dont help with anything other than making our visualization worse. Let's hide it!\n* There is one line that is almost over **Data Scientist** axis title, I will increase the range a little bit to avoid that."
"# 9. Adjusting hover information\nWhen we hover the mouse over the datapoints we are seeing **r** and **theta** values. This is not informative for anyone that is viewing the chart. Let's adjust a few things there.\n\n* Add the country name (so it's easy to relate the line to the country without needing to have multiple colors or legends)\n* Add the data point (so it's easy to relate the value of each data point to each line)\n* Format the data point as percentage without decimal points (no need to have any decimal points, because the analysis is more qualitative) \n"
"# 10. Telling a story\nNow we will highlight some specific countries that we want to tell a story about. For this chart I'm choosing United States vs China. What we will do:\n\n* Add color to highlighted countries lines\n    * Choose color from flags colors (red for China, blue for United States)\n* Increase width of highlighted countries lines\n* Increase opacity of highlighted countries lines\n* Show legend only for highlighted countries"
Now we are telling some story with this data! We are able to see the differences between China and United States! We can easily draw some conclusions from this chart!\n\n# 11. Adding a meaningful title\nNow we will add a meaninful title and the data source to the chart.\n\n* Bigger font size for title\n* Smaller font size for source\n* Both grey to not draw much attention
"# 12. Bringing all elements more close together\nUsing the standard autosize of Plotly is all elements too much separated from each other, let's bring them closer by defining the height and width size of the chart.\n\nAlso some minor adjustments to legend as well:\n* Changing color to grey\n* Change behaviour when click and doubleclick"
"# 13. More tips on how to do a great data storytelling\n\n## 13.1 Keep all your charts similar\nWhen showing multiple charts, stick to a few different chart types. The purpose of this is to make your readers lifes easier, as they will not need to learn how to read every new chart. After learning how to interpret one chart, reading all the other charts will be automatic.\n\n## 13.2 Stick to a few colors in every chart\nIf your highlighted data colors don't hold any meaning (unlike this example, where they encode the color of country flag) then you shouldn't use a lot of different colors. Keep them to a minimum.\n\n## 13.4 Tell just one story per chart\nIt might be tempting to highlight multiple things in a single chart. Instead create multiple charts to highlith different things, one single story per chart. "
"> The same chart, but showing also the median of the Sales and not divided by Year:"
"> Just as an observation, the mean and the median are very different, suggesting that some stores/departments might sell much more than others."
### Average Sales per Store and Department
"> Yeah, there are Sales difference between the Stores."
"> And there are Sales difference between the Departments too. Also some Depts are not in the list, like number '15', for example."
"> Correlation Metrics:\n    >* 0: no correlation at all\n    >* 0-0.3: weak correlation \n    >* 0.3-0.7: moderate correlaton\n    >* 0.7-1: strong correlation\n\n> Positive Correlation indicates that when one variable increase, the other also does. Negative is the opposite."
"> 'MarkDown' 1 to 5 are not strong correlated to 'Weekly_Sales' and they have a lot of null values, then we can drop them.\n\n> Also, 'Fuel_Price' is strong correlated to 'Year'. One of them must be dropped else they would carry similar information to the model. 'Year' will not be dropped, because it differentiate same Weeks for 'Store'+'Dept'.\n\n> Other variables that have weak correlation with 'Weekly_Sales' can be analyzed to see if they are useful."
\n## 2-Correlation Between Features
"**Outcome**    \n\n* Sex, Pclass, Fare and Embarked are associated with Survived. \n\n\n"
">* The first bar where there are 0 people responsible for DS workload in a company seems a bit strange to me , as a person calling himself a Data Scientist should atleast count himself.\n* In the last bar, the small companies(0-49,50-249 range) employing 20+ Data Scientists must be DS Consulting firms or companies fully based on AI like H20.ai or OpenAI.\n\n> There are two kinds of majorities here. \n* Small companies(0-49 employees) having 1-2 DS people to improve decision making or work on specific tasks like recommender systems, Sentiment Analysis, Sales Prediction etc.\n* MNCs with more than 10k employees having 20+ DS leveraging AI in full capacity like Facebook, Google etc.\n\nIt would also be useful to know which ML product companies use and you can add some experience in them to get you some edge over other contenders."
"> * Okay, so most of the companies do not use a machine learning product.\n* I have learned this from my own experience that most companies using Data Science do not want to use third party  APIs like Google Cloud Natural language or Google Cloud Translation or Google Cloud Speech-to-text or Google Cloud Vision. They prefer to have a propreitary in-house ML apllication(that's what they have hired the Data Scientists for, right?) and even when they do ,they use it as an intermediate process while building their ML applications.\n* Amazon Sagemaker is different from the above, it lets you build your own ML application on top of it's computing power and ML framework and hence it is mostly used by companies along with Google Cloud Machine Learning Engine.\n* I think, it takes a little time to get used to the quirks of Sagemaker when compared to the traditional methods. You might as well start with a free account."
"# What determines your Salary as a Data Scientist?\n\nThere are a lot of factors on which the salary of a Data Scientist depends, the common ones being the skill set and the country of work. Take a look.\n\n### What salary can a Data Scientist expect acording to his/her highest education?"
"In the mid to higher income range(let's say above 30,000 USD) we have much more people with a Master's degree.\n### How does your salary increases based on your years of experience?"
>* I think the most important factor in deciding your salary as a Data Scientist is the years of experience you have in the field.\n* The lowest salary range is dominated by people with very less experience(less than 2 years). Most of the Data Scientists having high income fall in the 5-10 years of experience range.
## Does the state of ML in the company decide the salary?\nI'm also interested to know whether the state of ML in the company has any effect on the salary (I'm pretty sure it does).
"> Some major deductions from the plot:\n* Most of the Highly paid Data Scientists come from companies which have well established ML methods. There models are in production and they generate revenue from it , therefore they invest more in their Data Science Team.\n* Companies that do not use ML methods( I don't know why would they hire a Data Scientist then), or are exploring ML methods or use ML to just generate insights pay less to their Data Scientists.\n* Most of the mid income DS are from companies that have just started using ML methods.\n\n## Does the DS Field play a role in deciding salary?\nI want to explore what is the trend in salary for different fields, let's dive into it."
"> Deductions:\n* Most of the people in ML belong to the high income Salary range and on the contrary most people in Computer Vision and NLP belong to the low and mid income range.\n* I had a hypothesis that has more to do with years of experience, since Machine learning is an older field compared to Deep Learning, so I made a plot to confirm this.\n* Seems like the hypothesis is true, most of the higher salary income people have a lot of experience.\n\n## Conclusions\n> Landing a Data Science job ain't that hard. You should know what it takes and where to look.\n* Having a university degree in Data Science is not that important, a lot of Data Scientists learn from MOOCs. However, it is very important to stay up-to-date with trends in the field.Medium Blogs and Kaggle Forums are a popular choice among Data Scientits.\n* Spend more time in cleaning and analyzing data than making models because that's what people do while solving real-world problems. Local environment with matplotlib is best for analyzing data.\n* Whenever solving a problem try to stick with easily interpretable algorithms before jumping to neural networks. If you want to enter the field of NLP or CV making an image or text classifier with good background on the working and basics of it would also work. You can also go for some not so common projects to give an edge to your resume.\n* Set realistic goals, after making a good profile( with some nice independent projects or open-source contributions) and a good understanding of what you do,apply at a start-up or a place that has more chances of hiring you.\n* The picture won't be that pretty and you will have a lot of responsibilities, but you will learn a lot here.\n* You can try your hand at learning to use a ML product.\n* Years of experience will gradually but surely add worth to you.\n"
#### GrLivArea
We can observe that there is almost **a linear relationship between Living Area and the Sale Price**. If the area is huge then price should also have also been high but the two rightmost points suggest something else. **These two points are outliers**. We will drop both of them below.
#### Garage Area
#### Total Basement Area
#### 1st Floor Area
"Before we move forward we need to understand the assumptions of linear regression:\n* Linearity\n* Homoscedasticity\n* No or little Multicollinearity\n* Independence of Error\n\nSince we fit a linear model, we assume that the relationship is linear, and the errors, or residuals, are pure random fluctuations around the true line. We expect that the variability in the dependent variable doesn't increase as the value of the independent increases, which is the assumptions of equal variance, also known as Homoscedasticity. We also assume that the observations are independent of one another(No Multicollinearity), and a correlation between sequential observations or auto-correlation is not there.\n\nNow, these assumptions are prone to happen altogether. In other words, if we see one of these assumptions in the dataset, it's more likely that we may come across with others mentioned above. Therefore, we can find and fix various assumptions with a few unique techniques.\n\nIn order to discover the linearity let's plot scatter plots for GrLivArea and MasVnrArea"
We can observe that the relationship between Sales Price and GrLivArea is much more linear than the relationship between Sales Price and MasVnrArea.\n\nLet's look at the residual plot for independent variable GrLivArea and our target variable SalePrice. \n\nA residual value is a measure of how much a regression line vertically misses a data point. Regression lines are the best fit of a set of data. You can think of the lines as averages; a few data points will fit the line and others will miss. A residual plot has the Residual Values on the vertical axis; the horizontal axis displays the independent variable.
"Ideally, if the assumptions are met, the residuals will be randomly scattered around the centerline of zero with no apparent pattern. The residual will look like an unstructured cloud of points centered around zero. However, our residual plot is anything but an unstructured cloud of points. Even though it seems like there is a linear relationship between the response variable and predictor variable, the residual plot looks more like a funnel. \n\nThe error plot shows that as GrLivArea value increases, the variance also increases, which is the characteristics known as Heteroscedasticity. The linear regression analysis requires the dependent variable to be multivariate normally distributed. A histogram, box plot, or a Q-Q-Plot can check if the target variable is normally distributed. \n\nLet's plot the three graphs again for the target feature."
We can observe that the SalesPrice feature now follows a normal distibution as the Histogram resembles bell-shape and the QQ-plot also overlaps with the red line. Let's again plot the scatter plot for GrLivArea and SalePrice to see whether Heteroscedasticity was removed.
"We can see that the scatterplot on the left had heteroscedasticity(funnel like shape) but after applying log transformation to the feature, it was removed and now it's Homoscedastic."
"Linear Regression indicates significant relationships between the dependent variable and the independent variable. It assumes that there is a linear relationship between the independent variables and the dependent variable but this is not always the case in real life. There are very less scenarios where this assumption holds true. Multiple Regression also suffers from multi-collinearity, auto-correlation, and heteroskedasticity.\n\nThe interpretation of the linear coefficient is that it represents the mean change in the dependent variable for 1 unit change in the independent variable when all the other independent variables are held constant.\n\nNow suppose that there are two correlated independent variables (A and B) and we are aiming to find the right coefficient for these independent variables. When the coefficient for A is calculated, will that coefficient be accurate? No, it won’t be right because changes in A are associated with shifts in B, and as we have already discussed that all the other variables need to be held constant, B can’t be held as a constant because A and B are correlated. This is known as Multicollinearity and it is one of the disadvantages of Linear Regression.\n\nIn order to solve this problem, we use various regularization techniques (l1, l2 etc) or other kind of regression techniuques like Elastic Net Regression, Lasso and Ridge Regression which automatically takes care of multicollinearity.\n\nCheck this [blog](https://medium.com/gdg-vit/overcoming-the-drawbacks-of-linear-regression-497fffcdd2d8) for better understanding"
"As the quality increases, price of the houses also increase"
"Autocorrelation is the correlation between two observations at different points in a time series. For example, values that are separated by an interval might have a strong positive or negative correlation. When these correlations are present, they indicate that past values influence the current value. Analysts use the autocorrelation and partial autocorrelation functions to understand the properties of time series data, fit the appropriate models, and make forecasts.\n\nIn this post, I cover both the autocorrelation function and partial autocorrelation function. You’ll learn about the differences between these functions and what they can tell you about your data. In later posts, I’ll show you how to incorporate this information in regression models of time series data and other time-series analyses.\n\n#### Autocorrelation and Partial Autocorrelation Basics\nAutocorrelation is the correlation between two values in a time series. In other words, the time series data correlate with themselves—hence, the name. We talk about these correlations using the term “lags.” Analysts record time-series data by measuring a characteristic at evenly spaced intervals—such as daily, monthly, or yearly. The number of intervals between the two observations is the lag. For example, the lag between the current and previous observation is one. If you go back one more interval, the lag is two, and so on.\n\nIn mathematical terms, the observations at yt and yt–k are separated by k time units. K is the lag. This lag can be days, quarters, or years depending on the nature of the data. When k=1, you’re assessing adjacent observations. For each lag, there is a correlation.\n\n#### Autocorrelation Function (ACF)\nUse the autocorrelation function (ACF) to identify which lags have significant correlations, understand the patterns and properties of the time series, and then use that information to model the time series data. From the ACF, you can assess the randomness and stationarity of a time series. You can also determine whether trends and seasonal patterns are present.\n\nIn an ACF plot, each bar represents the size and direction of the correlation. Bars that extend across the red line are statistically significant.\n\n#### Partial Autocorrelation Function (PACF)\nThe partial autocorrelation function is similar to the ACF except that it displays only the correlation between two observations that the shorter lags between those observations do not explain. For example, the partial autocorrelation for lag 3 is only the correlation that lags 1 and 2 do not explain. In other words, the partial correlation for each lag is the unique correlation between those two observations after partialling out the intervening correlations.\n\nAs you saw, the autocorrelation function helps assess the properties of a time series. In contrast, the partial autocorrelation function (PACF) is more useful during the specification process for an autoregressive model. Analysts use partial autocorrelation plots to specify regression models with time series data and Auto Regressive Integrated Moving Average (ARIMA) models. I’ll focus on that aspect in posts about those methods.\n\n*https://statisticsbyjim.com/time-series/autocorrelation-partial-autocorrelation/*"
 📖 Lag plots
## 5.4 Cholesterol Distribution 🥛
## 5.5 Gender Distribution based on Drug Type 👫💊
## 5.6 Blood Pressure Distribution based on Cholesetrol 🩸🥛
## 5.7 Sodium to Potassium Distribution based on Gender and Age 🧪👫👴
# 6. Dataset Preparation ⚙\n👉 This section will prepare the dataset before building the machine learning models.
"Since we have one missing value , i decided to fill it with the median value which will not have an important effect on the prediction."
"As we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled. \n\nIn this case, it is better to transform it with the log function to reduce this skew. "
"Since we have two missing values , i decided to fill them with the most fequent value of ""Embarked"" (S)."
"It seems that passenger coming from Cherbourg (C) have more chance to survive.\n\nMy hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown (Q), Southampton (S).\n\nLet's see the Pclass distribution vs Embarked"
"Indeed, the third class is the most frequent for passenger coming from Southampton (S) and Queenstown (Q), whereas Cherbourg passengers are mostly in first class which have the highest survival rate.\n\nAt this point, i can't explain why first class has an higher survival rate. My hypothesis is that first class passengers were prioritised during the evacuation due to their influence."
"## 4. Filling missing Values\n### 4.1 Age\n\nAs we see, Age column contains 256 missing values in the whole dataset.\n\nSince there is subpopulations that have more chance to survive (children for example), it is preferable to keep the age feature and to impute the missing values. \n\nTo adress this problem, i looked at the most correlated features with Age (Sex, Parch , Pclass and SibSP)."
"Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n\nHowever, 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers.\n\nMoreover, the more a passenger has parents/children the older he is and the more a passenger has siblings/spouses the younger he is."
**Heatmap**. The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:
"**Dendrogram**.The dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:"
\n# Column Types\n[Next](#t2_5)\n\nLet's look at the number of columns of each data type. int64 and float64 are numeric variables [(which can be either discrete or continuous)](https://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data). object columns contain strings and are [categorical features](https://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data). 
Correlations clustermap\n
\n# Visualizations\n\n[Back to Contents](#top)\n\n[Next](#t4)\n
Patient age quantile by sars cov2 exam result
"% of SARS COV2 exam result (0=Negative, 1=Positive)"
"\n# Encoding Variables\n\n[Back to Contents](#top)\n\n[Next](#t5)\n\nBefore we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as [LightGBM](Before we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM. Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process.\nYou can see this kaggle course: [Intermediate Machine Learning Home Page](https://www.kaggle.com/alexisbcook/categorical-variables)"
\n\n 2.2 Overview of univariate categorical variables\n
Overview of the categorical features shows the value counts of the strokes and no strokes for each categorical feature. This overview could give some insight where strokes are high in number.
 Gender Distribution \n
 HyperTension Distribution \n
 HeartDisease Distribution \n
 Marriage Distribution \n
 Residence Distribution \n
 Smoking Distribution \n
 Work Distribution \n
\n\n 2.4 Relationship between two continuous variables\n
"![](http://)3.2 Coordinate Mapping\n\nJust like we compared the travel time data and the number of passengers between the test and train sets, we can try and verify that the pickup location data in both sets are fairly similar and representative of one another. \n\n3.2.1 Pickup Locations\nTo do this, we utilise the city map border coordinates for New York, mentioned earlier in the kernel to create the canvas wherein the coordinate points will be graphed. To display the actual coordinates a simple scatter plot is used:"
"> We can tell from the two graphs that the pickup locations are quite similar, with the notable difference being that the train data set simply has more data points (which makes sense).\n\n3.2.2 Distance and Directionality\nThis next part is quite interesting. Thanks to Beluga's post, we can determine the distance and direction of a specific trip based on the pickup and dropoff coordinates. For this I've made three functions, as:"
"So, excellent! We can safely use the different date parts in their extracted forms as part of the modelling process. Let's take a look at the average speed and how it changes over time, specifically focusing on how the hour of the day, the day of the week, and the moth of the year affects average speed. It's important to note though that average speed is a function of distance and time so it wouldn't add anything to the modelling output. We'll therefore need to remove it eventually before we train our model."
"So the interesting thing to notice here is the average speed by hour of day. We're I'm from traffic usually peaks between 5am and 9am, and then again from about 4pm to around 6 or 7pm. But it would seem in manhattan that average speed diminishes as the day goes by from around 6am and picks up again around 7 or 8pm. So most of the travelling in the Big Apple happens during work hours. The average speed by weekday follows an expected trend. Over the weekend (Friday, Saturday, Sunday) the average speed picks up quite nicely, indicating less traffic. Finally, the average trip speed by month follows an expected trend. In the winter months there are less trips (see the previous timeseries plot we made) indicating less traffic in general in the city which means you can average a higher speed on the roads.\n\n\nThis next part uses the pick-up locations and the average speed data we've got and plots the average speed by location. "
"So clearly, by neighbourhood, the average speed definitely changes. To a greater extent the center of the city is the busiest (we'd expect this since the majority of activity in large cities is focused around the center) and the average speed picks up nicely around the outskirts.\n\nWe can expect good performance from our clustering data during modelling just from looking at how well we can distinguish average speeds by neighbourhood (i.e. cluster). Something definitely worth exploring, which could boost the performance of the XGBoost model significantly, is to create a data set that can be used with [Xiaolin Wu's line algorithm](https://en.m.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm ""Xiaolin Wu's line algorithm""). This would involve pixelating the graph area and recording every pixel crossed by the line from the pick-up location to the drop-off location. If you can make the resolution as high as possible some of the pixels shoudl encapsulate traffic junctions, traffic lights, bridges, etc. Using the ""has crossed coordinate X"" features one could potentially create an extra +-10 000 features to train the alogrithm with.\n\n**Note:** XGBoost will be able to handle up to about 10 000 columns (features) with about 1 million rows of data on a Mac Book Pro."
"- Looks like adding the `Reynolds` feature improved the misspredictions, especially the lower `l_chord` cases, looks quite neatly aligned now.\n\n\n    5.2 | HIGH FLOW SEPARATION MODEL\n\n\nTaking a look at the relation model prediction vs `spl` again:"
"- High flow separation cases are predicted much better than the lower flow separation case when using the same hyperparameter optimisation approach. \n- Some minor variation for higher `l_chord` cases can be noted. Let's see if the addition of `Reynolds` number can help increase the accuracy of the higher `l_chord` cases.\n- `Reynolds` feature also has a positive effect on model accuracy for the higher `aoa` model.\n\n\n    5.3 | GENERAL MODEL (BOTH CASES)\n\n\nThe Overall model should have similar mispredictions to both previous models, let's take a look:"
"- Looks like we have very similar misspredictions for the overall model (both lower and higher `l_chord` cases.\n- After add the additional feature, by the looks of it, we have made quite an accurate model for the prediction of `spl`.\n- The Gaussian based model knows can be a little too accurate for many problems, in that case you can simply adjust the hyperparameters, especially the matrix diagonal term. In this problem, we assumed that the noise hyperparameter, `sigma_n` is equal to 0.01 here.\n\n# 6 | CONCLUSIVE REMARKS\n\n- In this problem, we aimed to create a model that would predict the target variable `spl`, given a set of features. \n- A brief EDA was conducted, some interesting relations were noted. Each individual spectrum has its own unique relation to `spl`, it's not exactly possible to pinpoint exact spectrum noise sources, given we don't have anything visual to go by. \n- More notably was the `Reynolds` number relation that was noted in the experiment noise sources. This feature actually helped improve the model which was quite nice, \n- In the end, our customly created Gaussian Process Regression model was able to quite precisely predict the `spl`, with the addition of the new feature."
"### Growth Factor\nGrowth factor is the factor by which a quantity multiplies itself over time. The formula used is:\n\n**Formula: Every day's new (Confirmed,Recovered,Deaths) / new (Confirmed,Recovered,Deaths) on the previous day.**\n\nA growth factor **above 1 indicates an increase correspoding cases**.\n\nA growth factor **above 1 but trending downward** is a positive sign, whereas a **growth factor constantly above 1 is the sign of exponential growth**.\n\nA growth factor **constant at 1 indicates there is no change in any kind of cases**."
#### Growth Factor for Active and Closed Cases\nGrowth factor is the factor by which a quantity multiplies itself over time. The formula used is:\n\n**Formula: Every day's new (Active and Closed Cases) / new (Active and Closed Cases) on the previous day.**\n\nA growth factor **above 1 indicates an increase correspoding cases.**\n\nA growth factor **above 1 but trending downward is a positive sign.**\n\nA growth factor **constant at 1 indicates there is no change in any kind of cases.**\n\nA growth factor **below 1 indicates real positive sign implying more patients are getting recovered or dying as compared to the Confirmed Cases.**
#### Growth Factor constantly above 1 is an clear indication of Exponential increase in all form of cases.
"# 4.1 Feature Importance\nAmong the top 50 features, `Payment 2` has the highest average importance, which is also the feature that is the most negatively correlated with the target variable."
# 5 | Submission
"In addition to looking at the missing values of the monthly rent payment, it will be interesting to also look at the distribution of `tipovivi_`, the columns showing the ownership/renting status of the home. For this plot, we show the ownership status of those homes with a `nan` for the monthyl rent payment."
"The meaning of the home ownership variables is below:\n\n    tipovivi1, =1 own and fully paid house\n    tipovivi2, ""=1 own,  paying in installments""\n    tipovivi3, =1 rented\n    tipovivi4, =1 precarious\n    tipovivi5, ""=1 other(assigned,  borrowed)""\n    \nWe've solved the issue! Well, mostly: the households that do not have a monthly rent payment generally own their own home. In a few other situations, we are not sure of the reason for the missing information. \n\nFor the houses that are owned and have a missing monthly rent payment, we can set the value of the rent payment to zero. For the other homes, we can leave the missing values to be imputed but we'll add a flag (Boolean) column indicating that these households had missing values."
Lets Visualize our predictions in below scatter plot \n         -------
From the above result we can see that we are able to maintain the balance between Recall and Precision. \n\nPrecision of around 87% with Recall of 68% is not bad at all when we have such highly unbalanced data. \nThese numbers are not fixed and can vary . \n \n These numbers were different for Cross validation dataset and we shortlisted our Epsilon value by comparing the results of F1 Score.\n\nI will show you the result we achieved on Cross validation dataset again.
📌 From the graph we can infer that most of the area is filled with air as air has HU around -1000 and the next highest is water: HU around 0\n\n## Sorting our slices and GIF creation\n\nGo To Table of Contents\n
We are arranging the slices in order and skipping 5 slices at a time to look at wider variety of slices
## 3D Reconstruction using slices\n\nGo To Table of Contents\n
### Pixelspacing\n* The pixelspacing attribute you can find in the dicom files is an important one. It tells us how much physical distance is covered by one pixel. You can see that there are only 2 values that describe the x- and y-direction in the plane of a transversal slice.\n* For one patient this pixelspacing is usually the same for all slices.\n* But between patients the pixelspacing can differ due to personal or institutional preferences of doctors and the clinic and it also depends on the scanner type. Consequently if you compare two images in the size of the lungs it does not automatically mean that the bigger one is really larger in the physical size of the organ!
"We can see that the values really vary a lot from patient to patient! As they are given in mm and ct-scans usually cover 512 row and column values\n\n### Physical area & slice volume covered by a single ct-scan\n\nNow, we know some important quantities to compute the physical distance covered by a ct-scan!"
# Light EDA\n\nGo To Table of Contents\n
We can see an unusual heatmap this is because:\n\n📌 Most of our values are dervied which makes some features highly correlated
📌 Slice_thickness is directly proportional to the volume
"In both search methods, the `gbdt` (gradient boosted decision tree) and `dart` (dropout meets additive regression tree) do much better than `goss` (gradient based one-sided sampling). `gbdt` does the best on average (and for the max), so it might make sense to use that method in the future! Let's view the results as a barchart:"
__`gbdt` (or `dart`) it should be! Notice that random search tried `gbdt` about the same number of times as the other two (since it selected with no reasoning) while Bayesian optimization tried `gbdt` much more often. __\n\nSince `gbdt` supports `subsample` (using on a sample of the observations to train on in every tree) we can plot the distribution of `subsample` where `boosting_type=='gbdt'`. We also show the reference distribution.
There is a significant disagreement between the two methods on the optimal value for `subsample`. Perhaps we would want to leave this as a wide distribution in any further searches (although some subsampling does look to be beneficial).
"__The only clear distinction is that the score decreases as the learning rate increases.__ Of course, we cannot say whether that is due to the learning rate itself, or some other factor (we will look at the interplay between the learning rate and the number of esimators shortly). The learning rate domain was on a logarithmic scale, so it's most accurate for the plot to be as well (unfortunately I cannot get this to work yet)."
Now for the next four hyperparameters versus the score.
\n## 3.1. Exploration of Age
"The age distribution for survivors and deceased is actually very similar.  One notable difference is that, of the survivors, a larger proportion were children.  The passengers evidently made an attempt to save children by giving them a place on the life rafts. "
"Considering the survival rate of passengers under 16, I'll also include another categorical variable in my dataset: ""Minor"""
\n## 3.2. Exploration of Fare
"As the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model.  Passengers who paid lower fare appear to have been less likely to survive.  This is probably strongly correlated with Passenger Class, which we'll look at next."
\n## 3.3. Exploration of Passenger Class
"Unsurprisingly, being a first class passenger was safest."
\n## 3.4. Exploration of Embarked Port
"Passengers who boarded in Cherbourg, France, appear to have the highest survival rate.  Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown.  This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck).  It's also worth noting the size of the whiskers in these plots.  Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest.  The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker.  It's possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts."
\n## 3.5. Exploration of Traveling Alone vs. With Family
"Individuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it's likely that individuals traveling alone were likely male."
\n## 3.6. Exploration of Gender Variable
This is a very obvious difference.  Clearly being female greatly increased your chances of survival.
\n### 4.1.2. Feature ranking with recursive feature elimination and cross-validation\n\nRFECV performs RFE in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation.
"As we see, eight variables were kept. "
**Now let's put it all together again**
"Sweet, now we can see the Pawpularity score for a given image file. We pulled this information from two different places and put it together in a loop. \n\n**Let's do something a little different now and build a function that returns pet pictures based on given pawpularity scores.**"
### Pawpularity 10
"Don't wonder, the stuff inside the sine wave is a bit more complicated than what we used above. Instead of $y=sin(t)$ it holds $y=r \cdot sin(\omega  t)$ with $\omega=2 \pi f$ being [the angular frequency](https://en.wikipedia.org/wiki/Angular_frequency) and $r$ being the radius that we can set to adjust the amplitude $\hat{y}$ (max elongation). You can see that it's just a constant times frequency. So there is nothing new to learn. A higher frequency means more ups and downs of our wave within a fixed time period compared to a low frequency. "
"Ok, with these values we obtain one wave with high frequency and low radius and another wave with low frequency and higher radius. Adding up both we can see the same kind of ""problem"" like above - a ""big wave"" with a small one ""hidden"". "
"Ok, let's try it:\n\n* Perform the Fast Fourier Transform with $y_{1}$, $y_{2}$ and $y_{1}+y_{2}$\n* Compute the Power Spectral Density to get the components\n* Show that $y_{1}+y_{2}$ is like PSDs of $y_{1}$ and $y_{2}$ added up."
"### Insights\n\nCool, isn't it? :-) We can clearly see that the wave that was built by adding up the two sine waves has both components of them in its PSD. This is awesome as we can now use such kind of PSD components to filter out signals of our desire! To get closer to the competition I have added a noisy version of $y1+y2$ that shows a lot of fluctuations and high frequency components in the PSD.\n\nLet's try it out using the competition data!"
Now we can use the same idea as above to compute the Power Spectral Density:
"### Insights\n\n* We can see that we have more fluctuations in the PSD for high frequencies. :-( That's bad as there should be our hidden signal as well.\n* The low frequencies belong to our ""big waves"" that are caused by instrumental vibrations or terrestrical forces etc. Here we won't find our signal.\n\nLet's pick a few component examples (or regions in the PSD) to show which kind of wave belongs to it. This way we can compare with the original data and get some feeling about its structure:"
Let's take a look how the window functions change our data:
Hmm... what if our signal can be found in the beginning our end of the data? Then windowing would be very bad... wouldn't it?!
We can plot the bars horizontally as follows :-
#### Visualize `income` wrt `sex` variable
#### Interpretation\n\n\n- We can see that males make more money than females in both the income categories.
"In order to have a global view of the type of order performed in this dataset, I determine how the purchases are divided according to total prizes:"
"It can be seen that the vast majority of orders concern relatively large purchases given that $\sim$65% of purchases give prizes in excess of £ 200.\n\n____\n## 3. Insight on product categories\n\nIn the dataframe, products are uniquely identified through the **StockCode** variable. A shrort description of the products is given in the **Description** variable. In this section, I intend to use the content of this latter variable in order to group the products into different categories.\n\n___\n### 3.1 Products Description\n\nAs a first step, I extract from the **Description** variable the information that will prove useful. To do this, I use the following function:"
"This function takes as input the dataframe and analyzes the content of the **Description** column by performing the following operations:\n\n- extract the names (proper, common) appearing in the products description\n- for each name, I extract the root of the word and aggregate the set of names associated with this particular root\n- count the number of times each root appears in the dataframe\n- when several words are listed for the same root, I consider that the keyword associated with this root is the shortest name (this systematically selects the singular when there are singular/plural variants)\n\nThe first step of the analysis is to retrieve the list of products:"
"Using it, I create a representation of the most common keywords:"
___\n### 3.2 Defining product categories 
### Categorical variables\nLet's look into the categorical variables and the proportion of customers with target = 1
"As we can see from the variables **with missing values**,  it is a good idea to keep the missing values as a separate category value, instead of replacing them by the mode for instance. The customers with a missing value appear to have a much higher (in some cases much lower) probability to ask for an insurance claim."
### Interval variables\nChecking the correlations between interval variables. A heatmap is a good way to visualize the correlation between variables. The code below is based on [an example by Michael Waskom](http://seaborn.pydata.org/examples/many_pairwise_correlations.html)
"There are a strong correlations between the variables:\n- ps_reg_02 and ps_reg_03 (0.7)\n- ps_car_12 and ps_car13 (0.67)\n- ps_car_12 and ps_car14 (0.58)\n- ps_car_13 and ps_car15 (0.67)\n\nSeaborn has some handy plots to visualize the (linear) relationship between variables. We could use a *pairplot* to visualize the relationship between the variables. But because the heatmap already showed the limited number of correlated variables, we'll look at each of the highly correlated variables separately.\n**NOTE**: I take a sample of the train data to speed up the process. "
"#### ps_reg_02 and ps_reg_03\nAs the regression line shows, there is a linear relationship between these variables. Thanks to the *hue* parameter we can see that the regression lines for target=0 and target=1 are the same."
#### ps_car_12 and ps_car_13
"#### **Correlation Heatmap**\n\nWe shall do a correlation Heatmap to visualize which variables are more correlated with each other, in most context, correlation can help in predicting one attribute from another, which is a great way to impute missing values (we do have a lot of missing values and we will see some examples later). Of course, in regression wise, correlation matrix can be used to detect multi collinearity or say outliers.\n\n\nBy default, sns.heatmap will not include categorical features which are not factorized. As a result, the heatmap below did not include those categorical variables, only the columns with numerical are plotted against each other.\n\nSo since some categorical variables are not inside this map, we need to view their correlation with the other variables seperately. For example, our earlier plot suggests that ""sex/gender"" being female has a higher survival rate than males. Note that it is difficult to compare correlation of categorical and continuous variables and hence it brings up to the next section - where we plot more graphs to attempt to obtain an intuition between all the variables."
"### 3.2) Plot variables against Survived  \n\nNote that one should see that Name, Ticket, PassengerID will not play any crucial role in predicting the survival rate.\nHence I do not want to analyse them, later I will drop them. However, after reviewing many kernels by other experienced data analysts, we can do some feature engineering with Name - We will mention it later. We will do some basic overview of the survival rate in different categories."
"Bear in mind, we can use plot 1 when our categorical pclass does not have many categories/levels. What if pclass has 30 different levels. Then our graph will look messy. This is why I also used a line plot in plot 2 to illustrate that line plot can be a useful visualization tool as well.\n\n\n\nFrom the above 2 plots, we have the same conclusion, in Pclass 1, 63 percent of the passengers in Pclass 1 survived. In Pclass 2, 47.3 percent of passengers in Pclass 2 survived. In Pclass 3, 24.2 percent of Pclass 3 passengers survived. **So we have a feel that the the better your class, the higher your chance of survival.**\n\n\n\nBut do recall in our first part 3.3.1, we did see that gender DO affect one's chance of survival. We can further confirm our hypothesis by plotting Gender + Pclass vs Survival Rate.\n\n\n\n**How many men and women survived grouped by their Passenger Class?**\n\nHere is a neat code from https://www.kaggle.com/poonaml/titanic-survival-prediction-end-to-end-ml-pipeline to plot this.\n\nOne may ask what can we get from this plot? Well for a start, we do know that women has a much higher survival rate than men, so is this still true if we categorize them by different Pclass? Apparently, you will see later that for Pclass 1 and 2, women still maintain a very high survival rate, but in Pclass 3, we can see the women's survival rate dip down by quite a lot. This may suggest Pclass 3 is the ""poorer"" people's class - and hence they are valued less when evacuating (unfortunately life is unfair).\n\nAlso note that we can see in a correlation heatmap, fare and pclass are quite strongly correlated with a value of 0.55. So we reckon higher fare corresponds to Pclass 1.\n\n\n\n\nOne side note on the coding part below: catplot returns you a function called g which is a FacetGrid object with the plot on it for further tweaking. So the below code is how we usually add on different plottings on the facetgrid itself. Note there is a difference between the way we did on barplot, on barplot it returns you a function called ax which is a single grid.\n"
"Factor/cat Plot is quite useful for comparing multiple levels. Please explore it. The above plots can be interpreted as follows: In Pclass 1, about 38 percent of the **male** survived, for all the female passengers in Pclass 1, more than 90 percent of the **female** survived! \n\n\n\nSimilarly, we can find the trend for Pclass 2 and Pclass 3. \n\n\n\nA good conclusion we can make is: Female in Pclass 1 and 2 have a very high survival rate of 90 percent while their male counterparts are significantly lesser. All passengers in Pclass 3 have a low survival rate. My hypothesis is that male in general has a low survival rate irregardless of their Pclass."
**Pclass vs Embarked**\n
"Indeed, Passengers embarked from Chersboug has a high **proportion** of Pclass 1. This may account for one of the reasons that passengers who boarded from Chersboug has a significantly higher suvival rate than the rest of the cities."
**Gender vs Embarked**
"Although this plot did not tell us anything informative on whether females play a role in why Chersboug has a higher survival rate, it did hint to us that why Southampton has the lowest survival rate. A good reason is that there is way more males embarked from Southampton than females. "
"### 3.2.4) Plot Age against Survived  \n\nNow we do an EDA on Age vs Survival Rate. Age is a continuous variable, and we should not use bar chart here. I present one of the way to plot a KDE distribution below, where you superimpose the age plot for survived and the age plot for not survived. The idea of the code is you first plot the age plot for not survived, and then you plot the age plot for survived on the same axis: ax = myaxis, then the graph will present both graphs on the same grid.  "
"The above plot shows the age vs survival rate, represented by a density plot; for survivors and non survivors, the distribution seems to be similar. However, for survivors we see an obvious local maximum at around 0-5 years old, indicating small children has a higher survival rate. **Reminder: This distribution is before filling in the missing values of age.**\n\nBelow are some more visualizations graphs."
We have managed to find even better solution comparing to the RandomizedSearch.\nLet's look at the visualization of the process
"We can see that the movement of the parameters are quite random but the results become better with time: there are no extremely bad scores after 25 iterations but the number of good solutions increases. Algorithm started to predict quite good solutions, using information from the previous steps."
Let's plot best_cumulative_score vs. number_of_iterations for all approaches:
"We can see than TPE and Annealing algorithms actually keeps improving search results over time even on later steps while Random search randomly found quite a good solution in the beginning and then only slightly improved the results. The current difference between TPE and RandomizedSearch results is quite small but in some real life applications with more diversified range of hyperparameters hyperopt can give you significant time/score improvement.\n\nNote: in real life it is more correct to use time and not a number of iterations for comparison, but in our toy example the proportion of time spent on the additional calculations in tpe and annealing is to high comparing to cross_val_score calculation time so I have decided not to mislead you about computational speed of the hyperopt and plot scores in relation to the iteration number."
Let's try to separate these two classes by training an `Sklearn` decision tree. We will use `max_depth` parameter that limits the depth of the tree. Let's visualize the resulting separating boundary.
"And how does the tree itself look? We see that the tree ""cuts"" the space into 8 rectangles, i.e. the tree has 8 leaves. Within each rectangle, the tree will make the prediction according to the majority label of the objects inside it."
#### Example\nLet's generate some data distributed by the function $f(x) = e^{-x ^ 2} + 1.5 * e^{-(x - 2) ^ 2}$ with some noise. Then we will train a tree with this data and predictions that the tree makes.
We see that the decision tree approximates the data with a piecewise constant function.
"Yeah! :-) We can still see the true target and not the fooling target. That's amazing. But we can also see, that the background has increased intensitiy. Let's visualize the difference between the original true label and the adversarial image for $\epsilon = 16$: "
"### The gradient travel guide - natural fooling targets\n\nI'm happy that it was possible to fool our model but it's still diffuse and unclear where the one-step-gradient guides us through (remember we do not iterate with gradient ascent, we just take one step and size is given by strength of gradient times eta). I assume that some numbers are closer to each other in weight space than to others. As the model training draws decision boundaries dependent on the quality of the input data and flexibility of model architecture, there will be regions where a 3 is not predicted as 3 but as 8. Those regions where the model makes an incorrect prediction. And I think, that there are preffered numbers to be wrong predictions given a digit input image. Perhaps the fooling gradients drives us to those ""natural"" fooling target numbers? "
"Ok, we see that 8 was selected most often as fooling target. But 9, 3, 5 and 2 have high counts as well in contrast to 0, 1, 6 and 7. If our assumption is true that the gradient drives us to targets where the model tends to fail in prediction we should see a similar pattern of counts for wrong predictions:"
"Ok, so out of 16800 samples, the model failed to predict around 1600. That's why our intital accuracy score is close to 90 % (means 10 % failing). Now, which digit was selected as wrong prediction result most often?"
"Yes, that's the same pattern as for the fooling targets. As this is caused by the difficulty of our model to draw good decision boundaries we should see this pattern as well for the true labels of those digits that were wrong predicted:"
Now I want to see it in more detail: Which are the natural fooling targets (for successful foolings) for each digit?
"# 4. Advice to the tourists\n## 4.1 Average daily price per neighbourhood\nIn order to compare ""apples to apples"" I have only selected the most common type of accommodation, which is accommodation for 2 persons. As expected, accommodation in the city centre is the most expensive. "
"Below you can find the neighbourhoods on a map. This map is **interactive**. Hovering over the polygons shows the name of the neighbourhood, and the average price for 2-persons accommodations."
"## 4.2 Neighbourhood safety\nAs there is no data on neighbourhood safety in the Airbnb files, I searched for this online and came across the map below. As you can see, nowadays the western parts of the city are the most dangerous.\n\nPersonal note: Apparently ""de Bijlmer"" (south-eastern areas) is reasonably safe these days. However, I used to know somebody who lived there as a student years ago. At that time it certainly was not a good neighbourhood, and I was warned to ""stay under the balconies"" as people might throw rubbish and old furniture from their balconies. Getting an old couch on your head did not sound like the most pleasant thing in the world ;-).\n\n"
"## 4.3 Review scores location, and location scores versus price\nIn tis section, I am grouping the review scores for the location by neighbourhood (only listings with at least 10 reviews). Although I expect the distance to the city centre to an important factor, these score should also take other things into account. Other factors may include:\n\n* The safety of a location (as displayed in the previous section)\n* Noise. If a listing is centrally located, but surrounded by noisy bars, that should cost points in the location review score.\n* If a listing is located outside the city centre but well connected by public transportation, it should get bonus points for that.\n* Facilities near the listing. Are there any supermarkets, bars and restaurants nearby?\n* Some people may be looking for free parking if they come by car (parking is very expensive in Amsterdam in general).\n\nBelow we see that the central neighbourhoods, which were generally also the most expensive, generally also score higher on location review score.  If I would calculate the distance to the city centre for each listing, I expect to see pretty strong correlations between this distance with both price and location review score.\n\nWhen looking at the average review score, I am surprised to see that the average is above 8/10 for all neighbourhoods! I know that Amsterdam is a small city (much smaller than many people might think!). Therefore, it does not take much time to get to the city centre from anywhere, which might explain the high averages to a certain extend. My personal advice to tourists would be to consider more affordable accommodation outside the city centre, in a safe neighbourhood, and with good public transportation connections to the city centre anyway. However, are the differences between the best locations and outside neighbourhoods really that small? Let's find out in the next section!\n"
"## 4.4 How to use review scores\nIn addition to written reviews, guests can submit an overall star rating and a set of category star ratings. Guests can give ratings on:\n\n* Overall Experience. What was your overall experience?\n* Cleanliness. Did you feel that your space was clean and tidy?\n* Accuracy. How accurately did your listing page represent your space?\n* Value. Did you feel your listing provided good value for the price?\n* Communication. How well did you communicate with your host before and during their stay?\n* Arrival. How smoothly did their check-in go?\n* Location. How did you feel about the neighborhood?\n\nBelow you can see the scores distribution of all those categories. What caught my eye immediately is that scores seem really high across the board! A quick internet search told me that this seems common across Airbnb. It is explained well in this article: [Higher than the average rating? 95% of Airbnb listings rated 4.5 to 5 stars](https://mashable.com/2015/02/25/airbnb-reviews-above-average/?europe=true#1YLfzOC34sqd).\n\nAfter having seen the scores distributions, I would personally consider any score of 8 or lower to be not a good score. If I wanted to use any of these scores in a search for accomodation, I believe the ""Value"" seems most useful. First of all, I always like to get good value for money ;-). However, the number of ""10 averages"" is reasonably small, which makes the indicator a bit more ""distinguishable"" than other indicators."
"## 4.5 Finding a good host\nAt Airbnb you can get the status ""Superhost"". From Airbnb:\n* As a Superhost, you’ll have more visibility, earning potential, and exclusive rewards. It's our way of saying thank you for your outstanding hospitality.\n* How to become a Superhost: Every 3 months, we check if you meet the following criteria. If you do, you'll earn or keep your Superhost status.\n    * Superhosts have a 4.8 or higher average overall rating based on reviews from at least 50% of their Airbnb guests in the past year. \n    * Superhosts have hosted at least 10 stays in the past year or, if they host longer-term reservations, 100 nights over at least 3 stays. \n    * Superhosts have no cancellations in the past year, unless there were extenuating circumstances.\n    * Superhosts respond to 90% of new messages within 24 hours.\n\nBelow, we can see that only a small portion of the listings in Amsterdam do have a host who is Superhost."
"If I were to book accomomodation, I would not necessarily look for a superhost. Actually, I would be afraid that I would pay too much as superhost will likely increase their prices. However, I would also not want to host that responds badly, or cancels a lot.\n\nAs we can see, over 5,000 of the 20,000 listings have at least 10 reviews and respond to at least 90% of the new messages. I would consider those hosts ""proven"" good responders (which does not mean that a listing with less than 10 reviews cannot have good responding hosts; it is just not proven yet). Also, there are very few listings with hosts not replying to new messages within 24 hours."
"## 4.6 Availability over time\nThe calendar file holds 365 records for each listing, which means that for each listing the price and availablity by date is specified 365 days ahead."
### Correlation Matrix :
"- Both the matrix displayed are same! It is only done for visualization purpose. This trick can be used when the dataset has too many features to look into! \n- **CustomerID** displays a very high positive correlation with **Annual Income (k\\$)** as the customers are arranged in ascending order according to their **Annual Income (k\\$)**. We are not going to include **CustomerID** for modeling purpose.\n- **Gender** does not display any relation with other features. It is pretty much neutral with all the values sticking to 0.\n- **Spending Score (1-100)** and **Age** display a negative correlation i.e if value of one feature increases, then another feature's value decreases and vice-versa is true!\n- This information is gained from the **EDA** section the correlation matrix values further back the evidence. \n- We will now move to the modeling section by creating combinations of these features and finding different ways in which the mall customers can be segmented!"
### Original Dataset :\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 4**"
### Normalized Dataset :\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 3**"
### Age - Spending Score (1-100)
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 6**"
### Annual Income (k\$) - Spending Score (1-100)
"# **16. Visualize feature scores of the features** \n\n[Table of Contents](#0.1)\n\n\nNow, I will visualize the feature scores with matplotlib and seaborn."
"# **17. Build Random Forest model on selected features** \n\n[Table of Contents](#0.1)\n\n\nNow, I will drop the least important feature `doors` from the model, rebuild the model and check its effect on accuracy."
"And now we can use stemmer to see if it can reduce our these test words (""running"", ""runs"", ""run"") into their a single stemmed word. Conveniently we can test the stemmer on the fly as follows:"
"As we can see, the stemmer has successfully reduced the given words above into a base form and this will be most in helping us reduce the size of our dataset of words when we come to learning and classification tasks.\n\nHowever there is one flaw with stemming and that is the fact that the process involves quite a [crude heuristic in chopping off the ends of words](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) in the hope of reducing a particular word into a human recognizable base form. Therefore this process does not take into account vocabulary or word forms when collapsing words as this example will illustrate:"
"### Lemmatization to the rescue\n\nTherefore we turn to another that we could use in lieu of stemming. This method is called lemmatization which aims to achieve the same effect as the former method. However unlike a stemmer, lemmatizing the dataset aims to reduce words based on an actual dictionary or vocabulary (the Lemma) and therefore will not chop off words into stemmed forms that do not carry any lexical meaning. Here we can utilize NLTK once again to initialize a lemmatizer (WordNet variant) and inspect how it collapses words as follows:"
"**Revisiting our Term frequencies**\n\nHaving implemented our lemmatized count vectorizer, let us revist the plots for the term frquencies of the top 50 words (by frequency). As you can see from the plot, all our prior preprocessing efforts have not gone to waste. With the removal of stopwords, the remaining words seem much more meaningful where you can see that all the stopwords in the earlier term frequency plot "
"## 3b. Latent Dirichlet Allocation\n\nFinally we arrive on the subject of topic modelling and the implementation of a couple of unsupervised learning algorithms. The first method that I will touch upon is [Latent Dirichlet Allocation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). Now there are a couple of different implements of this LDA algorithm but in this notebook, I will be using Sklearn's implementation. Another very well-known LDA implementation is Radim Rehurek's [gensim](https://radimrehurek.com/gensim/), so check it out as well."
#  Bubble Chart
# 3D Scatter Plots
# Multivarite Numerical Plot
- **imports : It is another attribute that determines the reliance of the nations on other nations for the goods and services!**
"- **imports** stats of a nation describe the self reliance of a nation to solve their problems irrespective of being handicapped on one of the essential resources.\n- **Singapore**, **Malta**, **Luxembourg** & **Seychelles** are present in the top 5 of **exports** as well as **imports**. This is just an indication that highlight the nation's strategies of probably capitalizing on their resources and creating solid **exports** that gets countered by the heavy **imports** on something else. This just balances the books!\n- **Brazil** has the lowest **imports** out of all the nations with **11%**. **Sudan** is the only African country present in this lower end list with **17%**."
"- **income : Income of the per person is a key indicator about country's economic state. Higher the income of the citizens, more capabale they are to deal with uncertain situations!**"
"- Citizens of the **Qatar** have the highest **income** out of all the countries with a difference of **30k** more than the 2nd placed countries. **Singapore** & **Luxembourg** are again present in the top 5 of another feature.\n- Lower end of the **income** is dominated by the **African** nations. This is influenced by the damage done by colonization out of which the nations have not yet recovered. \n- The difference in the **income** of the nations in the top, middle and lower end is quite significant that will have an effect on every other features."
"- **inflation : It dictates the state of the economy, strength of the currency as well as demand for goods & services!**"
"- Higher **inflation** reduces the purchasing power of the citizens. Countries present at the top end of **inflation** have a devastating economic situation. Having such high inflation is a risk to the existence of the nation.\n- Similarly, the lower end of **inflation** has negative values i.e known as deflation. It signals a economy in a downward spiral leading to a recession or even a depression."
- **life_expec : Higher life expectancy displays citizens with health attributes physically as well as mentally!**
- **life_expec** depends alot on mental state as well as the lifestyle adopted by the citizens. **Singapore** is again present in the top of 5 of a feature.\n- None of the countries with a high **life_expec** are present in the top 5 of **health** that is related to the spending done by the citizen on health.\n- **African** countries are again present in the lower end for another feature.
- **total_fer : Economically backward countries have a high fertility rate!**
- **African** countries dominate the **total_fer** with values **6+**.\n- Mean **total_fer** value is **2** whereas lower end values of **1** concern abit as well.
- **gdpp : It is a feature that provides information about the contribution of a citizen to it's GDP!**
- It is a tricky feature as the population of the nation is a significant factor. One clear cut example of this is **China**. It has a huge population alongwith huge GDP.\n- **Luxembourg** is again present in the top ranks. **Switzerland** & **Qatar** are present in the top 5 similar to **income**.\n- Lower end is again dominated by **African** nations that labels them as the economically backward.
###  Numerical Features :
# Summary EDA
"- Many features have relationships with each other.\n- **child_mort** clearly increases when **income**, **gdpp** & **exports** decreases. Rise in **inflation** also leads to high **child_mort** cases. Economic conditions unfortunately act as an important factor!\n- Rise in **exports** clearly increases **gdpp**, **income** & **imports**.\n- Spending on **health** has a small rise in **life_expec** and also decreases the **child_mort**.\n- **income** & **gdpp** display a very high 0.9 correlation value. From the health perspective, high **income** has lead to higher **life_expec** but decreases the **total_fer** by some significant margin.\n- As expected high **inflation** has a negative effect on the financial features. High **inflation** displays a high **total_fer** and **child_mort**. This describes the typical features of a backward nation.\n- According to the data, higher **life_expec** displays a low **total_fer**. Higher **gdpp** has lead more spending on **health**. \n\n**We can clearly see that some features are essentially from the same category and they have the same reaction to other features of different category.**\n- The 3 categories of the features are :\n    - **health** : **child_mort**, **health**, **life_expec**, **total_fer**\n    - **trade** : **imports**, **exports**\n    - **finance** : **income**, **inflation**, **gdpp**\n- Hence, we will dissolve these features into these categories and normalize them!"
"**Observations**\n* The Fare distribution is very skewed to the left. This can lead to overweigthing the model with very high values.\n* In this case, it is better to transform it with the log function to reduce the skewness and redistribute the data."
**Observations**\nLog Fare categories are:\n* 0 to 2.7: less survivors\n* More than 2.7 more survivors
### Titles
"There are 4 types of titles:\n0. Mme, Ms, Lady, Sir, Mlle, Countess: 100%. \n1. Mrs, Miss: around 70% survival\n2. Master: around 60%\n3. Don, Rev, Capt, Jonkheer: no data\n4. Dr, Major, Col: around 40%\n5. Mr: below 20%"
## 3.1. Correlation analysis with histograms and pivot-tables
"**Observations for Age graph:**\n* 0 or blue represent women; 1 or orange represent men. Gender and age seem to have a stronger influece of the survival rate.\n* We start to find where most survivors are: older women (48 to 64 year old), and younger passengers.\n* What is statistically interesting is that only young boys (Age Category = 0) have  high survival rates, unlike other age groups for men.\n* We will create a new feature called young boys"
These plots are beautiful. It's a shame we don't have this data for the test set.
## Target vs. Atom Count
"When we look at the target `scalar_coupling_constant` in relation to the `atom_count` - there visually appears to be a relationship. We notice the gap in coupling constant values, between ~25 and ~75. It is rare to see a value within this range. Could this be a good case for a classification problem between the two clusters?"
# Super Simple Baseline Model [1.239 Public LB]\nThe simplest thing we can do as a model is predict that the target is the **average** value that we observe for that **type** in the training set!
"# Evaluation Metric\n\nSubmissions are evaluated on the Log of the Mean Absolute Error, calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type.\n\n![Eval Metric](https://i.imgur.com/AK6z3Dn.png)\n\nWhere:\n\n- `T` is the number of scalar coupling types\n- `nt` is the number of observations of type t\n- `yi` is the actual scalar coupling constant for the observation\n- `yi^` is the predicted scalar coupling constant for the observation\n\nFor this metric, the MAE for any group has a floor of 1e-9, so that the minimum (best) possible score for perfect predictions is approximately -20.7232."
Evaluation metric is important to understand as it determines how your model will be scored. Ideally we will set the loss function of our machine learning algorithm to use this metric so we can minimize the specific type of error.\n\nCheck out this kernel by `@abhishek` with code for the evaluation metric: https://www.kaggle.com/abhishek/competition-metric\n
# Feature Creation\nThis feature was found from `@inversion` 's kernel here: https://www.kaggle.com/inversion/atomic-distance-benchmark/output\nThe code was then made faster by `@seriousran` here: https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark
"\n    The following plot will compare the actual and predicted temperatures. Due to large amount of data, the following plot will use the first 15 data as samples to compare actual and predicted temperatures.\n\n"
## 6.2 | Evaluating Assumptions 🔍\n## 6.2.1 | Linearity Assumption\n
"\n    The scatter plot above shows that it is not a perfect linear relationship since the predicted line barely fits the diagonal line. The predictions are skewed towards lower values (between -10 and 0) and especially towards higher values (above 20). It is advised to apply nonlinear transformations or add polynomial terms to some predictors. If those fail to reflect the relationship between the predictors and the label, consider using more variables.\n    \n        . : . Linearity assumption not satisfied . : .\n    \n\n"
## 6.2.2 | Normality Assumption\n
"\n    The distribution is right-skewed since the median value is smaller than the mean value. In addition, the p-value distribution shows that it is smaller than 0.05, which indicates that the distribution is not normal, which may affect the confidence interval. It is suggested to perform non-linear transformations on variables.\n    \n        . : . Normality assumption not satisfied . : .\n    \n\n"
## 6.2.5 | Homoscedasticity Assumption\n
"\n    There is no uniform variance across the residuals, which is potentially problematic. This can be solved by either using weighted least squares regression instead of the standard OLS or transforming the dependent or highly skewed variables. A log transformation on the dependent variable is also a good starting point.\n    \n        . : . Homoscedasticity assumption not satisfied . : .\n    \n\n"
"# Parallel Coordinates\n\nWe now want to get another perspective on high dimensional data, such as the TF-IDF encoded questions. For that purpose I'll encode the concatenated questions into a set of N dimensions, s.t. each row in the dataframe then has one N dimensional vector associated to it.\nWith this we can then have a look at how these coordinates (or TF-IDF dimensions) vary by label.\n\nThere are many EDA methods to visualize high dimensional data, I'll show parallel coordinates here.\n\nTo make a nice looking plot, I've chosen N to be quite small, much smaller actually than you would encode it in a machine learning algorithm."
In the parallel coordinates we can see that there are some dimensions that have high TF-IDF features values for duplicates and others high values for non-duplicates.
"# Question character length correlations by duplication label\n\nThe pairplot of character length of both questions by duplication label is showing us that, duplicated questions seem to have a somewhat similar amount of characters in them.\n\nAlso we can see something quite intuitive, that there is rather strong correlation in the number of words and the number of characters in a question."
"# Model starter\n\nTrain a model with the basic feature we've constructed so far.\n\nFor that we will use Logisitic regression, for which we will do a quick parameter search with CV, plot ROC and PR curve on the holdout set and finally generate a submission."
"### ROC\n\nReceiver operator characteristic, used very commonly to assess the quality of models for binary classification.\n\nWe will look at at three different classifiers here, a strongly regularized one and two with weaker regularization. The heavily regularized model has parameters very close to zero and is actually worse than if we would pick the labels for our holdout samples randomly."
"# Precision-Recall Curve\n\nAlso used very commonly, but more often in cases where we have class-imbalance. We can see here, that there are a few positive samples that we can identify quite reliably. On in the medium and high recall regions we see that there are also positives samples that are harder to separate from the negatives."
# Prepare submission\n\nHere we read the test data and apply the same transformations that we've used for the training data. We also need to scale the computed features again.
### Correlation Heat Map
"The heatmap is the best way to get a quick overview of correlated features thanks to seaborn!\n\nAt initial glance it is observed that there are two red colored squares that get my attention. \n1. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables.\n2. Second one refers to the 'GarageX' variables. \nBoth cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. \n\nHeatmaps are great to detect this kind of multicollinearity situations and in problems related to feature selection like this project, it comes as an excellent exploratory tool.\n\nAnother aspect I observed here is the 'SalePrice' correlations.As it is observed that 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hello !' to SalePrice, however we cannot exclude the fact that rest of the features have some level of correlation to the SalePrice. To observe this correlation closer let us see it in Zoomed Heat Map "
#### SalePrice Correlation matrix
From above zoomed heatmap it is observed that GarageCars & GarageArea are closely correlated .\nSimilarly TotalBsmtSF and 1stFlrSF are also closely correlated.\n
"Visualisation of 'OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd' features \nwith respect to SalePrice in the form of pair plot & scatter pair plot for better understanding."
"Although we already know some of the main figures, this pair plot gives us a reasonable overview insight about the correlated features .Here are some of my analysis.\n\n- One interesting observation is between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area.\n\n- One more interesting observation is between 'SalePrice' and 'YearBuilt'. In the bottom of the 'dots cloud', we see what almost appears to be a exponential function.We can also see this same tendency in the upper limit of the 'dots cloud' \n- Last observation is that prices are increasing faster now with respect to previous years."
#### Box plot - OverallQual
#### Box plot - Neighborhood
#### Count Plot - Neighborhood
Based on the above observation can group those Neighborhoods with similar housing price into a same bucket for dimension-reduction.Let us see this in the preprocessing stage
With qualitative variables we can check distribution of SalePrice with respect to variable values and enumerate them. 
#### Housing Price vs Sales\n\n- Sale Type & Condition\n- Sales Seasonality
#### ViolinPlot - Functional vs.SalePrice
#### FactorPlot - FirePlaceQC vs. SalePrice 
#### Facet Grid Plot - FirePlace QC vs.SalePrice
"**Plot.ly Scatter Plot of feature importances**\n\nHaving trained the Random Forest, we can obtain the list of feature importances by invoking the attribute ""feature_importances_"" and plot our next Plotly plot, the Scatter plot.\n\nHere we invoke the command Scatter and as per the previous Plotly plots, we have to define our y and x-axes. However the one thing that we pay attention to in scatter plots is the marker attribute. It is the marker attribute where we define and hence control the size, color and scale of the scatter points embedded."
"Furthermore we could also display a sorted list of all the features ranked by order of their importance, from highest to lowest via the same plotly barplots as follows:"
"**Decision Tree visualisation**\n\nOne other interesting trick or technique oft used would be to visualize the tree branches or decisions made by the model. For simplicity, I fit a decision tree (of max_depth = 3) and hence you only see 3 levels in the decision branch, use the export to graph visualization attribute in sklearn ""export_graphviz"" and then export and import the tree image for visualization in this notebook."
## Ratings\nLet us find out the ratings
## Bird Seen by Country
# Audio Data Analysis
"## Spectrogram Analysis\n\n![](https://www.researchgate.net/profile/Phillip_Lobel/publication/267827408/figure/fig2/AS:295457826852866@1447454043380/Spectrograms-and-Oscillograms-This-is-an-oscillogram-and-spectrogram-of-the-boatwhistle.png)\n\n**What is a spectrogram?**\nA spectrogram is a visual way of representing the signal strength, or “loudness”, of a signal over time at various frequencies present in a particular waveform.  Not only can one see whether there is more or less energy at, for example, 2 Hz vs 10 Hz, but one can also see how energy levels vary over time.  In other sciences spectrograms are commonly used to display frequencies of sound waves produced by humans, machinery, animals, whales, jets, etc., as recorded by microphones.  In the seismic world, spectrograms are increasingly being used to look at frequency content of continuous signals recorded by individual or groups of seismometers to help distinguish and characterize different types of earthquakes or other vibrations in the earth. \n\n**How do you read a spectrogram?**\n\nSpectrograms are basically two-dimensional graphs, with a third dimension represented by colors. Time runs from left (oldest) to right (youngest) along the horizontal axis. Each of our volcano and earthquake sub-groups of spectrograms shows 10 minutes of data with the tic marks along the horizontal axis corresponding to 1-minute intervals.  The vertical axis represents frequency, which can also be thought of as pitch or tone, with the lowest frequencies at the bottom and the highest frequencies at the top.  The amplitude (or energy or “loudness”) of a particular frequency at a particular time is represented by the third dimension, color, with dark blues corresponding to low amplitudes and brighter colors up through red corresponding to progressively stronger (or louder) amplitudes.\n![](https://s3.amazonaws.com/pnsn-cms-uploads/attachments/000/000/583/original/6dd1240572ba9085af145892a1b4c1eacce3a651)\nAbove the spectrogram is the raw seismogram, drawn using the same horizontal time axis as the spectrogram (including the same tick marks), with the vertical axis representing wave amplitude. This plot is analogous to webicorder-style plots (or seismograms) that can be accessed via other parts of our website.  Collectively, the spectrogram-seismogram combination is a very powerful visualization tool, as it allows you to see raw waveforms for individual events and also the strength or “loudness” at various frequencies. The frequency content of an event can be very important in determining what produced the signal."
"# 4. Audio Features\n## Spectral Centroid\nThe spectral centroid is a measure used in digital signal processing to characterise a spectrum. It indicates where the center of mass of the spectrum is located. Perceptually, it has a robust connection with the impression of brightness of a sound."
# Spotify Music - EDA \n![](https://storage.googleapis.com/pr-newsroom-wp/1/2020/03/Header.png)\nIn continuation of previous kernel about spotify music data extraction -Part 1 \nhttps://www.kaggle.com/pavansanagapati/spotify-music-api-data-extraction-part1\n\nWe now will use the data extracted from Spotify to perform two steps as follows\n\n#### 1. Explore the Audio Features and analyze\n#### 2. Build a Machine Learning Model \n\n## 1. Explore the Audio Features and analyze
#### Let us first analyse at high level the data in the spotify music dataframe that we build by accessing the spotify data as shown in part 1 of this kernel https://www.kaggle.com/pavansanagapati/spotify-music-api-data-extraction-part1.
\n#### 4.2 Gender and Survived\n
This bar plot above shows the distribution of female and male survived. The x_label shows gender and the y_label shows % of passenger survived. This bar plot shows that 74% female passenger survived while only ~19% male passenger survived.
This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive.
\n#### 4.3 Pclass and Survived
"So it clearly seems that,The survival of the people belong to 3rd class is very least.\nIt looks like ...\n-  63% first class passenger survived titanic tragedy, while\n-  48% second class and\n-  only 24% third class passenger survived."
"This kde plot is pretty self explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that in, the plot; the third class passengers have survived more than second class passnegers. It is true since there were a lot more third class passengers than first and second.\n\n"
\n#### 4.5 Age and Survived
"There is nothing out of the ordinary of about this plot, except the very left part of the distribution. It shows that\n\nchildren and infants were the priority."
"This knowledge graph is giving us some extraordinary information. Guys like Javed Akhtar, Krishna Chaitanya, and Jaideep Sahni are all famous lyricists and this graph beautifully captures this relationship.\n\nLet’s see the knowledge graph of another important predicate, i.e., the “released in”:"
## 2. Conclusion  \n### I hope you have a good understanding on how to use Knowledge Graph .\n\n## Please do leave your comments /suggestions and if you like this notebook please do UPVOTE
"The Poisson distribution is a discrete distribution and is popular for modelling the number of times an event occurs in an interval of time or space. \n\nIt takes a value lambda, which is equal to the mean of the distribution.\n\nPMF: \n\n![](https://study.com/cimages/multimages/16/poisson1a.jpg)\n\nCDF: \n![](http://www.jennessent.com/images/cdf_poisson.gif)"
### Log-Normal Distribution
A log-normal distribution is continuous. The main characteristic of a log-normal distribution is that it's logarithm is normally distributed. It is also referred to as Galton's distribution.\n\nPDF: \n\n![](https://www.mhnederlof.nl/images/lognormaldensity.jpg)\n\nCDF:\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/29095d9cbd6539833d549c59149b9fc5bd06339b)\n\nWhere Phi is the CDF of the standard normal distribution.
## Summary Statistics and Moments 
Linear Regression can be performed through Ordinary Least Squares (OLS) or Maximum Likelihood Estimation (MLE).\n\nMost Python libraries use OLS to fit linear models.\n\n![](https://image.slidesharecdn.com/simplelinearregressionpelatihan-090829234643-phpapp02/95/simple-linier-regression-9-728.jpg?cb=1251589640)
"Here we observe that the linear model is well-fitted. However, a linear model is probably not ideal for our data, because the data follows a quadratic pattern. A [polynomial regression model](https://en.wikipedia.org/wiki/Polynomial_regression) would better fit the data, but this is outside the scope of this tutorial."
"If you thought GPUs are deep learning-exclusive, you are *horribly* mistaken.\n\nThe cuDF library, created by the open-source platform RAPIDs, enables you to run tabular manipulation operations on one or more GPUs.\n\nUnlike datatable, cuDF has a very similar API to Pandas, thus offering a less steep learning curve. As it is standard with GPUs, the library is super fast, giving it an edge over datatable when combined with its Pandas-like API.\n\nThe only hassle when using cuDF is its installation - it requires:\n- CUDA Toolkit 11.0+\n- NVIDIA driver 450.80.02+\n- Pascal architecture or better (Compute Capability >=6.0)\n\nIf you want to try out the library without installation limitations, Kaggle kernels are a great option. Here is a [notebook](https://www.kaggle.com/rohanrao/tutorial-on-reading-large-datasets?scriptVersionId=49328159&cellId=14) to get you started.\n\n### 🛠 GitHub and documentation\n- https://docs.rapids.ai/api/cudf/stable/\n- https://github.com/rapidsai/cudf\n\n### 💻 Demo\nHere is a snippet from the documentation that shows a simple GroupBy operation on the tips dataset:"
# 7. Automatic EDA libraries
Let's look at the leaders on individual grounds.
"Conclusions from the constructed visualization:\n* Obviously, cities located closer to the South Pole will have more sunny days per year :)\n* The most expensive cities are in Europe. This is due to the high standard of living, as well as high wages in these countries.\n* The highest life expectancy - in the cities of Europe and Japan. It is also associated with a high standard of living.\n* The dirtiest air and longest working hours are in developing countries, as well as in China. This is due to the low requirements at the level of legislation of the countries."
"Let's build scatterplots, take the level of happiness as the target variable."
There is a positive and inverse correlation with some features.\nHypothesis: There is multicollinearity in the data.
\n### Correlations (Part I)
"**Correlations test (click ""code"")** "
"Well, this is immportant, there is a **strong correlation** between:\n- angular_velocity_Z and angular_velocity_Y\n- orientation_X and orientation_Y\n- orientation_Y and orientation_Z\n\nMoreover, test has different correlations than training, for example:\n\n- angular_velocity_Z and orientation_X: -0.1(training) and 0.1(test). Anyway, is too small in both cases, it should not be a problem."
"**Normal distribution**\n\nThere are obviously differences between *surfaces* and that's good, we will focus on that in order to classify them better.\n\nKnowing this differences and that variables follow a normal distribution (in most of the cases) we need to add new features like: ```mean, std, median, range ...``` (for each variable).\n\nHowever, I will try to fix *orientation_X* and *orientation_Y* as I explained before, scaling and normalizing data.\n\n---\n\n### Now with a new scale (more more precision)"
### Histogram for main features
"> **We've been able to significantly shrink the magnitude of ""f2"".**\n> \n> Let's plot the Probability Density Function Estimation for the transformed features:"
"> The distributions of the transformed features looks more bell-shaped-like but still...\n> A good idea is to transform features separately, adding them into a model and monitor the model performance. It might be a very time consuming process taking into consideration not only log transformations. We will leave it as an exercise for the reader."
"# 2. Data Augmentation using OpenCV-Python\n\n![](https://opencv-python-tutroals.readthedocs.io/en/latest/_static/opencv-logo-white.png)\n\n[OpenCV](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html) essentially stands for Open Source Computer Vision Library. Although it is written in optimized C/C++, it has interfaces for Python and Java along with C++. \nOpenCV-Python is the python API for OpenCV. You can think of it as a python wrapper around the C++ implementation of OpenCV. OpenCV-Python is not only fast (since the background consists of code written in C/C++) but is also easy to code and deploy(due to the Python wrapper in foreground). This makes it a great choice to perform computationally intensive programs.\n"
\n 2.1 Flipping with opencv  \n\nThe image is flipped according to the value of flipCode as follows:\n\n* flipcode = 0: flip vertically\n* flipcode > 0: flip horizontally\n* flipcode < 0: flip vertically and horizontally
"# 3. Data Augmentation using imgaug  \n  \n\n[imgaug](https://imgaug.readthedocs.io/en/latest/) is a library for image augmentation in machine learning experiments. It supports a wide range of augmentation techniques, allows to easily combine these and to execute them in random order or on multiple CPU cores, has a simple yet powerful stochastic interface and can not only augment images, but also keypoints/landmarks, bounding boxes, heatmaps and segmentation maps.\n![](https://cdn-images-1.medium.com/max/800/1*QT3A5EZIp1EXSVIiB4SlCA.png)\n\n"
 2.1 Flipping with imgaug  
[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n**4.5. Show Average Face**  
[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n**4.6. Show Eigen Faces** 
[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n**4.7. Classification Results**  
Application
"This looks rather,well,odd but it helps us to get a general **picture of the outlines of our image in 3 dimensional perspective.** It helps to understand the size with a 3-dimensional spatial representatiion and at the same time helps to understand the limits of our image in a 3 dimensional scale. It was originally used for Data Science Bowl 2017 by Guido Zuldhof, but I have adapted it for here (this competition).\n\nA recommended approach would be to **resample** the images before plotting it like this. Here we can clearly see what is inside the image and where the isolated cells are (it forms a figure vaguely resemblant of a skull). We can also get a touchy-feely sense of this whole thing with Plot.ly (it is a real toll on my poor computer with 3 gigabyte RAM)"
**3. Benign and malignant tumors**
Benign image viewing
These are the first 30 melanoma images with benign tumors. Let's check the distribution of values in `benign_malignant`.
Malignant image viewing
** 4. Which part of the body?**
"It seems like we have a lot of issues with the torso, and after that the extremities of the body (upper/lower). We have about 100-200 cases of cancer in the mouth or genitalia (the areas with the lowest rate of cancerous growth) and the palms and soles also are safe (probably because they are not exposed to any external sources in the day-to-day life of the person). \n\nOne could indeed fathom that the torso is frequently exposed either during the occasional workout or the occasional swim, or in some cases, the occasional extreme adventure. It also could be that the torso was exposed to UV light (which is the cause of melanoma) in highly populated regions (where pollution allows the sun's UV rays to come in)."
"Our earlier hypothesis now has more depth to it, with half (approximately?) of the skin cancer cases located in the torso. However, the torso has more square area than any other affected part, so it seems like the area on a body part is correlated with number of cases on that body part (as we can see)."
"### Violin Plot -\nWhen someone says seaborn, two things come to my mind first greyjoys ( GOT fan I am ;) ) and second violin plots, I know seaborn package because of it's violin plots, so I will explain the distributions of next variable with violin plots. Following are few facts about violin plots -  \n- Violin plot can be made using seaborn package in python and with split\n- Here we are using them to check the distributions, and horizontal lines inside them shows the quartiles\n- Green one is vendor 1 and red one is vendor 2 and trip_duration is plotted on log scale"
"**Findings** -\n- There are trips for both the vendor with zeros passengers and few of these trips have negative time as well, I don't understand how can a taxi trip have negative time, possibly they aren't right data points to train the model, we would remove them before making model\n- Trips with zero passengers can be trips when a taxi is called to a particular location and the customer is charged for getting the taxi there, that is one possible explanation.\n- Distributions are similar for both the vendors, but vendor one has more number of larger trips than vendor two for passenger count 2 and 3\n- There are very less number of trips with passenger count 7, 8 and 9"
## Box-Plots \n**Interpretation**\n- Most popular plots to check the distribution of variables\n- box covers data from second and third quadrant and rest is shorn by bars\n- Dots on the both side of bars shows outliers
"**Findings** -\n- From the boxplot above we can see that 75%ile of avg trip duration on Sunday(0) and Saturday(6) is less than 2000 seconds. i.e. around 33 minutes\n- Time taken by Monday, Tuesday, Wednesday, and Thursday are greater than rest of the days."
## line-plots \n- Simple lineplots can explain how the trip duration is changing with time for different days of week\n- very easy to interpret
"**findings** - \n- Its clear from the above plot that on day 0, that is Sunday and day 6 that is Saturday, the trip duration is very less that all the weekdays at 5 AM to 15 AM time. \n- See this, on  Saturday around midnight, the rides are taking far more than usual time, this is obvious through now verified  using given data"
## Alluvial Plots or parallel coordinates - \nAlluvial(in R) or Parallel coordinates(py) is use to plot the cluster characteristics. \n- passenger count 5 and 6 are mostly travelled with vendor 2 only \n- Cluster 4 has mostly vendor 2 cabs while cluster 5 has both in almost equal amount\nSuch kind of observations about clusters can be seen using parallel coordinates and its offered by pandas package.
"# Extracting same features for Test data - \n- We will extract same features for test dataset and then will train xgboost regressor to see how we are doing with prediction, how much predicting power out extracted features have. If we didn't get satisfactory accuracy we will extract extra features and so on.."
\n# Data Pre-Processing\n\n #### Croping Images\n\nhttps://stackoverflow.com/questions/13538748/crop-black-edges-with-opencv
- Croping Images randomly for resizing.
"##  3c. Distribution Plots (Satisfaction - Evaluation - AverageMonthlyHours)\n***\n**Summary:** Let's examine the distribution on some of the employee's features. Here's what I found:\n - **Satisfaction** - There is a huge spike for employees with low satisfaction and high satisfaction.\n - **Evaluation** - There is a bimodal distrubtion of employees for low evaluations (less than 0.6) and high evaluations (more than 0.8)\n - **AverageMonthlyHours** - There is another bimodal distribution of employees with lower and higher average monthly hours (less than 150 hours & more than 250 hours)\n - The evaluation and average monthly hour graphs both share a similar distribution. \n - Employees with lower average monthly hours were evaluated less and vice versa.\n - If you look back at the correlation matrix, the high correlation between evaluation and averageMonthlyHours does support this finding.\n \n**Stop and Think:** \n - Is there a reason for the high spike in low satisfaction of employees?\n - Could employees be grouped in a way with these features?\n - Is there a correlation between evaluation and averageMonthlyHours?"
"##  3d. Salary V.S. Turnover\n***\n**Summary:** This is not unusual. Here's what I found:\n - Majority of employees who left either had **low** or **medium** salary.\n - Barely any employees left with **high** salary\n - Employees with low to average salaries tend to leave the company.\n \n**Stop and Think:** \n - What is the work environment like for low, medium, and high salaries?\n - What made employees with high salaries to leave?"
"##  3f. Turnover V.S. ProjectCount \n***\n**Summary:** This graph is quite interesting as well. Here's what I found:\n - More than half of the employees with **2,6, and 7** projects left the company\n - Majority of the employees who did not leave the company had **3,4, and 5** projects\n - All of the employees with **7** projects left the company\n - There is an increase in employee turnover rate as project count increases\n \n**Stop and Think:** \n - Why are employees leaving at the lower/higher spectrum of project counts?\n - Does this means that employees with project counts 2 or less are not worked hard enough or are not highly valued, thus leaving the company?\n - Do employees with 6+ projects are getting overworked, thus leaving the company?\n\n"
##  3g. Turnover V.S. Evaluation \n***\n**Summary:** \n - There is a biomodal distribution for those that had a turnover. \n - Employees with **low** performance tend to leave the company more\n - Employees with **high** performance tend to leave the company more\n - The **sweet spot** for employees that stayed is within **0.6-0.8** evaluation
##  3h. Turnover V.S. AverageMonthlyHours \n***\n**Summary:** \n - Another bi-modal distribution for employees that turnovered \n - Employees who had less hours of work **(~150hours or less)** left the company more\n - Employees who had too many hours of work **(~250 or more)** left the company \n - Employees who left generally were **underworked** or **overworked**.
##  3i. Turnover V.S. Satisfaction \n***\n**Summary:** \n - There is a **tri-modal** distribution for employees that turnovered\n - Employees who had really low satisfaction levels **(0.2 or less)** left the company more\n - Employees who had low satisfaction levels **(0.3~0.5)** left the company more\n - Employees who had really high satisfaction levels **(0.7 or more)** left the company more
"##  3j. ProjectCount VS AverageMonthlyHours \n***\n\n**Summary:**\n - As project count increased, so did average monthly hours\n - Something weird about the boxplot graph is the difference in averageMonthlyHours between people who had a turnver and did not. \n - Looks like employees who **did not** have a turnover had **consistent** averageMonthlyHours, despite the increase in projects\n - In contrast, employees who **did** have a turnover had an increase in averageMonthlyHours with the increase in projects\n\n**Stop and Think:** \n - What could be the meaning for this? \n - **Why is it that employees who left worked more hours than employees who didn't, even with the same project count?**"
"##  3k. ProjectCount VS Evaluation\n***\n**Summary:** This graph looks very similar to the graph above. What I find strange with this graph is with the turnover group. There is an increase in evaluation for employees who did more projects within the turnover group. But, again for the non-turnover group, employees here had a consistent evaluation score despite the increase in project counts. \n\n**Questions to think about:**\n - **Why is it that employees who left, had on average, a higher evaluation than employees who did not leave, even with an increase in project count? **\n - Shouldn't employees with lower evaluations tend to leave the company more? "
"##  3l. Satisfaction VS Evaluation\n***\n**Summary:** This is by far the most compelling graph. This is what I found:\n - There are **3** distinct clusters for employees who left the company\n \n**Cluster 1 (Hard-working and Sad Employee):** Satisfaction was below 0.2 and evaluations were greater than 0.75. Which could be a good indication that employees who left the company were good workers but felt horrible at their job. \n - **Question:** What could be the reason for feeling so horrible when you are highly evaluated? Could it be working too hard? Could this cluster mean employees who are ""overworked""?\n\n**Cluster 2 (Bad and Sad Employee):** Satisfaction between about 0.35~0.45 and evaluations below ~0.58. This could be seen as employees who were badly evaluated and felt bad at work.\n - **Question:** Could this cluster mean employees who ""under-performed""?\n\n**Cluster 3 (Hard-working and Happy Employee):** Satisfaction between 0.7~1.0 and evaluations were greater than 0.8. Which could mean that employees in this cluster were ""ideal"". They loved their work and were evaluated highly for their performance. \n - **Question:** Could this cluser mean that employees left because they found another job opportunity?"
##  3m. Turnover V.S. YearsAtCompany \n***\n**Summary:** Let's see if theres a point where employees start leaving the company. Here's what I found:\n - More than half of the employees with **4 and 5** years left the company\n - Employees with **5** years should **highly** be looked into \n \n**Stop and Think:** \n - Why are employees leaving mostly at the **3-5** year range?\n - Who are these employees that left?\n - Are these employees part-time or contractors? 
"## 3n. K-Means Clustering of Employee Turnover\n***\n**Cluster 1 (Blue):** Hard-working and Sad Employees\n\n**Cluster 2 (Red):** Bad and Sad Employee \n\n**Cluster 3 (Green):** Hard-working and Happy Employee \n\n**Clustering PROBLEM:**\n    - How do we know that there are ""3"" clusters?\n    - We would need expert domain knowledge to classify the right amount of clusters\n    - Hidden uknown structures could be present"
"# Feature Importance\n***\n**Summary:**\n\nBy using a decision tree classifier, it could rank the features used for the prediction. The top three features were employee satisfaction, yearsAtCompany, and evaluation. This is helpful in creating our model for logistic regression because it’ll be more interpretable to understand what goes into our model when we utilize less features. \n\n**Top 3 Features:**\n1. Satisfaction\n2. YearsAtCompany\n3. Evaluation\n"
"# 4a. Modeling the Data: Logistic Regression Analysis\n***\n**NOTE:** This will be an in-depth analysis of using logistic regression as a classifier. I do go over other types of models in the other section below this. **This is more of a use-case example of what can be done and explained to management in a company.**\n\nLogistic Regression commonly deals with the issue of how likely an observation is to belong to each group. This model is commonly used to predict the likelihood of an event occurring. In contrast to linear regression, the output of logistic regression is transformed with a logit function. This makes the output either 0 or 1. This is a useful model to take advantage of for this problem because we are interested in predicting whether an employee will leave (0) or stay (1). \n\nAnother reason for why logistic regression is the preferred model of choice is because of its interpretability. Logistic regression predicts the outcome of the response variable (turnover) through a set of other explanatory variables, also called predictors. In context of this domain, the value of our response variable is categorized into two forms: 0 (zero) or 1 (one). The value of 0 (zero) represents the probability of an employee not leaving the company and the value of 1 (one) represents the probability of an employee leaving the company.\n\n**Logistic Regression models the probability of ‘success’ as: **\n\n \nThe equation above shows the relationship between, the dependent variable (success), denoted as (θ) and independent variables or predictor of event, denoted as xi. Where α is the constant of the equation and, β is the coefficient of the predictor variables\n\n"
"## Using Logistic Regression Coefficients \n***\nWith the elimination of the other variables, I’ll be using the three most important features to create our model: Satisfaction, Evaluation, and YearsAtCompany.\n\nFollowing overall equation was developed: \n\n**Employee Turnover Score** = Satisfaction*(**-3.769022**) + Evaluation*(**0.207596**) + YearsAtCompany*(**0.170145**) + **0.181896**\n"
Let's analyze the discrete variables and see how they are related with the target variable SalePrice. 
"There tend to be some relationships between the variables and the SalePrice, for example some are monotonic like OverallQual, some almost monotonic except for a unique values like OverallCond. We need to be particulary careful for these variables to extract maximum values for a linear model. "
Continous Variables\n\nLet's find out the continous variables. 
"As you can see these variables are not normally distributed including our target variable ""SalePrice"". In order to maximise performance of linear models, we can use log transformation. We will describe more on this in following parts. We will do the transformation in the feature engineering section. Let's now see how the distribution might look once we do the transformation. "
"As you can see, we get a better spread of data once we use log transformation.\n\nI want to focus our attention on the target variable which is **SalePrice.** We already know that our target variable is not normally distributed. However, lets go into more details. If we want to create any linear model, it is essential that the features are normally distributed. This is one of the assumptions of multiple linear regression. In previous codes we have seen that log transformation can help us to make features more like normally distribute. I will explain more on this later."
"These **three** charts above can tell us a lot about our target variable.\n* Our target variable, **SalePrice** is not normally distributed.\n* Our target variable is right-skewed. \n* There are multiple outliers in the variable.\n\n\n**P.S.** \n* If you want to find out more about how to customize charts, try [this](https://matplotlib.org/tutorials/intermediate/gridspec.html#sphx-glr-tutorials-intermediate-gridspec-py) link. \n* If you are learning about Q-Q-plots for the first time. checkout [this](https://www.youtube.com/watch?v=smJBsZ4YQZw) video. \n* You can also check out [this](https://www.youtube.com/watch?v=9IcaQwQkE9I) one if you have some extra time. \n\nLet's find out how the sales price is distributed."
We will observe that the total charges increases as the monthly bill for a customer increases.
"### E.) Finally, let's take a look at out predictor variable (Churn) and understand its interaction with other important variables as was found out in the correlation plot. "
1. Lets first look at the churn rate in our data
"In our data, 74% of the customers do not churn. Clearly the data is skewed as we would expect a large majority of the customers to not churn. This is important to keep in mind for our modelling as skeweness could lead to a lot of false negatives. We will see in the modelling section on how to avoid skewness in the data."
"**i.) Churn vs Tenure**: As we can see form the below plot, the customers who do not churn, they tend to stay for a longer tenure with the telecom company. "
"**ii.) Churn by Contract Type**: Similar to what we saw in the correlation plot, the customers who have a month to month contract have a very high churn rate."
**iii.) Churn by Seniority**: Senior Citizens have almost double the churn rate than younger population.
**iv.) Churn by Monthly Charges**: Higher % of customers churn when the monthly charges are high.
**v.) Churn by Total Charges**: It seems that there is higer churn when the total charges are lower.
"## After going through the above EDA we will develop some predictive models and compare them.\n\nWe will develop Logistic Regression, Random Forest, SVM, ADA Boost and XG Boost"
"An interesting question was asked to kagglers in this survey : 'Are you actively looking to switch careers to data science?'.\nOnly 3012 respondents answered this question. My guess is that the other respondents are either already working as data scientists, in which case Yes/No would have no sens, or are still students."
"Without surprise, most of working kagglers who aren't yet working as data scientists would love to switch careers !\nThat being said, some of them don't which raises the question of the motive of using Kaggle for people who wouldn't want to work in Data Science.    \nThis question was included in the survey and it's one of the very few questions that only came as a free form question, i.e choices were not given.     \nIn the following, we're going to see which words were the most often used by non-switchers who answered this question. The following words *['data', 'science', 'mining', 'big', 'bigdata', 'machine', 'learning']* are deliberately plotted in shades of green.    \nLet's see what we've got ! \n> EDIT : Credits to @Chris Crawford who inspired me with his very nice goose-shaped WordCloud that you can check [here](http://www.kaggle.com/crawford/analyzing-the-analyzers)."
"Without suprise, the chosen key words in green are among the most used in the answers that were given. Let's see how all of the words are linked : \n* **Curiosity and interest (and interesting) ** were frequently used, it suggests that some answers were 'Interest towards machine learning' for example.\n* **hobby and fun** were also frequently used, it suggests that some people practice data science and participate in competitions as a hobby, to have fun, not because they want to work in that field.\n* **challenge, project and competition** were popular too, and yeah kaggle is a pretty great platform for fierce competitors looking for challenging subjects :)"
"**5.2 Contour chart for depicting density(and distribution) of HP, Speed, Sp. Attack, Sp. Defense of different generations of pokemon based on their Attack and Defense**"
# 6. Bubble charts\nYou can use a bubble chart instead of a scatter chart if your data has three data series that each contain a set of values. The sizes of the bubbles are determined by the values in the third data series. Bubble charts are often used to present financial data.
"**6.1 Categorical bubble chart with Attack on X-axis, Defense on Y-axis and HP as size for different generations of fire pokemon**"
"# 7. Treemaps\nIn information visualization and computing, treemapping is a method for displaying hierarchical data using nested figures, usually rectangles. **Treemaps are often used in R kernels due to their interactivity. Python offers squarify to do the same. We can combine squarify and pyplot to plot treemaps.**"
"# 8. Bullet chart\nStephen Few's Bullet Chart was invented to replace dashboard gauges and meters, combining both types of charts into simple bar charts with qualitative bars (ranges), quantitiative bars (measures) and performance points (markers) all into one simple layout. ranges typically are broken into three values: bad, okay, good, the measures are the darker bars that represent the actual values that a particular variable reached, and the points or markers usually indicate a goal point relative to the value achieved by the measure bar. **It is good for seeing if a pokemon's attributes are good or not for given data.**"
**Show generated images**\n> show_generated_img()
### Training Loop
"As we have seen,\n\n* ph : 14.98 %\n* Sulfate : 23.84 %\n* Trihalomethanes : 4.94 %"
- Another method to show the missing values by using **[missingno library](https://github.com/ResidentMario/missingno)** \n
- **Missingno bar** gives us bar cahrt version of the completeness & nullity of the variables.
Lets look at the first document from this collection:
"Now we convert the data into something Vowpal Wabbit can understand, and we throw away words shorter than of 3 symbols. Here we skip many important NLP stages (stemming and lemmatization, for example), however, we will later see that VW solves the problem even without these steps."
"Now we load predictions, compute AUC and plot the ROC curve:"
AUC value we get states that here we've achieved high classification quality.
"**Findings:** Parch isn't balanced as levels of Parch(8) are not equally represented in its distribution. Over one thousand passengers were without parents or children, followed by 170 passengers had one parents or children. In other words, over 76.5% passengers were without parents or children while rest of the 23.5% had few parents or children.\n\n## 4.2 Numerical Variables \nWe would like to analyse numerical variables using histogram, density plot, and summary statistics. To analyse numerical variables, we will create 2 custom functions. The 1st one will plot histogram and density plot for each numerical variable. And the 2nd one will calculate summary statistics including skewness."
### 4.2.1 Fare 
"If the reconstruction error is greater than the threshold(blue-dotted line), classify the ECG as abnormal."
"Looking at the picture above, it seems that the threshold was set by considering the Normal and Anomaly distributions well. In particular, it seems that recall is more important than precision for abnomaly detection. In actual application, it may be necessary to consider fine-tuning the threshold."
Let's carry out the Shapiro-Wilk test to check for the normality of the distribution of numerical variables.
"The test for normality has not been passed, therefore, to find the relationship of our variables, it is necessary to use non-parametric tests. Since we mostly have categorical variables, we use Pearson's Chi-square test to check."
"The purpose of the test is to determine if two variables are related to each other.\n\nNull hypothesis:\nWe start by defining the null hypothesis ( H0 ), which states that there is no relationship between the variables.\nAn alternative hypothesis could argue that there is a significant relationship between the two."
"In our dataset, there is a statistically significant relationship between the target variable and categorical features. This means that you should try to make a small prediction of the target feature."
Or in a more illustrative way:
Let's go deeper:
## 2.1 Duration
"##### PLease note: duration is different from age, Age has 78  values and Duration has 1544 different values"
***Do you have longer names?***
***Whats in the name?***
"* **Data scientists and machine learning engineers are not only highly paid, they also have higher job satisfaction.** \n* **Interestingly, even if researchers are underpaid, their job satisfaction is quite high compared to their salaries. That's why it is important to learn new things everyday.**"
"**Although experiences of most respondents in coding are lower than 5 years, employment rate is quite high.**"
**[Social capital](http://www.investopedia.com/terms/s/socialcapital.asp) is probably the most important thing in the professional world regardless of sectors.**
"* **Experience from work in company related to ML and Kaggle competitions seem most important way to prove knowledge in data science. Although 60% of respondents have master or doctoral degree, degree is not considered as proof of knowledge in data science.** \n* **Github portfolio and certificates of online courses are even graded better than master and doctoral degree. But it is important to keep in mind that most respondents are employees, not employers.**"
"**It is critical to learn logistic regression, random forests and support vector machine. To learn math behind these algorithms to narrate non-technical audiences is  more critical.**"
**2018 will be the year of Deep Learning!**
"**It would be great if we would download and build models on the famous iris data-set everyday but we should spend  our most time (%60-%70) to clean the data, not to build cool algorithms and models.**"
"* **There are two powerful tools and communities in data science: Python and R. Somehow it is hard to select which language a newbie starts in data science. Although  I am a big fan of Python, I would not give many comments in this section. **\n* **These graphs speak for themselves!**"
"\n# 4. \n## Exploratory Data Analysis\n\n\n### 4.1 - Correlation matrix\n\nNow that missing values and outliers have been treated, I will analyse each feature in more detail. This will give guidance on how to prepare this feature for modeling. I will analyse the features based on the different aspects of the house available in the dataset.\n\n"
"- Using this correlation matrix, I am able to visualise the raw highly influencing factors on SalePrice.\n- I am looking for these because I will create polynomial features from the highly correlating features, in an attempt to capture the complex non-linear relationships within the data.\n\n***\n\n\n### 4.2 - Feature engineering\n\nThis section is quite lengthy, so I have added hyperlinks to each subsection below in case you want to skip through...\n\n- 4.2.1 - [Polynomials](#polynomials)\n- 4.2.2 - [Interior](#interior)\n- 4.2.3 - [Architectural & Structural](#architectural_&_structural)\n- 4.2.4 - [Exterior](#exterior)\n- 4.2.5 - [Location](#location)\n- 4.2.6 - [Land](#land)\n- 4.2.7 - [Access](#access)\n- 4.2.8 - [Utilities](#utilities)\n- 4.2.9 - [Miscellaneous](#miscellaneous)\n\n\n#### 4.2.1 - Polynomials\n\nThe most common relationship we may think of between two variables, would be a straight line or a linear relationship. What this means is that if we increase the predictor by 1 unit, the response always increases by X units. However, not all data has a linear relationship and therefore it may be necessary for your model to fit the more complex relationships in the data. \n\nBut how do you fit a model to data with complex relationships, unexplainable by a linear function? There are a variety of curve-fitting methods you can choose from to help you with this.\n\n- The most common way to fit curves to the data is to include polynomial terms, such as squared or cubed predictors.\n- Typically, you choose the model order by the number of bends you need in your line. Each increase in the exponent produces one more bend in the curved fitted line. It’s very rare to use more than a cubic term."
\n#### 4.2.2 - Interior\n\n***BsmtQual***\n\n- Evaluates the height of the basement.
"- SalePrice is clearly affected by BsmtQual, with the better the quality being meaning the higher the price. \n- However, it looks as though most houses have either 'Good' or 'Typical' sized basements.\n- Since this feature is ordinal, i.e. the categories represent different levels of order, I will replace the values by hand."
***BsmtCond***\n- Evaluates the general condition of the basement.
"- As the condition of the basement improves, the SalePrice also increases.\n- However, we see some very high SalePrice values for the houses with ""Typical"" basement conditions. This perhaps suggests that although these two features correlate positively, BsmtCond may not have a largely influential contribution on SalePrice.\n- We also see the largest number of houses falling into the ""TA"" category.\n- Since this feature is ordinal, I will replace the values by hand."
***BsmtExposure***\n- Refers to walkout or garden level walls
"- As the amount of exposure increases, so does hte typical SalePrice. Interestingly, the average difference of SalePrice between categories is quite low here, telling me that some houses sold for very high prices, even with no exposure.\n- From this analysis I would say that it is positively correlating with SalePrice, but it isn't massively influential.\n- Since this feature is ordinal, I will replace values by hand."
***BsmtFinType1***\n- Rating of basement finished area
"- This is very interesting, it seems as though houses with an unfinished basement on average sold for more money than houses having up to an average rating...\n- However, houses with a good finish within the basement still demand more money than unfinished ones.\n- This is an ordinal feature, however as you can see this order does not necessarily cause a higher SalePrice. By creating an ordinal variable it was suggest that as the order of the feature increases then the target variable would also. We can see that this is not the case. Therefore, I will create dummy variables from this feature."
***BsmtFinSF1***\n- Type 1 finished square feet.
"- This feature has a positive correlation with SalePrice and the spread of data points is quite large. \n- It is also clear that the local area (Neighborhood) and style of building (BldgType, HouseStyle and LotShape) has a varying effect on this feature.\n- Since this is a continuous numeric feature, I will bin this into several categories and create dummy features."
***BsmtFinType2***\n- Rating of basement finished area (if multiple types)
"- There seems as though there are a lot of houses with unfinished second basements, and this may cause the skew in terms og SalePrice's being relatively high for these...\n- There also looks to be only a few values for each of the other categories, with the highest average SalePrice coming from the second best category.\n- Although this is intended to be an ordinal feature, we can see that the SalePrice does not necessarily increase with order. Hence, I will cerate dummy variables here."
## 4.1 Avocado Prices
## 4.2 Boston House Prices
# 5. Data pre-processing
### 5.1.7 | Outlet_Type
"\n    👉 There are 4 outlet types namely grocery store, supermarket type 1, supermarket type 2, and supermarket type 3.\n    👉 Supermarket type 1 becomes the outlet type with the most number (65.43%).\n    👉 However, the smallest number outlet type is supermarket type 2 (only 10.89%).\n"
### 5.2.2 | Continuous Column Distribution 📈\n\n    👉 This section will show the continuous column distribution using histograms and box plots.\n
"\n    👉 Item_Weight, Item_MRP, and Outlet_Establishment_Year distribution is normal (no outliers detected in these columns). \n    👉 For Item_Visibility and Item_Outlet_Sales distribution, the distribution is right-skewed distributions (has a long right tail, the mean position is on the right side of the data). These outliers will be pre-processed in the next section.\n"
## 5.3 | Missing Values Exploration ❓\n\n    👉 This section will show missing values exploration for all columns.\n
"# 6. | Data Pre-processing ⚙\n\n    👉 Data pre-processing will be performed in this section to ensure high-quality data by cleaning dirty data, imputing missing values, and handling outliers.\n\n\n    "
## 5.1 Distribution of AMT_CREDIT
## 5.2 Distribution of AMT_INCOME_TOTAL
## 5.3 Distribution of AMT_GOODS_PRICE
## 5.4 Who accompanied client when applying for the  application
## 5.5 Data is balanced or imbalanced
___\n* _How customers are distributed in Brazil? (a 30k orders sample from 2018 in a map)_\n___
___\n* _**HeatMaps:** a good view to see where are the core of brazilian e-commerce customers_\n___
"By the map we showed above, we have already the insight that the southeast of Brazil has the highest number of orders given through e-commerce. So, let's see it in a HeatMap!"
"**Nice!** Another good view is to use the folium plugin _[HeatMapWithTime](https://nbviewer.jupyter.org/github/python-visualization/folium/blob/master/examples/HeatMapWithTime.ipynb)_ to see the evolution of e-commerce orders among time.\n\nFor [limitations](https://github.com/python-visualization/folium/issues/859) purpose (i.e. jupyter and Chrome limitations for total number of points shown at HeatMapWithTime, we will show the evolution of orders from January 2018 to July 2018)\n\nAlso, it's possible that the plugin HeatMapWithTime doesn't work properly from a [issue](https://github.com/python-visualization/folium/issues/1221) fixed on version 0.11 (it's seems that the version of the kernel is 0.10). It it is the case for you, just updating the version of folium library would fix it."
\n3.3 E-Commerce Impact on Economy\n\nGo to TOC
"One of the datasets provided have informations about order's payment. To see how payments can take influence on e-commerce, we can build a mini-dashboard with main concepts: `payments type` and `payments installments`. The idea is to present enough information to clarify how e-commerce buyers usually prefer to pay orders."
"In fact, we can see by the line chart that payments made by credit card really took marjority place on brazilian e-commerce. Besides that, since 201803 it's possible to see a little decrease on this type of payment. By the other side, payments made by debit card is showing a growing trend since 201805, wich is a good opportunity for investor to improve services for payments like this.\n\nOn the bar chart above, we can see how brazilian customers prefer to pay the orders: mostly of them pay once into 1 installment and it's worth to point out the quantity of payments done by 10 installments."
Let's show now distribution of combinations of features. We create a function to show a heatmap.
Let's see first what consonant diacritics and vowel diacritics appears together.
#### Some sssential imports
\n#### Configuration
"# Elbow method to select number of clusters\nThis method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the ""elbow criterion"". This ""elbow"" cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance.\n# Basically, number of clusters = the x-axis value of the point that is the corner of the ""elbow""(the plot looks often looks like an elbow)"
"As the elbow corner coincides with x=2, we will have to form **2 clusters**. Personally, I would have liked to select 3 to 4 clusters. But trust me, only selecting 2 clusters can lead to best results.\nNow, we apply k-means algorithm."
"Now, let's visualize the results."
"So, the blue cluster represents China(Mainland), USA and India while the red cluster represents all the other countries.\nThis result was highly probable. Just take a look at the plot of cell 3 above. See how China, USA and India stand out. That has been observed here in clustering too.\n\nYou should try this algorithm for 3 or 4 clusters. Looking at the distribution, you will realise why 2 clusters is the best choice for the given data"
"- UK, USA, Germany"
### Please be sure to leave a reference when using code or taking ideas in my visualizations.
# Relationship between targets\nColored by `SN_filter` although I'm not clear from the data description what this column represents.\n
"# Simple Baseline Using Simple LightGBM [0.47706 LB]\n\nThis model still predicts the same value for each id in the test set, but the predicted value is based off of the sequence data."
# Improve Baseline by adding: **structure** and **predicted_loop_type** features [0.47520 LB]
"# Modeling approach. Fit Line for Reactivity?\nLets test and see what a regression line looks like for some example samples. Since we are only given 68 values in the training set and will predict 93 in the test, this might be a good idea for extending the trend beyond 93. "
### Confusion Matrix
### Let's try different values of n_neighbors to fine tune and get better results
## Classification using Support Vector Classifer (SVC)\n
### Let's visualize the import features which are taken into consideration by decision trees.
## Classification using Random Forest.\n
###  Classification report \n\n#### **Let's use yellowbrick for classification report as they are great for visualizing in a tabular format**
## Classification using Gradient Boosting\n
Up to outlines
### Thanks for reading
This part is inspired by: https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\nVery goodjob with the ARIMA models ! It is more simple when we have directly a stationary Time series. It is not our case...\n\nWe will use the Dickey-Fuller Test. More informations here: https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test
Our Time Series is stationary ! it is a good news ! We can to apply the ARIMA Model without transformations.
Good job ! We have a Time Series Stationary ! We can apply our ARIMA Model !!!
"We expose the naive decomposition of our time series (More sophisticated methods should be preferred). They are several ways to decompose a time series but in our example we take a simple decomposition on three parts.\nThe additive model is Y[t] = T[t] + S[t] + e[t]\nThe multiplicative model is Y[t] = T[t] x S[t] x e[t]\nwith:\n\n 1. T[t]: Trend \n 2. S[t]: Seasonality \n 3. e[t]: Residual\n\nAn additive model is linear where changes over time are consistently made by the same amount. A linear trend is a straight line. A linear seasonality has the same frequency (width of cycles) and amplitude (height of cycles).\nA multiplicative model is nonlinear, such as quadratic or exponential. Changes increase or decrease over time. A nonlinear trend is a curved line.A non-linear seasonality has an increasing or decreasing frequency and/or amplitude over time.\nIn ou example we can see it is not a linear model. So it is the reason why we use a multiplicative model."
From above plot we can see that number of characters in test and train set was in same range.In selected text the range flows from 3 to 138 Characters.
We can see that number of unique words in train and test sets range from 1 to 26. In selected text most number  
"Following scatter stripplot shows touch screen or not and ram values according to price range.\n\n0= not touch screen, \n1= has touch screen"
 \n#### 5- 3G-RAM and Price Range
"Following boxplots show 3G or not and ram values according to price range.\n\n0= not 3G, \n1= has 3G"
 \n#### 6- 4G-RAM- PRICE RANGE
"Following boxplots show 4G or not and ram values according to price range.\n\n0= not 4G, \n1= has 4G"
 \n#### 7- NUMBER OF CORES OF PROCESOR(n_cores)- RAM and PRICE RANGE
"Following violin plots shows wifi or not and ram values according to price range.\n\n0- has not wifi, \n1- has wifi"
 \n#### 14- DUAL SIM- RAM AND PRICE RANGE
"Following distibution plots show dual sim or not and ram values according to price range.\n\n0- has not dual , \n1- has wifi"
 \n#### 15- Screen Height of mobile(sc_h) AND Screen Width of mobile(sc_w)- RAM and Price Range
"# 7. | Model Implementation 🛠️\n\n    This section will implement various machine learning models as mentioned in Introduction section. In addition, explanation for each models also will be discussed.\n"
"## 7.1 | Logistic Regression\n\n    \n        Logistic regression is a statistical method that is used for building machine learning models where the dependent variable is dichotomous: i.e. binary. Logistic regression is used to describe data and the relationship between one dependent variable and one or more independent variables. The independent variables can be nominal, ordinal, or of interval type.\n    The name ""logistic regression"" is derived from the concept of the logistic function that it uses. The logistic function is also known as the sigmoid function. The value of this logistic function lies between zero and one.\n        \n            \n            🖼 Logistic Function by Simplilearn\n        \n    \n\n"
 \n## Create Model\n* Lets put them all together
\nUp to this point we create 2 layer neural network and learn how to implement\n* Size of layers and initializing parameters weights and bias\n* Forward propagation\n* Loss function and Cost function\n* Backward propagation\n* Update Parameters\n* Prediction with learnt parameters weight and bias\n* Create Model\n\n Now lets learn how to implement L layer neural network with keras.
"Hypotheses:\n* The distribution of age is close to normal, the value of ticket prices has a long ""tail"".\n* Boxplots of both features indicate the presence of possible outliers for these variables (separate patterns in the data are possible)."
Let's visualize categorical variables depending on the survival of passengers.
"We use statistical tests to test hypotheses. First, let's test the hypotheses about the normality of the distribution of numerical variables."
"The hypotheses about the normal distribution were confirmed. For the effect of these variables on the target, 2 different tests should be used. For the Fare variable, we use the Mann-Whitney U-test, for the Age variable, we use the Student's T-test."
"Next, we will conduct statistical tests of the influence of categorical variables on the target categorical variable. To do this, we use the Chi-square-Pearson test."
All variables have an effect on the target variable. You need to add them to the forecast building model. Gender and ticket class have the most influence.
## 6.2 | Jointplot between Sepal Length and Sepal Width 🌹\n
## 6.3 | Distribution of Numerical Distributions 📊\n
\n    👉 Iris-setosa is having smaller feature and less distributed. Iris-versicolor is average distributed and has average features. Iris-virginica is highly distributed with large number of values and features.\n
## 6.4 | Heatmap 🔥\n
# 7. | Dataset Pre-processing ⚙\n\n    👉 This section will prepare the dataset before building the machine learning models.\n
### Numerical Features vs Target Variable (DEATH_EVENT) :
"- Cases of **DEATH_EVENT** initiate from the age of **45**. Some specific peaks of high cases of **DEATH_EVENT** can be observed at 45, 50, 60, 65, 70, 75 and 80.\n- High cases of **DEATH_EVENT** can be observed for **ejaction_fraction** values from **20 - 60**.\n- **serum_creatinine** values from **0.6** to **3.0** have higher probability to lead to **DEATH_EVENT**.\n- **serum_sodium** values **127 - 145** indicate towards a **DEATH_EVENT** due to heart failure."
#### anaemia vs Numerical Features :
"- Irrespective of **anaemia**, **age** group of **55 - 75** and **ejaction_fraction** values of **20 - 40** are prone to **DEATH_EVENT**.\n- Similarly, **serum_creatinine** levels between **1 - 2** and **serum_sodium** levels of **130 - 140** display a higher chance of confronting a **DEATH_EVENT**."
#### diabetes vs Numerical Features :
"- For **creatinine_phosphokinase**, values from **0 - 500** and **platelets** range from **2x10^5 - 3x10^5** detect more cases of heart failure.\n- Similarly, **serum_creatinine** levels between **1 - 2** and **time** feature's values from **0 - 100** highlight more heart failure cases."
#### high_blood_pressure vs Numerical features :
"- Due to **high_blood_pressure**, **age** at which the **DEATH_EVENT** occurs for heart failure increases its range of values. The lower threshold of age limit drops just **below 55** and upper limit extends **over 70**.\n- Chances of confronting a **DEATH_EVENT** due to **high_blood_pressure** lowers the values of **time** feature's values and increases the chances of heart failure."
#### sex vs Numerical Features :
"- For female(0) population, **age** group **50 - 70** and male(1) population's **age** group **60 - 75** are more prone to heart failure leading to **DEATH_EVENT**.\n- **ejaction_fraction** values for female(0) population of **30 - 50** and **20 - 40** for male(1) population leads to cases of **DEATH_EVENT**.\n- **serum_sodium** values indicating **DEATH_EVENT** due to heart failure is different for male and female."
#### smoking vs Numerical Features :
"- **age** group of **60 - 70** dominates the cases for **DEATH_EVENT** due to **smoking**. However, range of values ,**50 - 75** , increases for cases of **DEATH_EVENT** that do not **smoke**.\n- **Smoking** reduces the range of values for feature **time** to **0 - 75** that someone might face a **DEATH_EVENT**."
### Numerical features vs Numerical features w.r.t Target variable(DEATH_EVENT) :
"- For **time vs age** plot, **DEATH_EVENT** peaks can be found at **age** values of 50, 60, 70 and 80 for **time** value range between **50 - 100**.\n- **creatinine_phosphokinase** values between 0 - 500 are dominant in recording **DEATH_EVENT** irrespective of other features.\n- Similarly, **ejaction_fraction** values between **20 - 40** record high number of cases of **DEATH_EVENT**.\n- **platelets** range of values between **2x10^5 - 4x10^5** and **time** between **0 - 50** is a strong indicator for **DEATH_EVENT**.\n- Another indicator for **DEATH_EVENT** is **serum_creatinine** values from **0 - 2** with **time** values from **0 - 50**.\n- **serum_sodium** range of values from **130 - 140** record high number of cases for **DEATH_EVENT**."
"We see that the isup_grade 0 and 1 i.e no cancer, has the most number of values and that's what expected in case of most medical datasets , the target class will always be underrepresented and that's also the most important challenge when performing machine learning tasks on Medical DATA\n\nNow let's Look at how much data is provided by which data-provider"
"I learned about adding the numerical values i.e count and percentage on the countplot of seaborn from Rohit's kernels . I thought I knew EDA very well but indeed I was wrong . You can learn more about ax.patches here : https://medium.com/@dey.mallika/transform-your-graphs-with-seaborn-ea4fa8e606a6\nAfter learning go ahead and play around the parameters of ax.test() and ask any queries in the comment section\nOne tip for Better visualization : always go through the documentation of the function you are using to visualize and play with all the parameters it takes , you will surprise yourself with suprisingly new things"
From this graph it is also clear that the data will be baised towards non-cancer examples
"# Image EDA\nNow we can finally move on to Image EDA . BUT since we are complete beginners, let's first+ understand the format of image that is provided to us and all the image related jargons that we will be using further\n\n## Q1) What is .tff format and Why it is used?\n\nTagged Image File Format (TIFF) is a variable-resolution bitmapped image format developed by Aldus (now part of Adobe) in 1986. TIFF is very common for transporting color or gray-scale images into page layout applications, but is less suited to delivering web content.\n\nReasons for Usage:\n* IFF files are large and of very high quality. Baseline TIFF images are highly portable; most graphics, desktop publishing, and word processing applications understand them.\n* The TIFF specification is readily extensible, though this comes at the price of some of its portability. Many applications incorporate their own extensions, but a number of application-independent extensions are recognized by most programs.\n* Four types of baseline TIFF images are available: bilevel (black and white), gray scale, palette (i.e., indexed), and RGB (i.e., true color). RGB images may store up to 16.7 million colors. Palette and gray-scale images are limited to 256 colors or shades. A common extension of TIFF also allows for CMYK images.\n* TIFF files may or may not be compressed. A number of methods may be used to compress TIFF files, including the Huffman and LZW algorithms. Even compressed, TIFF files are usually much larger than similar GIF or JPEG files.\n* Because the files are so large and because there are so many possible variations of each TIFF file type, few web browsers can display them without plug-ins.\n\n## Q2) What are image levels?\nIn some image formats the image data has a fixed amount of possible intensities. For instance an image may be defined as uint8 (unsigned integer 8-bit) which means that each pixel can have a value (intensity) between 0-255, and each intensity is a whole number (integer) in that range. So that gives 256 possible intensity levels. Another way to interpret this would be layers. An RGB (red green blue) type image uses three layers to define colour (a single layer would define a large-scale image, some image types contain more than 3 layers). For each pixel there are 3 intensity levels, 1 for each colour, are defined and together (using a kind of mixing of the colours) they define the colour of that pixels. Similarly for a grayscale there can be two levels i.e black and white\n\n## Q3) What is Down-sampling and Up-sampling in Image processing?\nDownsampling and upsampling are two fundamental and widely used image operations, with\napplications in image display, compression, and progressive transmission. Downsampling is\nthe reduction in spatial resolution while keeping the same two-dimensional (2D) representation. It is typically used to reduce the storage and/or transmission requirements of images.\nUpsampling is the increasing of the spatial resolution while keeping the 2D representation\nof an image. It is typically used for zooming in on a small region of an image, and for\neliminating the pixelation effect that arises when a low-resolution image is displayed on a\nrelatively large frame\n\nNow that we know all this we are good to go.\n\n\n I will be using openslide to display images as I learned it in this competition from a very informative kernel:\nhttps://www.kaggle.com/wouterbulten/getting-started-with-the-panda-dataset\nThe benefit of OpenSlide is that we can load arbitrary regions of the slide, without loading the whole image in memory. Want to interactively view a slide? We have added an interactive viewer to this notebook in the last section.\n\nYou can read more about the OpenSlide python bindings in the documentation: https://openslide.org/api/python/"
\n\n#### 5.2 |Distplot
"\n\nChart report  \n    \n\n\n**From the first graph, we can observe that the values of the ""age"" variable are mostly distributed as values of 50, 60 and 65 - 70. In addition, there are many peaks in the distribution graph of the ""age"" variable. This is an indicator that the values of the variable are not normally distributed. Looking at the second graph, we can observe that the values of the ""platelets"" variable are mainly distributed between 200000 and 400000. At the same time, the graph does not have many peaks, skewness and kurtosis are low. It can be said that the values of this variable are normally distributed.**"
\n\n#### 5.3 |Boxplot
\n\n#### 5.4 |Pairplot
"\n\nChart report  \n    \n\n\n**From the graph above, we can observe whether there is a correlation between the numerical variables, as well as the distribution of those variables**"
\n\n#### 5.5 |Scatterplot
\n\nChart report  \n    \n\n**As it can be seen there is not any correlation between the variables**
"\n\nChart report  \n    \n\n**When we look at the graphs, we do not see a significant difference between the classes of the 'sex' variable according to the variables mentioned.**"
"\n\nChart report  \n\n    \n**When we look at the graphs, we do not see a significant difference between the classes of the 'diabetes' variable according to the variables mentioned.**"
\n\n#### 5.6 |Smooth Kernel Density with Marginal Histograms
\n\n#### 5.7 |Regplot and heatmap
"\n\nChart report  \n\n**From the graphs here, we do not observe any correlation between the variables**"
"\n\nChart report  \n\n**From the heatmap above, we do not observe any correlation between the variables**"
"I notice that Statten Island and the Bronx are highly underrepresented in this dataset. For Statten Island, the reason is that the population of the island is small. However, this can't be the case for the Bronx which has a population comparable (~1.4mln) to Manhattan (~1.6mln) or for for Brooklyn /Queens with their populations of ~2.5mln and ~2.4mln, respectively. \n\nThis makes sense: Queens, the Bronx  and, to a fair extent Brooklyn, are residential neighborhoods unlike Manhattan which is a business centre as well as a tourist destination."
### 1.4.3 Longitude and latitude
Longitude and latitude are somewhat correlated with each other. This is because the locations of properties tend to come from clusters.
### 1.4.4 Room type
"As far as room types, this dataset is balanced away from 'Shared room' properties. The proportions of private room and entire home/apt rentals are close, with entire home/apt dominating prive room by <10%."
### 1.4.5 Minimum nights
### 1.4.6 Reviews per month
"The distribution of the number of reviews per month is highly skewed however way we cut it. This is because there is a large weight on small numbers: there are a lot of properties which only get a few reviews and a rather fat tail of properties which get a lot of reviews. \n\nOne explanation would be that the properties which are available a larger fraction of the year get more reviews. However, a scatter plot of reviews_per_month and availability_365 variables shows no evidence of a relationship so that explanation would appear to not be valid."
"This distribution is highly skewed towards the low and high end. The dataset contains a hiuge number of properties that are available only for a couple of days each year, and a decent number that are available for > 300 days. "
### Feature engineering
### 1.5.0 Pearson correlation matrix
"There don't appear to exist obvious, strong correlations between these variables. \n\nHowever, the number of reviews per month is fairly (40%) correlated with the total number of reviews and the the total number of reviews is correlated (at 30%) with the availability of the property. Both of these correlations make sense.\n\nIt's also interesting that the longitude is anticorrelated (at 20%) with the price. That also makes sense - property in the Bronx and in Queens is cheaper than Manhattan and Brooklyn."
### 1.5.1 PairPlot
## 1.6 Encoding categorical features
"Immediately evident is a dramatic improvement in our model's convergence rate. After only one epoch, the model using the encoded features has a validation loss comparable to the unencoded model's final validation loss."
How do the models compare on the validation data?
## Distribution Age over Week
## FVC - The forced vital capacity
### FVC vs Percent
FVC seems to related Percent linearly. Makes sense as both terms are proportional.
### FVC vs Age
Males have higher FVC than females irrespective of age
### FVC vs Weeks
### Pick one patient for FVC vs Weeks
Person never smoked has FVC lower than smoker. Some Ex-smoker have very high FVC.
"### Pclass, Sex & Embarked vs. Survival"
"From the above plot, it can be seen that:\n- Almost all females from Pclass 1 and 2 survived.\n- Females dying were mostly from 3rd Pclass.\n- Males from Pclass 1 only have slightly higher survival chance than Pclass 2 and 3."
### Age vs. Survival
"From *Pclass* violinplot, we can see that:\n- 1st Pclass has very few children as compared to other two classes.\n- 1st Plcass has more old people as compared to other two classes.\n- Almost all children (between age 0 to 10) of 2nd Pclass survived.\n- Most children of 3rd Pclass survived.\n- Younger people of 1st Pclass survived as compared to its older people.\n\nFrom *Sex* violinplot, we can see that:\n- Most male children (between age 0 to 14) survived.\n- Females with age between 18 to 40 have better survival chance."
"From the above figures, we can see that:\n- Combining both male and female, we can see that children with age between 0 to 5 have better chance of survival.\n- Females with age between ""18 to 40"" and ""50 and above"" have higher chance of survival.\n- Males with age between 0 to 14 have better chance of survival."
# Diagnosis Distribution\n\nThis part we can't use in our model but it's giving us some insights about this disease so we can inspect that too. You can see the details below:
# Loading Image Meta Features\n\nThis is the part where we get basic info directly from images themselves.
"# Image Sizes\n\nWe can see some kind of relation between size and target, but is it meaningful? Too soon to say..."
"## Getting Image Attributes\n\nYou can get these attributes by using the code below, I commented it out here and imported it as a data becasue it's time consuming process."
# Image Colors and Their Effects on Results
"# How are the Image Sizes Affecting Targets in Our Data\n\nWe have important observation here, you can see whole 1920x1080 set in test data which is not present in train data. That can have huge impact on final scores, mind that in your models. You might want to leave out image size related info in your models or regularize your models to smooth that effect. It can cause overfitting because of high correlation between image sizes and target, but these correlation might not be the case in test set (most likely) so keep that in mind."
# The Mysterious Images\n\nThe name 'Mystery' comes from Chris Deotte from the comments down below and I decided to investigate them further. In last part we found out a new set of images with the resolution of 1920x1080 and they aren't present in train data at all. So we can assume these images weren't selected randomly for this competition. Down below I'm going to compare them with the rest of data.\n\n* It looks like without the 1920x1080 set mean colors are much more similar between train and test.\n* Again image size distribution gets closer between train and test without the mystery set.\n\n\nGonna check other features and add them here soon...
"I was curious about if these 1920x1080 images belong to high scan patients including 200+ one but it seems these observations are grouped around 10 scans, so it makes things more interesting..."
"Since we checking this mystery patch of images, let's check other features about them too maybe we can find other factors effecting this test sampling..."
Looks like our 1920x1080 set images consisting little bit younger patients than the rest. Interesting...
"How much meaning are you really getting out of this visualization ??    \n\nThis seems to happen a lot when data is spread over a large range. The highest values 'drown' out the ability to see the relationships in the heights of the smaller values...  \n\nData in the field of data science seems to include data distributions like this a lot, perhaps we can fix this visualization issue.   You see plots like the above a fair amount. "
"There is nothing wrong with keeping the original plot, but a subplot should be created for the area 'east' of the Cut, to see the RELATIONSHIP between the data points.   \n\nI will now plot the datapoints east of the cut in its own subplot, for visualization clarity..."
**Concept Reference**  -  You will see [this guy](https://mgoblue.com/roster.aspx?rp_id=19098) in the NFL some day\n  * Formulated during the viewing of [this](https://raw.githubusercontent.com/tombresee/Temp/master/ENTER/michvsnd.png) game (there was some beer involved)
"Side Note:\n* I did considerable analysis to see if the windspeed would have any factor in the outcome of the running play, and found **no real evidence based on the data that it made any type of difference**\n* I would posit that windspeed and direction would have a considerable impact on passing plays though (go outside and play catch with someone when its windy, a football is not a baseball, even if you put zip on it a football will drift a bit under high wind conditions\n* I wish I had more temp and humidity data, my guess is that it would make a difference in yards gained as defensive linemen started getting fatigued, but then again the offensive line would get fatigued as well possibly the same amount, hard to say without the data \n* Personal Opinion:  IF one is introduced in doing a serious analysis of this data, and diving beyond the surface, I think its important to constantly keep your eye out for the types of distributions seen (age/height/weight may be gaussian for instance, but there are MANY features/attributes that are inherently skewed, even the actual yards gained).  Where am I going with this ?  You cannot apply the same advanced statistics principles on skewed data that you can on gaussian distributions.  But either way, a general guideline I follow here is that when a feature is skewed in some form, it is IMPORTANT to realize that a better measure of central tendency of the data is median over mean (average)...\n* I am guilty of initial bias:\n  * I was **convinced** that turf versus grass would make a difference in the running back performance, and yet I found no real evidence that running backs perform better on one surface versus the other.  I have no included this analysis but will probably post at some point.  \n  * I will wager a guess:  Based on my experience playing rugby, a *muddy* field seems to slow down even the fastest runners, and offers an advantage to the 'forwards' (think linemen), but the quality and conditions of NFL playing fields never really result in a truly muddy field, and thus to a certain extent it is likely that a well-kept grass field and a turf field are both going to allow the runner to run as fast as he desires... \n  * I read online extensively on this 'turf vs grass' debate, and there seems to be the understanding that neither field type really offers a substantial quantifiable advantage...so for now we will drop this line of discussion...\n"
"Let's now begin:  Initial Examination of overall running (rushing) yards per play:\n* Let's take a look at the most important feature, the yards, which we will need to be able to predict going forward after our machine learning model is created\n    "
"\nHistogram Hit:  \n\nIts important to understand that a histogram is a great starting point to examine the data's distribution, but there is some data that is smoothed out by doing this (as it is inherently a binning process).  It is understood that when examining extremely large datasets it is important to start somewhere, and the histogram in general has many many positives, but I'm creating the term `Histogram Hit` so it is understood it is somewhat of a smoothing process AND depending on the bin size chosen can steer the visualization in many different directions.  I am aiming to create an equation to quantity the actual 'hit' one takes when creating histogrames, but for now I'll introduce the term and come back to this at some later point.  Please understand that a histogram is an A-, its great, it is a quick way of visualizing data, but it is not flawless..."
2018 - Top 10 Longest Rushes:\n* Let's take a look at the top ten most spectacular rushes:\n
"The first was NOT a kickoff return, it was a **handoff**.    Now look at HOW far back he is in the endzone !   \nThe guy ran probably 107 yards on that play (but gets credit for 99 yards from line of scrimmage at their own 1 yard line; that day he ran for a total of a staggering 238 yards).  This tied the NFL record for longest rush, set 35 years earlier.   \n\n\nDerrick Henry from the Tennessee Titans is a [*beast*](https://twitter.com/NFL/status/1070863698791550976?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1070863698791550976&ref_url=https%3A%2F%2Fwww.theguardian.com%2Fsport%2F2018%2Fdec%2F06%2Fderrick-henry-touchdown-titans-jaguars-nfl-99-yards).  He is so money you could hold him sideways, swipe him across an ATM machine, and money would just keep streaming out for hours...\nThe best part:  He celebrated by striking the Heisman pose, which is perfectly fine, since he won it in 2015.  \n\n1. Derrick Henry - Tennessee Titans\n1. Lamar Miller	- Houston Texans \n1. Nick Chubb	- Cleveland Browns\n1. Adrian Peterson	- Washington Redskins	\n"
"Yards vs Down:\n* Plotting the distribution of yards by series 'Down'.  Note that I have configured many of my plots to show granular ultra-precise 1-yard increments ! \n* In this case, I have removed a few outliers so as to see the general trend of the data \n* I am doing this for a very specific reason, you will see in a sec      - *Tom Bresee*\n    "
"* >Note:   I am creating a new term I will call 'unit grid'. When creating plots where the y or x axis, depending on the plot, is in a relatively short magnitiude range (lets say approximately 20 units or below), I find it helps to actually use the grid lines to expand to the plot on a somewhat granular basis.  When that is needed or helpful, spacing the grid at 'unit' levels shall be known as 'unit grid', i.e. the grids are every 1 unit on the scale.  I think it helps the viewer quickly quantify actual values, to the point where it approaches the information transfer of a barplot..."
"* First and second down we see almost an identical distribution of runs (i.e. In the first two quarters, the teams run about the same distribution of runs in the quarters)\n  * What i find suprising is that in the first two quarters, examining the plots we see that 25% of the plays generated **less** than 1 yard total gained. Running the ball has risk, it does not always end with yards gained.   \n* In the third down, we see a slight drop in the number of yards gained, and the median yards gained has dropped a solid yard.  In a game of inches, this is a big deal. \n* Fourth down performance is relatively poor, but then again, most of the time a team does NOT run the ball on 4th down, due to the risk. "
"\nWarning: It is important to list the sample size for each of the histograms, because one may draw the erroneous conclusion that the number of times the ball was run was the 'same' for each of the downs, when in fact it wasn't..."
"![](http://)> **INSIGHT**:   For a sample size of all NFL rush plays over the course of two entire years, only **7.83%** of the runs were on 3rd down, and less than **1%** were rushes on 4th down. "
"Yards vs Quarter of the Game:\n* Plotting the distribution of yards by game quarter, where 5Q symbolizes overtime..."
Yards Gained vs Box Defender Count:\n* Plotting the distribution of yards gained vs number of defenders in the box.  We will call this the defensive 'density' count...\n* A helpful reference image I created is shown below. 
### 4.4.2 plot utility score vs Accuracy
You will only get a utility score if your prediction accuracy is higher than 50%.
### 4.5 Heatmap of features.
Threre are many features which are highly correlated\nI think we should look into some highly correlated features to see their distribution and relation.\n\nI used plotly so that you can zoom into the feature.\nYou can see that most of high correlated feature are near the center diagonal line\nMost important patter is in the bottom right corner if you zoom into it you will see that\nThere is alternate high and low correlations\n\nAnother interesing one is 18 to 36 feature\n\nBut there is no feature which is highly correlated to the resp value.
### 4.6 Pairplot for highly related features (18 to 39)
"😆😆😆\nOh man EDA is fun.\nI mean look at it, It looks like those optical illusion images"
## 4.9 Distribution of Resps
## 4.10 Distplot of some features\n\nI am going to plot distribution plot of those 10 features which has highest difference in mean\nof data with negative resps and positive resps.
## 4.11 Count of neg and pos resps in train_data
Number of datapoints in negative and positive resps are almost same which is a good news\nwe do not have to worry about training on unbalanced dataset. 
# 4.2 Locality of schools with their counts 
# 4.3 Top Donor State Which Donated Highest Money
### Top 5 Donor States\n* **California (46+ Million)**\n* **New York (24+ Million)**\n* **Texas (17+ Million)**\n* **Illinois (14+ Million)**\n* **Florida (13+ Million)**
# 4.5 Average amount funded by top 5 states in terms of number of projects
# 4.6 Percentage of donors as teachers
# 4.7 Top Donor Checked Out carts
# 4.8 Average Percentage of free lunch based on Metro Type
# 4.10 Top 5 projects with their count
## Merging schools and projects to derive insights from them
### Visualize Projects count according to School Metro Type
#### Inference:- A lot of projects are allocated to urban areas. 
### Binary target visualisation per tissue slice \n\nBefore we will take a look at the whole tissue let's keep it a bit simpler by looking at the target structure in the x-y-space for a handful of patients:
### Insights\n\n* Sometimes we don't have the full tissue information. It seems that tissue patches have been discarded or lost during preparation. \n* Reading the paper (link!) that seems to be related to this data this could also be part of the preprocessing.
Let's use an example patient with id 13616: 
"### Insights\n\n* The tissue on the left is shown without target information.\n* The image on the right shows the same tissue but cancer is stained with intensive red color. \n* Comparing both images it seems that darker, more violet colored tissue has a higher chance to be cancer than those with rose color. \n* But as one can see it's not always the case. So we need to ask ourselves if violet tissue patches have more mammary ducts than rose ones. If this is true we have to be careful. Our model might start to learn that mammary ducts are always related to cancer! \n\nSometimes it's not possible to load an image patch as the path is ill defined. But in our case, we were able to load them all:"
## Target distributions \n\nLet's take a look at the target distribution difference of the datasets: 
We can see that the test data has more cancer patches compared to healthy tissue patches than train or dev. We should keep this in mind!
Another important value on this Dataset is the Repayment Interval.... \nMight it is very meaningful about the loans to sectors  
Humm.. An high number of loans have a Irregular Repayment... It's very interesting.... \nLet's explore further the Sector's
 I will plot the distribuition of loan by Repayment Interval to see if they have the same distribuition 
And with the Lender Count? How is the distribuition of lenders over the Repayment Interval
"Intesresting behavior of Irregular Payments, this have a little differenc... The first peak in lenders distribuition is about the zero values that I add 1."
Let's take a better look on Sectors and Repayment Intervals in this heatmap of correlation
"We have 3 sector's that have a high number of Irregular Repayments. Why this difference, just because is the most frequent ? "
And what's the most frequent countrys? 
"The highest mean value without the filter is Cote D'Ivoire is 50000 because have just 1 loan to this country, that was lended by 1706 lenders, how we can look below. "
"If you want to see the heatmap correlations below, click on Show Output"
On this heatmap correlation above we can see that just Kenya have Weekly payments and Kenya have the highest number of Irregular payments
We can look a lot of interesting values on this heatmap.\n- Philipines have high number of loan in almost all sectors\n- The USA is the country with the highest number of Entertainment loans\n- Cambodia have the highest number of Loans to Personal Use\n- Paraguay is the country with highest Education Loan request\n- Pakistan have highest loan requests to Art and Whosale\n- Tajikistan have highest requests in Education and Health\n- Kenya and Philipines have high loan requests to Construction\n- Kenya also have high numbers to Services Loans
Let's verify the most frequent currency's 
PHP THAT IS THE CURRENCY PHILIPINE\nUSD EVERYONE KNOWS\nKES THAT IS THE CURRENCY OF KENYA
"\n  👀 We now know that the majority of respondents are Students and Data Scientists, although 💃ladies do prefer a Data Analyst job, vs the 🎩gents who would rather go for Data Engineer. We also know that they love and use Python the most, while starting to lose interest in R. Jupyter Notebooks are losing interest vs Visual Studio, which has been skyrocketing in 2021. In terms of special sauces, Personal Laptops are most used, as the pool of respondents is younger, therefore have less experience & resources. An interesting trend is the rise of TPU and the apparent fall of GPU, seen in both men and women, although 🎩gents use much more special hardware than 💃ladies do.\n\n\n# 3. Music at ""the Wireless""\n\n> 📻 Fun fact - ""the Wireless"" started being popular in the 1920s and it soon became an interesting point of attraction in households, especially for the youth. The elder and more ""conservative"" people (that lived most of their life in the second half of the 1800s) were reluctant to use it, saying ""it's just a passing phase"". If you haven't guessed it, ""the wireless"" was to be and it's now named ... the radio. At that time, you had to have it installed by a professional and you would usually hear some news, live presidential/royal speeches and music (most popular genres at the time were blues, jazz, swing, regtime etc.).\n\n\n\nNote: Inspiration from the TV series ""Downton Abbey"".\n\n## 3.1 ""Jazzy"" Visualizations\n\n* Matplotlib, Seaborn and Plotly remain unchallenged in their top 3 ranking for the most used visualization libraries. Something to mention is that for all libraries (besides GGplot, where women tend to have bigger usage than men), men seem to be much more interested in the visualization packages.\n* Matplotlib increased a few percentages in 2021 vs 2018, however Seaborn received the most attention, gaining 15% popularity points for 🎩gents and 17% popularity points for 💃ladies.\n* All other visualization libraries seem to either remain stagnant or even decrease in usage. Why is that? Is it a lack of knowledge from the respondents/users, or just a lack of overall interest? I am thinking this might be the case because D3.js, Geoplotlib, Shiny, Leaflet etc. are very specific libraries for specific problems and not for day-to-day usage."
"## 3.2 Machine Learning on the ""Blues""\n\nI. The Algorithms\n\n* If you shift through years, there is an apparent decrease in specialized respondents (that have more than 2 years of experience in doing ML). This links very well with the fact that we know that there are more youngsters on the platform, that are using their personal computers in order to navigate and learn Machine Learning. Nevertheless, these youngsters will increase the percentage of experienced coders, usage of specialized hardware and overall ""heavy-duty"" tools and libraries in the years to come. Just wait for it! 🧐\n* 🎩Gents tend to have more numbers within the 3+ years of ML Experience than 💃ladies do. This is linked to the fact that more men come from an Engineering background (which requires coding aptitudes) vs. women that come more from Data/Business Analytics (which requires less or even no coding).\n* The most used algorithms for both genders are Trees Ensembles and Random Forests, Gradient Boosting Algorithms and CNNs (Convolutional Neural Networks). This matches quite well the overall scheme of competitions we have been having on Kaggle, which tends to be either tabular or requires Computer Vision (of course, there have been many more other types, forecasting and NLP to name a few - however most competitions do require in most of the cases at least one of the above-mentioned algorithms)."
"II. The Frameworks\n\n* Scikit-learn, Tensorflow and Keras are the most used ML frameworks for both genders. However, 💃ladies do tend to have less interest within them than 🎩men do, as it shows within the smaller numbers of usage for each framework.\n* Most frameworks have a steady trend of usage, with no apparent substantial increase or decrease in popularity. I believe that the most ""apparent"" improvement in the last 4 years was for PyTorch, which increased 8% popularity points for 🎩gents and 6% popularity points for 💃ladies in 2021 vs 2018.\n\nNote: As the number of respondents that answered they used Other Frameworks or None at all was very small (less than 3% for both genders), I excluded them from the graphs."
"## 3.3 Deep Learning ""Swing""\n\n* In regards to Computer Vision, Image Classification (e.g.: ResNet, EffNet etc.) is the most used method for both genders. However, the second most used method differs for the 2: for 🎩gents is Object Detection (e.g.: RetinaNet), while for the 💃Ladies is Image Segmentation (e.g.: U-Net), although the 2 are very close in terms of popularity from one to another.\n* In terms of NLP (Natural Language Processing), we can see a very strong increase in popularity for Transformer Language Models (e.g.: BERT) for both genders. It even surpassed in popularity of the EncoderDecorderModels (e.g.: seq2seq), which were much more popular in 2019."
"II. The Size of the Business\n\n*  Around 20% of respondents (same pattern in both genders) say that they are employed in very small companies (between 1 and 49 employees). This trend remained quite constant from 2019 through 2021.\n*  Another 20%-25% of respondents are at the other end of the spectrum, saying that they work within large and very large companies of more than 1,000 employees (shown for both ladies and gents).\n*  There was a smaller percentage of respondents that answered this question (be aware, not in absolute numbers - but in percentage on year and gender) in 2020 vs 2019. In 2021 it stabilized a little, but I am wondering if the decrease in 2020 was marked by some elderly more experienced people on the platform that preferred to keep these pieces of information private."
"## 4.2 Estate's Profitability\n\nI. The Salary\n\n>  📌 Note: The salaries were adjusted on respondent's country, meaning that the pay (in US dollars) was balanced to be showing the power of purchase within the place of residence, rather than just the overall brute amount. The adjustment was done as I did last year, meaning that I have converted the salary to ""number of McMeal units a person can buy in their country"". The more units, the bigger the purchasing power. Unfortunately, I could not adjust the pay on years too as I could not find data for all 4 years - hence there might be some inflation bias within the numbers.\n\n*  🎩 For gentlemen, the most prominent change between 2018 and 2021 is that there are many more respondents that have lower salaries. This is also evident in the general mean of the amount, which increased in 2019 but dropped at the lowest in 2021.\n*  💃 This trend is the exact same for ladies too. In general, there are many more respondents with lower income in 2021 vs 2018. The average pay dropped suddenly for women in 2020 vs 2019 and remained at the same value in 2021. \n*  I strongly believe this decrease in overall pay is not due to general lower income within the Data Science/ Machine Learning Industry. These industries actually have some of the biggest pays around the world. I would rather think this trend shows because there are many more young people that took the survey, hence they naturally have lower income vs somebody with 5+ years of experience. I am confident we will see a surge in higher pays in the following years."
"Curiosity Bonus: Salaries & Roles\n\nI know pay is a taboo subject, but I could not help myself to also look within the top 6 most frequent Roles and what is the average salary for each of them. I have come up with an extremely informative graph:\n\n* For 🎩gents, the highest paying jobs are Project Manager (1st place) and Data Scientist (2nd place). Coming next, the 3rd best-paying salary is for Software Engineering - and we know men love this job, as it is also the 3rd most frequent role found within our respondents.\n* For 💃ladies, the first 2 most high-paying roles are Project Manager (1st place) and Data Scientist (2nd place) as well! What I find the most interesting is that for them, on the 3rd place on the list of the best paying jobs is Business Analyst (very close to Data Analyst I might add), which are also extremely popular roles for women (as we saw in the chart in Chapter 2.1).\n* What I find interesting is that each gender has in the top 4 best paying jobs roles that they are really good at. For women there is Data Science, Data Analysis and Business Analysis, while for men is Data Science, Software Engineering and Research."
"II. The (Machine Learning) Investment\n\nAre companies interested in ML? How much are they willing to pay for it?\n\nSomething that I found rather odd is that there is some sort of decrease in Machine Learning interest from companies, at least from the respondent's perspective. I would have actually expected an increase in companies that have started using ML or have been implementing it for a while. However, not only the percentage of interest decrease but also the overall number of respondents that answered this question decreased too. I would put a part of this decrease on the lack of administrative knowledge from respondents (if they don't know for sure they might choose to hold back from answering)."
" The investing activity is quite correlated to the interest in Machine Learning. For both genders, we can see that ~30% of respondents say that their company either doesn't invest in ML at all, or it invests a very small amount of money (less than 100 dollars yearly). On the other end of the spectrum, very few respondents (less than 5% for 💃ladies and less than 🎩10% for men) say that their company invests more than 100K dollars in ML.\n\n Again, I am reluctant to put this behavior on the lack of interest or lack of administrative knowledge. As we know from previous graphs, there were quite a few people that said they work in big companies, however the percentages don't match with the investment - meaning that large companies don't seem to necessarily invest a lot in Machine Learning."
"## 4.3 Manpower within the Estate\n\nA first note here would be that I have explored the activities that make up an important role for a respondent and there are no major changes throughout the years. The most frequent activities remain ""building and running Machine Learning services"", while the least frequent activity is ""performing research to advance the state of the art of Machine Learning"".\n\nIn regards to the number of people that make up the Data Science teams we can observe the following:\n\n* For both 🎩men and 💃women the team size correlates with the size of the company. There are around 35% of respondents that affirm that the DS team within the company is quite small (less than 4 people).\n* On the opposite side there are ~16% (💃ladies) to almost 20% (🎩gents) that state they are working in teams equal or greater than 10 members.\n* There is no significant shift within the percentages of categories throughout the years."
"\n  👀 🎩Gents and 💃ladies are found the most within the Academics and Technology industries. The company and team size are split almost 50-50 (between the respondents that answered these questions), meaning that ~half of them are part of small companies and teams, while the other half are part of larger companies and Data Science teams size. These companies don't seem to pay greater attention to ML nor invest more within this discipline in 2021, however the respondent's bias and administrative knowledge might influence this answer. Lastly, there has been a decrease in pay due to an increase in young (fresh) respondents, with some of the best paying jobs (for both genders) being Project Manager and Data Scientist.\n\n\n# 5. Getting Social - Sharing is Caring\n\n## 5.1 Acquiring Knowledge\n\nThe evolution of learning platforms has been as follows:\n\n* Kaggle Learn had the biggest increase in popularity since 2018, having 34% of the 🎩gents and 30% of the 💃ladies saying that they use Kaggle Learn on a regular basis. Another platform that has increased in popularity has been Udemy, but at a slower pace than Kaggle Learn.\n* Besides these two, all the other learning platforms have been decreasing in usage in the last 4 years."
"## 5.2 Sharing and Contribution\n\n* GitHub, Kaggle and Colab are the most popular spaces out of all to share Data Science work (for both genders). All three mentioned have been increasing in usage in 2021 vs 2020.\n* Worth mentioning is that there are fewer women (in percentage points, not in absolute numbers) that answered this question versus men."
"## 5.3 Where the gossip is at\n\n* Kaggle and Youtube remain in 2021 (as in the past years too) the most used platforms for gathering information, news and interesting events about Data Science. I personally would have expected Twitter to be very used as well, but it has only 15% usage for gents and ~10% for ladies.\n* For both 💃ladies and 🎩gents there has been an overall decrease within almost all media sources, due to a decrease in the percentage of respondents that answered this question. However, Youtube and Newsletters are the only two platforms that had a slight increase in popularity in 2021 compared to the last years."
"\n  👀 Kaggle has the general lead in all 3 social categories - learning, sharing DS work and media source - and it increased in all categories in 2021 vs the last years. GitHub and Colab are the other two very popular sharing platforms, while as for media sources we can affirm that Youtube and Personal Blogs remain in lead.\n\n\n# 6. Ending the Party in Style\n\n\n\nA dataset is very volatile. In my time as a Data Analyst, and then Data Scientist, there is one thing I can say I have discovered and I live by: looking at a dataset is not a straightforward recipe. A dataset is like a living, breathing organism.\n\nSometimes it confirms some conclusions that you maybe might have already known, or let's say had a hunch, an intuition on them. Sometimes it does the exact opposite, giving the reader some insights they would have never expected. Sometimes it even shocks you. And other times it doesn't really say anything. Nothing new, nothing you already knew, nothing at all. But in these ""nothing"" insights there are still some conclusions to be taken, even actions to be made.\n\nThe following are some of my own, personal insights and findings on this beautiful dataset. However, I would encourage you to read and draw you own conclusions, even challenge mine. A healthy debate brings us more forward than always agreeing with one another - right? :)\n\n\n  \n    \n  \n    \n  \n      💃 Ladies\n    As we have seen so far throughout the analysis, there are no major differences between ladies and gents in terms of preferences towards tools. I will be honest, I have expected to see many more dissimilarities than I am seeing now, however ladies and gents do seem to have in common much more than one would anticipate.\n    The percentage of women that responded to the survey has maintained through time at around 20%. Most ladies are from USA, India and more recently from Nigeria, Egypt and Indonesia. They work as Data Scientists, Data Analysts or Business Analysts in companies oriented towards Tech and Medical industries, or they can be found studying/teaching within Academia. \n    Regarding the tools and skills, ladies are very fond of Python and are starting to rapidly lose interest in R. They do use less special hardware and have fewer years of experience in coding due to the nature of their role/job, which doesn't involve mandatory coding practice.\n  \n\n\n\n\n\n  \n    \n  \n    \n  \n      🎩 Gentlemen\n    Gentlemen are very similar to ladies in terms of preferences and used tools. However, they do have a few areas where they stand out, having different aspects than the ladies. \n    Around 80% of men are responding to the survey each year, with this number maintaining throughout all 5 analyzed years. Usually, they have rezidency within India, USA, Japan, with a big surge in people from Nigeria and Pakistan. Most of the gents are Data Scientists or Software Engineers working within the Tech and Finance industries, or they are teachers/students within Academia. \n    Python is and remains king in the programming languages arena, and they do seem to be liking to work more with specialized hardware, such as GPU and TPU. Gents do tend to have a bit more seniority within the machine learning and coding arena, as their role within companies requires more necessary knowledge on this part.\n  \n\n\n\n\n\n  \n    \n  \n    \n  \n      🕒 Time\n    Many of the shifts observed within the evolutions were affected by the increase in youth (people aged 18 to 24 years old). Hence, the decrease we have observed in Doctorate/ Masters respondents, in overall coding experience and in income were naturally influenced by this aspect.\n\n    For programming languages we see more activity within C/C++, while R is losing popularity each year. Visual Studio is also enjoying an increase in usage, vs Jupyter who had a dip in 2021. TPU is growing more and more popular each year, and it seems that it does so to the detriment of GPU, which is slowly decreasing. Another algorithm that had a steep increase is the Transformer (e.g.: BERT).\n    Regarding the companies and their structure, the respondents seem to be split quite in the middle: around half saying they work in small companies and teams, while the other stated that they work in larger companies and teams (with no apparent change through time). However, I was surprised that the interest and investment in ML don't tend to increase throughout the years and it doesn't correlate with the company size - meaning that larger companies don't seem to necessarily invest more in ML.\n  \n\n\n\n# 7. Giving Thanks\n\n* Font Generator for ParkLane (Great Gatsby Font). (n.d.). Font Generator. Retrieved November 19, 2021, from https://www.font-generator.com/fonts/ParkLane/?size=58&color=000000&bg=none\n* Sources for graph images. (n.d.). Pinterest Board. Retrieved November 19, 2021, from https://ro.pinterest.com/andrada_teodora/1920/\n* Templates for the D3 Graphs. (n.d.). The D3.Js Graph Gallery. Retrieved November 22, 2021, from https://www.d3-graph-gallery.com/index.html\n* Special loving thanks: to my amazing partner, who has been extremely supportive and eager to help in this analysis. This notebook would have never looked like it does now without him, and for his patience and kindness, I am forever grateful.\n\n\n## My Specs\n\n> My W&B Dashboard is growing:\n\n\n\n\n* 🖥 Z8 G4: Workstation\n* 💾 2 CPUs & 96GB Memory\n* 🎮 2x NVIDIA A6000\n* 💻 Zbook Studio G7 on the go"
"In terms of job category once again there is no difference between men and women, but we can see that job category 3 tends to take bigger credit amounts for longer duration.  \n\nAnd at the end if someone likes 3D plots here you go."
"**Clustering with KMeans**\n\nFor clustering, I will create a subset containing only numerical variables (Age, Credit amount, Duration). "
I will create a function which plots three histograms - one for each variable. 
Let's look at the histograms.
I will check how inertia changes for various number of clusters.
The plot above shows that inertia decreases with increasing number of clusters. \n\nThis method allows for assessment of cluster separations and fitting of each observation in its own cluster.  The highest score the better.  I will perform this analysis for various seeds as well.
The heatmap above shows silhouette scores for various combinations of random state and number of clusters. The highest scores are for 2 and 3 clusters and they are relatively insensitive to seed. \n\nI will chose 3 clusters to get more insight into data.
Below I will create silhouette graph for 3 clusters in order to visually depict fit of each point within its own cluster (modified code from scikit-learn doc).
I will define a function showing clusters on the scatter plot.  
"In this algorithm there are two relevant parameters: preference and dumping. It means that we don’t define upfront number of clusters, algorithm itself chooses their number. I will fix dumping and check number of clusters in function of preference parameter.  "
Together with decreasing value of preference parameter number of clusters goes down as well and levels for very small preference values. I will check four clusters option. 
"\n\nGoal: for each building and meter pair, visualize where target is missing and where target is zero VS time.\n\n\n\n"
"- Vertical blue lines may be suspicious\n\nLegend:\n* X axis: hours elapsed since Jan 1st 2016, for each of the 4 meter types\n* Y axis: building_id\n* Brown: meter reading available with non-zero value\n* Light blue: meter reading available with zero value\n* White: missing meter reading"
"Let's also check out what the distribution of answered_correctly looks like if we groupby the (10,000 unique) task_container_id's."
"Below I am plotting the number of answers per user_id against the percentage of questions answered correctly (sample of 200). As some users have answered huge amounts of questions, I have taken out the outliers (user_ids with 1000+ questions answered). As you can see, the trend is upward but there is also a lot of variation among users that have answered few questions."
"Does it help if the 'prior_question_had_explanation'? Yes, as you can see the percent answered correctly is about 17% higher when there was an explanation. Although it is probably better to treat not having an explanation as a disadvantage as there was an explanation before the vast majority of questions.\n\nIn addition, it is also interesting to see that the percent answered correctly for the missing values is closer to True than to False."
"prior_question_elapsed_time: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n\nAt first glance, this does not seem very interesting regarding our target. For both wrong and correct answers, the mean is about 25 seconds."
"However, as the feature works with regards to the CV (see Baseline model), I also wanted to find out if there is a trend. Below, I have taken a sample of 200 rows. As you can see, there is s slightly downward trend."
"# 1.2 Exploring Questions\n\nMetadata for the questions posed to users.\n\n* question_id: foreign key for the train/test content_id column, when the content type is question (0).\n* bundle_id: code for which questions are served together.\n* correct_answer: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n* part: the relevant section of the TOEIC test.\n* tags: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.\n"
"As you can see, the differences are significant!"
"However, we should also realize that the tag with the worst percent_correct only has about 250,000 answers. This a low number compared to the tags with most answers."
"What are the so-called ""Parts""? When following the link provided in the data description we find out that this relates to a test.\n\n> The TOEIC L&R uses an optically-scanned answer sheet. There are 200 questions to answer in two hours in Listening (approximately 45 minutes, 100 questions) and Reading (75 minutes, 100 questions). \n\nThe listening section consists of Part 1-4 (Listening Section (approx. 45 minutes, 100 questions)).\n\nThe reading section consists of Part 5-7 (Reading Section (75 minutes, 100 questions))."
### Image Label Groups by No. of Images
\n\nBasic Image Exploration
## Frequencies Distribution
## Time stamps Distribution
"## Simulating gravitational waves and evaluating their detectability in Python\n\nRiroriro is a Python package to simulate the gravitational waveforms of binary mergers of black holes and/or neutron stars, and calculate several properties of these mergers and waveforms, specifically relating to their observability by gravitational wave detectors.""\n\n""The gravitational waveform simulation of Riroriro is based upon the methods of Buskirk and Babiuc-Hamilton (2019), a paper which describes a computational implementation of an earlier theoretical gravitational waveform model by Huerta et al. (2017), using post-Newtonian expansions and an approximation called the implicit rotating source to simplify the Einstein field equations and simulate gravitational waves. Riroriro's calculation of signal-to-noise ratios (SNR) of gravitational wave events is based on the methods of Barrett et al. (2018), with the simpler gravitational wave model Findchirp (Allen et al. (2012)) being used for comparison and calibration in these calculations.""\n\nLink for the paper\n\n#### cc. of GW simulation: this part was written by Geir Drange and presented by Marília Prata\n\n\n\n\n""Riroriro is a set of Python modules containing functions to simulate the gravitational waveforms of mergers of black holes and/or neutron stars, and calculate several properties of these mergers and waveforms, specifically relating to their observability by gravitational wave detectors. Riroriro combines areas covered by previous gravitational wave models (such as gravitational wave simulation, SNR calculation, horizon distance calculation) into a single package with broader scope and versatility in Python, a programming language that is ubiquitous in astronomy. Aside from being a research tool, Riroriro is also designed to be easy to use and modify, and it can also be used as an educational tool for students learning about gravitational waves.""\n\n""The modules “inspiralfuns”, “mergerfirstfuns”, “matchingfuns”, “mergersecondfuns” and “gwexporter”, in that order, can be used to simulate the strain amplitude and frequency of a merger gravitational waveform. The module “snrcalculatorfuns” can compare such a simulated waveform to a detector noise spectrum to calculate a signal-to-noise ratio (SNR) for that signal for that detector. The module “horizondistfuns” calculates the horizon distance of a merger given its waveform, and the module “detectabilityfuns” evaluates the detectability of a merger given its SNR.""\n\nMore information on the pip installation can be found here: https://pypi.org/project/riroriro/\n\nTutorials for Riroriro can be found here: https://github.com/wvanzeist/riroriro_tutorials\n\nFull documentation of each of the functions of Riroriro can be found here: https://wvanzeist.github.io/\n\nhttps://github.com/wvanzeist/riroriro\n"
"### How Gravitational waves get detected\n\n""When a gravitational wave passes by Earth, it squeezes and stretches space. LIGO can detect this squeezing and stretching. Each LIGO observatory has two “arms” that are each more than 2 miles (4 kilometers) long. A passing gravitational wave causes the length of the arms to change slightly. The observatory uses lasers, mirrors, and extremely sensitive instruments to detect these tiny changes."""
### Resample the signal to 2048Hz (only the orthogonal part)
### Visualizing Frequency Vector
### Visualizing spectrum in frequency domain using Constant-Q transform
### Amplitude vs. Distance(Inverse square law verification)
"### Why the surprising results?\n\nSurprisingly we can notice that gravitational waves amplitude doesn't follow the inverse square law but why?\n\nIn order to answer this question we have to explain the difference between monopolic, diapolic and quadrapolic signals.\nFirst off, there are fundamental ways that light and gravitational waves are the same. They both:\n\n* do carry energy,\n* do reach infinite distances,\n* do spread out over space (in roughly a sphere) as you move farther away,\n* and will be detectable, at a certain distance, in proportion to the magnitude of the signal.\n\nBecause the geometry of space is the same for both light and gravitation, the difference between these two behaviors must lie in the nature of the signal that we can detect.\n\nTo understand that, we need to understand how gravity is a fundamentally different kind of force than electromagnetism. This will lead us to better understand how gravitational radiation (our gravitational waves) behave differently than electromagnetic radiation (light) when we allow it to propagate across the vast distances of intergalactic space.\n\n\n\nIf you want to create electromagnetic or gravitational radiation, how could you do it? The simplest way you could imagine — which (spoiler) doesn't work — would be to spontaneously create or destroy charge in a region of space. Having a charge pop into (or out of) existence would create radiation of a very specific type: monopole radiation. Monopole radiation is what happens when you have a change in the amount of charge that's present.\n\n\n\nWe cannot do this for either electromagnetism or gravitation, however. In electromagnetism, electric charge is conserved; in gravitation, mass/energy is conserved. The fact that we don't get monopole radiation is important for the stability of our Universe. If charge or mass could spontaneously be created or destroyed, existence would be extremely different!\n\nIf charge and mass/energy are conserved, then the next step is to either move your charges (or masses) rapidly back-and-forth, or to take charges of opposite signs and change the distance between them. This would create what we call dipole radiation, which changes the distribution of charge without changing the total amount of charge.\n\nIn electromagnetism, this creates radiation, because moving an electric charge back-and-forth changes the electric and magnetic fields together. This matters, because changing electric and magnetic fields that are mutually perpendicular to each other and in-phase if wis what an electromagnetic wave actually is. This is the simplest way to make light, and it radiates just like you're familiar with. The light carries energy, and the energy is what we detect, which is why objects appear dimmer as 1/r2 the farther away they are.\n\n\n\nIn gravity, however, freely moving a mass doesn't make gravitational radiation, because there's a conservation rule about masses in motion: the conservation of momentum. Similarly, separating masses doesn't make gravitational radiation either, because the center of mass remains constant. There's also a conservation rule about masses moving at a certain distance from the center of mass: the conservation of angular momentum.\n\nBecause energy, momentum, and angular momentum are conserved, you have to go past both monopole and dipole moments; you need a specific change in how the masses are distributed around their mutual center of mass. The simplest way to imagine this is to take two masses and have them mutually rotate around their center of mass, which results in what we call quadrupole radiation.\n\n\n\nThe amplitude of gravitational quadrupolar radiation falls off as 1/r, meaning the total energy falls off as 1/r2, just as it did for electromagnetic radiation. But this is where the fundamental difference between gravitation and electromagnetism comes in. There's a big difference between what you can physically detect for quadrupole and dipole radiation.\n\nFor electromagnetic (dipole) radiation, when the photons hit your detectors, they get absorbed, causing a change in the energy levels, and that change in energy — which remember, falls off as 1/r2 — is the signal you observe. That's why objects appear to dim according to an inverse square law.\n\nFor gravitational (quadrupole) radiation, however, it doesn't get directly absorbed in a detector. Rather, it causes objects to move towards or apart from one another in proportion to the amplitude of the wave. Even though the energy falls off as 1/r2, the amplitude only falls off as 1/r. That's why gravitational waves fall off according to a different law than electromagnetic waves.\n\n\n\nBut the amplitude, as we received it, compressed and expanded the entire Earth by about the diameter of three protons. The energy is huge and falls off as 1/r2, but we cannot detect energy for gravitational waves. We can only detect amplitude, which (thankfully) only falls off as 1/r, which is a very good thing. The amplitudes may be tiny, but if we can detect any signal at all, it's only a small step forward to detecting that same magnitude signal at any distance.\n\n For more check this! "
"## Plot explained variance ratio with number of dimensions\n\n- An alternative option is to plot the explained variance as a function of the number of dimensions.\n\n- In the plot, we should look for an elbow where the explained variance stops growing fast.\n\n- This can be thought of as the intrinsic dimensionality of the dataset.\n\n- Now, I will plot cumulative explained variance ratio with number of components to show how variance ratio varies with number of components."
### Comment\n\nThe above plot shows that almost 90% of variance is explained by the first 12 components.
Looking the State variable\n- pledge log by state\n- goal log by state\n- goal log x pledged log
"Cool. As we saw in your stastical test, the difference between "
We have a very interesting distribuition in goal values.
## I will take a further look at top 10 sucessful and failed categorys.\nI will look at:\n- Goal\n- Pledged\n- diff_pleded_goal 
We can see that almost all categorys in sucessful have the same distribuition of values but some video games projects have the highest values in % difference of Pledged by Goal 
Now I will start to Investigating the 3 top sucess and fail projects\n
Main Category
"In the musics with sucess the most frequent is Indie, and fails is Rock and Hip Hop! \n\nAnother interesting thing, is that Documentary is a significant value in both states... "
NOTE:\n- Higher class passengers (low `Pcass`) have better average survival than the low class(high `Pclass`) passengers.
### 2) Sex vs. Survival
"### 4) Pclass, Sex & Embarked vs. Survival"
"NOTE:From the above plot, it can be seen that:\n- Almost all females from Pclass 1 and 2 survived.\n- Females dying were mostly from 3rd Pclass.\n- Males from Pclass 1 only have slightly higher survival chance than Pclass 2 and 3."
*Alley*\n-------
All missing value indicate that particular house doesn't have an alley access.we can replace it with 'None'.
*Fireplaces*\n------------
Having 2 fireplaces increases house price and fireplace of Excellent quality is a big plus. 
"### Next let's create a function that will process the text data for us. \nFor this purpose, we will be using the spacy library. This function will convert text to lower case, remove punctuation, and find and remove stopwords. For the parser, we will use en_core_sci_lg. This is a model for processing biomedical, scientific or clinical text."
Applying the text-processing function on the **body_text**. 
\n\n Kurtosis grater than 3\n    \nIt is leptokurtic. It will signify that it produces outliers rather than a normal distribution.
"\n\n The graph shows positive skewness\n    \nWe see positive skewness from the graph above. As the graphs shows, more weight is on the left side of the distribution. We will fix it using 'norm' function and ""log1p"" function of numpy"
"## 4.1. Linear Regression for one country \n\nSince we are interested into predicting the future time evolution of the pandemic, our first approach consists on a simple Linear Regression. However, remind that **the evolution is** not linear but **exponential** (only in the beginning of the infection), so that a preliminar log transformation is needed. \n\nVisual comparison of both cases for Spain and with data from last 10 days informed, starting on March 1st:"
"As you see, the log transformation results in a fancy straight-like line, which is awesome for Linear Regression. However, let me clarify two important points:\n\n* This ""roughly **exponential behavior**"" **is only true for the initial infection stages of the pandemic** (the initial increasing of infections on the SIR model), but that's exactly the point where most countries are at the moment.\n\n* Why do I only extract the last 10 days of data? For three reasons:\n    1. In order to capture exactly the very short term component of the evolution\n    2. To prevent the effects of certain variables that have been impacting the transmition speed (quarantine vs free circulation)\n    3. To prevent differences on criteria when confirming cases (remember that weird slope on the China plot?)"
"# >> 5. Using a linear regression. \n\nBased on the existing data, one can calculate the best fit line between two variables, from checking correlations.\n\nIt is worth mentioning that linear regression models are sensitive to outliers."
- Interesting corelation!
# Training Function
# Validation Function
"## 1.6 Survival rate by cabin\nCabin is supposed to be less distingushing, also taking into consideration that most of the values are missing."
"## 1.7 Correlation of the variables\n* Pclass is slightly correlated with Fare as logically, 3rd class ticket would cost less than the 1st class.\n* Pclass is also slightly correlated with Survived\n* SibSp and Parch are weakly correlated as basically they show how big the family size is.\n"
"## 2. FEATURE SELECTION AND ENGINEERING\n## 2.1 Impute values\nNB: The calculation of values to impute should only be done on train set. For example, you want to impute the mean of age in the mussing values in test set. The mean of age should only be calculated on train set to avoid data leakage.\n\nFirst, we check how many nas there is in general. If there is only small amount then we can just exclude those individuals. Considering that there are 891 training samples, 708 do not have missing values. 183 samples have na values. It is better to impute. There are different techniques one can impute the values."
Check if we disrupted the distribution somehow.
## 2.2 ENGENEER VALUES
"### Visualize the loss function over time\n\nWhile it's helpful to print out the model's training progress, it's often *more* helpful to see this progress. [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) is a nice visualization tool that is packaged with TensorFlow, but we can create basic charts using the `matplotlib` module.\n\nInterpreting these charts takes some experience, but you really want to see the *loss* go down and the *accuracy* go up."
"## Evaluate the model's effectiveness\n\nNow that the model is trained, we can get some statistics on its performance.\n\n*Evaluating* means determining how effectively the model makes predictions. To determine the model's effectiveness at Iris classification, pass some sepal and petal measurements to the model and ask the model to predict what Iris species they represent. Then compare the model's prediction against the actual label.  For example, a model that picked the correct species on half the input examples has an *[accuracy](https://developers.google.com/machine-learning/glossary/#accuracy)* of `0.5`. Figure 4 shows a slightly more effective model, getting 4 out of 5 predictions correct at 80% accuracy:\n\n\n  \n    \n    \n    \n  \n  \n    Example features\n    Label\n    Model prediction\n  \n  \n    5.93.04.31.511\n  \n  \n    6.93.15.42.122\n  \n  \n    5.13.31.70.500\n  \n  \n    6.0 3.4 4.5 1.6 12\n  \n  \n    5.52.54.01.311\n  \n  \n    Figure 4. An Iris classifier that is 80% accurate. \n  \n"
### Observations:\n1)The Toddlers(age<5) were saved in large numbers(The Women and Child First Policy).\n\n2)The oldest Passenger was saved(80 years).\n\n3)Maximum number of deaths were in the age group of 30-40.
The Women and Child first policy thus holds true irrespective of the class.
### Chances for Survival by Port Of Embarkation
The chances for survival for Port C is highest around 0.55 while it is lowest for S.
"### Observations:\n1)Maximum passenegers boarded from S. Majority of them being from Pclass3.\n\n2)The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers.\n\n3)The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around **81%** didn't survive. \n\n4)Port Q had almost 95% of the passengers were from Pclass3."
"### Observations:\n\n1)The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Pclass.\n\n2)Port S looks to be very unlucky for Pclass3 Passenegers as the survival rate for both men and women is very low.**(Money Matters)**\n\n3)Port Q looks looks to be unlukiest for Men, as almost all were from Pclass 3.\n"
The crosstab again shows that larger families were in Pclass3.
### Observations:\n\nHere too the results are quite similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.\n\nThe chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when somebody has >4 parents on the ship.
"According to the curve above, betting on 10% of the matches leads to an average ROI of 58%.\n\nWe decide to bet on **10%** of the matches : the 10% with the highest confidence between 2013 and 2018 (~1100 matches).\n\nWe have to check first that these matches are well spread between 2013 and 2018."
The matches we are the most confident in are well spread accross the study period.\n\nThat indicates a certain stability in the betting market.\n\nNow let's see our ROI for the consecutive sections of 100 matches between 2013 and 2018.
"There is a great variability but the process seems quite stationary. \n\nTo sum up, a strategy that gives a good ROI in 2013 still gives a good ROI in 2018, which is quite reassuring.\n\nBut such a variability would force us to wait much more that 117 matches to guarantee the 58% ROI...\n\n**Now let's bet on 35% of the matches (~3850 matches). The average ROI over our study period in 20%.**"
Let's check how the distribution of survival variable  depending on the title.
People with 'Master' have the highest survival rate. Maybe because people with the master are mainly boys under 13 years old.
* Plot illustrating positive effect of having dependents (kids & teens) on number of deals purchased:
"* Plots illustrating the positive effect of income and negative effect of having kids & teens on advertising campaign acceptance:\n\nNote: For the purposes of the following plot, limiting income to < 200000 to remove outlier"
"# Section 02: Statistical Analysis\n\nPlease run statistical tests in the form of regressions to answer these questions & propose data-driven action recommendations to your CMO. Make sure to interpret your results with non-statistical jargon so your CMO can understand your findings.  \n\n### What factors are significantly related to the number of store purchases?  \n\n* We will use use a linear regression model with `NumStorePurchases` as the target variable, and then use machine learning explainability techniques to get insights about which features predict the number of store purchases\n* Begin by plotting the target variable:"
"* Drop uninformative features\n    - `ID` is unique to each customer\n    - `Dt_Customer` will be dropped in favor of using engineered variable `Year_Customer`\n* Perform one-hot encoding of categorical features, encoded data shown below:"
"* Explore the directionality of these effects, using SHAP values:\n    - Findings:\n        - The number of store purchases increases with higher number of total purchases ('TotalPurchases')\n        - The number of store purchases decreases with higher number of catalog, web, or deals purchases ('NumCatalogPurchases', 'NumWebPurchases', 'NumDealsPurchases')\n    - Interpretation:\n        - Customers who shop the most in stores are those who shop less via the catalog, website, or special deals"
"### Does US fare significantly better than the Rest of the World in terms of total purchases?\n\n* Plot total number of purchases by country:\n    - Findings: \n        - Spain (SP) has the highest number of purchases\n        - US is second to last, therefore the US does not fare better than the rest of the world in terms of the total number of purchases"
"* Plot total amount spent by country: \n    - Findings: \n        - Spain (SP) has the highest total amount spent on purchases\n        - US is second to last, therefore the US does not fare better than the rest of the world in terms of the total amount spent on purchases"
"### Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent an above average amount on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test\n\n* Plot relationship between amount spent on gold in the last 2 years (`MntGoldProds`) and number of in store purchases (`NumStorePurchases`):\n    - Findings: There is a positive relationship, but is it statistically significant?"
* Perform Kendall correlation analysis (non-parametric test since `MntGoldProducts` is not normally distributed and contains outliers):\n    - Findings: There is significant positive correlation between `MntGoldProds` and `NumStorePurchases`
"### Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do ""Married PhD candidates"" have a significant relation with amount spent on fish? \n\n* We will compare `MntFishProducts` between Married PhD candidates and all other customers:\n    - Findings: Married PhD candidates spend significantly less on fish products compared to other customers."
"### What other factors are significantly related to amount spent on fish?\n\n* Like with the analysis of `NumStorePurchases` above, we will use use a linear regression model with `MntFishProducts` as the target variable, and then use machine learning explainability techniques to get insights about which features predict the amount spent on fish\n* Begin by plotting the target variable:"
"* Fit linear regression model to training data (70% of dataset)\n* Evaluate predictions on test data (30% of dataset) using RMSE:\n    - Findings: The RMSE is exceedingly small compared to the median value of the target variable, indicating good model predictions"
> The next category that we'll be looking into is **benefits**!
" Inference: \n\nThis was the respondent's answer to the question, **'Does your employer provide mental health benefits?'.**\n\n* We see that around 38% of the respondents said that their employer provided them mental health benefits, whereas a significant number ( 32% ) of them didn't even know whether they were provided this benefit.\n* Coming to the second graph, we see that for the people who **YES** said to mental health benefits, around 63% of them said that they were seeking medical help. \n* Surprisingly, the people who said **NO** for the mental health benefits provided by the company, close to 45% of them who want to seek mental health treatment.   "
"This was the respondent's answer to the question, **'Do you know the options for mental health care your employer provides?'.**\n\nSince this graph is more or less similar to the benefits one, we won't be discussing it in more detail. \n\n> Moving forward, the next category is wellness program. Lets try understanding that! \n"
" Inference: \n\nThis is the respondents answer to the question, **'Has your employer ever discussed mental health as part of an employee wellness program?'.**\n\n* About 19% of the repondents say **YES** about becoming a part of the employee wellness program and out of those 60% of employee want to get treatment. \n* One shocking revealation is that more than 65% of respondents say that there aren't any wellness programs provided by their company. But close to half of those respondents want to get treatment, which means the company needs to fulfil its duty and provide it soon. \n \n> The next category is **seek_help**, we will be leaving it as it is more or less similar to care_options, benefits and wellness_program. Our next category is anonymity."
" Inference: \n\nThis is the respondent's answer to the question, '**Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources?**'.\n\n* Around 65% of the people were not aware whether anonymity was provided to them and 30% said yes to the provision of anonymity by the company.\n* Looking at the second graph, we see that out of the people who answered yes to the provision of anonymity, around 60% of them were seeking help regarding their mental condition. Possible reasoning for this may be that the employee feels that the company has protected his/her privacy and can be trusted with knowing the mental health condition of it's workers. The most basic reason behind hiding this from the fellow workers can be the social stigma attached to mental health.\n\n> The next factor that we will be discussing is '**leave.**'\n"
"This is the respondent's answer to the question, '**How easy is it for you to take medical leave for a mental health condition?**'\n\n* While close to 50% of the people answered that they did not know about it, suprisingly around 45% of those people sought help for their condition.\n* A small percent of people ( around 8% ) said that it was very difficult for them to get leave for mental health and out of those, 75% of them sought for help.\n* Employees who said it was 'somewhat easy' or 'very easy' to get leave had almost 50% people seeking medical help.\n\n> The next category that we'd be looking into is **mental health consequence.**\n\n\n"
" Inference: \n\nThis is the respondent's answer to the question, '**Do you think that discussing a mental health issue with your employer would have negative consequences?**'.\n\n* Around same number of people ( around 40% each ) answered **Maybe** as well as **No** for the negative impact of discussing mental health consequences with the employer and about 23% said **Yes** to it.\n* 23% is a significant number who feel that discussing their mental health might create a negative impact on their employer. This may be because of the stigma, decreased productivity, impact on promotions or any other preconcieved notion.\n* It is nice to know that out of the people who answered No, there were only around 40% of the people who actually sought after help, whereas in both the other categories, it is more than 50%.\n\n> The next factor that we are going to discuss is **physical health consequence.** It will be interesting to compare both of these two together."
" Inference: \n\nThis is the respondent's answer to the question, '**Do you think that discussing a physical health issue with your employer would have negative consequences?**'\n\n* There is a starking difference between the reponses for the same question regarding mental and physical health. More than 70% of the employees believe that their physical health does not create a negative impact on their employer and only 5% of them believes that it does. \n* While it maybe incorrect for us to draw any conclusions about whether they seek mental help on the basis of their physical condition, because it is more or less same for all the three categories, we must keep in mind about **how differently mental and physical health are treated as a whole.**"
"This is the respondent's answer to the question, '**Would you be willing to discuss a mental health issue with your coworkers?**'\n\n* Around 62% of the employees said that they might be comfortable discussing some type of mental problems with their coworkers, and out of them around 50% actually sought for medical help.\n* 20% of the employees believed that discussing mental health with their coworkers wasn't a good option for them.\n\n> The next category is **supervisor.** Lets find out whether the employees are comfortable sharing their mental health with their supervisor."
" Inference: \n\nThis is the respondent's answer to the question, **'Would you be willing to discuss a mental health issue with your direct supervisor(s)?'**.\n\n* This graph is quite different from the one of the coworker. Here, around 40% of the workers believe in sharing their mental health with their supervisors. This may have something to do with their performance etc.\n* Looking at the second graph, employees who actually sought for help regarding their mental health was more or less similar for all the three categories.\n\n> This has become really tiring now! Anyway, just 2-3 categories more left for analysis. Let's move forward with our next  variable, which is **'mental_health_interview'** "
"This is the respondent's answer to the question, '**Do you think that discussing a mental health issue with your employer would have negative consequences?**'.\n\n* As our intution might suggest us, 80% of the respondents believe that it is a good option to discuss your mental health with the future employer. This is actually a good thing! This might not have been the case 15 years ago.\n* While around 15% of the candidates seem confused about whether they should be discussing their mental conditions with the future employer or not, less than 5% think that it may not be a good option discussing it.\n\n> The next category is **physical_health_interview**. Let's see if there's any difference in the respondent's answer for this one with the previous one."
" Inference: \n\nThis is the respondent's answer to the question, **'Would you bring up a physical health issue with a potential employer in an interview?'**.\n\n* While a majority of the people are still dubious about discussing their physical health condition with the future employer, however, close to 17% believe that there is no issue in discussing their physical health conditions.\n* Around 50% of the people still remain confused about whether it is a good option to discuss their condition or not.\n\n> Coming to the last but one, **mental_vs_physical**. Let's see what insights can be drawn from this category!"
"This was the respondent's answer to the question, **'Do you feel that your employer takes mental health as seriously as physical health?'**\n\n* While close to 50% people said that they didn't know, the number of people who answered **Yes** as well as **No** were completely equal. \n* For the people who answered Yes as well as the ones who answered No, more than 505 of them sought after medical help for their mental health, whereas it was not the case for the one's belonging to the 'Don't know' category.\n\n> Coming to the last column, we have finally reached to **obs_consequence**. This definitely calls for a meme!\n\n![Alt_Text](https://www.awesomeinventions.com/wp-content/uploads/2016/02/Not-Tired1.jpg)"
" Inference: \n\nThis was the respondent's answer to the question, **'Have you heard of or observed negative consequences for coworkers with mental health conditions in your workplace?'**\n\n* Majority ( 85% ) of the people, answered **No** to this question. This is quite important to note  that IT being an organised sector, follows strict guidelines of employee satisfaction etc. Thus, we didn't come across any major issue regarding the employer behavior as such!\n\n> Anyway, I think we're done with the EDA. Let me know if we left something out and I will try top cover that as well!"
### Plots of relation to target for all numerical features
"**Conclusion from EDA on numerical columns:**\n\nWe see that for some features like 'OverallQual' there is a strong linear correlation (0.79) to the target.  \nFor other features like 'MSSubClass' the correlation is very weak.  \nFor this kernel I decided to use only those features for prediction that have a correlation larger than a threshold value to SalePrice.  \nThis threshold value can be choosen in the global settings : min_val_corr  \n\nWith the default threshold for min_val_corr = 0.4, these features are dropped in Part 2, Data Wrangling:  \n'Id', 'MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF',  'LowQualFinSF',  'BsmtFullBath', 'BsmtHalfBath', 'HalfBath',   \n'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'\n\nWe also see that the entries for some of the numerical columns are in fact categorical values.  \nFor example, the numbers for 'OverallQual' and 'MSSubClass' represent a certain group for that feature ( see data description txt)"
### 🎗️ Main Training Function\n\n> How does it work:\n
### 🎗️ Experimenting\n\n> 📌 **Note**: This cell below was ran locally on my ZBook Studio (as the Kaggle environment is too slow for all the data). I am sharing with you the logs that I've got :)
"# Step 7: Evaluate Predictions #\n\nBefore making your final predictions on the test set, it's a good idea to evaluate your model's predictions on the validation set. This can help you diagnose problems in training or suggest ways your model could be improved. We'll look at two common ways of validation: plotting the **confusion matrix** and **visual validation**."
## Confusion Matrix ##\n\nA [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) shows the actual class of an image tabulated against its predicted class. It is one of the best tools you have for evaluating the performance of a classifier.\n\nThe following cell does some processing on the validation data and then creates the matrix with the `confusion_matrix` function included in [`scikit-learn`](https://scikit-learn.org/stable/index.html).
#### Visualize frequency distribution of `RainTomorrow` variable
"#### Interpretation\n\n- The above univariate plot confirms our findings that -\n\n   - The `No` variable have 110316 entries, and\n   \n   - The `Yes` variable have 31877 entries.\n\n"
We can plot the bars horizontally as follows :
"### Findings of Univariate Analysis \n\n\n-	The number of unique values in `RainTomorrow` variable is 2.\n\n-	The two unique values are `No` and `Yes`.\n\n-	Out of the total number of `RainTomorrow` values, `No` appears 77.58% times and `Yes` appears 22.42% times.\n\n-	The univariate plot confirms our findings that –\n\n     -  The `No` variable have 110316 entries, and         \n     \n     - The `Yes` variable have 31877 entries.\n"
"# Score Predictor\n\nSo now when we have everything in place, we will now build the score predictor. The main function working under the hood will be the Similarity function, which will calculate the similarity between movies, and will find 10 most similar movies. These 10 movies will help in predicting the score for our desired movie. We will take the average of the scores of the similar movies and find the score for the desired movie.\n\nNow the similarity between the movies will depend on our newly created columns containing binary lists. We know that features like the director or the cast will play a very important role in the movie's success. We always assume that movies from David Fincher or Chris Nolan will fare very well. Also if they work with their favorite actors, who always fetch them success and also work on their favorite genres, then the chances of success are even higher. Using these phenomena, lets try building our score predictor."
#### Let's check our prediction for Godfather
"As you can see below, these examples are too rare and also the sales are low. I'm open your suggestions for these families. I won't do anything for now but, you would like to improve your model you can focus on that.\n\nBut still, I want to use that knowledge whether it is simple and I will create a new feature. It shows that the product family is active or not."
"We can catch the trends, seasonality and anomalies for families."
"We are working with the stores. Well, there are plenty of products in the stores and we need to know which product family sells much more? Let's make a barplot to see that.\n\nThe graph shows us GROCERY I and BEVERAGES are the top selling families."
#### Does onpromotion column cause a data leakage problem?\n\nIt is really a good question. The Data Leakage is one of the biggest problem when we will fit a model. There is a great discussion from [Nesterenko Marina](https://www.kaggle.com/nesterenkomarina) [@nesterenkomarina](https://www.kaggle.com/nesterenkomarina). You should look at it before fitting a model.\n\n- https://www.kaggle.com/c/store-sales-time-series-forecasting/discussion/277067
How different can stores be from each other? I couldn't find a major pattern among the stores actually. But I only looked at a single plot. There may be some latent patterns. 
"# 6. Holidays and Events\n\nWhat a mess! Probably, you are confused due to the holidays and events data. It contains a lot of information inside but, don't worry. You just need to take a breathe and think! It is a meta-data so you have to split it logically and make the data useful.\n\nWhat are our problems?\n- Some national holidays have been transferred.\n- There might be a few holidays in one day. When we merged all of data, number of rows might increase. We don't want duplicates.\n- What is the scope of holidays? It can be regional or national or local. You need to split them by the scope.\n- Work day issue\n- Some specific events\n- Creating new features etc.\n\n\nEnd of the section, they won't be a problem anymore!"
"# 9. ACF & PACF for each family\n\nThe lag features means,shifting a time serie forward one step or more than one. So, a lag feature can use in the model to improve it. However, how many lag features should be inside the model? For understanding that, we can use ACF and PACF. The PACF is very useful to decide which features should select.\n\nIn our problem, we have multiple time series and each time series have different pattern of course. You know that those time series consists of store-product family combinations and we have 54 stores and 33 product families. We can't examine all of them one by one. For this reason, I will look at average sales for each product but it will be store independent.\n\nIn addition, the test data contains 15 days for each family. We should be careful when selecting lag features. We can't create new lag features from 1 lag to 15 lag. It must be starting 16. "
"I decided to chose these lags 16, 20, 30, 45, 365, 730 from PACF. I don't know that they will help me to improve the model but especially, 365th and 730th lags may be helpful. If you compare 2016 and 2017 years for sales, you can see that they are highly correlated."
"Let's say that we want to show both value counts and the probability distribution (value_count / total).  \nWe'll need to generate 2 plots. We'll do that by using subplots.  \nIn our imports we imported subplots from plotly, we'll be using that module."
"Since the 2 distributions are the same but with different y-axis scaling, we can consolidate the 2 subplots into 1 by using the secondary_y attribute."
"A Density Heatmap is kinda like a 2D Histogram, where the x and y axis are dedicated to 2 variables and the ""z axis"" (or rather the color of the Heatmap cell) is dedicated to count/sum of occurences."
"We want to use the count aggregation on our data, so we'll have to create a new row for each passenger.  \nSo the first record will be mapped to 112 records, the second one to 118 records etc.  \nWe'll be flexing our pandas skills to do this."
"### Feature Importances for Tree-Based Models, top 10 values:"
"## Predictions\n\nNow that we have tried different preprocessing and modeling techniques, resulting in a final best pipeline, let's use it to predict the test data provided by kaggle.\n\n**Remember:** All transformations that were done in the training dataset must be done in the test set."
### ***Following integrated development environments (IDE's) do Data Analysts use on a regular basis?:***
- Most of the job roles are using VScode and Jupyter except for developer advocate they are using Vscode as well as not using any IDE in parallel.***
### ***Top 10 Libraries Used For Visualization By Data Analysts:***
- Most of the analystsb roles are using Matplotlib and Seaborn except for Statitician they are using ggplot.\n- So here we need to learn matplot or seaborn for detailed or impressive visualization of data .
### ***Which of the following machine learning frameworks Data Analysts use on a regular basis?***
- Scikit learn is mostly used all job roles. But MLop use tensorflow keras and Sklearn simultanously more than anyone else.***
### ***Which of the following ML algorithms Data Analyst use on a regular basis?***
\nMost commanly use of ML algorithms by data analysts are linear/ logistic or decision tree algorothms. 
### ***Which categories of computer vision methods does Data Analysts use on a regular basis?***
- According to graph mostly data analysts does not need of computer vision but if in any case of analyzation image classification or general purpose network are on the top position.***
### ***Which of the following natural language processing (NLP) methods Data Analysts use on a regular basis?***
- NLP represents unstructed data or this language word embedding or word to vectors is often use.***
### ***Business Intelligence Tools Used By Data Analysts:***\n
***Data Architect and Data Analyst are using BI tools more than any one else. and this graph shows microsoft power Bi tableau on top rated.***
### ***Which of the following ML model hubs/repositories do Data Analysts use most often?***
- Kaggle Data_sets are mostly used by data analysts
"### ***Do Data analysts use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
- Most commanly use of data base in the role of data analysts is My SQL data base then came others databases.
### ***Do Data Analysts use any of the following automated machine learning tools?***
- Acording to this graph max data analysts are not preffered to use automated machine learning tools but if they are consider this then the top name of automated tool of google cloud autoML.
### ***Favorite Media Sources for Data_analysts***
- Data Analysts job roles are using Youtube and Kaggle and blogs except for data administrtatpr and managers they don't use any media source.
"# Part 2. Visual data analysis\n\n## 2.1. Correlation matrix visualization\n\nTo understand the features better, you can create a matrix of the correlation coefficients between the features. Use the initial dataset (non-filtered).\n\n### Task:\n\nPlot a correlation matrix using [`heatmap()`](http://seaborn.pydata.org/generated/seaborn.heatmap.html). You can create the matrix using the standard `pandas` tools with the default parameters.\n\n### Solution:"
"**Question 2.1. (1 point).** Which pair of features has the strongest Pearson's correlation with the *gender* feature?\n\n1. Cardio, Cholesterol\n2. Height, Smoke\n3. Smoke, Alco\n4. Height, Weight"
#### Group 1 : Customer Information : \n#### gender | SeniorCitizen | Partner | Dependents |
"- Customer churning for **male** & **female** customers is very similar to each other!\n- Similarly, number of **SeniorCitizen** customers is pretty low! Out of that, we can observe a near about 40% churn of **SeniorCitizen** customers. It accounts for a total of 476 customers out of 1142 **Senior Citizen** customers.\n- Customers who are housing with a **Partner** churned less as compared to those not living with a **Partner**.\n- Similary, churning is high for the customers that don't have **Dependents** with them!"
"#### Group 2: Services Subscribed by the Customer :\n\n- **For visualization purposes, we will create 2 groups!**\n\n#### PhoneService | MultipleLines | InternetService | StreamingTV | StreamingMovies |"
"- For **PhoneService**, despite having no phone service, more customers were retained as compared to the number of customers who dropped the services.\n- In case of **MultipleLines**, churn rate in when the **Multiplelines** are present or not is the same. \n- A high number of customers have displayed their resistance towards the use of **Fiber optic** cables for providing the **InternetService**. On the contrary, from the above graph, customers prefer using **DSL** for their **InternetService**!\n- **StreamingTV** and **StreamingMovies** display an identical graph. Irrespective of being subscribed to **StreamingTV** & **StreamingMovies**, a lot of customers have been churned. Looks like the streaming content was not entirely at fault!"
#### Group 2: Services Subscribed by the Customer : \n#### OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport |
"- When it comes down to catering the customers, services w.r.t **OnlineSecurity**, **OnlineBackup**, **DeviceProtection** & **TechSupport** are crucial from the above visualizations! \n- A high number of customers have switched their service provider when it comes down poor services with the above mentioned features."
#### Group 3 : Contract | PaperlessBilling | PaymentMethod |
"- Customer churning for a **Month-to-Month** based **Contract** is quite high. This is probably because the customers are testing out the varied services available to them and hence, in order to save money, 1 month service is tested out!\n- Another reason can be the overall experience with the internet service, streaming service and phone service were not consistent. Every customer has a different priority and hence if one of the 3 was upto par, the entire service was cutoff!\n- **PaperlessBilling** displays a high number of customers being churned out. This is probably because of some payment issue or receipt issues.\n- Customers clearly resented the **Electronic check** **PaymentMethod**. Out of the **2365** number of bills paid using **Electronic check**, a staggering 1071 customers exited the pool of service due to this payment method. Company definitely needs to either drop **Electronic check** method or make it hassle-free and user-friendly."
### Numerical Features :\n\n#### Distribution of Numerical Features :
- **tenure** and **MonthlyCharges** kind of create a **bimodal distribution** with peaks present at **0 - 70** and **20 - 80** respectively.\n- **TotalCharges** displays a **positively or rightly skewed distribution**.
Another plot we can make is the passenger count distribution colored by the fare bin.
"There does not appear to be much difference between the number of passengers. To get a more accurate picture, we can calculate the actual stats."
"5.3 CREATE A LEARNING RATE SCHEDULER\n\n---\n\nAdapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time. Sometimes this is called learning rate annealing or adaptive learning rates. Here we will call this approach a learning rate schedule. See [**this article**](https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/) for a basic tutorial on learning rade schedules.\n\n\nWe will utilize a basic step function following a warmup phase. Warmup is commonly used in learning rate schedule where we start training a model with a much smaller learning rate and increase it during the first few epochs/steps until the initial learning rate is used.\n\nIntuitively, this method will allow a model to adjust itself less before it becomes more familiar with the dataset. This usually prevents breaking pretrained weights. For adaptive optimisers like Adam, warmup also allows the optimizers to compute bettere statistics of the gradients."
5.4 WRAP THE CONFIGURATION DETAILS IN A CLASS OBJECT FOR EASY ACCESS\n\n---\n
6.4 CUSTOM TRAIN LOOP\n\n---\n\nINFORMATION
6.5 JUST-IN-CASE SAVE\n\n---\n\nINFORMATION
6.6 VIEW PREDICTIONS & DISTRIBUTION OF LEVENSHTEIN DISTANCE FOR VAL DATASET\n\n---\n\nINFORMATION
\n\n\n\n\n\n7  INFER ON TEST DATA    ⤒\n\nIn this section we will use our trained model to generate the predictions we will use to submit to the competition
"To check that our dataset pipeline is working, let's plot some of the images and masks as an overaly."
"As for the loss, we will use a symmetric version of [Lovasz](https://arxiv.org/pdf/1705.08790.pdf) (more on this loss in the final section): "
"One other question to ask is: how to move from **binary semantic** segmentation to **multiclass segmentation**?\n\nThere are few things we need to change: \n\n\n- the metric computation of course.\n- the dataset processing, especially the mask.\n- the choice of the loss\n\n\nAs for the metric computation, let's assume it is IoU. A simple extension is to compute this \nmetric for each class then take the mean of everything, this gives the mIoU. \nOften, it is better to exclude the background class from the mIoU computation.\n\nAs for the loss, if the different classes are balanced, we can use categorical cross  entropy.\nIf the classes are unbalanced, we can use the focal loss for example.\n\n\nFinally, for the mask, we can encode each class as an integer from 0 to n-1 where n is the number of classes. For example, if we have 3 classes, let's say background, car, and pedestrian, we can have the following mapping: \n\n- 0 => background\n- 1 => car\n- 2 => pedestrian\n\nThe mask can look like this (we will reuse the same code from the first part but adapted): \n\n"
"Nice. That's enough with multiclass segmentation, let's move to metrics libraries."
"In order to find an appropriate number of clusters, the elbow method will be used. In this method for this case, the inertia for a number of clusters between 2 and 10 will be calculated. The rule is to choose the number of clusters where you see a kink or ""an elbow"" in the graph."
"The graph above shows the reduction of a distortion score as the number of clusters increases. However, there is no clear ""elbow"" visible. The underlying algorithm suggests 5 clusters. A choice of 5 or 6 clusters seems to be fair. \n\nAnother way to choose the best number of clusters is to plot the silhuette score in a function of number of clusters. Let's see the results."
Silhouette score method indicates the best options would be 5 or 6 clusters. Let's compare both.
"The biggest cluster is a cluster number 1 with 79 observations (""medium-medium"" clients). There are two the smallest ones each containing 23 observations (cluster 3 ""high-high"" and cluster 0 ""low-high"" clients). Below there is a 3D projection of 5 generated clusters. It is not very helpful in terms of a visualisation in a static mode but if you run the code in an interactive environment (e.g. Spyder) you can rotate it!"
Below a Plotly version:
To check the quality of each cluster we can examine the Silhuette plot.
**6 CLUSTERS**
### Feature Ranking via the Random Forest \n\nThe Random Forest classifier in Sklearn also contains a very convenient  attribute **feature_importances_** which tells us which features within our dataset has been given most importance through the Random Forest algorithm. Shown below is an Interactive Plotly diagram of the various feature importances.
### Visualising Tree Diagram with Graphviz\n\nLet us now visualise how a single decision tree traverses the features in our data as the DecisionTreeClassifier object of sklearn comes with a very convenient **export_graphviz** method that exports the tree diagram into a .png format which you can view from the output of this kernel.
"### Feature Ranking via the Gradient Boosting Model\n\nMuch like the Random Forest, we can invoke the feature_importances_ attribute of the gradient boosting model and dump it in an interactive Plotly chart"
"### CONCLUSION\n\nWe have constructed a very simple pipeline of predicting employee attrition, from some basic Exploratory Data Analysis to feature engineering as well as implementing two learning models in the form of a Random Forest and a Gradient Boosting classifier. This whole notebook takes less than a minute to run and it even returns a 89% accuracy in its predictions.\n\nThat being said, there is quite a lot of room for improvement. For one, more features could be engineered from the data.  Furthermore one could squeeze performance out of this pipeline by perhaps using some form of blending or stacking of models. I myself am quite keen to implement a classifier voting where a handful of classifiers votes on the outcome of the predictions and we take the majority vote. "
Number of faults in the system according to their Fault_Type\n
  \n\n
"\n    \nVoltage or Current graph, where there is large fluctuation in the graph, there faults have occurred\n    \n"
 Separating Faults into different Categories
  
2. Faulty System with Line A to Ground Fault\n
"### Check the distribution of variables\n\n\nNow, I will plot the histograms to check variable distributions to find out if they are normal or skewed. "
We can see that all the variables in the dataset are positively skewed. 
"### Interpretation \n\n- The correlation coefficient ranges from -1 to +1. \n\n- When it is close to +1, this signifies that there is a strong positive correlation. So, we can see that there is a strong positive correlation between `Class` and `Bare_Nuclei`, `Class` and `Uniformity_Cell_Shape`, `Class` and `Uniformity_Cell_Size`.\n\n- When it is clsoe to -1, it means that there is a strong negative correlation. When it is close to 0, it means that there is no correlation. \n\n- We can see that all the variables are positively correlated with `Class` variable. Some variables are strongly positive correlated while some variables are negatively correlated."
"There's a very important fact. Recordings come from very different sources. As far as I can tell, some of them can come from mobile GSM channel.\n\nNevertheless,** it is extremely important to split the dataset in a way that one speaker doesn't occur in both train and test sets.**\nJust take a look and listen to this two examlpes:"
Even better to listen:
There are also recordings with some weird silence (some compression?):\n
"It means, that we have to prevent overfitting to the very specific acoustical environments.\n"
Let's plot mean FFT for every word
"## 2.5. Gaussian Mixtures modeling\n \n\nWe can see that mean FFT looks different for every word. We could model each FFT with a mixture of Gaussian distributions. Some of them however, look almost identical on FFT, like *stop* and *up*... But wait, they are still distinguishable when we look at spectrograms! High frequencies are earlier than low at the beginning of *stop* (probably *s*).\n\nThat's why temporal component is also necessary. There is a [Kaldi](http://kaldi-asr.org/) library, that can model words (or smaller parts of words) with GMMs and model temporal dependencies with [Hidden Markov Models](https://github.com/danijel3/ASRDemos/blob/master/notebooks/HMM_FST.ipynb).\n\nWe could use simple GMMs for words to check what can we model and how hard it is to distinguish the words. We can use [Scikit-learn](http://scikit-learn.org/) for that, however it is not straightforward and lasts very long here, so I abandon this idea for now."
## 2.7. Anomaly detection\n \n\nWe should check if there are any recordings that somehow stand out from the rest. We can lower the dimensionality of the dataset and interactively check for any anomaly.\nWe'll use PCA for dimensionality reduction:
"Notice that there are *yes/e4b02540_nohash_0.wav*, *go/0487ba9b_nohash_0.wav* and more points, that lie far away from the rest. Let's listen to them."
## Unigrams
## Bigrams
"# Wordcloud\n\n## Again most common words in more visual format, not giving us extra feature but looks cool right? :)"
# Getting Sentence Embeddings\n\n### We're given small number of training instances. With huge models especially neural networks we're always in risk of overfitting. For this and baseline purposes I implemented a simple sentence embedding model based on fine tuned roberta. After getting embedding matrix we are going to train that using Bayesian Ridge Regression...
### Non-English European 
"We can see that German and English are the most common European languages to feature in the dataset, although Spanish and Greek are not far behind."
"This plot shows that middle-eastern languages such as Arabic and Persian are represented more than languages from the Indian subcontinent or south-east Asia, such as Hindi, Vietnamese, and Indonesian. There is not a single comment in Mandarin, Korean, or Japanese!"
We can see that Africa is not as well represented as the other continents in the dataset. The two most common African languages in the dataset are Afrikaans and Somali.
### Distribution of comment words
"From the plot above, we can see that the distribution of comment words has a strong rightward (positive) skew with maximum probability denisty occuring at around 13 words. As the number of words increases beyond 13, the frequency reduces sharply."
### Average comment words vs. Language
"I have plotted the average comment words in each language above. Certain languages tend to have more words on average than other languages. For example, comments written in Akan, Persian, and Sinhala have more than 300 words on average! This may be due to the small number of samples in these languages and presence of one or two outliers."
### Average comment length vs. Country
"In the world plot above, we can see that certain regions in the world tend to have a higher average comment length comment length than other countries. Persian, Arabic, and Hindi comments all have at least 100 words on average! Most long-comment languages seem to originate in Asia."
"### Negative sentiment\n\nNegative sentiment refers to negative or pessimistic emotions. It is a score between 0 and 1; the greater the score, the more negative the abstract is."
"From the above plot, we can see that negative sentiment has a strong rightward (positive) skew, indicating that negativity is usually on the lower side. This suggests that most comments are not toxic or negative. In fact, the most common negativity value is around 0.04. Virtually no comments have a negativity greater than 0.8."
### Negativity vs. Country
"In the world plot, we can see that the language with highest average negativity is Afrikaans. Also, languages from western Europe and south-east Asia tend to have higher toxicity than Hindi and Russian."
### Negativity vs. Toxicity
"I have plotted the distribution of negativity for toxic and non-toxic comments above. We can clearly see that toxic comments have a significantly greater negative sentiment than toxic comments (on average). The probability density of negativity peaks at around 0 for non-toxic comments, while the negativity for toxic comments are minimum at this point. This suggests that a comment is very likely to be non-toxic if it has a negativity of 0."
"### Positive sentiment\n\nPositive sentiment refers to positive or optimistic emotions. It is a score between 0 and 1; the greater the score, the more positive the abstract is."
"From the above plot, we can see that positive sentiment has a strong rightward (positive) skew, indicating that positivity is usually on the lower side. This suggests that most comments do not express positivity explicitly. In fact, the most common negativity value is around 0.08. Virtually no comments have a positivity greater than 0.8."
# Proportion of Kagglers for each Job Title\n\nFirst let's create a metric to enable us compare those two different datasets and also different countries. The following radar charts express the number of Kagglers and job listings proportionally to the total of each country. See the example below for better understanding.\n\n#### Example Country\nData Scientists = 30 answers (or job listings if Glassdoor)\nDevelopers = 90 answers \nTotal = 120 answers\n\n**We will be plotting:**\nData Scientists = 25%\nDevelopers = 75%\n\n\n  Warning! Having a higher value doesn't mean a greater absolute value of answers or jobs listings. But a higher proportion when compared to the rest in the same country.\n
# USA vs China
"Most countries follows the same proportion of Kagglers as United States, with proportionally more data scientists than other professions. And it does make sense because Kaggle is focused on Data Science. But we do have some outliers. Starting with China, where software engineers and Data Engineers are massively present in Kaggle. **Are they trying to learn data science and change their careers? Or is it just a knowledge they want to add up to their current roles?**"
"Looking at Glassdoor jobs listings we see that China has much more jobs in Software Engineering than in Data Science. \n\n\n  Warning! If you live and China, are a software engineer and are thinking about migrating to Data Science, then reconsider that. You'll likely have a much harder time finding a job in this new career.\n\n\n  USA wants data analysts! If you live in the United States and want to be a Data Scientist in a few years, then consider getting a job as data analyst. It will probably be much easier as there is proportionally more jobs in that area than in DS.  "
"Looking at salaries reported by Kagglers we see that China is paying better for Statisticians and Research Scientists when compared to other positions. However, unfortunately, there aren't many jobs available in this area as the previous chart shows. In USA the highest salaries are in Data Science, followed by Data Engineering and Software Engineering. And of course... USA pays more than any other country for almost all positions."
# Brazil vs India
"We see that Brazil and India have very similar proportions of Kagglers across all job titles. However Brazil has a higher proportion of Statistician and Research Scientists, it might be an indication that in Brazil there are more people doing DS in the academia than in India."
"Looking at jobs listings, we find out that Brazil has a similar shape to China: huge demand for Software Engineers. \n\n\n  Data Scientists from Brazil that are struggling to find a job! Consider learning software engineering and joining a company as junior developer.\n  \n\n  Kagglers from India! There is huge opportunities to start your career as Business Analyst.\n"
"Look at that! Salaries in Brazil are looking great for Data Engineering! Not so easy to get a DE job though (remember that Software Engineers are in much higher demand). In India, you'll better paid as Data Scientist. If you want to earn even more, then consider moving to another country."
# Europe
"The Netherlands have more professionals interested in Kaggle, consequently in Data Science. For other countries we see a lack of Data Engineers and Business Analysts. **Are they happy in their current positions?**"
"In Europe the jobs listings are almost equalised (almost same proportion for all titles) which means it won't make much difference, in terms of job offerings, if you wanna be a Data Scientist or a Business Analyst. But in terms of competition, there will be probably more candidates per listing for data science if we extrapolate the Kaggle survey to the real world.\n\n\n  Software Engineers from Europe! Might be easier for you to find a job in The Netherlands!\n  "
"Finally when we compare salaries in Europe we find out that they are at almost the same range (a little bit higher for UK, maybe because of stronger currency compared to Euro). And now we can tell that Business Analysts are indeed quite happy in Europe, they are earning on average even more than technical folks.\n\n\n  Software Engineers, Data Engineers and Data Scientists from UK and Germany! Stop coding now and become Business Analysts!\n\n  I'm joking. Don't do that. Or do."
"Thanks to https://www.kaggle.com/startupsci/titanic-data-science-solutions\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning, we can narrow down our choice of models to a few. These include:\n\n- Linear Regression, Logistic Regression\n- Naive Bayes \n- k-Nearest Neighbors algorithm\n- Perceptron\n- Support Vector Machines and Linear SVR\n- Stochastic Gradient Descent, GradientBoostingRegressor, RidgeCV, BaggingRegressor\n- Decision Tree Classifier, Random Forest, AdaBoostClassifier, XGBRegressor, LGBM, ExtraTreesRegressor \n- Gaussian Process Classification\n- MLPRegressor (Deep Learning)\n- Voting Classifier\n\nEach model is built using cross-validation (except LGBM). The parameters of the model are selected to ensure the maximum matching of accuracy on the training and validation data. A plot is being built for this purpose with [learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html?highlight=learning_curve#sklearn.model_selection.learning_curve) from sklearn library."
### 5.1 Linear Regression \n\n[Back to Table of Contents](#0.1)
\n\nDistribution Plots
## **Cell Type Distribution**
\n\nImage View
## **Training Images**
### Activation Function\n---\n* Activation functions is used for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the **inputs and response variable.** They introduce non-linear properties to our Network.\n* Their main purpose is to convert a ***input signal of a node in a A-NN to an output signal.** That **output signal** now is used as a **input in the next layer** in the stack.\n![](https://cdn-images-1.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png)
#### Generate Fake Data
#### Activation Function plot from data
Section : 2. Neural Network\n---
Visualize Training & Validation Metrics
Training Loss vs Validation Loss
Training Accuracy vs Validation Accuracy
![Upvote!](https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)
## **Data Source** 
## **Age** 
## **Sex** 
\n\nReferences
"**What if we want to observe all correlation between features?** Yes, you are right. The answer is heatmap that is old but powerful plot method."
"Well, finaly we are in the pub and lets choose our drinks at feature selection part while using heatmap(correlation matrix)."
"After drop correlated features, as it can be seen in below correlation matrix, there are no more correlated features. Actually, I know and you see there is correlation value 0.9 but lets see together what happen if we do not drop it."
"Well, we choose our features but **did we choose correctly ?** Lets use random forest and find accuracy according to chosen features."
"Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. \nNow lets see other feature selection methods to find better results."
"Best 5 feature to classify is that **area_mean, area_se, texture_mean, concavity_worst and concavity_mean**. So lets se what happens if we use only these best scored 5 feature."
"Accuracy is almost 96% and as it can be seen in confusion matrix, we make few wrong prediction. What we did up to now is that we choose features according to correlation matrix and according to selectkBest method. Although we use 5 features in selectkBest method accuracies look similar.\nNow lets see other feature selection methods to find better results."
"Finally, we find best 11 features that are **texture_mean, area_mean, concavity_mean, texture_se, area_se, concavity_se, symmetry_se, smoothness_worst, concavity_worst, symmetry_worst and fractal_dimension_worst** for best classification. Lets look at best accuracy with plot.\n"
"Lets look at what we did up to this point. Lets accept that guys this data is very easy to classification. However, our first purpose is actually not finding good accuracy. Our purpose is learning how to make **feature selection and understanding data.** Then last make our last feature selection method."
"# 8. | Model Implementation ⚙\n\n    👉 This section will implement various machine learning models as mentioned in Introduction section. In addition, explanation for each models will be provided.\n"
"## 8.1 | Logistic Regression\n\n    \n    Logistic regression is a statistical method that is used for building machine learning models where the dependent variable is dichotomous: i.e. binary. Logistic regression is used to describe data and the relationship between one dependent variable and one or more independent variables. The independent variables can be nominal, ordinal, or of interval type.\n    The name ""logistic regression"" is derived from the concept of the logistic function that it uses. The logistic function is also known as the sigmoid function. The value of this logistic function lies between zero and one.\n    \n    \n    🖼 Logistic Function by Simplilearn\n    \n    \n\n"
Let's see how scores for both Top20% and Bottom 80% are distributed in test data?
"We see on the above chart that the two classes are very well defined, and distinct from each other. This is confirmed when we plot the ROC curve.\n\n> In a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner (100% sensitivity, 100% specificity). Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test (Zweig & Campbell, 1993)."
\n## 4.2 WordCloud 🌟
"back to table of content\n\n# 5. Baseline Model and Comparison\n\nCurrently, we have the messages as lists of tokens (also known as lemmas) and now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with.\n\nWe'll do that in three steps using the bag-of-words model:\n\n1. Count how many times does a word occur in each message (Known as term frequency)\n2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n\nLet's begin the first step:\n\nEach vector will have as many dimensions as there are unique words in the SMS corpus. We will first use SciKit Learn's **CountVectorizer**. This model will convert a collection of text documents to a matrix of token counts.\n\nWe can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message.\n\n![vectorizer.png](attachment:fe0ac5b9-f924-4a7c-b8e3-306485322784.png)"
"So lag features let us fit curves to lag plots where each observation in a series is plotted against the previous observation. Let's build same plots, but with ***'lag'*** feature:"
"# 3.3 Some more statistics & visualizations\nIn this block we are going to explore data. Firstly, let's count for each category in each dataset, ***value_counts()***:"
Here we can see stats for **df_holidays**:
Here we count values for some columns of **df_stores**:
Let's plot **pie chart** for ***'family'*** of **df_train**:
"# 3.4 BoxPlot\nIn addition, we can build some **boxplots**: for **df_oil** & **df_trans**."
"# 3.6. Trend. Forecasting Trend\nWe'll use a function from the **statsmodels** library called **DeterministicProcess**. Using this function will help us avoid some tricky failure cases that can arise with time series and linear regression. The order argument refers to polynomial order: 1 for linear, 2 for quadratic, 3 for cubic, and so on."
"Here we can see **Linear Trend** & **Linear Trend Forecast** for **Transactions** (plots 1,2) and **Sales** (plots 3,4)."
"# 3.7 Seasonality\nTime series exhibits **seasonality** whenever there is a regular, periodic change in the mean of the series. Seasonal changes generally follow the clock and calendar -- repetitions over a day, a week, or a year are common. Seasonality is often driven by the cycles of the natural world over days and years or by conventions of social behavior surrounding dates and times. Just like we used a moving average plot to discover the trend in a series, we can use a **seasonal plot** to discover seasonal patterns."
#### 4. Shifting
#### 5. Pitch
"- From the above types of augmentation techniques i am using noise, stretching(ie. changing speed) and some pitching."
"As a final check, we can get a rough measure of the differences between the clinical data and supplemental data by performing an adversarial validation. The goal with adversarial validation is to see if a classifier can tell the two datasets apart. We'll use ROC AUC score to inform us of differences. If the two sets appear to be very similar, the classifier will not be able to tell them apart, and thus will have an ROC AUC score of 0.5. If they are easy to tell apart - and thus are very dissimilar - then the ROC AUC score will approach 1. "
"With an AUC ROC score of 0.939, we can see that the classifier can easily tell the two datasets apart. This suggests that they are very dissimilar in nature, as was indicated as part of the competition. Caution should be used when mixing these two datasets, as they are very different in nature."
## 1.4.2 - Protein Data\n\nLet's take a look at the values we have for protein data.
Protein expression frequency values appear to have a wide range of values. We'll use a quick kernel density estimate to get an idea of where frequencies are clustered. We'll use a logarithmic scale due to the large values and potential variability involved in the expression frequencies.
"As we can see, there is a lot of variability regarding the actual protein expression frequencies. We'll look more into the distribution of various proteins and their association to the UPDRS scores in section 2 below. For now, the key observation we have is that normalized protein expression is highly variable, as indicated by the min, max, and standard deviation of the feature."
## 1.4.3 - Peptide Data\n\nLet's take a look at what we have for the peptide data.
"Again, we see a wide variation in the abundance of peptides. The min, max, and standard deviation tell us that peptide abundances will likely vary greatly depending on the particular peptide we are looking at. Again, we can plot kernel density estimates to give us an idea of where the bulk of our values exist."
"Once again, we'll look at peptide data - specifically peptide sequences and how they relate to UPDRS scores - in section 2 below."
"# 2 - Feature Exploration\n\nIn this section, we will examine each of the features we have to work with in more detail.\n\n# 2.1 - Visit Month\n\nThe visit month has an impact across all of the different datasets, and subsequently, through many different features that we have. Let's take a look at them in turn.\n\n# 2.1.1 - Visit Month vs UPDRS \n\nFor each visit month, we have observations about the target features - UPDRS scores. According to Holden et al (2018), the findings in each part of the UPDRS were dependent on whether or not the patient was taking medication. We should sub-divide the UPDRS score observations into groups that were taking medication, and those that were not. For the purposes of this exploration, a null value found in clinical data regarding medication state will be considered to be `Off`."
"Some general observations when OFF medication:\n    \n* There is a large amount of variance and outliers across each of the UPDRS parts and their respective visit months.\n* In general across UPDRS Parts 1 - 3, the trendline of score remains relatively flat.\n    * With UPDRS Part 4, we see a gradual increase in score."
"Some general observations when ON medication:\n    \n* There is a large amount of variance and outliers across each of the UPDRS parts and their respective visit months.\n* In general across UPDRS Parts 1, 2, and 4, the trendline of score remains relatively flat.\n    * With UPDRS Part 3, we see a gradual increase in score.\n\nAs mentioned by Holden et al (2018), the maximum score of the UPDRS is 272. In prior versions of the UPDRS, there was a linear progression of UPDRS score as time progressed. We should look at the sum total of the UPDRS scores to see if there is a score increase over time within this data."
"With the sum of all UPDRS scores while OFF medication, we see an upwards trend as the visit month increases, which indicates that overall, disease progression is occurring. While ON medication, the trendline remains relatively flat until months > 96, which sees an increase in overall score. Again, this indicates that disease progression is occurring. If we combine both ON and OFF medication status:"
"Overall, we see a trend of increasing UPDRS scores. This observation is important, as it suggests that our scores should likely see increases as time progresses, rather than decreases. This can be used as a post-processing check to ensure predictions being made by our machine learning algorithms make sense."
"# 2.1.2 - Visit Month vs Protein Data\n\nWithout diving too much into the actual protein data, we should check to see if there are general trends regarding the breakdown of protein data by month."
"Unsurprisingly, we see stable amounts of protein expressions across each month category. There is a large amount of variance regarding the protein expression frequencies, but overall, we're seeing the same mean total NPX values repeated across the months. This likely means that there will be differences in the actual proteins expressed, rather than their absolute numbers. \n\nWe can check to see if there are any significant increases or decreases in `UniProt` proteins across the months. Looking at all 227 proteins is going to be challenging. For this, we'll look at proteins that have significant increases or decreases across the months. We'll examine protein counts for all 227 proteins, and then pick out ones that appear to have very large standard deviations compared to their mean. For this EDA, we'll look at any protein expression data that has a standard deviation of more than 25% of the mean value."
"**4* Linear Regression with ElasticNet regularization (L1 and L2 penalty)**\n\nElasticNet is a compromise between Ridge and Lasso regression. It has a L1 penalty to generate sparsity and a L2 penalty to overcome some of the limitations of Lasso, such as the number of variables (Lasso can't select more features than it has observations, but it's not the case here anyway)."
"The optimal L1 ratio used by ElasticNet here is equal to 1, which means it is exactly equal to the Lasso regressor we used earlier (and had it been equal to 0, it would have been exactly equal to our Ridge regressor). The model didn't need any L2 regularization to overcome any potential L1 shortcoming."
\n\n##### 5.5| Regplot
\n\n##### 5.6| Heatmap
\n\n##### 5.7| Pandas crosstab
"### Robust Scaler\nWhen working with outliers we can use Robust Scaling for scakling our data,\nIt scales features using statistics that are robust to outliers. This method removes the median and scales the data in the range between 1st quartile and 3rd quartile. i.e., in between 25th quantile and 75th quantile range. This range is also called an Interquartile range. \nThe median and the interquartile range are then stored so that it could be used upon future data using the transform method. If outliers are present in the dataset, then the median and the interquartile range provide better results and outperform the sample mean and variance. \nRobustScaler uses the interquartile range so that it is robust to outliers"
"# 3. Handling Categorical Variables\n\nCategorical variables/features are any feature type can be classified into two major types:\n- Nominal\n- Ordinal\n\nNominal variables are variables that have two or more categories which do not have any kind of order associated with them. For example, if gender is classified into two groups, i.e. male and female, it can be considered as a nominal variable.Ordinal variables, on the other hand, have “levels” or categories with a particular order associated with them. For example, an ordinal categorical variable can be a feature with three different levels: low, medium and high. Order is important.\n\nIt is a binary classification problem:\nthe target here is **not skewed** but we use the best metric for this binary classification problem which would be Area Under the ROC Curve (AUC). We can use precision and recall too, but AUC combines these two metrics. Thus, we will be using AUC to evaluate the model that we build on this dataset.\n\nWe have to know that computers do not understand text data and thus, we need to convert these categories to numbers. A simple way of doing that can be to use :\n- Label Encoding\n```python\nfrom sklearn.preprocessing import LabelEncoder\n```\n- One Hot Encoding\n```python\npd.get_dummies()\n```\n\nbut we need to understand where to use which type of label encoding:\n\n**For not Tree based Machine Learning Algorithms the best way to go will be to use One-Hot Encoding**\n- One-Hot-Encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. \n- The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality. In these cases, I typically employ one-hot-encoding followed by PCA for dimensionality reduction. I find that the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes. PCA finds the linear overlap, so will naturally tend to group similar features into the same feature\n\n**For Tree based Machine Learning Algorithms the best way to go is with Label Encoding**\n\n- LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat. Still there are algorithms like decision trees and random forests that can work with categorical variables just fine and LabelEncoder can be used to store values using less disk space."
"\n## Title Extractor\n\nThis function extracts prefixes from passenger names and groups them in more generalized titles. We know male survival is low but we also know women and children first, right? So we can extract 'Master' title which is given for young males and their survival ratio might differ.\n\n### [Back To Table of Contents](#toc_section)"
"\n## Title Encoder\n\nThis function is for getting dummy variables for titles, might or might not use it.\n\n### [Back To Table of Contents](#toc_section)"
"\n## Scaler\n\nAnother function for getting rid of outliers in Fare values with using boxcox. I wasn't sure about if I use this or binning, but model did little better at the end with binning.\n\n### [Back To Table of Contents](#toc_section)"
\n## Embarked Processor\n\nA function for filling missing values and then hot encoding for each value.\n\n### [Back To Table of Contents](#toc_section)
\n## Ticket Cleaner\n\nThis function extract prefixes from ticket and assign them to specific group based on their prefix. Again I found this approach adding complexity only.\n\n### [Back To Table of Contents](#toc_section)
"\n## Housekeeping\n\nThis function is for cleaning the redundant features after extracting useful information from them. The cabin, passengerId, last name, ticket and ofc our target survived is going to be dropped. These features did their job on feature engineering and we can leave them in peace now.\n\n### [Back To Table of Contents](#toc_section)"
"\n## Correlation Matrix\n\nAlright one last table I'm going to check is correlation matrix. So we can spot linear relations between features. Especially the ones which effects survival rate. It seems Sex, the Mr. title and family survival ratio is most related features to survival.\n\nWell... That's it then let's continue with modelling now!\n\n### [Back To Table of Contents](#toc_section)"
\n# Modelling\n\nSince preprocessing done we are ready for training our models. We start with loading packages and splitting our transformed data so we have 22 features and and 891 observations to train our estimators. Our test set has 418 observations to make predictions.\n\n### [Back To Table of Contents](#toc_section)
##### Scatter plot\n\nThe *scatter plot* displays values of two numerical variables as *Cartesian coordinates* in 2D space. Scatter plots in 3D are also possible.\n\nLet's try out the function [`scatter()`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.scatter.html) from the `matplotlib` library:
"We get an uninteresting picture of two normally distributed variables. Also, it seems that these features are uncorrelated because the ellipse-like shape is aligned with the axes.\n\nThere is a slightly fancier option to create a scatter plot with the `seaborn` library:"
"The function [`jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) plots two histograms that may be useful in some cases.\n\nUsing the same function, we can also get a smoothed version of our bivariate distribution:"
"This is basically a bivariate version of the *Kernel Density Plot* discussed earlier.\n\n##### Scatterplot matrix\n\nIn some cases, we may want to plot a *scatterplot matrix* such as the one shown below. Its diagonal contains the distributions of the corresponding variables, and the scatter plots for each pair of variables fill the rest of the matrix."
"Sometimes, such a visualization may help draw conclusions about data; but, in this case, everything is pretty clear with no surprises.\n\n#### 3.2 Quantitative vs. Categorical\n\nIn this section, we will make our simple quantitative plots a little more exciting. We will try to gain new insights for churn prediction from the interactions between the numerical and categorical features.\n\nMore specifically, let's see how the input variables are related to the target variable Churn.\n\nPreviously, you learned about scatter plots. Additionally, their points can be color or size coded so that the values of a third categorical variable are also presented in the same figure. We can achieve this with the `scatter()` function seen above, but, let's try a new function called [`lmplot()`](https://seaborn.pydata.org/generated/seaborn.lmplot.html) and use the parameter `hue` to indicate our categorical feature of interest:"
"It seems that our small proportion of disloyal customers lean towards the top-right corner; that is, such customers tend to spend more time on the phone during both day and night. But this is not absolutely clear, and we won't make any definitive conclusions from this chart.\n\nNow, Let's create box plots to visualize the distribution statistics of the numerical variables in two disjoint groups: the loyal customers (`Churn=False`) and those who left (`Churn=True`)."
"From this chart, we can see that the greatest discrepancy in distribution between the two groups is for three variables: *Total day minutes*, *Customer service calls*, and *Number vmail messages*. Later in this course, we will learn how to determine feature importance in classification using *Random Forest* or *Gradient Boosting*; there, we will see that the first two features are indeed very important for churn prediction.\n\nLet's look at the distribution of day minutes spoken for the loyal and disloyal customers separately. We will create box and violin plots for *Total day minutes* grouped by the target variable."
"In this case, the violin plot does not contribute any additional information about our data as everything is clear from the box plot alone: disloyal customers tend to talk on the phone more.\n\n**An interesting observation**: on average, customers that discontinue their contracts are more active users of communication services. Perhaps they are unhappy with the tariffs, so a possible measure to prevent churn could be a reduction in call rates. The company will need to undertake additional economic analysis to find out whether such measures would be beneficial.\n\nWhen we want to analyze a quantitative variable in two categorical dimensions at once, there is a suitable function for this in the `seaborn` library called [`catplot()`](https://seaborn.pydata.org/generated/seaborn.factorplot.html). For example, let's visualize the interaction between *Total day minutes* and two categorical variables in the same plot:"
"From this, we could conclude that, starting with 4 calls, *Total day minutes* may no longer be the main factor for customer churn. Perhaps, in addition to our previous guess about the tariffs, there are customers that are dissatisfied with the service due to other problems, which might lead to fewer number of day minutes spent on calls.\n\n#### 3.3 Categorical vs. Categorical\n\nAs we saw earlier in this article, the variable *Customer service calls* has few unique values and, thus, can be considered either numerical or ordinal. We have already seen its distribution with a *count plot*. Now, we are interested in the relationship between this ordinal feature and the target variable *Churn*.\n\nLet's look at the distribution of the number of calls to customer service, again using a *count plot*. This time, let's also pass the parameter `hue=Churn` that adds a categorical dimension to the plot:"
"**An observation**: the churn rate increases significantly after 4 or more calls to customer service.\n\nNow, let's look at the relationship between *Churn* and the binary features, *International plan* and *Voice mail plan*."
"**An observation**: when *International Plan* is enabled, the churn rate is much higher; the usage of the international plan by the customer is a strong feature. We do not observe the same effect with *Voice mail plan*.\n\n##### Contingency table\n\nIn addition to using graphical means for categorical analysis, there is a traditional tool from statistics: a *contingency table*, also called a *cross tabulation*. It shows a multivariate frequency distribution of categorical variables in tabular form. In particular, it allows us to see the distribution of one variable conditional on the other by looking along a column or row.\n\nLet's try to see how *Churn* is related to the categorical variable *State* by creating a cross tabulation:"
## Identity info as a function of time
"## Compare Numeric Features in Train and Test\nSimilar to above but for the transaction data, specific examples that look interesting."
"### Clustering for Segments\n#### K-Means Clustering\nThe K-means clustering belongs to the partition based\centroid based hard clustering family of algorithms, a family of algorithms where each sample in a dataset is assigned to exactly one cluster.\n\nBased on this Euclidean distance metric, we can describe the k-means algorithm as a simple optimization problem, an iterative approach for minimizing the within-cluster sum of squared errors (SSE), which is sometimes also called cluster inertia. So, the objective of K-Means clustering is to minimize total intra-cluster variance, or, the squared error function: \n![image](https://www.saedsayad.com/images/Clustering_kmeans_c.png)\n\nThe steps that happen in the K-means algorithm for partitioning the data are as given follows:\n1. The algorithm starts with random point initializations of the required number of centers. The “K” in K-means stands for the number of clusters.\n2. In the next step, each of the data point is assigned to the center closest to it. The distance metric used in K-means clustering is normal Euclidian distance.\n3. Once the data points are assigned, the centers are recalculated by averaging the dimensions of the points belonging to the cluster.\n4. The process is repeated with new centers until we reach a point where the assignments become stable. In this case, the algorithm terminates.\n\n##### K-means++\n- Place the initial centroids far away from each other via the k-means++ algorithm, which leads to better and more consistent results than the classic k-means.\n- To use k-means++ with scikit-learn's KMeans object, we just need to set the init parameter to k-means++ (the default setting) instead of random.\n\n#### The Elbow Method\n  \nUsing the elbow method to find the optimal number of clusters. The idea behind the elbow method is to identify the value of k where the distortion begins to increase most rapidly. If k increases, the distortion will decrease, because the samples will be closer to the centroids they are assigned to. \n\nThis method looks at the percentage of variance explained as a function of the number of clusters. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the ""elbow criterion"". This ""elbow"" cannot always be unambiguously identified.Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance."
"Note that by the Elbow method from a K equal to 3 we already observed low rates of gain in the decay of the distortions with the decrease of K reaching the limit of 10% with the K equal to 7. With this in mind, we will begin to evaluate the options more deeply with 3, and 7, starting with the silhouette analysis."
"#### Silhouette analysis on K-Means clustering\n\nSilhouette analysis can be used to study the separation distance between the resulting clusters, as a strategy to quantifying the quality of clustering via graphical tool to plot a measure of how tightly grouped the samples in the clusters are. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. \n\nIt can also be applied to clustering algorithms other than k-means\n\nSilhouette coefficients has a range of \[-1, 1\], it calculated by:\n1. Calculate the cluster cohesion a( i )as the average distance between a sample x( i )   and all other points in the same cluster.\n2. Calculate the cluster separation b( i ) from the next closest cluster as the average distance between the sample x( i ) and all samples in the nearest cluster.\n3. Calculate the silhouette s( i )  as the difference between cluster cohesion and separation divided by the greater of the two, as shown here:\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/3d80ab22fb291b347b2d9dc3cc7cd614f6b15479)\nWhich can be also written as:\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/ab5579a6c7150579af8a0d432b6630ba529376f0)\n\nWhere:\n- If near +1, it indicate that the sample is far away from the neighboring clusters. \n- a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. \n- If most objects have a high value, then the clustering configuration is appropriate. \n- If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n- A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters\n- Negative values indicate that those samples might have been assigned to the wrong cluster.\n\nThe silhouette plot can shows a bad K clusters pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. A good k clusters can found when all the plots are more or less of similar thickness and hence are of similar sizes.\n\nAlthough we have to keep in mind that in several cases and scenarios, sometimes we may have to drop the mathematical explanation given by the algorithm and look at the business relevance of the results obtained.\n\nLet's see below how our data perform for each K clusters groups (3, 5 and 7) in the silhouette score of each cluster, along with the center of each of the cluster discovered in the scatter plots, by amount_log vs recency_log and vs frequency_log."
"When we look at the results of the clustering process, we can infer some interesting insights:\n\n- First notice that all K clusters options is valid, because they don't have presence of clusters with below average silhouette scores. \n- In the other hand, all options had a some wide fluctuations in the size of the silhouette plots. \n\nSo, the best choice may lie on the option that gives us a simpler business explanation and at the same time target customers in focus groups with sizes closer to the desired. \n\n#### Clusters Center:\nLet's look at the cluster center values after returning them to normal values from the log and scaled version. "
"#### Clusters Insights:\n\nWith the plots and the center in the correct units, let's see some insights by each clusters groups:\n\n***In the three-cluster:***\n- The tree clusters appears have a good stark differences in the Monetary value of the customer, we will confirm this by a box plot.\n- Cluster 1 is the cluster of high value customer who shops frequently and is certainly an important segment for each business.\n- In the similar way we obtain customer groups with low and medium spends in clusters with labels 0 and 2, respectively.\n- Frequency and Recency correlate perfectly to the Monetary value based on the trend (High Monetary-Low Recency-High Frequency).\n\n***In the five-cluster:***\n- Note that clusters 0 and 1 are very similar to their cluster in the configuration with only 3 clusters.\n- The cluster 1 appears more robust on the affirmation of those who shop often and with high amount.\n- The cluster 2 are those who have a decent spend but are not as frequent as the cluster 1\n- The cluster 4 purchases medium amounts, with a relatively low frequency and not very recent\n- The cluster 3 makes low-cost purchases, with a relatively low frequency, but above 1, and made their last purchase more recently. This group of customers probably response to price discounts and can be subject to loyalty promotions to try increase the medium-ticket, strategy that can be better defined when we analyzing the market basket. \n- The silhouette score matrix says that the  five cluster segments are less optimal then the three cluster segments. \n\n***In the five-cluster:***\n- Definitely cluster 6 defines those who shop often and with high amount.\n- Clusters 1 and 5 show good spending and good frequency, only deferring in how recent were their last purchases, where 5 is older, which suggests an active action to sell to group 5 as soon as possible and another to 1 seeking to raise its frequency.\n- Cluster 0 presents the fourth best purchase and a reasonable frequency, but this is a long time without buying. This group should be sensible to promotions and activations, so that they do not get lost and make their next purchase.\n- Cluster 5 is similar to 0, but has made its purchases more recently and has a slightly better periodicity. Then actions must be taken to raise their frequency and reduce the chances of them migrating to cluster 0 by staying longer without purchasing products.\n\n#### Drill Down Clusters:\n\nTo further drill down on this point and find out the quality of these difference, we can label our data with the corresponding cluster label and then visualize these differences. The following code will extract the clustering label and attach it with our customer summary dataset."
"Once we have the labels assigned to each of the customers, our task is simple. Now we want to find out how the summary of customer in each group is varying. If we can visualize that information we will able to find out the differences in the clusters of customers and we can modify our strategy on the basis of those differences.\n\nThe following code leverages plotly and will take the cluster labels we got for each configurations clusters and create boxplots. Plotly enables us to interact with the plots to see the central tendency values in each boxplot in the notebook. Note that we want to avoid the extremely high outlier values of each group, as they will interfere in making a good observation around the central tendencies of each cluster. Since we have only positive values, we will restrict the data such that only data points which are less than 0.95th percentile of the cluster is used. This will give us good information about the majority of the users in that cluster segment.\n\nI've used these charts to review my previously stated insights, but follow the same for you to explore:"
### Correlation Between The Features
"Interpreting The Heatmap\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\nPOSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between SibSp and Parch i.e 0.41. So we can carry on with all features."
### Pairplots\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us.
"###  Model\nNow we are ready to train a model and predict the required solution. There are lot of predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\nLogistic Regression\n\nKNN\n\nSupport Vector Machines\n\nNaive Bayes classifier\n\nDecision Tree\n\nRandom Forrest\n\nLinear Discriminant Analysis\n\nAda Boost Classifier \n\nGradient Boosting Classifier\n\nAnd also compared above given classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure and plot accuracy based confusion matrix"
## LogisticRegression
## Random Forest Classifier 
## Support Vector Machines
## KNN Classifier
## Gaussian Naive Bayes
## AdaBoost
## Linear Discriminant Analysis
## Gradient Boosting Classifier
## Model evaluation\nWe can now rank our evaluation of all the models to choose the best one for our problem.
## The text version of scatter plot looks messy but you can zoom it for great results
Let's see what happens when we use a spaCy based bigram tokenizer for topic modelling
"- Initially, we will look for the bigger picture of the features : **AveragePrice**, **Total Volume**, **Total Bags**. Then we will proceed to visualize the features derived from the above features."
"- From the above graphs, we can say that **AveragePrice** vs **Total Volume** & **Total Bags** is negatively correlated or complement each other. \n- **AveragePrice** is inversely proportional to **Total Volume** & **Total Bags** whereas **Total Volume** displays a directly proportional relationship with **Total Bags**.\n- Assuming the graphs are superimposed, the crests & troughs of **AveragePrice** would overlap the troughs & crests of **Total Volume** & **Total Bags** respectively.\n- This relationship between **AveragePrice** vs **Total Volume** & **Total Bags** w.r.t **Date** can be associated with the **Law of Supply and Demand**.\n    - If the supply increases and demand stays the same, the price will go down. \n    - If the supply decreases and demand stays the same, the price will go up. \n    \n- **AveragePrice**, **Total Volume** and **Total Bags** displays an uptrend with **Date**."
"- Avocados with PLU (Product Lookup Code) **4046** and **4225** kind of display the same patterns w.r.t **Date**. Crests and troughs are very similar to each other. \n- **4770** type of avocado has a seen a decrease in it's demand as time progresses. It has encountered a sharp drop in demand during the later months of 2016 and has not recovered since then.  \n- Use of **bags** w.r.t avocados has definitely been on the up! All the **bags** display a rising graph w.r.t **Date**. From the values present on y-axis, customers prefer to use the **Small Bags**."
#### Total Volume vs region w.r.t year :
"- This graph is unable to provide significant insights about year on year difference in avocado **Total Volume** consumed in different cities due to overlapping of data points.\n- **region** has mixed elements with the names of the cities and division of the country based on cardinal direction & non-cardinal directions like : **[Midsouth, Northeast, SouthCentral, Southeast, West, TotalUS]**.\n- All the elements based on cardinal directions & non-cardinal directions display a huge spike in demand for avocado. This is probably because of the data collected for these elements is a combination of mulitple city data.\n- **California**, **Great Lakes**, **Los Angeles** and **Plains** are some of the cities that highlight heavy consumption of avocado through **Total Volume**. "
#### Total Volume vs region w.r.t type :
"- This graph is very similar to the graph above in terms of the positions of the spike. \n- Elements based on cardinal directions & non-cardinal directions display the same spikes with a clear cut preference towards **Conventional(0)** avocados that supports the information gained from other visualizations.\n- Cities can like **DallasFtWorth**, **Denver**, **Houston**, **New York**, **PhoenixTucson** and **San Francisco** can be added to the previous of **California**, **Great Lakes**, **Los Angeles** and **Plains** that showcase their preference towards **Conventional(0)** avocado."
#### Total Bags vs region w.r.t year :
"- Use of **bags** is highly correlated with **Total Volume**.\n- From the data point **TotalUS** on the y-axis of the graph, we can say that there is crystal clear rise in use of bags."
#### Total Bags vs region w.r.t type :
- This graph is also very similar to the graphs above. No new information was gained.
"\nCohort Analysis\n\nA cohort is a group of people sharing something in common, such as the sign-up date to an app, the month of the first purchase, geographical location, acquisition channel (organic users, coming from performance marketing, etc.) and so on. In Cohort Analysis, we track these groups of users over time, to identify some common patterns or behaviors."
\nCustomer Lifetime Value\n\nCustomer lifetime value is how much money a customer will bring your brand throughout their entire time as a paying customer.
\nFrequency of Repeat Transactions
\nCredits\n    \n- https://www.businessnewsdaily.com/15918-how-to-use-crm-analytics.html\n    \n- https://www.analyticsvidhya.com/blog/2021/06/cohort-analysis-using-python-for-beginners-a-hands-on-tutorial/\n    \n- https://benalexkeen.com/bg-nbd-model-for-customer-base-analysis-in-python/
Bin age feature into groups. This will be helpful for filling missing values like expenditure according to age.
**Expenditure**
Calculate total expenditure and identify passengers with no expenditure.
**Passenger group**
Extract passenger group and group size from PassengerId.
"We can't really use the Group feature in our models because it has too big of a cardinality (6217) and would explode the number of dimensions with one-hot encoding.\n\nThe Group size on the other hand should be a useful feature. In fact, we can compress the feature further by creating a 'Solo' column that tracks whether someone is travelling on their own or not. The figure on the right shows that group size=1 is less likely to be transported than group size>1."
**Cabin location**
"Extract deck, number and side from cabin feature."
"*This is interesting!* It appears that Cabin_number is grouped into chunks of 300 cabins. This means we can compress this feature into a categorical one, which indicates which chunk each passenger is in.\n\n*Other notes:* The cabin deck 'T' seems to be an outlier (there are only 5 samples)."
**Last name**
Calculate family size from last name.
# Missing values
"Missing values make up about 2% of the data, which is a relatively small amount. For the most part, they don't seem to be happening at the same time (except the features made from splitting Cabin and Name), but let's inspect closer."
"*Notes:*\n* Missing values are independent of the target and for the most part are isolated. \n* Even though only 2% of the data is missing, about 25% of all passengers have at least 1 missing value.\n* PassengerId is the only (original) feature to not have any missing values. \n\n\n*Insight:*\n* Since most of the missing values are isolated it makes sense to try to fill these in as opposed to just dropping rows.\n* If there is a relationship between PassengerId and other features we can fill missing values according to this column."
Most of the people feel that the title is fine. Around 1700 people feel that the title fits poorly with what they do.!\n\nLet us now look how many people write codes to analyze data of the respondents.
"Majority of the respondents who work, write codes to analyze the data. Let us now look at the top employer type to understand more about them."
"Most of the repondent's employers have a size of 10,000 or more employees. "
Most of the employers are from Technology industry followed by Academics.\n\nNow let us look at the the number of years the employer is using advanced analytics / datascience.
There seems to be sudden drop in 6 to 10 years.
"**Analysis on interesting questions asked to all participants:**\n\nIn this part, let us do some analysis on the interesting questions that has been asked to all the participants.\n\n**Do you currently consider yourself a Data Scientist?**\n\nWhat a question to begin with. Let us now see hoe people responded to that."
"Interestingly, higher number of people said ""no"" than ""yes"". I was expecting the other way around though.\n\nWe can now see how these numbers change based on Employment status."
"Interestingly again, people who are employed answered ""No"" for this question than ""Not Employed but looking for work"" people.\n\n**How did you first start your DS / ML training?**\n\nLet us see what is the response for this question."
Most people responded as online courses followed by university and self-taught. **There are quite a few people who started  through Kaggle competitions as well.**\n\n**Which ML tool or technology are you most excited to learn next year?**\n\nThis is a very interesting question and I would love to know the answer as well.\n\n
"Tensorflow seems to be a clear winner followed by Python and R. Big data technologies like Spark / MLlib, Hadoop are fast catching up as well.\n\n**Which ML method are you most excited to learn next year?:**\n\nHere is one more interesting question."
"In line with Tensorflow, Deep learning is the most popular method which people want to learn next year.!\n\n** Which language would you recommend to a new DS to learn first?:**\n\nOne more interesting question to start the language war."
Wow. Python is so tall followed by R. This also goes well with the previous two questions since tensorflow primarily used python and also many deep learning methods.\n\n**Most important way to prove knowledge of ML / DS:**\n\nOne more nice question and let us see what people think.\n
Experience from work turns out to be the most important one. Guess what came second? Kaggle competitions.!\n\n**Time spent on different aspects of DS projects during work:**\n\nIn this section let us see how people spend time on different aspects of Data science like\n\n* Gathering and cleaning data\n* Model building\n* Putting the work Production\n* Visualizing data\n* Finding insights in the data\n* Others\n\nSum of all the different aspects should sum up to 100 as per the instructions given during the survey.
"From the graphs, most part of the time is spent on cleaning / pre-processing the data. It is atleast inline with my experience as well. Looks like productioning the models take lesser time compared to others.\n\n**Percentage of current ML training categories:**\n\nIn this section, let us analyze the different catefories that contributes to the ML / DS training of a person. The sum of all the actegories should be 100. So we could get the percentage share of each of the different categories."
"""Self taught"" seems to be the most important contributor followed by ""Online courses"".\n\nAs a follow up of the previous plot, we can also look at how long people have been learning data science."
Almost 50% of the respondents mentioned that it is less than 1 year that they have started learning DS.\n\nNow let us also check how many hours people spend studying DS in a week.
"**Work Details Analysis:**\n\nIn this section, let us analyze the different work related details that has been asked as part of the survey. Let us start with the dataset size which people used to work with as part of their job."
A right skewed normal distribution with a peak at 1 GB.\n\n**Computing Hardware used in work:**
### Class Distribution
### Batch Image
## P Curve
## PR Curve
## F1 Curve
## R Curve
## 4.3.4. Code Example:
# > 4.5. Affinity Propagation clustering algorithm
## 4.5.1. Code Examples
# > 4.6. Mean-Shift clustering algorithm
## 4.6.3. Code Example
"# > 4.7. OPTICS algorithm\n**OPTICS Clustering stands for Ordering Points To Identify Cluster Structure.** It draws inspiration from the DBSCAN clustering algorithm. It adds two more terms to the concepts of DBSCAN clustering.\n\nThey are:-\n- **Core Distance:** It is the minimum value of radius required to classify a given point as a core point. If the given point is not a Core point, then it’s Core Distance is undefined.\n- **Reachability Distance:** It is defined with respect to another data point q(Let). The Reachability distance between a point p and q is the maximum of the Core Distance of p and the Euclidean Distance(or some other distance metric) between p and q. Note that The Reachability Distance is not defined if q is not a Core point.\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20190711114717/reachability_distance1.png)"
## 4.7.2. Code Example: 
# > 4.8. Agglomerative Hierarchy clustering algorithm\nThe agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It's also known as AGNES (Agglomerative Nesting). The algorithm starts by treating each object as a singleton cluster.
## 4.8.3. Code Example: 
# > 4.9. DIANA or Divisive Analysis
## 4.10.3. Code Example 
"# 5. Comperative Analysis of Algorithms\nCredit: [*Sunit Prasad, Different Types of Clustering Methods and Applications*](https://www.analytixlabs.co.in/blog/types-of-clustering-algorithms/)"
4.5 VISUALIZE & INVESTIGATE INDIVIDUAL CELLS\n\n---\n\nLet's review a single image and then look into the cells that make up that image\n
4.6 ADD BBOXES TO DATAFRAME\n\n---\n\nLet's iterate over the images and add the bounding box coordinates to the dataframe in order (matching the RLE)\n\n* We also add the 0-1 normalized version of the bboxes to the dataframe for later training purposes\n
4.7 STATS REGARDING BOUNDING BOXES\n\n---
"\n\n\n\n\n\n\n    5  MODELLING    ⤒\n\n\n---\n\nEfficientDET for Object Detection, Classification, and Segmentation"
5.3 INSTANIATE OUR DATALOADER\n\n---\n\nAugmentations are breaking the masks... so disablled for now
5.4 CREATE MODEL AND LOAD PRETRAINED WEIGHTS\n\n---\n\nCOCO weights
5.6 VALIDATE THE MODEL IS LEARNING\n\n---\n
5.7 IOU ON VALIDATION DATASET\n\n---\n
"We now define our training loop and evaluation loop functions. Much of the code is inspired by @abhishek's [kernel](https://www.kaggle.com/abhishek/bert-multi-lingual-tpu-training-8-cores). As you will see later, these functions are run on each of the 8 cores.\n\nTo get the loss of a batch, since the data is spread across the 8 cores, we have to _reduce_ the loss. Now the `loss_reduced` will be the same on all the cores (average of the 8 losses). `xm.master_print` can be used to print this value once from only one core.\n\nPyTorch XLA requires that the optimizer be stepped using their own function `xm.optimizer_step(optimizer)`."
"We finally define our main function (which itself calls the above functions) that will be spawned by PyTorch XLA multiprocessing. This function will be run on each of the 8 cores. There are several steps for 8-core training:\n1. We need to use a `DistributedSampler` that will appropriately distribute the dataset across the 8 cores.\n2. We are using `num_workers=0` as that decreases memory usage (only master process loading data). On higher memory VMs, this could be increased to speed up the training.\n3. The learning rate is scaled by the number of TPU cores (`xm.xrt_world_size()`)\n4. We put the model onto the TPU\n5. We use `ParallelLoader` which is a PyTorch XLA-specific DataLoader for loading data onto the TPU.\n6. We save the model at the end of training with `xm.model_save`"
That's it! Now we can submit the results of our XLM-R model!\n\n### Post-processing:\n\nLet's try @christofhenkel's post-processing trick from [here](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/160980)
"# Acknowledgments:\n- Based on data from [Abhishek's code](https://www.kaggle.com/abhishek/bert-multi-lingual-tpu-training-8-cores-w-valid)\n- Model based on [xhlulu's code](https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta)\n- Original attempt from [Aditya's code](https://www.kaggle.com/adityaecdrid/simple-xlmr-tpu-pytorch)\n- Discussion with Davide Libenzi and Daniel Sohn (PyTorch XLA team) - [code](https://www.kaggle.com/davidelibenzi/simple-xlmr-tpu-pytorch)\n- Fruitful discussions with Abhishek and Aditya\n\n\n# Fin\n\nIf you have any questions or suggestions, please drop a comment! :)"
#### Count of Observation by Game/Video title
#### Count by World
# 11. Simple Baseline \n Top\n\n
Here as well the distribution is very similar to the previous one. No wonder the correlation between the two variables are also high.\n\n**Bathroom Count:**
"There is an interesting 2.279 value in the bathroom count.\n\nEdit: As MihwaHan pointed in the comments, this is the mean value :)\n\nNow let us check how the log error changes based on this."
**Bedroom count:**
3.03 is the mean value with which we replaced the Null values.
**YearBuilt:**\n\nLet us explore how the error varies with the yearbuilt variable.
There is a minor incremental trend seen with respect to built year.\n\nNow let us see how the logerror varies with respect to latitude and longitude.
There are no visible pockets as such with respect to latitude or longitude atleast with the naked eye.\n\nLet us take the variables with highest positive correlation and highest negative correlation to see if we can see some visible patterns.
"There are no visible patterns here as well. So this is going to be a hard competition to predict I guess.\n\nJust for fun, we will let the machine form some arbitrary pattern for us :D"
Hurray.! Finally we got some nice pattern in the data :P\n\nWe had an understanding of important variables from the univariate analysis. But this is on a stand alone basis and also we have linearity assumption. Now let us build a non-linear model to get the important variables by building Extra Trees model.
"Seems ""tax amount"" is the most importanct variable followed by ""structure tax value dollar count"" and ""land tax value dollor count""\n\n"
"Using xgboost, the important variables are 'structured tax value dollar count' followed by 'latitude' and 'calculated finished square feet' "
"Judging by the plots, our model was able to successfully approximate the initial time series, capturing the daily seasonality, overall downwards trend, and even some anomalies. If you look at the model deviations, you can clearly see that the model reacts quite sharply to changes in the structure of the series but then quickly returns the deviation to the normal values, essentially ""forgetting"" the past. This feature of the model allows us to quickly build anomaly detection systems, even for noisy series data, without spending too much time and money on preparing the data and training the model."
"We'll apply the same algorithm for the second series which, as you may recall, has trend and a 30-day seasonality."
#### Here we will be taking the prefix values of the cabin number. The missing values will be replaced with 'X'.
#### These cabins will be mapped with a numeric value.
"We see clearly that happier countries tend to be **older, and less populous**. \n\nI've included Europe for reference.\n\nWhat about Fertility rate?"
"As I suspected, happier countries tend also to have **fewer children**. Likely this is due to access to contraception."
I am surprised that **population density doesn't effect happiness** - though that is due to personal preferences!
# Have there been changes over time?\n\nDo the unhappy get happier?\n\nIs this just a snapshot of a moment in time? Or are the trends more long-standing?
"Of concern here is that the **unhappy stay unhappy, and worse still, they appear to be getting unhappier.**\n\nIs this trend consistent? Or do some countries scores improve over time?"
"Let's explore changes over time a little more.\n\nAbove, I took a sample of a few countries. Let's plot their changes in a **slopegraph from 2007 to 2020** to see if we can learn anything"
Clearly there has indeed been a lot of movement through the years.\n\nWhich countries have seen the biggest changes?
Let's compare the biggest gainer & the biggest loser in terms of the happiness index scores: Bulgaria & Jordan.\n\nWe'll compare how they both got on over the years
"Whilst I explore this idea of temporal change, I want to look at a continental perspective.\n\nFor example, are all countries in Western Europe 'happy'?"
Europe though seems almost exclusively without worry. Is this true? Doubtful.\n\nLet's explore Europe a little more...
"so within Europe there are indeed countries that might be considered 'unhappy', such as North Cyprus and Greece.\n\nAs I noted above, the North-South split is a well known issue within Europe, and even within countries such as Italy.\n\nLet's explore the variance in happiness index scores across all continents now..."
"As I suspected, there is of course variance within every continent/region.\n\nHowever, highlighted above is the Middle East & North Africa because it has such a large spread in happiness scores.\n\nLet's repeat the plot from earlier on to see how each country in that region has scored over the years..."
"This region contains many countries that are in vastly different political realities - from war to peace, which of course will have a large effect on happiness scores.\n\nDespite the intercontinental variance, it is actually the case that there are only a few countries to have ever topped the leaderboard. \n\nIt does not seem like this will be changing any time soon either.\n"
"# Last steps: Clustering\n\nNow we have a clear understanding of our data.\n\nEarlier, I mentioned that there appeared to be **three clusters** in our dataset between regions. \n\nLet's formally cluster our data and see if the intuitions, **uncovered purely by exploratory data analysis**, prove correct."
"I'll use **K-Means** and  **the Elbow Method** to select the number of clusters.\n\nIn the Elbow method, we are actually varying the number of clusters (K) from 1 – 6 in this caase. \n\nFor each value of K, we are calculating WCSS (Within-Cluster Sum of Square). WCSS is the sum of squared distance between each point and the centroid in a cluster. When we plot the WCSS with the K value, the plot looks like an Elbow. As the number of clusters increases, the WCSS value will start to decrease. WCSS value is largest when K = 1. When we analyze the graph we can see that the graph will rapidly change at a point and thus creating an elbow shape. From this point, the graph starts to move almost parallel to the X-axis. The K value corresponding to this point is the optimal K value or an optimal number of clusters.\n\nA good read and source for above description:\n(https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/)"
"### 3.3 Exploratory visual analysis\n\nAs always, it may be helpful and instructive to look at a graphical representation of your data.\n\nWe will create a time series plot for the whole time range. Displaying data over such a long period of time can give clues about seasonality and conspicuous abnormal deviations.\n\nFirst, we import and initialize the `Plotly` library, which allows creating beautiful interactive plots:"
"We also define a helper function, which will plot our dataframes throughout the article:"
Let's try and plot our dataset *as is*:
"High-frequency data can be rather difficult to analyze. Even with the ability to zoom in provided by `Plotly`, it is hard to infer anything meaningful from this chart apart from the prominent upward and accelerating trend.\n\nTo reduce the noise, we will resample the post counts down to weekly bins. Besides *binning*, other possible techniques of noise reduction include [Moving-Average Smoothing](https://en.wikipedia.org/wiki/Moving_average) and [Exponential Smoothing](https://en.wikipedia.org/wiki/Exponential_smoothing), among others.\n\nWe save our downsampled dataframe in a separate variable because further in this practice we will work only with daily series:"
"Finally, we plot the result:"
"This downsampled chart proves to be somewhat better for an analyst's perception.\n\nOne of the most useful functions that `Plotly` provides is the ability to quickly dive into different periods of timeline in order to better understand the data and find visual clues about possbile trends, periodic and irregular effects. \n\nFor example, zooming-in on a couple of consecutive years shows us time points corresponding to Christmas holidays, which greatly influence human behaviors.\n\nNow, we're going to omit the first few years of observations, up to 2015. First, they won't contribute much into the forecast quality in 2017. Second, these first years, having very low number of posts per day, are likely to increase noise in our predictions, as the model would be forced to fit this abnormal historical data along with more relevant and indicative data from the recent years."
"In the resulting dataframe you can see many columns characterizing the prediction, including trend and seasonality components as well as their confidence intervals. The forecast itself is stored in the `yhat` column.\n\nThe Prophet library has its own built-in tools for visualization that enable us to quickly evaluate the result.\n\nFirst, there is a method called `Prophet.plot` that plots all the points from the forecast:"
"This chart doesn't look very informative. The only definitive conclusion that we can draw here is that the model treated many of the data points as outliers.\n\nThe second function `Prophet.plot_components` might be much more useful in our case. It allows us to observe different components of the model separately: trend, yearly and weekly seasonality. In addition, if you supply information about holidays and events to your model, they will also be shown in this plot.\n\nLet's try it out:"
\n#  6. Data Visualization \n
* 26.6 % of customers switched to another firm.\n* Customers are 49.5 % female and 50.5 % male.
* There is negligible difference in customer percentage/ count who chnaged the service provider. Both genders behaved in similar fashion when it comes to migrating to another service provider/firm.
* About 75% of customer with Month-to-Month Contract opted to move out as compared to 13% of customrs with One Year Contract and 3% with Two Year Contract
"* A lot of customers choose the Fiber optic service and it's also evident that the customers who use Fiber optic have high churn rate, this might suggest a dissatisfaction with this type of internet service.\n* Customers having DSL service are majority in number and have less churn rate compared to Fibre optic service."
* Customers without dependents are more likely to churn
* Customers that doesn't have partners are more likely to churn
* It can be observed that the fraction of senior citizen is very less.\n* Most of the senior citizens churn.
**Let's look at Number of siblings/spouses aboard**
"**Looks like single person Non-survived count is almost double than survived, while others have 50-50 % ratio**"
**Now Looking at Port of embarkation**
**Can't say much!**
**Look in to relationships among dataset**
**Configure the heatmap**
**Fare vs Embarked**
"- The wider fare distribution among passengers who embarked in Cherbourg. It makes scence - many first-class passengers boarded the ship here, but the share of third-class passengers is quite significant.\n- The smallest variation in the price of passengers who boarded in q. Also, the average price of these passengers is the smallest, I think this is due to the fact that the path was supposed to be the shortest + almost all third-class passengers."
**Fare vs Pclass**
"We can observe that the distribution of prices for the second and third class is very similar. The distribution of first-class prices is very different, has a larger spread, and on average prices are higher.\n\nLet's add colours to our points to indicate surviving status of passenger (there will be only data from training part of the dataset):"
Let's look at some maximum and minimum values of features!
one of the effectitve way to fill the null values is by finding correlation
"> **Pclass and age, as they had max relation in the entire set we are going to replace missing age values with median age calculated per class**"
 Experience level
🎈 Observations: Most of the staff have senior working experience
 Employment type
🎈 Observations: Almost all employees have Full-Time job
 Remote ratio
"🎈 Observations: For each year(2020,2021,2022), the count for work type-remote is high.The number of remote working is huge. The cause is most likely due to the impact of the COVID-19 epidemic, changing the working trend."
Employee residence
"🎈 Observations: Most of the empolyees(TOP 5) are from US, UK, India, Canada and Germany."
Company location
"🎈 Observations: Most of the countries(TOP 5) are located in US, UK, Canada, Germany and India."
Work year
🎈 Observations: This shows a positive trend and shows that data science jobs are becoming more valuable as the years pass.
 Salary in USD based on company location
"Experience Level, Salary, and Company Size"
"Remote Ratio, Salary, and Employment Type"
"Experience level, Salary, Job title"
## How much data is covered per cluster?
### Take-away\n\n* We can see that most of our clusters cover around  5 % of our data. \n* Nonetheless there exist some clusters with verly low or very high coverage.\n\nWe should try to find out which kind of products and feature ranges belong to these clusters. This way we should try to find out if our clusters are inhomogenous or if we should use more components for our mixture model.
## Can we reveal the hidden product type?
"### Take-away\n\n* First of all: YES! We can find hidden product categories. That's amazing! No one told Gaussian Mixture something about product names or types. Only dealing with nutrition table information it was able to group similar products. We can find various interesting clusters of chocolate, pasta, ice cream, cheese, yoghurts, juice, grains, sauces, meat, water and nuts.\n* On the goarse-grained view our clustering was very successful. But if you take a closer look you may wonder why some clusters hold similar products or seem to be of a mixed type like cluster 6.\n\nFor this reason, some more question arise:\n1. How sure is our model about our cluster assignments?\n2. How valid is the data in context of app user errors given a cluster?"
To obtain a first impression of the similarity of our clusters we can look at the correlation of the cluster center:
"### Take-away\n\n* Many clusters do not correlate or show anti-correlation. This is what we like to have as we want them do be dissimilar in the feature space. This way we can make sure that they can cover different type of products. \n* Unfortunately we also find some high correlating clusters that have nearby cluster centers like the pasta cluster 9 and the whole grain cluster 16. In this case it seems to make sense. Even though these kind of products are very similar, pasta seems to be different from whole grain. That's cool! But we should try to understand in the feature space what makes this difference."
## Finding reason for missing data using Dendrogram \nA dendogram is a tree diagram of missingness. It groups the highly correlated variables together.
">Let's read the above dendrogram from a top-down perspective: \n* Cluster leaves which linked together at a distance of zero fully predict one another's presence—one variable might always be empty when another is filled, or they might always both be filled or both empty, and so on(missingno documentation)\n>\n>![Screenshot%202020-04-25%20at%208.19.56%20AM.png](attachment:Screenshot%202020-04-25%20at%208.19.56%20AM.png)\n>\n>* the missingness of Embarked tends to be more similar to Age than to Cabin and so on.However, in this particluar case, the correlation is high since Embarked column has a very few missing values.\n\nThis dataset doesn't have much missing values but if you use the same methodology on datasets having a lot of missing values, some interesting pattern will definitely emerge."
"### Bathrooms Features\nIt's time to take a break and go to the toilet, to our luck there are 4 bathroom variables in our data set. FullBath has the largest correlation with SalePrice between than. The others individually, these features are not very important. \n"
"![image](http://www.danlanephotography.com/wp-content/uploads/20-funny-toilet-paper-holders-funny-toilet-paper-holders.jpg)\nHowever, I assume that I if I add them up into one predictor, this predictor is likely to become a strong one. A half-bath, also known as a powder room or guest bath, has only two of the four main bathroom components-typically a toilet and sink. Consequently, I will also count the half bathrooms as half."
"As we have seen, porch features have low correlation with price, and by the graphics we see all most has low bas and high variance, being a high risk to end complex models and fall into ouverfit.\n\n### Slope of property and Lot area\nEveryone knows that the size of the lot matters, but has anyone seen any ad talking about the slope?\n![image](https://www.abedward.com/wp-content/uploads/2015/10/upside-down-house1-1024x730.jpg)"
"It is interesting to note that the slope has a low correlation, but as an expected negative. On the other hand, the lot size does not present such a significant correlation, contrary to the interaction between these two characteristics, which is better and also allow us to identify some outliers. Let's take a look at the effect of removing the outliers."
### Neighborhood\nLet's watch how much the neighborhood may be influencing the price.\n![image](https://files.sharenator.com/634178883988989120_NeighborhoodWatch_FailsWins_and_Motis-s800x600-87267-1020.jpg)
"As we can see prices are affected by the neighborhood, yes, if more similar more they attract. But we will delve a little and see how the year and month of the sale also has great influence on the price variation and confirm the seasonality.\n![image](http://blogs.tallahassee.com/community/wp-content/uploads/2015/10/real-estate-seasonality-impact.gif)"
"### Test hypothesis of better feature: Construction Area\nLet's call a specialist to help us create a new feature that sum all area features, the construct area, and evaluates if is better than their parcels. \n![image](https://im.ziffdavisinternational.com/ign_fr/screenshot/default/simpsonblackboard_h9ga.jpg)"
"As we can see, our built metric performs better than its parcels, even more than the living area. Besides better correlation, it presents less bias and variance.\n\nThis may lead us to think of a model option that uses only the constructed area, without including any of the parcels, that would be replaced by an indication variable of existence or not if there is no categorical variable associated with it.\n\nWe can also use them to compose other variables and finally remove them.\n\nAnyway the **living area** seems ***useless*** now, to prove it let's go see how a single linear regressor perform with this options:"
\nCLUSTER ANALYSIS
The optimal number of clusters is 4.
"**How to Interpret:** \nAlthough these two heatmaps use the same features, they different in the relation in which the percentage is taken. For the first plot on the left, the percentages add up to 100% by **column**, while the plot on the right has is **normalized into percentages by row**.\n\n**Interpretation:** \nThe dominance of the *General* size is consistent across the various categories within **Department Name**. There a notable overall between *General Petite* and *Department Name*.\n\n***\n\n**Class Name by Department Name:**"
"**Interpretation:** \nHere we get a closer glimpse at the breakdown of specific clothing types. Up to now, the dominance of dress popularity has been evident, but not that of ""Knits"". This is a kind of thickly thread and colorful top item which I must confess I have not seen much of out in the real world."
"**Interpretation:**\nThis normalization of percentage by column and index explains how clothing types are distributed across departments. This provides a clear way to see which products are dominant within each category. Following up on knits, it appears that the runner up in the ""Tops"" category, ""Blouses"", is not that far behind.\n\n***\n**Division Name by Department Name:**"
"**Interpretation:** \nI think this plot wraps up the interplay between Blouses, Dresses, and Knits by showing that most reviews revolve around the normal sized version of the products. It is interesting to note that Dresses attract higher proportion of ""Petite"" sized customers.\n\n***\n### 3.2  Continuous Variable by Categorical Variable\n\nHere I want to look at the behavior of the continuous variables when sliced by various categorical variables. The general theme of this section is that there is no clear slicing of continuous on categorical variables that provide a clear, distinct pattern.\n\n**Positive Feedback Count Distribution by Rating, Department Name, Recommended IND, and Class Name**"
"**Interpretation:** \nSince Positive Feedback Count is in log form, the higher frequency of non-recommended [0] has a bigger effect than visually suggested. The more popular reviews are not recommended, which suggest that the content is in the form of constructive criticism."
"**Interpretation:** \nThe difference is not huge, but nevertheless, a higher gini coefficient signigies higher inequality. This means that there is a bigger divergence between recommended reviews than there is between non-recommended reviews.\n\n***\n\n**Positive Feedback Count by Class Name:** "
Not much to say here. There are too many classes to include a legend.. A statistical test method would operate better at this dimensionality.\n\n***\n**Age Distribution by the Usual Suspects.. round them up**
"**Interpretation:** \nUnlike Positive Feedback Count, Age has not been transformed into a logarithm. For these reasons, slight noise between the age distribution by these features are nothing to worry about. Age doesn't seem to receive influence on these dimensions.\n\n***\n\n### 3.3 Continuous Variables  on Continuous Variables\n\nTime for some scatter plots. with [Seaborn Joint Plot](https://tryolabs.com/blog/2017/03/16/pandas-seaborn-a-guide-to-handle-visualize-data-elegantly/)."
"**How to Interpret:** \nDon't be deceived by the seemingly numerous amount of points over the Positive Feedback Count value of 0! The distribution plot up top clearly shows that most points reside at ZERO!\n\n**Interpretation:** \nThere appears to be a slight correlation between age and positive feedback count received. It would be interesting to focus on the textual anatomy of high positive feedback reviews.\n***\n\n### 3.4 Percentage Standardize Distribution Plots\n\nSince many variables are severely unbalanced, I employ standardization by percentage to see if the proportion is consistent between categorical classes. This is the same idea used previously on heatmaps now applied to barcharts!\n\n[Percentage Standardize in Seaborn - Stackoverflow](https://stackoverflow.com/questions/34615854/seaborn-countplot-with-normalized-y-axis-per-group)"
"**Code Explanation:** \nMany transformation are conducted here.\n- Groupby([x])[hue]: Groups the data by the x variable, what will become the X axis of the barplot.\n- Value_counts(normalized=True): Then the hue variable, which is rowed by the x variable, is ordered by most frequent to least, and that value is converted to decimal percentage.\n- rename().mull(100): Then this is renamed to ""Percentage"", and the decimal value is multiplied by 100 to be in proper percentage units.\n\n***\n**Recommended IND by Department and Division**"
**Interpretation:** \nThe finding here is the same as the earlier heatmap. Nothing tremendous.\n\n***\n**Rating by Department and Divison Name**
**Interpretation:** \nDepartment and Divison are consistent with the overall distribution of Rating.\n\n***\n**Positive Feedback Count over 40 by Recomended IND and Rating**
"**Code Explanation:** \nWhile I have mostly built my multi-plot visualization configuration from scratch, here is an facetplot example which is less complex, but nevertheless, requires careful planning of new variables/dimensions, such as my ""Cutoff"" variable.\n\nThe red vertical line corresponds to the cutoff rule. Note that KDE likes to smooth out its tails, even though the hard cutoff would contradict this. More realistic representation would appear with a barplot, but the clutter would be too in-intelligible.\n\n**Interpretation:** \nAs a follow-up on the preview analysis on the dominant high positive feedback count rate of reviews recommended by the customer, this plot offers even more nuance.\n\nFirst finding is the bump in on the bottom left: Cutoff = True | Recommended IND = 0. Now, this plot explores un-hopeful criticism about certain products, which is why the light blue's (rating = 1) second bump dominates the ~110 positive feedback count range.\n\nThe second finding is the the bottom right plot. Here, these are popular reviews which are recommended. It it interesting to see the high spread of the yellow distribution, rating = 3. This indicates that hopeful reviews which offer constructive criticism are the most socially appreciated.\n\n***\n**Rating by Recommended IND**\n"
"**Interpretation:** \nThis is a big one, which returns to my question: ""How do customers express their dislike for a Product"". There is a conflicting interest between the customers personal interaction with the product, such as the personal size fit, experience, and other personal synergies, and what the customer would invision for other customers.\n\nMy theory is that when customers give product a low rating, but nevertheless recommend the item, the customer is protesting about a personal complaint they have, such as a fit issue or customer service and product handling problem all the while still expressing admiration for the product, an approval of style worthy for the body of another.\n\nLooking at the data, it appears like five star ratings are void of non-recommendations, but low rated products are recommended a small amount of the time.\n\nThe more even occurrence between recommended and non-recommended on products with three rating is a phenomenon worth getting to the bottom of. Especially the recommended portion of the reviews, which might shed light on the biggest limitations of the retailers personal servicing, and the customers personal clothing experience.\n\n***\n\n## 4. Multivariate Analysis and Descriptive Statistics\n\n\nIn this section, I will no longer look at merely observation count by feature, but also look at how averages and other descriptive statistics behave when cut up.\n\n**Rating by Recommended IND**"
"**Interpretation:** \nRating is just under max rating when recommended, and halfed when not recommended. Trend is consistent across Division and Department.\n\n***\n**Correlating Average Rating and Recommended IND by Clothing ID** \nAnalysis of data grouped by Clothing ID."
"**How to Interpret:** \nI must stress the *Grouped By Clothing ID* aspect of this analysis. This aggregation investigates if there is trend between average rating and number of reviews by product. This is a different lense of analysis than merely running a correlation on *all customers reviews*.\n\n**Interpretation:** \nThis correlation heatmap suggest that there is in fact no correlation between count and average value, which means that the popularity of the item does not lead to differential treatment when it comes to average scoring. The age variable behaves in this same as well.\n\nHowever, There is a strong positive correlation of .80 between rating and recommended IND mean."
"**Interpretation:** \nHere is a closer look at this correlation of interest. And look at that p-value! Someone call a publisher.\n\nJokes aside, perhaps the dots are the bottom left could be the products that unarguably need attention from the retailer, in the hope of preserving brand image."
"Follow-up on the previous correlation plot. This plot displays that these outliers are not very strongly represented. Indeed, the average count for the **LOW QUADRANT**, as labeled at the bottom left of the plot, is only 2.3. For these reasons, hyper negative reviews may be unrepresentative outliers, and not taken as the public's general opinion. \n\nA practise I could envision tackling this problem is to include the average rating of the product class, such as ""Dress"", in order to relieve customers who may be worried about product with low, hyper negative reviews.\n\n***\n**Correlating Average Rating and Recommended IND by Class Name** \n- [Stackoverflow Annotating Outliers](https://stackoverflow.com/questions/43010462/annotate-outliers-on-seaborn-jointplot)"
Image: parade.com
"Jingle Bell Rock - Let's sing along - Lyrics: \nJingle bell, jingle bell, jingle bell rock/\nJingle bells swing and jingle bells ring/\nSnowing and blowing up bushels of fun/\nNow the jingle hop has begun/\nJingle bell, jingle bell, jingle bell rock/\nJingle bells chime in jingle bell time/\nDancing and prancing in Jingle Bell Square/\nIn the frosty air/\nWhat a bright time, it's the right time/\nTo rock the night away/\nJingle bell time is a swell time/\nTo go gliding in a one-horse sleigh/\nGiddy-up jingle horse, pick up your feet/\nJingle around the clock/\nMix and a-mingle in the jingling feet/\nThat's the jingle bell rock/\nJingle bell, jingle bell, jingle bell rock/\nJingle bells chime in jingle bell time/\nDancing and prancing in Jingle Bell Square/\nIn the frosty air/\nJingle bell, jingle bell, jingle bell rock/\nJingle bells chime in jingle bell time/\nSnowing and blowing up bushels of fun/\nNow the jingle hop has begun/\nJingle bell, jingle bell, jingle bell rock/\nJingle bells chime in jingle bell time/\nDancing and prancing in Jingle Bell Square/\nIn the frosty air/\nWhat a bright time, it's the right time/\nTo rock the night away (rock the night away)/\nJingle bell time is a swell time/\nTo go gliding in a one-horse sleigh/\nGiddy-up jingle horse, pick up your feet/\nJingle around the clock/\nMix and a-mingle in the jingling feet/\nThat's the jingle bell/\nThat's the jingle bell/\nThat's the jingle bell (rock)/    https://www.youtube.com/watch?v=VfLf7A_-1Vw  113,514,564 views"
"\n📝 Note: Instead of using combined_df to calculate the group_size we could use row['Parch'] + row['SibSp'] + 1 but we dont know if each group ticket is a single familie or a familie at all. So since we have the data available, we can use it."
\n💭 Thougts: This looks like a more likely price distribution.
\n📝 Note: This is interesting. Being alone in the first class seems to have a negative impact on your chance of survival. Being in a large group (more than 4 people) in the third class seems to lower your survival probability as well. This could be a useful feature to predict whether someone has survived.
Here is one final cumulative graph of a pair plot that shows the relations between all of the different features
"\n# **5. Feature Engineering**\nBecause values in the Sex and Embarked columns are categorical values, we have to represent these strings as numerical values in order to perform our classification with our model. We can also do this process through **One-Hot-Encoding**."
"# Data Visualization\nWe live in a world where data visualisations are done through intricate code and graphic design. From Tableau to Datawrapper and Python and R, numerous possibilities exist for visualising compelling stories.\n""Often, the most important thing is to give a truthful impression of the data.""\nHowever, the goal of visualisation does not always have to be to encode information in such a way that it is easy to read off exact values. Often, the most important thing is to give a truthful impression of the data. And, the most technically correct visualisations may not always be the best way to convey that impression.[6]"
"Matplotlib is the most favorite library for data visualization by 37.8%, followed by Searborn, 27%, Plotly 12.4%, and Ggplot 10.9%. Other libraries are less popular with less than 3% users. [7] Unfortunately popular data visualization like Tableau, Google Data Studio, Looker and many others were not asked in the survey which I believe also used by many Kagglers."
# Cloud Computing Platforms\nWhich of the following cloud computing platforms do you use on a regular basis?
"Most of the large cloud providers have jumped in with both feet into the machine learning space. Amazon, Google, IBM, and Microsoft have all added core capabilities for machine learning model development, management, and iteration as well as capabilities for data preparation, engineering, and augmentation. [9]\nAmazon Web Services is the number one of user's cloud platform choice with 31.7%, followed by Google Cloud Platform, 26.8%, Microsoft Azure, 20.9%, and IBM Cloud/Red Hat in the 4th position, with 4.9% far behind the top three."
"# Big Data Products\nWhich of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?"
"The number one favorite database is still MySQL (24.2%), then half percentage of it followed by PostgresSQL (15.1%), and MSSQL (13.3%). Other databases are almost half of percentage of the top three, with MongoDB (7.4%), Oracle Database (6.9%), Google BigQuery (6.5%), and the rest with less than 5%."
# Cloud Computing and Data Storage Products\n1. Do you use any of the following cloud computing products on a regular basis?\n2. Do you use any of the following data storage products on a regular basis?
"Amazon is the top choice of cloud computing and data storage products, Amazon Elastic Compute Cloud, (49.4%), and on data storage Amaxon Simple Storage Service, with 50.3%, and almost one third followed by Google Cloud Filestore (16.8%), then Amazon Elastic File System (14.9%)."
# Share EDA\nWhere do you publicly share your data analysis or machine learning applications?
"Github is the number one for open source communities (31%), and Kaggle is not surprisingly, at number two position (20.7%). In this chart, I take out users who do not share their work in the chart (21.4%). How come a Kaggler does not share their work anyway. "
 Out of all the survivers there were many females. this may due to the fact that higher priority given to save the children and women on the ship. and also there would be a high priority to the class 1 passengers. So class of the ticket you are buying will have a significant effect to the survival of tragic incidents.
 This shows the above mentioned fact of given higher priority to the class 1 passengers. hence there are many survivars from class 1 while there are many who could not survive in class 3.
 Although there is a high percentage of survivers in who's port of embarkation is churbog the no of passengers in churbog is low compared to southampton and also this may happen due to the fact that many in churbog are class 1 passengers.
 This shows the above mentioned fact that churbog has higher number of class 1 passengers compared to other classes.
### Part 3. Explaining the model
Confusion matrix is quite balanced.
RobustScaler subtracts the column median and divides by the interquartile range.
Let's check the minimums and maximums for each column after RobustScaler.
StandardScaler is scales each column to have 0 mean and unit variance.
"Qutie a nice chart, don't you think? You can see that all features now have 0 mean."
"# Normalizer\n\nNote that normalizer operates on the rows, not the columns. It applies l2 normalization by default."
Let's check the minimums and maximums for each column after scaling.
"As an option, you can save the plot in an html file:"
#### Bar chart\n\nLet's use a *bar chart* to compare the market share of different gaming platforms broken down by the number of new releases and by total revenue.
#### Box plot\n\n`plotly` also supports *box plots*. Let's consider the distribution of critics' ratings by the genre of the game.
"Using `plotly`, you can also create other types of visualization. Even with default settings, the plots look quite nice. Additionally, the library makes it easy to modify various parameters: colors, fonts, captions, annotations, and so on."
* **Residual plot shows residual error VS. true y value.**
"* **Residualplot showing a clear pattern, indicating Linear Regression no valid!**"
"--------------------------------------------------------\n# Checking Correlations\n\nFirst, draw a heatmap to find features that are related to classes."
" Observation:\n    \nV3, V7, V10, V11, V12, V14, V16, and V17 have a strong correlation with target(class) compared to other features."
" Observation:\n\n**Looking at the above pictures, the V14 and V17 features have a high correlation with target class. Let's dig deeper into these features.**\n\n**V14 and V17 features seem to have more correlation with Class than other features. Let's analyze the difference before and after sampling with a scatter plot of these two features.**"
### Plotting histogram Plot
 Observation:\n    \nThe imbalacne also seems to be large. Let's think about how we can solve this.
### Visualizing after Dimension Reduction
 Observation:\n    \n* Wow! It is beatiful!\n* The target is severely imbalaced. Level 1 target is too small compared to level 0 target.    
" Observation:\n\nIf you look at the distribution of the created zero-target data, you can see that it is spread over a wide range."
### Plotting Scatter Plot
" Observation:\n\nIf you look at the result of SMOTE, it looks like lines connected between points. This is because SMOTE uses an interpolation technique."
"## Visualising Age KDEs\nSummarizing the data with Density plots to see where the mass of the data is located. [A kernel density estimate plot](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) shows the distribution of a single variable and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn kdeplot for this graph.\n\n### Distribution of Ages w.r.t Target"
\n### Distribution of Ages w.r.t gender
## Distribution of Diagnosis
# 4. Visualising Images : JPEG\n\n## Visualizing a random selection of images
We do see that the JPEG format images vary in sizes
## Visualizing Images with Malignant lesions
"## Histograms\n\nHistograms are a graphical representation showing how frequently various color values occur in the image i.e frequency of pixels intensity values. In a RGB color space, pixel values range from 0 to 255 where 0 stands for black and 255 stands for white. Analysis of a histogram can help us understand thee brightness, contrast and intensity distribution of an image. Now let's look at the histogram of a random selected sample from each category.\n\n### Benign category"
### Malignant category
"# 5 Preprocessing DIOCOM files \n[Digital Imaging and Communications in Medicine (DICOM)](https://en.wikipedia.org/wiki/DICOM) is the standard for the communication and management of medical imaging information and related data.DICOM is most commonly used for storing and transmitting medical images enabling the integration of medical imaging devices such as scanners, servers, workstations, printers, network hardware, and picture archiving and communication systems (PACS) from multiple manufacturers\n\nDICOM images have the extension dcm. A DICOM file has two parts: the header and the dataset. The header contains information on the encapsulated dataset. It consists of a File Preamble, a DICOM prefix, and the File Meta Elements.\nFortunately we have a library in Python called Pydicom which can be used to read the DIOCOM files.pydicom makes it easy to read these complex files into natural pythonic structures for easy manipulation. Modified datasets can be written again to DICOM format files.\n\nThere is very nice [kernel](https://www.kaggle.com/schlerp/getting-to-know-dicom-and-the-data) from a competition couple of years ago which serves as a great introduction to DIOCOM image files.I have borrowed the below mentioned code from there.\nKernel: https://www.kaggle.com/schlerp/getting-to-know-dicom-and-the-data\n"
#### 4.2.2.1 Histogram Distribution 📉
#### 4.2.2.2 Violin Plot 🎻
"*   The distribution of **Applicant income, Co Applicant Income, and Loan Amount** are **positively skewed** and **it has outliers** (can be seen from both histogram and violin plot).\n*   The distribution of **Loan Amount Term** is **negativly skewed** and **it has outliers.**\n\n"
### 4.3.1 Heatmap 🔥
👉 There is positive correlation between Loan Amount and Applicant Income
### 4.3.2 Categorical 📊 - Categorical 📊
"👉 Most male applicants are already married compared to female applicants. Also, the number of not married male applicants are higher compare to female applicants that had not married."
## 3.1 Exploration of Age
## 3.2 Exploration of Fare
## 3.3 Exploration of Passenger Class
## 3.4 Exploration of Embarked Port
## 3.5 Exploration of Traveling Alone vs. With Family
"Individuals traveling without family were more likely to die in the disaster than those with family aboard.  Given the era, it's likely that individuals traveling alone were likely male."
## 3.6 Exploration of Gender Variable
## 4. Logistic Regression and Results
Test data set:
"# NOTE\nI know what are you thinking, how much sense does it make to take a mean of a moving average and other derivations of it but according to the feature importance of lbgm it does. In a contrast if we were to take all 150 000 rows and calculate what we have calculated for each of them. Than (approximately) 9 minutes times 150 000 is around\n# **ONE LIFETIME**\n worth of time of calculations. Now I do understand that it is actually not that since we are taking the mean for every one ofthe variables etc.. but STILL it is a long time!"
"**We can also look at feature importance**, to see which features (out of the ones we've already created) are the most important. This way, we can choose afterwards which one to choose when creating the more complex models (more details are coming in my second notebook).\n\n\n📌 Note:We can see that freq_sum, freq_min and freq_mean are the most important features, although we have more than 11,700 columns for the words in our texts. It means that the word_frequencies dataset is actually helpful!\n"
"\n📌 Yay! This is a big improvement! The RMSE dropped from a value of 2.31 to around 0.82! I would call this a win, especially because we didn't really do much to our dataset.\n\n\n\n\n\n\n# ⌨️🎨 My Specs\n\n* **Z8 G4** Workstation 🖥\n* 2 CPUs & 96GB Memory 💾\n* NVIDIA **Quadro RTX 8000** 🎮\n* **RAPIDS** version 0.17 🏃🏾‍♀️\n\n\n> 📌 **Leaderboard**: And the leaderboard score for the XGBRF Model using Repeated Folds is **0.93** (if you have any questions on how to submit, don't hesitate to ask - don't forget to name your submission `submission.csv` so you won't get an error!)\n"
"That seems reasonable. Let's also check with a **[Receiver Operator Curve (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)**,"
"Another common metric is the **Area Under the Curve**, or **AUC**. This is a convenient way to capture the performance of a model in a single number, although it's not without certain issues. As a rule of thumb, an AUC can be classed as follows,\n\n- 0.90 - 1.00 = excellent\n- 0.80 - 0.90 = good\n- 0.70 - 0.80 = fair\n- 0.60 - 0.70 = poor\n- 0.50 - 0.60 = fail\n\nLet's see what the above ROC gives us,"
"So, it looks like the most important factors in terms of permutation is a thalessemia result of 'reversable defect'. The high importance of 'max heart rate achieved' type makes sense, as this is the immediate, subjective state of the patient at the time of examination (as opposed to, say, age, which is a much more general factor).\n\nLet's take a closer look at the number of major vessles using a **Partial Dependence Plot** (learn more [here](https://www.kaggle.com/dansbecker/partial-plots)). These plots vary a single variable in a single row across a range of values and see what effect it has on the outcome. It does this for several rows and plots the average effect. Let's take a look at the 'num_major_vessels' variable, which was at the top of the permutation importance list,"
"So, we can see that as the number of major blood vessels *increases*, the probability of heart disease *decreases*. That makes sense, as it means more blood can get to the heart.\n\nWhat about the 'age',"
"That's a bit odd. The higher the age, the lower the chance of heart disease? Althought the blue confidence regions show that this might not be true (the red baseline is within the blue zone).\n\nWhat about the 'st_depression',"
"Interestingly, this variable also shows a reduction in probability the higher it goes. What exactly is this? A search on Google brought me to the following description by Anthony L. Komaroff, MD, an internal medicine specialist [5](https://www.sharecare.com/health/circulatory-system-health/what-st-segment-electrocardiogram-ecg) .... *""An electrocardiogram (ECG) measures the heart's electrical activity. The waves that appear on it are labeled P, QRS, and T. Each corresponds to a different part of the heartbeat. The **ST segment** represents the heart's electrical activity immediately after the right and left ventricles have contracted, pumping blood to the lungs and the rest of the body. Following this big effort, ventricular muscle cells relax and get ready for the next contraction. During this period, little or no electricity is flowing, so the ST segment is even with the baseline or sometimes slightly above it. The faster the heart is beating during an ECG, the shorter all of the waves become. **The shape and direction of the ST segment are far more important than its length. Upward or downward shifts can represent decreased blood flow to the heart from a variety of causes, including heart attack, spasms in one or more coronary arteries (Prinzmetal's angina), infection of the lining of the heart (pericarditis) or the heart muscle itself (myocarditis), an excess of potassium in the bloodstream, a heart rhythm problem, or a blood clot in the lungs (pulmonary embolism).""***\n\n    [6](https://www.cvphysiology.com/CAD/CAD012)"
"So, this variable, which is described as 'ST depression induced by exercise relative to rest', seems to suggest the higher the value the higher the probability of heart disease, but the plot above shows the opposite. Perhaps it's not just the depression amount that's important, but the interaction with the slope type? Let's check with a 2D PDP,"
"It looks like a low depression is bad in both cases. Odd.\n\nLet's see what the SHAP values tell us. These work by showing the influence of the values of every variable in a single row, compared to their baseline values (learn more [here](https://www.kaggle.com/dansbecker/shap-values))."
"Simple training loop. I prefer to write those myself from scratch each time, because then you can tweak it to do whatever you like."
## The model
"## Embarked \n`Embarked` tells us where a passenger boarded from.  \nThere are three possible values for it: Southampton, Cherbourg and Queenstown.  \nIn the training data, more than 70% of the people boarded from Southampton, slightly under 20% from Cherbourg and the rest from Queenstown.  \nCounting survivors by boarding point, we see that more people who embarked from Cherbourg survived than those who died."
"Since we don't expect that a passenger's boarding point could change the chance of surviving, we guess this is probably due to the higher proportion of first and second class passengers for those who came from Cherbourg rather than Queenstown and Southampton.  \nTo check this, we see the class distribution for the different embarking points."
"The claim is correct and hopefully justifies why that survival rate is so high.  \nAgain this feature might be useful in detecting groups at a deeper level of a tree and this is the only reason why I keep it.\n## Name\nThe `Name` column contains useful information as for example we could identify family groups using surnames.  \nIn this notebook, however, I extracted only the passengers' title from it, creating a new feature for both train and test data."
"This should help our model a little, so I think we are fine here.\n## SibSp\n`SibSp` is the number of siblings or spouses of a person aboard the Titanic.  \nWe see that more than 90% of people traveled alone or with one sibling or spouse.  \nThe survival rate between the different categories is a bit confusing but we see that the chances of surviving are lower for those who traveled alone or with more than 2 siblings.  \nFurthermore, we notice that no one from a big family with 5 or 8 siblings was able to survive."
"## Parch\nSimilar to the `SibSp` column, this feature contains the number of parents or children each passenger was traveling with.  \nHere we draw the same conclusions as `SibSp`: we see again that small families had more chances to survive than bigger ones and passengers who traveled alone."
"## Family type\nSince we have two seemingly weak predictors, one thing we can do is combine them to get a stronger one.  \nIn the case of `SibSp` and `Parch`, we can join the two variables to get a family size feature, which is the sum of `SibSp`, `Parch` and 1 (who is the passenger himself). "
"Plotting the survival rate by family size it is clear that people who were alone had a lower chance of surviving than families up to 4 components, while the survival rate drops for bigger families and ultimately becomes zero for very large ones."
"To further summarize the previous trend, as my final feature I created four groups for family size."
Here is the final result: I think we discovered a nice pattern.
"After all these considerations it is finally time to put everything together in a simple and quite efficient model.\n# Modeling\nWe start by selecting the features we will use and isolating the target.  \nAs I said, I will not consider `Cabin` and in the end, I also excluded `Age` as the relevant information which is being a young man is encoded in the Master title.  \nI also did not use `Sex` as it is not useful given the `Title` column: adult males and young children have the same sex but are really different categories as we saw before, so we don't want to confuse our algorithm.  \nIf you don't extract the `Title` column, remember to put `Sex` in your models as it is pretty important!"
Lets obtain the predictions of the model
"In this implementation, I have not traiened this network for longer epoochs, but for better predictions, you can train the network for larger number of epoochs say somewhere in the range of 500 - 1000. \n\n## 2.3 UseCase 3: Sequence to Sequence Prediction using AutoEncoders\n\n\nNext use case is sequence to sequence prediction. In the previous example we input an image which was a basicaly a 2 dimentional data, in this example we will input a sequence data as the input which will be 1 dimentional. Example of sequence data are time series data and text data. This usecase can be applied in machine translation. Unlike CNNs in image example, in this use-case we will use LSTMs. \n\nMost of the code of this section is taken from the following reference shared by Jason Brownie in his blog post. Big Credits to him. \n- Reference : https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n\n#### Autoencoder Architecture  \n\nThe architecuture of this use case will contain an encoder to encode the source sequence and second to decode the encoded source sequence into the target sequence, called the decoder. First lets understand the internal working of LSTMs which will be used in this architecture. \n\n- The Long Short-Term Memory, or LSTM, is a recurrent neural network that is comprised of internal gates.   \n- Unlike other recurrent neural networks, the network’s internal gates allow the model to be trained successfully using backpropagation through time, or BPTT, and avoid the vanishing gradients problem.   \n- We can define the number of LSTM memory units in the LSTM layer, Each unit or cell within the layer has an internal memory / cell state, often abbreviated as “c“, and outputs a hidden state, often abbreviated as “h“.   \n- By using Keras, we can access both output states of the LSTM layer as well as the current states of the LSTM layers.  \n\nLets now create an autoencoder architecutre for learning and producing sequences made up of LSTM layers. There are two components: \n\n- An encoder architecture which takes a sequence as input and returns the current state of LSTM as the output  \n- A decoder architecture which takes the sequence and encoder LSTM states as input and returns the decoded output sequence\n- We are saving and accessing hidden and memory states of LSTM so that we can use them while generating predictions on unseen data. \n\nLets first of all, generate a sequence dataset containing random sequences of fixed lengths. We will create a function to generate random sequences. \n\n- X1 repersents the input sequence containing random numbers  \n- X2 repersents the padded sequence which is used as the seed to reproduce the other elements of the sequence  \n- y repersents the target sequence or the actual sequence \n"
Lets visualize Surviving rates for families:
"The same thing as in (*) we gonna do for those Surnames who do not exist in test set.\nAgain, i gonna do it manually, cause i dont wanna work with automatization here =)\n\nBut just before this we will put some extra feature which mean ""Has any namesakes"" to determin if the person in som relatio-like group. It may look the same as SibSp, but guess, that som connections between people is hidden."
I desided to delete values with low frequancy - for our model not to be messed up.
"###   1.3.2 Parch and Age\nThis features may have crucial effect on the target. Parants was saving there children befor them, and many other connections may be inside of the just two attributes"
"#   2. Data preparation\n##   2.1 Filling None values\n###   2.1.1 Filling ""Age"" None values"
"For the ""Age"" feature we just can replace None values with some mean value, but we gonna do it in a bit more complex way.\nWe will locate some groups, based on Sex and Pclass - lets check the difference in ages in several groups."
"### Transported(Dependent, Nominal)  \nThe target value is a binary label consisting of True and False. Therefore, it is necessary to make sure that it is balanced. I checked the number and percentage of each category using countplot. When I checked, I found that True was 50.36% and False was 49.64%."
"-------------------------------------------------------------------------------\n### HomePlanet (Nominal)  \nThese variables are nominal variables, so we decided to check the distribution with countplot. To see if this variable can explain the dependent variable well, we checked the distribution of the dependent variable again. I confirmed the following results.\n\n1) Among the Homeplanets, 54.19% of the earth's share is the highest.  \n2) The ratio of Europa to Mars is almost the same.  \n3) Among Europa, the percentage of Transported is certainly high.  \n4) The false percentage of Transported in Earth is certainly high.  \n5) There is not much difference in Transported among Mars.\n\n= > More than half of people belong to Earth. In addition, the difference between Earth and Europa's transported ratio is certain, so the sorting algorithm can work well. Therefore, you will be able to use the variable well to solve this problem."
-------------------------------------------------------------------------------\n### Other Continuous Variables\nThey have a one-sided distribution. Most people didn't pay for this service. These can be extracted from variables such as total consumption.
"-------------------------------------------------------------------------------\n## 4. Multinomial Explore  \nYou should also consider the relationship between two or more variable combinations and dependent variables. For example, if you look at Home Planet and CryoSleep together, you can better define the relationship between the dependent variables:"
"\n🔑conclusion:   \nPassengers in suspended sleep are generally likely to be transmitted. Especially in Europa and Mars, most passengers during housekeeping sleep were forwarded.\n"
"### 3.3 Dendrogram\n\nThis technique is specific to the agglomerative hierarchical method of clustering. The method starts by considering each point as a separate cluster and starts joining points to clusters in a hierarchical fashion based on their distances. To get the optimal number of clusters for hierarchical clustering, we make use a dendrogram which is tree-like chart that shows the sequences of merges or splits of clusters."
"# 4. K-Means\n\nK-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.\nIt's a centroid-based algorithm and the simplest unsupervised learning algorithm.\nThe algorithm tries to minimize the variance of data points within a cluster. It's also how most people are introduced to unsupervised machine learning.\n\n**K-means++** (default init parameter for K-Means in sklearn) is the algorithm which is used to overcome the drawback posed by the k-means algorithm. The goal is to spread out the initial centroid by assigning the first centroid randomly then selecting the rest of the centroids based on the maximum squared distance. The idea is to push the centroids as far as possible from one another.\n\nAlthough the initialization in K-means++ is computationally more expensive than the standard K-means algorithm, the run-time for convergence to optimum is drastically reduced for K-means++. This is because the centroids that are initially chosen are likely to lie in different clusters already."
"# 8. Visualizing model performance\n\nLet's plot the model accuracy and loss for the training and the validating set. These plots show the accuracy and loss values for the second round of training. Since we initially trained the model with 30 epochs, these would be epochs 31-45. Note that no random seed is specified for this notebook. For your notebook, there might be slight variance."
We see that the accuracy for our model is around 98%. Finetune the model further to see if we can achieve a higher accuracy.
"## 3.1 The Skills Gap\n\nA lot of enterprises express that their recruits fresh out of their degree courses often lack certain fundamentals to seamlessly fit into the workplace. This may be attributed to their universities not prioritising certain key skills or perhaps due to the lack of initiative taken by the students in learning by themselves. In [Anaconda's State of Data Science Report 2020](https://www.anaconda.com/state-of-data-science-2020), respondents working in industry backgrounds were asked what were some of the essential skills that their newer recruits lacked. On the other side of things, students were even asked what they chose to study in preparation for their work.\n\n    Source: Anaconda's State of DS Report 2020 - \n     The Skills Gap\n    \n"
"While these numbers are specifically for the field of data science, it does still give us an idea of where there is a disconnect between what the students learn and what is actually expected of them in the workplace. Some of the areas where students seem to be lacking are:\n* **Big data management:** When people talk about big data, one of the first things that come to a lot of peoples' mind is deep learning and how it leverages massive volumes of data to generate powerful models. Rarely does one think of big data management which actually encompasses the entire range of **policies, procedures and technologies that are utilised in the organization and administration of large repositories of data**. Often students will glaze over this part of data management because its one of the more mundane parts of the job.\n* **Deep Learning and Machine Learning:** Based on the specifics of the role, companies also place a lot of importance on the recruit having a good foundation in the **working of machine learning and deep learning algorithms** as well as a hands-on experience of **when to apply** these to specfic problems.\n* **Advanced Mathematics:** While admittedly there are slight variations on the exact content of ""advanced mathematics"", it is a safe bet that if one wants to work with data they should be **well-versed with the basics of algebra, statistics, calculus and trigonometry**, and must be comfortable to delve deeper into these topics should their work require it"
## 3.2 Development environments: IDEs and Hosted notebooks\nLet us see if there are any note-worthy trends in terms of what tools data professionals use during their development work.
"\n    Source: Kaggle 2020 DS and ML Survey - \n     Q9. Which of the following integrated development environments (IDE's) do you use on a regular basis?\n    \n\n\nWhile Jupyter products hold the top spot in this graph, most of the **other IDEs do share a sizable chunk of the audience**. One might even be tempted to experiment with language specific IDEs like PyCharm, RStudio, Spyder, MATLAB, etc, because of the greater amount of customisation and **language specific features** that might aid the development process."
"\n    Source: Kaggle 2020 DS and ML Survey - \n     Q10. Which of the following hosted notebook products do you use on a regular basis?\n    \n\n\nHosted notebooks provide us access to a hassle free environment in which we may collaborate and share our results with others. For example, even though I dont have R installed on my system, Kaggle will happily let me open up an R notebook and start coding immediately without any setup on my side. \n\nAmong those that do use hosted notebooks **Colab and Kaggle notebooks are the top two choices**. With 28% of individuals responding that they dont any hosted notebooks, we understand that **not all roles require their use**."
## 3.4 What languages should I learn?\n\nContinuing on the topic of experience a very relevant question is what languages do I need to know in order to work as a data professional? Lets try to understand what languages students tend to learn versus whats actually utilised in the industry.\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q7. What programming languages do you use on a regular basis?\n    \n
"Some of the key takeaways are:\n- The students seem to be on a strong start here - learning **Python** is not only a safe bet because of its widespread use in the industry, but its also by far the language **most recommended** to beginners. \n- Having at least a **basic understanding of SQL** and database management would go a long way in your career.\n- While languages like C, C++ and Java are what many of us start out with, students should **learn to quickly adapt** and transfer their knowledge to more commonly used languages."
## 3.5 Visualization Tools\nLet's see if there are any favorites among data professionals when it comes to visualising their data. Also **be sure to check the hover info for additional stats and information** about each of the tools.\n\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q14. What data visualization libraries or tools do you use on a regular basis? \n    \n
"From the above graph my main takeaway is that with visualization greatly aiding analysis its often **useful to have experience with at least one** of these tools under your belt. If your job role hinges on **creating impactful visualisations**, then maybe you might need to upskill yourself and learn to utilise tools with interactivity and higher degrees of customisation that help to convey your message better."
## 3.7 What Machine Learning frameworks should I opt for?\n\nLet's try to see what ML frameworks are commonly used in the industry.\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q16. Which of the following machine learning frameworks do you use on a regular basis? \n    \n
"A couple things to note:\n* **Scikit** is the most common choice among respondents. **Tensorflow and Keras** also seem to be popular choices in the second and third place.\n* **Xgboost vs LightGBM vs CatBoost**: A common question asked on Kaggle - when considering only Data Scientists, we see that **49% use Xgboost** making it the preferred choice over LightGBM at 26% and Catboost at 14%.\n\nWhile exploring the data for this analysis I noticed a rather peculiar behaviour - for some reason **Statisticians adopted Caret to a much greater extent** than any other field. On further analysis I realised that because this was a **group that had much higher usages of R**, an **R ML framework** like Caret had a much higher adoption rate. Let's see how the same chart differs if we instead look at those who only use R.\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q16. Which of the following machine learning frameworks do you use on a regular basis? \n    \n"
"Seems like our results have changed when only considering R users. If students prefer R as their language of choice, **Caret** seems to be a good option to look into (Also notice how another R package like **TinyModels suddenly jumped up in the list**)\n\nThe lower percentages in general compared to the previous graph, as well as the presence of ""None"" in second place shows that most **R users value the language for purposes other than machine learning**. \n\n*I'm assuming we're seeing certain python packages popping up in this graph because the respondents had knowledge of them but didn't consider Python as a language they used regularly, in the survey responses.*"
"## 3.8 Learning Platforms\n\nEven with all the amazing resources available online, at times we all need a little guidance in developing a systematic learning plan and thats where learning platforms come to our rescue. Let's take a look at what we can learn from the way that data professionals consume online course content.\n\nWhile exploring the data I noticed how at each age group there were noticable **differences between both groups' learning habits** so I created an interactive piece to help you see just that. *(PRO TIP: After you've selected an age group option, using the up and down arrow keys to go through the options makes the graph a lot more interesting to use.)*\n\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q37. On which platforms have you begun or completed data science courses? \n    \n"
"* A common trend we see is that **data professionals are more likely to leverage these resources** - even when only comparing individuals in the same age group.\n* When considering online platforms we see that **Coursera** has cemented its place in our #1 spot. In the next few spots **Udemy, Kaggle Learning courses and Data Camp** are also popular among the two groups.\n* While **University Courses** do often pop up in the top 5 of our list, we see that **less than 30% of all data professionals have completed a data science course in university**. This goes to show that having **a degree in data science isn't always necessary** to work in this field."
## 3.9 Media and learning\n\nThe field of data science and machine learning is advancing at a rapid rate. Let's look at some of the tools that will help you to continuously learn and keep in touch with the latest in your field.\n\n\n    Source: Kaggle 2020 DS and ML Survey - \n     Q39. Who/what are your favorite media sources that report on data science topics?\n    \n
"Using media is a great way to catch up on trends and updates in our field. They provide information in a **more entertaining and informal manner** that one can always pick up at their own convenience, making them a great way to **supplement our more structured learning activities**. On seeing the trend that professionals seem to utilise more of these, perhaps students might want to look into a couple ML/DS related content creators on their favorite platforms.\n\nOther notable sources that didn't quite make the top 5 here are - **email newsletters, podcasts, slack communities, reddit**, etc."
"## 3.10 Obstacles to working with data \nWhile studying in preparation for working with data we tend to live in this bubble of ours, assuming that things will work out since we probably are qualified enough to work in our field of choice. We often neglect actually getting a sense of how the job market is and what are the struggles faced by those actively hunting for work in the same field.\n\nTo get an idea of the trappings that prevent students from landing that dream data role, we look at Ananconda's report where they asked students what was the biggest factor that prevented them from securing their ideal data science job.\n\n\n    Source: Anaconda's State of DS Report 2020 - \n     In your opinion, what is the biggest obstacle to obtaining your ideal data science job? \n    \n"
"I initially expected that a lack of job openings would be a major factor, but turns out that this was hardly the case. Most students claimed that their reason for not securing their ideal job was that they **lacked the experience** for it. Often this will be due to companies asking for ""*x years of experience in...*"" which will turn away most students early in their careers, otherwise it might be experience working in a particular field or with specific tools.  While there may be no direct workaround for this, **seeking internships** in a related area of work would go a long way as initial work experience. Data science **side-projects** are also a great way to showcase your ability, so long as the content is novel and well-thought out."
"**Few notes of why we might need to use probability calibration techniques:**\n* A well-calibrated classifier can help to prevent incorrect decisions based on the classifier's predictions, especially if those decisions are based on threshold values for the predicted probabilities.\n* Poorly calibrated classifiers can lead to incorrect decisions, which can be especially problematic in high-stakes situations.\n* Probability calibration techniques can adjust the predicted probabilities to better reflect the true probabilities of positive outcomes, leading to more accurate and reliable predictions.\n\n**The process:**\n\nTo assess the calibration of a binary classifier, we can plot a calibration curve that shows the predicted probabilities on the x-axis and the proportion of true positive outcomes for each predicted probability bin on the y-axis. The calibration curve can also show the proportion of true negative outcomes for each predicted probability bin. A perfectly calibrated classifier would have a calibration curve that is close to the diagonal line, indicating that the predicted probabilities accurately reflect the true probabilities of both positive and negative outcomes.\n\nIf the probabilities are not calibrated we can build in a sense the second level model fitting our predictions into Logistic regression or any other chosen algorithm against the target.\nAfter that we repeat the process by plotting the calibration curve and returning the new score.\n\n**Methods:**\n\nTo calibrate a binary classifier, the data set of scores and their corresponding binary outcomes is used. The aim is to find a function that can accurately estimate the relationship between the scores and the true probabilities as determined empirically in the calibration set. There are several methods of calibration that can be used, including:\n\n* Platt Scaling\n* Isotonic Regression\n* Beta Calibration\n* SplineCalib\n\nThe choice of calibration method depends on factors such as the size of the calibration set, the complexity of the classifier, and the desired level of calibration accuracy.\n\n**Let's plot the calibration plot and histogram first:**\n"
**Note**:\n\nIt might look like the classifier is not well calibrated in a range ~(0.35 < x < 0.85) and we need to focus on that. But the important thing is to look at the histogram which shows that routhly 90% of our predictions < 0.5 and we need to **zoom in**:
### 3. Modelling
"As you can see, the MA(1) model fits $\beta_1 = 0.5172$, which is quite close to the `beta_1 = 0.5`. However, the predicted values seem to be quite off as well in this case - similarly to the AR(p) cases."
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1) and near-adjacent (lag = 2) observations\n\n| | AR($p$) | MA($q$) | ARMA($p$, $q$) |\n|-|-|-|-|\n|ACF|Tails off (Geometric decay) |  Significant at lag $q$ / Cuts off after lag $q$   |Tails off (Geometric decay) |\n|PACF| Significant at each lag $p$ / Cuts off after lag $p$ |Tails off (Geometric decay) |Tails off (Geometric decay) |\n\n-> We can use an **MA(2) model** to model this process.\n\nSo that for MA(2), we would model the MA(q) formula\n$\hat{y}_t = \epsilon_t + \beta_1 \epsilon_{t-1} + \dots + \beta_{q} \epsilon_{t-q}$\nto the following:\n\n$\hat{y}_t = \epsilon_t + \beta_1 \epsilon_{t-1} + \beta_2 \epsilon_{t-2}$"
"As you can see, the MA(2) model fits $\beta_1 = 0.5226$ and $\beta_2 = -0.5843$, which is quite close to the `beta_1 = 0.5` and `beta_2 = 0.5` which we have set. However, the predicted values seem to be quite off as well in this case - similarly to the MA(1) case."
## Periodical\nThe following time series is periodical with T=12. It consists of 48 timesteps.
"### 1. Check Stationarity\nThe sample data is stationary. Therefore, we do not need to difference the time series."
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1) and near-adjacent observations\n- From both the ACF and PACF plot, we can see a strong correlation with the adjacent observation (lag = 1) and also at a lag of 12, which is the amount of T.\n\n\n| | AR($p$) | MA($q$) | ARMA($p$, $q$) |\n|-|-|-|-|\n|ACF|Tails off (Geometric decay) |Significant at lag $q$ / Cuts off after lag $q$  |Tails off (Geometric decay) |\n|PACF|  Significant at each lag $p$ / Cuts off after lag $p=12$ |Tails off (Geometric decay) |Tails off (Geometric decay) |\n\n-> We can use an AR(12) model to model this process."
## Trend\n\nThe following time series is the same as [Periodical](#Periodical) (periodical with T=12) with added trend. It consists of 48 timesteps.
### 1. Check Stationarity\nThe sample data is non-stationary. 
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1) and near-adjacent observations"
"Spreading the Spectrum\n\nFrom the histogram, you can see that most pixels are found between a value of 0 and and about 25.  The entire range of grayscale values in the scan is less than ~125.  You can also see a fair amount of ghosting or noise around the core image.  Maybe the millimeter wave technology scatters some noise?  Not sure... Anyway, if someone knows what this is caused by, drop a note in the comments.  That said, let's see what we can do to clean the image up.\n\nIn the following function, I first threshold the background.  I've played quite a bit with the threshmin setting (12 has worked best so far), but this is obviously a parameter to play with.\n\nNext we equalize the distribution of the grayscale spectrum in this image.  See this tutorial if you want to learn more about this technique.  But in the main, it redistributes pixel values to a the full grayscale spectrum in order to increase contrast.\n\n"
Here's the histogram:\n![Raw Histogram](https://storage.googleapis.com/kaggle-datasets-jbf/tsa_datasets/spread_spectrum_hist.png)\n\n*Image output removed at the request of DHS.  Uncomment the unit test and run in your own environment.*
"Masking the Region of Interest\n\nUsing the slice lists from above, getting a set of masked images for a given threat zone is straight forward.  The same note applies here as in the 4x4 visualization above, I used a cv2.resize to get around a size constraint (see above), therefore the images are quite blurry at this resolution.  Note that the blurriness only applies to the unit test and visualization.  The data returned by this function is at full resolution.\n"
*Image output removed at the request of DHS.  Uncomment the unit test and run in your own environment.*
"Cropping the Images\n\nUsing the crop lists from above, getting a set of cropped images for a given threat zone is also straight forward.  The same note applies here as in the 4x4 visualization above, I used a cv2.resize to get around a size constraint (see above), therefore the images are quite blurry at this resolution.  If you do not face this size constraint, drop the resize.  Note that the blurriness only applies the unit test and visualization.  The data returned by this function is at full resolution."
"The following algorithm is the simplest and easiest to understand among all the the clustering algorithms without a fixed number of clusters.\n\n\nThe algorithm is fairly simple:\n1. We start by assigning each observation to its own cluster\n2. Then sort the pairwise distances between the centers of clusters in descending order\n3. Take the nearest two neigbor clusters and merge them together, and recompute the centers\n4. Repeat steps 2 and 3 until all the data is merged into one cluster\n\nThe process of searching for the nearest cluster can be conducted with different methods of bounding the observations:\n1. Single linkage \n$d(C_i, C_j) = min_{x_i \in C_i, x_j \in C_j} ||x_i - x_j||$\n2. Complete linkage \n$d(C_i, C_j) = max_{x_i \in C_i, x_j \in C_j} ||x_i - x_j||$\n3. Average linkage \n$d(C_i, C_j) = \frac{1}{n_i n_j} \sum_{x_i \in C_i} \sum_{x_j \in C_j} ||x_i - x_j||$\n4. Centroid linkage \n$d(C_i, C_j) = ||\mu_i - \mu_j||$\n\nThe 3rd one is the most effective in computation time since it does not require recomputing the distances every time the clusters are merged.\n\nThe results can be visualized as a beautiful cluster tree (dendogram) to help recognize the moment the algorithm should be stopped to get optimal results. There are plenty of Python tools to build these dendograms for agglomerative clustering.\n\nLet's consider an example with the clusters we got from K-means:"
"## Accuracy metrics\n\nAs opposed to classfication, it is difficult to assess the quality of results from clustering. Here, a metric cannot depend on the labels but only on the goodness of split. Secondly, we do not usually have true labels of the observations when we use clustering.\n\nThere are *internal* and *external* goodness metrics. External metrics use the information about the known true split while internal metrics do not use any external information and assess the goodness of clusters based only on the initial data. The optimal number of clusters is usually defined with respect to some internal metrics. \n\nAll the metrics described below are implemented in `sklearn.metrics`.\n\n**Adjusted Rand Index (ARI)**\n\nHere, we assume that the true labels of objects are known. This metric does not depend on the labels' values but on the data cluster split. Let $N$ be the number of observations in a sample. Let $a$ to be the number of observation pairs with the same labels and located in the same cluster, and let $b$ to be the number of observations with different labels and located in different clusters. The Rand Index can be calculated using the following formula: \n\n$$\Large \text{RI} = \frac{2(a + b)}{n(n-1)}.$$ \n\nIn other words, it evaluates a share of observations for which these splits (initial and clustering result) are consistent. The Rand Index (RI) evaluates the similarity of the two splits of the same sample. In order for this index to be close to zero for any clustering outcomes with any $n$ and number of clusters, it is essential to scale it, hence the Adjusted Rand Index: \n\n$$\Large \text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}.$$\n\nThis metric is symmetric and does not depend in the label permutation. Therefore, this index is a measure of distances between different sample splits. $\text{ARI}$ takes on values in the $[-1, 1]$ range. Negative values indicate the independence of splits, and positive values indicate that these splits are consistent (they match $\text{ARI} = 1$).\n\n**Adjusted Mutual Information (AMI)**\n\nThis metric is similar to $\text{ARI}$. It is also symmetric and does not depend on the labels' values and permutation. It is defined by the [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) function and interprets a sample split as a discrete distribution (likelihood of assigning to a cluster is equal to the percent of objects in it). The $MI$ index is defined as the [mutual information](https://en.wikipedia.org/wiki/Mutual_information) for two distributions, corresponding to the sample split into clusters. Intuitively, the mutual information measures the share of information common for both clustering splits i.e. how information about one of them decreases the uncertainty of the other one.\n\nSimilarly to the $\text{ARI}$, the $\text{AMI}$ is defined. This allows us to get rid of the $MI$ index's increase with the number of clusters. The $\text{AMI}$ lies in the $[0, 1]$ range. Values close to zero mean the splits are independent, and those close to 1 mean they are similar (with complete match at $\text{AMI} = 1$).\n\n**Homogeneity, completeness, V-measure**\n\nFormally, these metrics are also defined based on the entropy function and the conditional entropy function, interpreting the sample splits as discrete distributions: \n\n$$\Large h = 1 - \frac{H(C\mid K)}{H(C)}, c = 1 - \frac{H(K\mid C)}{H(K)},$$\n\nwhere $K$ is a clustering result and $C$ is the initial split. Therefore, $h$ evaluates whether each cluster is composed of same class objects, and $c$ measures how well the same class objects fit the clusters. These metrics are not symmetric. Both lie in the $[0, 1]$ range, and values closer to 1 indicate more accurate clustering results. These metrics' values are not scaled as the $\text{ARI}$ or $\text{AMI}$ metrics are and thus depend on the number of clusters. A random clustering result will not have metrics' values closer to zero when the number of clusters is big enough and the number of objects is small. In such a case, it would be more reasonable to use $\text{ARI}$. However, with a large number of observations (more than 100) and the number of clusters less than 10, this issue is less critical and can be ignored.\n\n$V$-measure is a combination of $h$, and $c$ and is their harmonic mean:\n$$\Large v = 2\frac{hc}{h+c}.$$\nIt is symmetric and measures how consistent two clustering results are.\n\n**Silhouette**\n\nIn contrast to the metrics described above, this coefficient does not imply the knowledge about the true labels of the objects. It lets us estimate the quality of the clustering using only the initial, unlabeled sample and the clustering result. To start with, for each observation, the silhouette coefficient is computed. Let $a$ be the mean of the distance between an object and other objects within one cluster and $b$ be the mean distance from an object to an object from the nearest cluster (different from the one the object belongs to). Then the silhouette measure for this object is \n\n$$\Large s = \frac{b - a}{\max(a, b)}.$$\n\nThe silhouette of a sample is a mean value of silhouette values from this sample. Therefore, the silhouette distance shows to which extent the distance between the objects of the same class differ from the mean distance between the objects from different clusters. This coefficient takes values in the $[-1, 1]$ range. Values close to -1 correspond to bad clustering results while values closer to 1 correspond to dense, well-defined clusters. Therefore, the higher the silhouette value is, the better the results from clustering.\n\nWith the help of silhouette, we can identify the optimal number of clusters $k$ (if we don't know it already from the data) by taking the number of clusters that maximizes the silhouette coefficient."
"To conclude, let's take a look at how these metrics perform with the MNIST handwritten numbers dataset:"
"## 4. Demo assignment\n\nTo practice with PCA and clustering, you can complete [this assignment](https://www.kaggle.com/kashnitsky/a7-demo-unsupervised-learning) where you'll be analyzing data from accelerometers and gyros of Samsung Galaxy S3 mobile phones. The assignment is just for you to practice, and goes with [solution](https://www.kaggle.com/kashnitsky/a7-demo-unsupervised-learning-solution)."
# Technique 4: Random Row Selection\nThis is nothing but performing a **random sampling** of the data and make best use of it.\nFor this we can build a list of random rows by writing a function 
Suppose we want to fetch a random sample of 10000 lines out of the total dataset. i.e we need to fetch a list of `lines - 1 - 10000` random numbers ranging from 1 to 796373. \n\nNote that while we are generating such a long list also takes a lot of space and  some time. So let us use the Technique 1 mentioned above and make sure to use del and gc.collect() when complete!!!!\n\nNow let us generate list of lines to skip
\n        \n            1. Logestic Regression\n        \n
\n        \n            2. Support Vector Machines\n        \n
\n        \n            3. KNeighborsClassifier\n        \n
\n        \n            4. DecisionTreeClassifier\n        \n
\n        \n            5. RandomForestClassifier\n        \n
\n        \n            6. ExtraTreesClassifier\n        \n
\n        \n            7. AdaBoostClassifier\n        \n
\n        \n            8. XGB Classifier\n        \n
\nComparing Multiple Models\n
"Uh it seems the rasterizer renders rather well the satellite and semantic views, and both in conjunction help one to get a good sense of the positioning of each vehicle in relation to the road. You can easily understand the placement and motion of the vehicles and highway layout in satellite by taking a good look at the semantic view too."
"Again, box and stub will also give a good representaton of the data albeit with less low-level detail than the semantic view, seeing as the highways are not into much consideration here. The box view helps to just take a low-level look at the vehicles and their projected path whereas the stub view functions similarly to semantic. We can now proceed to taking a good look at the metadata provided by kkiller and potentially train a good model. The ones to check now will be stub and satellite to check."
"We have a literal wealth of information that we can use here to our benefits, including familiar features like:\n1. x, y,  and z coords\n2. yaw\n3. probabilites of other extraneous factors."
"Here we can extrapolate that the variables **centroid_x** and **centroid_y** have strongly negative correlations, and the strongest correlations are between **extent_z** and **extent_x** more than any other, coming in at 0.4. We can also try using an XGBoost/LightGBM model as kkiller has demonstrated in his brilliant kernel as an alternative approach to the problem."
### centroid_x and centroid_y
It seems like the two centroids have a somewhat strongly negative correlation and seemingly similar variable distributions. It seems that as such there is a negative correlation between both the variables.
"### extent_x, extent_y and extent_z"
"It seems both the distributions of extent X and extent Y are heavily right skewed, as is centroid X. However, I have left out extent Z is order for readability of the plot, let's look at it now.\n\nTry to smooth the data and get:"
"Once again, we have a right-skewed distribution as is the same with all the `extent` variables. "
### yaw
"So yes it seems like this distribution has several ""protrusions"" as I shall call them. We can now move on to exploring the frames data to check how feasible it is for our tabular purposes."
"First of all, we have nine ego rotation columns corresponding to each. So I would want to do a quick check of the correlation of these variables before moving on to some more high-level analyses."
Things to note from this correlation analysis:\n1. The rotation coordinates with `y` and `z` seem to be uncorrelated most of the time\n2. The coordinates which have `x` are correlated strongly with the z-dimensional rotation (could this be indicative of something? I very much think so)
## Result Visualisation
#### The best result is captured at k = 11 hence 11 is used for the final model
"- Now, let's plot a histogram for the number of ratings represented by the `rating_counts` column in the above dataframe. "
"- From the above plot, we can see that most of the movies have received less than 50 ratings and there are no movies having more than 100 ratings."
"- Now, we will plot a histogram for average ratings."
"- We can see that the integer values have taller bars than the floating values since most of the users assign rating as integer value i.e. 1, 2, 3, 4 or 5. \n\n- Furthermore, it is evident that the data has a weak normal distribution with the mean of around 3.5. There are a few outliers in the data as well."
"- Movies with a higher number of ratings usually have a high average rating as well since a good movie is normally well-known and a well-known movie is watched by a large number of people, and thus usually has a higher rating. \n\n- Let's see if this is also the case with the movies in our dataset. We will plot average ratings against the number of ratings."
"- The graph shows that, in general, movies with higher average ratings actually have more number of ratings, compared with movies that have lower average ratings."
  \nDistribution of Gender\n
Male: 577\nFemale: 314
  \nPclass and Age vs Survived\n
  \nCabin vs Survived\n
#### Age:
- Infants (age<=5) and childrens (between 10 and 15 years old) are most likely to survive.\n- elder passengers (>75) survived.\n- most passengers are between 15 and 40 years old.\n\n**insights:** It's good to convert the age feature to age band groups of length 5.
\n        \n            Discovering the correlation between the features:\n        \n\n
- Passenger Id has no correlation with any feature.\n- PClass has strong negative correlation with age and Fare.\n- Age has negative correlation with parch and sibsp.
#### Pclass - Age - Survived:
"- Pclass=3 had most passengers, Most if them did not survive.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived.\n- Most passengers in Pclass=1 survived."
#### Sex - Age - Survived:
"- as we saw before, Females are most likely to survive.\n- Elder passengers (>=70) are all males.\n"
"\n\n5.8 COMBINE THE RANKS OF THE IMPORTANT COLUMNS\n\n---\n\nThis is weird... because we seemingly ignore the calculations we just performed (prior to ranking) in favour of the non-normalized values. \n\nBasically the combination is the following equation\n\n$$\nf(x,y,z) = (x*y*z)^{\dfrac{1}{3}}\n$$\n\n\n\n\n\n"
\n\n5.9 CREATE SUBMISSION FILE USING COMBINED NORMALIZED RANK VLAUE\n\n---\n\nStraightforward
"We start with an **overview plot of the feature relations:** Here we show a *correlation matrix* for each numerical variable with all the other numerical variables. We excluded *PassengerID*, which is merely a row index. In the plot, stronger correlations have brighter colours in either red (positive correlation) or blue (negative correlation). The closer to white a colour is the weaker the correlation. "
"*Positive vs negative correlation* needs to be understood in terms of whether an increase in one feature leads to an increase (positive) or decrease (negative) in the correlated feature. Perfect correlation would have a correlation index of 1; perfect anti-correlation (= negative correlation) would have -1 (obviously each feature is perfectly correlated with itself; leading to the deep red diagonal). The upper right vs lower left triangle that make up this plot contain the same information, since the corresponding cells show the correlation coefficients of the same features. \n\nThe matrix gives us an overview as to which features are particularly interesting for our analysis. Both strongly positive or negative correlations with the *Survived* feature are valuable. Strong correlations between two other features would suggest that only one of them is necessary for our model (and including the other would in fact induce noise and potentially lead to over-fitting).\n\n**We learn:**\n\n- *Pclass* is somewhat correlated with *Fare* (1st class tickets would be more expensive than 3rd class ones)\n- *SibSp* and *Parch* are weakly correlated (large families would have high values for both; solo travellers would have zero for both)\n- *Pclass* already correlates with *Survived* in a noticeable way"
"In addition, we plot a **Pairplot** of the numerical features. This kind of plot is a more detailed visualisation of relationships between variables. It shows scatter plots for the different feature combinations plus a distribution of each feature on the diagonal. Again, the upper right and lower left triangle contain the same information. This kind of plot is vastly more useful for a set of continuous variables, instead of the categorical or integer values we have here. Nonetheless, it is a valuable exploratory tool that has a place in everyone's toolbox.\n\nThis plot is inspired by, and realised much more aesthetically in, the [comprehensive Ensemble Stacking Kernel by Anisotropic](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)  "
"Now we continue to examine these initial indications in more detail. Earlier, we had a look at the *Survived* statistics of the individual features in the overview figure. Here, we want to look at correlations between the predictor features and how they could affect the target *Survived* behaviour.\n\nUsually it's most interesting to start with the strong signals in the correlation plot and to examine them more in detail."
"**We learn:**\n\n- For females the survival chances appear to be higher between 18 and 40, whereas for men in that age range the odds are flipped. This difference between 18-40 yr olds might be a better feature than *Sex* and *Age* by themselves.\n\n- Boys have proportional better survival chances than men, whereas girls have similar chances as women have. Rather small numbers, though. "
"We study the correlation of *Age* with *Pclass* using a *violin plot*, which is also split between survived (right half) and not survived (left half). Check out the other visualisations in your forked copy."
"*Violin plots* are a modified version of boxplots, where the shape is a ""kernel density estimate"" of the underlying distribution. These estimates are smoothed and therefore extend beyond the actual values (look closely at the dotted zero level). I have also indicated *Age == 10*, which we will use to define children (vs teenagers) in the engineering part below.\n\n**We learn:**\n\n- Age decreases progressively as Pclass decreases from 1st to 3rd\n- Most older passengers are 1st class, but very few children are. This conflates the impact of *Age* and *Pclass* on the survival chances.\n- In 1st class, younger adults had better survival chances than older ones.\n- Most children in 2nd class survived, and the majority in 3rd class did too."
"Also, we will start to use *factorplots*, i.e. groups of *pointplots*, from the *seaborn* plotting package to visualise the categorical relations:"
"**We learn:**\n\n- Both the factorplot and the mosaicplot indicate that almost all females that died were 3rd class passengers.\n- For males being in 1st class gives a survival boost, otherwise the proportions look roughly similar.\n- Except for 3rd class, the survival for *Embarked == Q* is close to 100% split between male and female.\n\nLet's follow up the numbers for *Pclass vs Embarked* with a *pandas crosstab plot*:"
"**We learn:**\n\n- a high percentage of those embarked at ""C"" were 1st class passengers.\n- almost everyone who embarked at ""Q"" went to 3rd class (this means that the clear separation in the factorplot for ""Q"" isn't very meaningful, unfortunately).\n\nThe 2nd point is somewhat curious, since we recall from above that the survival chances for ""Q"" were actually slightly better than for ""S"". Not significantly so, of course, but certainly not worse even though ""S"" had a higher percentage of 1st and 2nd class passengers.\n\nIt seems that embarking at ""Q"" improved your chances for survival if you were a 3rd class passenger. Let's investigate that a bit more:"
"Ok, now from here it looks more like ""S"" is the interesting port since survival is less probably for that one if you are a 3rd class passenger. Otherwise  there is no significant difference within each class.\n\nThere seems to be some impact here that isn't captured by the passenger class. What about the other strong feature, Sex?"
"Now this is somewhat expected since it explains the difference between ""S"" and the other ports. Therefore, it seems that between more 1st class passengers embarking at ""C"" and more men at ""S"" there doesn't seem to be much actual influence in the port of embarkation.\n\nHowever, the last plot should also indicate that ..."
# 4. Evaluate the model\n## 4.1 Training and validation curves
## 4.2 Confusion matrix
Confusion matrix can be very helpfull to see your model drawbacks.\n\nI plot the confusion matrix of the validation results.
# 5. Prediction and submition
## 5.1 Prediction validation results
## 5.2 Submition
"The **ROC curve**. In a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold.\n\nAs the area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test, the areas under ROC curves are used to compare the usefulness of tests. The term ROC stands for Receiver Operating Characteristic."
## I hope this kernel is helpfull for you -->> upvote will appreciate me for further work.
"\n\n4.5 MASK SIZES/AREAS\n\n---\n\nWe know that every other number in an RLE encoding represents a run of mask... so if we add up all those numbers we get the total number of masked pixels in an image. This is much faster than opening and closing each image.\n\nOBSERVATIONS\n\n* It's observable that the distributions of mask area is mostly normal although it skews slightly to the smaller side...\n* All the distributions are similar although the Stomach distribution has an odd gap between 400-750 pixels.\n* It's interesting to note that, while not common, we do have some VERY large masks (>7500 pixels)\n    * Also, it's kind of funny that the biggest masks are for **small** bowel\n    "
"\n\n4.6 MASK DATASET CREATION, CLASS OVERLAP & MASK HEATMAP\n\n---\n\nIt's important to determine if the the masks overlap one another (**multilabel**) or not (**multiclass**). To do this, we will quickly create a dataset of **`npy`** files. During this creation process we will check for overlap.\n\nOBSERVATIONS\n\n* There is overlap, and while it is not that common, some images exhibit a high degree of overlap.\n* This means that we cannot frame the problem as simple categorical semantic segmentation.\n* We must instead frame the problem as multi-label semantic segmentation\n* This means our mask will take the form --> $W \times H \times 3$\n    * Where the channel dimensions are binary masks for each respective segmentation type\n    * This will allow for the masks to overlap\n\n\n**NOTE ON THE PLOTTED IMAGE BELOW:**\n* In the examined image below we can see a section of the small bowel is completely inside of a larger section of larger bowel.\n* This shows why treating this as multi-label semantic segmentation is so important!"
Plotting the duplicate Images
"We have other methods to find duplicates that will help us in identifying more soft duplicates if any in the dataset , that will come in later versions of this kernel"
### Industry and profession
"I know that many people has shown this graph, but still let's look at it again. I think that most of these titles can be joined into several groups:\n- Students can be a separate group,\n- Let's leave DS also by themselves;\n- People in research;\n- Next we have analysts - DA, BA and others, who need a different set of skills, but could be considered a level before DS;\n- DE and SE who build production systems;\n- Managers to lead the products;\n- And others;\n\nThis grouping is arbitrate and could be wrong, but let's see what will be the result."
Now we see that the number of DE and DS is almost equal and the number of DA isn't far behind.\nIt is worth noticing that different companies could have very different titles. For example in Facebook DS could work as DA; in some companies situation could be opposite.
### Years of learning ML vs self-confidence
"Quite interesting. Usually people with 2 or more years of ML-experience are confident that they are DS, but in Russia people with 1-2 or even less that 1 year of experience consider themselves to be DS."
"\n Data augmentation is a technique through which one can increase the size of the data for the training of the model without adding the new data. Techniques like padding, cropping, rotating, and flipping are the most common methods that are used over the images to increase the data size. \n\nThere are six different augumentations/transformations implemented in this notebook :\n* Random Brighness\n* Random Contrast \n* Random Saturation\n* Random Crop or Pad\n* Random Rotate\n* Sharpness\n"
#### Difference between normal tf.image functions and stateless ones:\n* There are two sets of random image operations: `tf.image.random*` and `tf.image.stateless_random*`. \n\n* Using `tf.image.random*` operations is strongly discouraged as they use the old RNGs from TF 1.x. (TensorFlow Website)
"# Confusion matrix\n\nLet's also take a look at the confusion matrix, this will give us an idea about what classes the model is mixing or having a hard time."
"# Visualize predictions\n\nFinally, it is a good practice to always inspect some of the model's prediction by looking at the data, this can give an idea if the model is getting some predictions wrong because the data is really hard, of if it is because the model is actually bad.\n\n\n### Class map\n```\n0: Cassava Bacterial Blight (CBB)\n1: Cassava Brown Streak Disease (CBSD)\n2: Cassava Green Mottle (CGM)\n3: Cassava Mosaic Disease (CMD)\n4: Healthy\n```\n\n\n## Train set"
Note that in the previous plot *iteration* is one iteration (or *minibatch*) of SGD. In one epoch there are \n(num_train_samples/num_iterations) of SGD.\n\nWe can see the plot of loss versus learning rate to see where our loss stops decreasing:
"The loss is still clearly improving at lr=1e-2 (0.01), so that's what we use. Note that the optimal learning rate can change as we train the model, so you may want to re-run this function from time to time."
## 10.2. Plotting simple liner regression
- It clearly underfits.
## 12.3. Ploting
- It still underfits.
## 13.3. Ploting
- A fair fit now.
"**Observations:**\n\n* trtbps and chol looks like they are normally distributed, with some outliers highly skewed towards right.\n* In case of thalachh the data is highly skewed towards right!"
**By the pair plot we can see data destribution and identfy outlier**
"Unfortunately matplotlib (the most common library for plotting in Python) doesn't come with a way to visualize a function, so we'll write something to do this ourselves:"
Here's what our function looks like:
**OBSERVATIONS**: \n* From the above plot we can see most of the questions had an explanation. Also we can see near right bottom some points in groups. This maybe because a large number of students took their test at the same time.\n* Another thing that we can notice is a faint blue line along the y-axis where x is 0. This is where the timestamp is 0 and there were no prior explanations.
Above plot tells us that most of the answers are correct
**OBSERVATIONS**\n* In the bottom right we can see that `starter` value in the `type_of` column has a range of 4-7 in part.\n* There are a lot of `lecture_id` with `intention` value in the `type_of` column
As we noted before `IDs` do not explain anything much
There is a lot of data in part 5 and 6 and a lot of tags are of the type concept
**OBSERVATIONS** - \n* There are no other parts for intention field other than part-2\n* The starter type lecture has only parts above 5th part\n* Most of the 5th part comes from concept lecture type
Maybe there are some null values. Let us check with a plot
okay so we have one missing value let us drop that row
Many interesting trends which we will discuss in detail down below...
"Not much can be seen from this plot as there is a lot of data. However, we can see as part increases the question_id value also increases. Interesting...."
We can see the same plot as above. Let us see if `question_id` and `bundle_id` columns are same or not
![](https://media.tenor.com/qoDOnF_uBYIAAAAC/office-motion-graphics.gif)
"\n    From the above Analysis, Most of the Kagglers are working as Data Scientist (1929 members ) and  Data Analyst(1538).\n     Almost 1435 Kagglers are Currently not employed.\n"
\n\n    \n        NOTEBOOK PRODUCTS USED BY KAGGLERS ON A REGULAR BASIS\n    \n    
\n\n    \n        IDE'S USED BY KAGGLERS ON A REGULAR BASIS\n    \n    
\n\n    \n        DATA VISUALIZATION LIBRARIES USED BY KAGGLERS ON A REGULAR BASIS\n    \n    
"The word **Love** is the most commonly used word in movie titles. **Girl**, **Day** and **Man** are also among the most commonly occuring words. I think this encapsulates the idea of the ubiquitious presence of romance in movies pretty well."
"**Life** is the most commonly used word in Movie titles. **One** and **Find** are also popular in Movie Blurbs. Together with **Love**, **Man** and **Girl**, these wordclouds give us a pretty good idea of the most popular themes present in movies. "
## 5.1 Project proposal is Approved or not ?
* Training data is highly imbalanced that is approx. 85 % projetcs were approved and 15 % project were not approved. Majority imbalanced class is positive.
### 5.2.a Distribution of School states
"* Out of 50 states, **California(CA)** having higher number of projects proposal submitted **approx. 14 %**  followed by **Texas(TX)(7 %)** and **Tennessee(NY)(7 %)**."
"### 5.2.b Distribution of project_grade_category (school grade levels (PreK-2, 3-5, 6-8, and 9-12))"
"* Out of 4 school grade levels, Project proposals submission in school grade levels is higher for **Grades Prek-2** which is approximately **41 %** followed by **Grades 3-5** which has approx. **34 %**."
### 5.2.c Distribution of category of the project
"* Out of 51 Project categories,  Project proposals submission for project categories is higher  for  **Literacy & Language** which is approx. **27 %** followed by **Math & Science** which has approx. **20 %**."
### 5.2.d Distribution of number of previously posted applications by the submitting teacher
### 5.2.e Distribution of subcategory of the project
"* Out of 1,82,020 Project subcategories, Project proposals submission for project sub-categoriesis is higher  for **Literacy** which is approx. **16 % ** followed by **Literacy & Mathematics** which has approx. **16 %** ."
### 5.2.f Distribution of Project titles
"* Out of 1,82,080 project titles, Project proposals submission for project titles is higher for **Flexible seating** which is approx. **27 %** followed by **Whiggle while your work** which has approx. **14 %**."
## 5.6 What is highest level of formal education of Developers?
"**About highest level of formal education of Developers :**\n  * Approx. 46 % Developers having Bachelor's degree\n  * Approx. 23 % Developers having Master's degree\n  * Approx. 12 % having some college/university study without earning a degree\n  * Approx. 9 % having Secondary schools(eg. American high school etc.)\n  * Approx 3 % having Associate degree\n  * Approx. 2 % having other doctral degree (Phd. , Ed.D etc.)\n  * Approx. 2 % having Primary/elementry school\n  * Approx. 2 % having Professional degree (JD, MD etc.)\n  * Approx. 1 % never completed any formal education."
## 5.7 Main field of study of Developers
"* **Main field of study of Developers :** Most of the Developers **(63.7 %)** having **Computer Science, Computer engineering or Software engineering** field of study followed by **8.79 %** Developers having **Another engineering discipline(ex. civil, electrical or mechanical)** followed by **8.23 %** Developers having **Information Systems, Information Tecnology or system administration**. Only Approx 1 % Developers never declared their field of study."
## 5.8 How many people are employed by the company or organization they work for?
"**Number of people are employed by the company or organization they work for :**\n  * ** Approx. 10 %** people work in company or organization having **Fewer than 10 employees**.\n  * ** Approx. 11 %** people work in company or organization having **10 to 19 employees**.\n  * ** Approx. 24 %** people work in company or organization having **22 to 99 employees**.\n  * ** Approx. 20 %** people work in company or organization having **100 to 499 employees**.\n  * ** Approx. 7 %** people work in company or organization having **500 to 999 employees**.\n  * ** Approx. 11 %** people work in company or organization having **1,000 to 4,999 employees**.\n  * ** Approx. 4 %** people work in company or organization having **5,000 to 9,999 employees**.\n  * ** Approx. 14 %** people work in company or organization having **10,000 or employees**."
## 5.9 Description of people who participated in the survey
**Description of peoples who participated in the survey :**\n  * Approx. **19 %** peoples called themselves **Back-end developers**.\n  * Approx. **16 %** peoples called themselves **Full-stack developers**.\n  * Approx. **13 %** peoples called themselves **Front-end developers**.\n  * Approx. **7 %** peoples called themselves **Mobile developer**.\n  * Approx. **6 %** peoples called themselves **Desktop or enterprise application developer**.\n  * Approx. **6 %** peoples called themselves **Student**.\n  * etc.
## 5.12 How easy or difficult was this survey to complete?
**People thought about difficulty of survey :**\n  * Approx. **37 %** peoples think survey was **Somewhat easy**.\n  * Approx. **33 %** peoples think survey was **Very easy**.\n  * Approx. **23 %** peoples think survey was **Neither easy nor difficult**.\n  * Approx. **6 %** peoples think survey was **Somewhat difficult**.\n  * Approx. **1 %** peoples think survey was **Very difficult**.
## 5.13 What peoples think about the length of the survey ?
**Peoples thinking about length of the survey :**\n  * Approx **50 %** peoples thinks **The survey was an appropriate length.**\n  * Approx. **49 %** peoples thinks **The survey was too long**.\n  * Approx. **1 %** peoples thinks **The survey was too short**.
## 5.14 for how many years have peoples been coding ?
**for how many years have peoples been coding :**\n  * 3-5 years : Approx. 25 % peoples\n  * 6-8 years : Approx. 21 % peoples\n  * 9-11 years : Approx. 13 % peopels\n  * 0-2 years : Approx. 11 % peoples\n  * etc.
## 5.15 For how many years have peoples coded professionally (as a part of your work)?
**For how many years have peoples coded professionally  :**\n  * 0-2 years : Approx. 30 % peoples\n  * 3-5 years : Approx. 27 % peoples\n  * 6-8 years : Approx. 15 % peoples\n  * 9-11 years : Approx. 10 % peoples\n  * etc.
## 5.16  Peoples hope to be doing in the next five years
**Peoples hope to be doing in the next five years :**\n  * ** Approx. 34 % peoples** : Working in a different or more specialized technical role than the one I'm in now.\n  * **Approx. 26 % peoples ** : Working as a founder or co-founder of my own company.\n  * **Approx. 19 % peoples** : Doing the same work.\n  * **Approx. 10 % peoples** : Working as a product manager or project manager.\n  * **Approx. 3 % peoples** : Working in a career completely unrelated to software development.\n  * **Approx. 2 % peoples** : Retirement
"It is very important to look at the response column, which holds the information, which we are going to predict. In our case we should look at 'deposit' column and compare its values to other columns. \n First of all we should look at the number of 'yes' and 'no' values in the response column 'deposit'."
"On the diagram we see that counts for 'yes' and 'no' values for 'deposit' are close, so we can use accuracy as a metric for a model, which predicts the campaign outcome."
> Let's create a kernel density estimation (KDE) plot colored by the value of the target. A kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It will allow us to identify if there is a correlation between the Age of the Client and their ability to pay it back.
#### Education
"This value is safely below the point where pressure relief valve opens (at 70 cmH20) in order to prevent excessive pressures in the lung, thus reducing any barotrauma risk.\n\nThe pressures in the training data have the following distribution"
with a median value of 
"Note however that in this competition the expiratory phase is not scored, so for practical purposes we are only really interested in the pressure for `u_out=0`, *i.e.* the first second of the experiments:"
"We have nine combinations of experiments; `C` can be 10, 20 or 50, and `R` can be 5, 20 or 50. Lets take a quick look at an example of each"
# Positive end-expiratory pressure (PEEP)\nIt is worth noting that even before the experiments start (*i.e.* the `time_step=0` and `u_in=0`) there is a positive pressure in the airway. The system is maintained above atmospheric pressure to promote gas exchange to the lungs.
The average value of PEEP at the beginning of each cycle is
Both of these breaths have a somewhat unusual aspect
"Note that all of the instances of negative pressure occur only in the `R=50` (high restriction) with `C=10` (thick latex) systems.\n# Simple feature engineering\nWe shall add a new feature, which is the [cumulative sum](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cumsum.html) of the `u_in` feature:"
"The thinking behind this feature is that it is reasonable to assume the pressure in the lungs is approximately proportional to how much air has actually been pumped into them. It goes almost without saying that this feature is not useful when breathing out, but given that the expiratory phase is not scored in this competition this should not be too much of a problem.\n\n### Shifting `u_in`\nLet us take a look at the first second of `breath_id=928`, which is an excellent example of an oscillatory experiment"
"It can be observed that there is a lag between `u_in` and the resulting `pressure` of around 0.1 seconds. I am sure it is with this in mind that [Chun Fu](https://www.kaggle.com/patrick0302) wrote his excellent notebook [""*Add lag u_in as new feat*""](https://www.kaggle.com/patrick0302/add-lag-u-in-as-new-feat/notebook), which introduces a new *shifted* `u_in` feature. Here we shall use a shift of 2 rather than his original shift of 1, which is now more in line with the delay seen:"
"> **📌 Points to note :**\n* The education status of the females is impressive with the majority (~46%)having a Master's degree followed closely by a Bachelor's degree(27%). There are also 16% PhDs who answered the survey. \n* The analysis also reveals that there is a certain proportion who have had no formal education past high school. In spite of this, they took the survey which in itself is a commendable thing. They should be encouraged to complete their education either full time or through part-time courses.\n\n## 4.2 Educational qualifications of the female respondents, country wise"
"> **📌 Points to note :**\n* The U.S has maximum number of women with a Master's and Doctoral degrees followed closely by India. However, It should be kept in mind that a lot of women in India and other countries, generally move to the U.S for their Masters and PhDs.\n* India also tops the list with the maximum number of Bachelor degrees. This is pretty obvious since the majority of women respondents are students in their 20s.\n* There is a general predominance of Master's over other degrees, among all the countries except for Japan which has a higher incidence of a Professional degree."
# 5. Professional Experience\n![](https://cdn-images-1.medium.com/max/800/1*Ncg-xZhA-ZOM9InqStVm6w.jpeg)\n*PC: www.freepik.com *\n    \nWe already know that most of the female respondents comprising mainly of a young population having mostly students. Let's now see the various professional roles that females occupy in the industry.\n\n\n## 5.1 Female respondents' roles over the years
"> **📌 Points to note :**\n* Data Scientist seems to be the principal role for the female respondents since 2017 followed by a Data Analyst. Other roles like Developers, researchers, and project managers can also be seen in the population.\n\n## 5.2 Top 20 roles for female respondents in 2019"
"> **📌 Points to note :**\n* If we exclude students from the result, Data Scientists(~19.5%) form the chunk of the population who took the survey. This is closely followed by women in the Data Analyst role(~11%). \n* Interestingly, there are also women who are not employed but have responded to the survey. These women could not be working by choice or may be looking for jobs. We could connect to these women to understand if they are willing to work and could assist them in the same.\n\n## 5.3 Female respondents' Current Roles country wise in 2019\nLet's combine some of the roles to create broader groups. For instance, Data Engineer and DBA/Database Engineer can be clubbed together. Similarly, Data Analyst and Business Analyst can also be merged. Let's then see the distribution geographically.    "
"> **📌 Points to note :**\n* Again, leaving out the students, U.S has the maximum number of Data Scientists who took part in the survey, followed by India.\n* The U.S also has the maximum number of Data Analysts participating in the survey, again followed by India.\n* India has the number of Software Engineers participating in the survey.\n* The percentage of unemployed females respondents(~<2%) is also high in India.\n\n## 5.4 Percentage of Female Data Scientists over the years "
"# 6. Salary\n![](https://cdn-images-1.medium.com/max/800/1*8qpREVGhO0noT07tqE0C2g.png)\n*PC: www.freepik.com*\n    \nEven though some [researches](http://www.timothy-judge.com/Judge,%20Piccolo,%20Podsakoff,%20et%20al.%20(JVB%202010).pdf) say otherwise, salary is a big motivational factor in retaining and acquiring new talent. Let's see how well our ladies are paid in the Data Science space.\n\n## 6.1 Salary Range of Female respondents in 2019\nWe shall first analyze the general trend of the salary of the female respondents in 2019. For the responses, which were blank I have included them in the 'Didnot Disclose' category."
"> **📌 Points to note :**\n* The majority of female respondents did not wish to disclose their annual salary. Of the remaining, (~10%) have an annual salary of fewer than 1000 dollars. This makes sense since a major chunk of that population is students who may currently not be having permanent jobs. \n* The next common salary range is (1k–2k) dollars and (10k-15k) dollars. It appears that the salary range of females in 2019 is highly varied.\n* There is also a tiny percentage of females who make more than 200k and 300k dollars a year.\n\n## 6.2 Comparison of Salaries of Female respondents in 2018 and 2019\n\nLet's see if this pattern of salaries is exclusive to 2019 only or is it a recurring phenomenon. Let's compare it with the 2018 Salary range. For this, we shall do some preprocessing of the data so that we get a common salary range for both the years. \n*Note that I did not include the 2017 salary data since it had more than 10 different currencies.*"
"> **📌 Points to note :**\n* The general pattern amongst the salary distribution appears to be the same in 2018 and 2019.\n* The percentage of females earning less than 10k USD in 2019 is less as compared to last year. \n* For all the other salary ranges, annual compensation in 2019 is marginally better than it was in 2018, which is good.\n* Another important point is that unlike in 2018, 2019 does have some females who earn more than 500k USD.\n\n## 6.3 Comparison of Male and Female salaries in 2019.\nWe saw things were slightly better in 2019 as compared to 2018. Let's now see compare the salaries genderwise.\n"
## 6.5 A look at Salaries of Female Data Scientists worldwide
"> **📌 Points to note :**\n* There are definitely salary differences between the U.S and other countries. Data Scientists in U.S are paid relatively higher than in other countries.\n* The majority of Female Data Scientists in the U.S earn between 100–200k USD, while a lot of Data Scientists in India earn less than 1000 dollars a year. Indian Data Scientists are highly underpaid as compared to other countries.\n* The proportion of Data Scientists earning more than 100k annually is also higher than in her countries."
`2` Now let's visualize the correlations:
"Unfortunately, there is __no correlation between our features and the injury.__"
"Below is our code for wdecay, the code is pretty intuitive to understand and does exactly what we described above. This will be a wrapper around our optimizer."
Now we create our optimizer with simple grouped param intialization and optimizer params as defined above.
"Stochastic Weight Averaging\n\nIntroduction\n\nSnapshot ensembling is a technique where we take weights snapshot while training the same network and then after training create an ensemble of nets with the same architecture but different weights. This allows to improve test performance, and it is a very cheap way too because you just train one model once, just saving weights from time to time.\n\nIn SWA (Stochastic Weight Averaging) the authors propose to use a novel ensembling in the weights space. This method produces an ensemble by combining weights of the same network at different stages of training and then uses this model with combined weights to make predictions. There are 2 benefits from this approach:\n - when combining weights, we still get one model at the end, which speeds up predictions\n - it can be applied to any architecture and data set and shows good result in all of them.\n \nIdea\n![swa](https://miro.medium.com/max/1766/1*_USiR_z8PKaDuIcAs9xomw.png)\n\nIntuition for SWA comes from empirical observation that local minima at the end of each learning rate cycle tend to accumulate at the border of areas on loss surface where loss value is low (points W1, W2 and W3 are at the border of the red area of low loss in the left panel of figure above). \nBy taking the average of several such points, it is possible to achieve a wide, generalizable solution with even lower loss (Wswa in the left panel of the figure above).\n\nHere is how it works. Instead of an ensemble of many models, you only need two models:\n - the first model that stores the running average of model weights (w_swa in the formula). This will be the final model after the end of the training which will be used for predictions.\n - the second model (w in the formula) that will be traversing the weight space, exploring it by using a cyclical learning rate schedule.\n \n![swa2](https://miro.medium.com/max/502/1*Afu2bqxzC6p1BpIRTDWJtg.png)\n \nAt the end of each learning rate cycle, the current weights of the second model will be used to update the weight of the running average model by taking weighted mean between the old running average weights and the new set of weights from the second model (formula provided in the figure on the left). \n By following this approach, you only need to train one model, and store only two models in memory during training. For prediction, you only need the running average model and predicting on it is much faster than using ensemble described above, where you use many models to predict and then average results."
"Ending Notes\n\n- There are many more stable training strategies which I haven't covererd and which one do further research on,\n    - Early Stopping\n    - Training Iterations: Longer Fine-Tuning\n    - Transferring via an Intermediate Task - STILTs Training\n    - Weight initialization and data order\n    - Mixed Precision Training\n    \n- I will be sharing a FineTuning kernel with all of the above idea and results soon.\n\n- More comprehensive repository for learning and implementing Transformers for various tasks can be found [here](https://notebooks.quantumstat.com/), [here](https://huggingface.co/transformers/master/community.html#community-notebooks) and [here](https://huggingface.co/transformers/notebooks.html)  \n\n- I want to acknowledge once more that this kernel has code implementations from the potpourri of best papers out there on Stable and Robust Transformer Fine-Tuning Strategies.\n\n - [REVISITING FEW-SAMPLE BERT FINE-TUNING](https://arxiv.org/pdf/2006.05987.pdf)\n - [ON THE STABILITY OF FINE-TUNING BERT](https://arxiv.org/pdf/2006.04884.pdf)\n - [SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models](https://arxiv.org/pdf/1911.03437.pdf)\n - [Fine-Tuning Pretrained Language Models:Weight Initializations, Data Orders, and Early Stopping](https://arxiv.org/pdf/2002.06305.pdf)\n - [MIXOUT: EFFECTIVE REGULARIZATION TO FINETUNE LARGE-SCALE PRETRAINED LANGUAGE MODELS](https://arxiv.org/pdf/1909.11299.pdf)\n - [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/pdf/1905.05583.pdf)\n - [Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks](https://arxiv.org/pdf/1811.01088.pdf)\n \nThanks & Please Do Upvote!"
"Finally, select the area with the ""best"" values of $C$."
"Recall that these curves are called validation curves. Previously, we built them manually, but sklearn has special methods to construct these that we will use going forward."
# 4-Data Visualization\n**Numerical variables correlation with survival**
# Numerical variable: SibSp
"# The numerical features\n\nIf we plot histograms of the 175 numerical features, we see that they have all kinds of distributions:"
"**Insight:** Histograms with white space at the left or right end can indicate that the data contain outliers. We will have to deal with these outliers. But are these data really outliers? Maybe they are, but they could as well be legitimate traces of rare events. We do not know...\n"
"Let's look at B_19. All values are between 0 and 1.01. To get a high-resolution histogram, we spread it over eleven diagrams. The histogram show only rectangles of width 0.01, which indicate that the values are uniformly distributed in intervals of width 0.01, but every interval has another probability. This means that B_19 originally had some other range, but was scaled, rounded and got added some uniform noise by applying the following function:\n\n```\ndef anonymize(data):\n    data -= data.min()\n    data /= data.max()\n    data = data.round(2)\n    rng = np.random.default_rng()\n    data += rng.uniform(0, 0.01, len(data))\n    return data\n```\n"
"**Insight:** We don't care about the scaling, we cannot do anything against the rounding, but we should remove the artificial noise, e.g. by applying a function such as `x['B_19'] = x['B_19'].apply(lambda t: np.floor(t*100))`"
"## Plot Example \nTo verify the concept \nLet's plot \n* The original TS (take the length of the sliding window + the label size - 90 days + 28 days)\n* The first array of trainX - which is the first sequence\n* The first array of trainy , which is the first label "
"## Pytorch Tensors \nPytorch use tensors as the input to the model \nVariable is a wrapper to the tensor \nThis kernel is only a preliminary starter, \n\nSo I use the Variable wrapper\nA more common way is to train with batches and use the dataset class\nBut this is for later.\n\nIf you want to learn more about Tensors \nRead this tutorial \n\nhttps://pytorch.org/tutorials/beginner/former_torchies/tensor_tutorial.html"
"Since the validation dataset was used to tune hyperparameters (the number of iterations), I predict targets for the testing dataset which the model hasn't seen yet. The metrics reported here are accuracy, precision and recall. They all have sensible values which is also confirmed by the confusion matrix shown below."
"From the confusion matrix it is clear that the model tries to predict both classes and doesn't prefer one over the other due to their imbalance. The latter is a common mistake and if you see accuracy scores without the confusion matrix, be very skeptical about those results. If over-/undersampling isn't applied for imbalanced classes, the classifier will opt for the constant prediction in favor of the majority class. In this case the accuracy for this testing dataset will be quite high 0.909 (see below), although this classifier doesn't have any predictive power! This skewness in class predictions is very visible on the confusion matrix."
"# 4. Feature importances\n\nDuring the early draft of this project the analysis of feature importances helped me to realize that the [Lending Club dataset provided by Wendy Kan from Kaggle](https://www.kaggle.com/wendykan/lending-club-loan-data) was actually including features that aren't available for investors. So I decided to include this analysis here in case someone finds it useful.\n\nAmong all features I selected 10 with the largest importance values (see below). The top 3 features are `loan_amnt`, `mths_since_recent_inq` and `revol_util`. The importances of `mths_since_recent_inq` and `revol_util`, however, are quite close to each other and the rest of the features so this ranking might slightly change for a different train-test split."
In gradient boosting the importances of highly correlated features usually split between them. From the correlation heatmap (see below) the feature `revol_util` (top 3) is quite highly correlated with `bc_util` (top 5) which leads to the decreased importance of `revol_util`.
It is also useful to look at the distributions of the features to see how their values influence predictions.\n\nFrom the histogram for the feature `loan_amnt` (top 1) the loan is more likely to be returned (good loans) if the loan amount is lower. This makes sense because smaller loan amounts usually have smaller monthly installments that are easier to pay.
We can visual the latent space described by the model for used in unsupervised applciations. 
"Unlike many unsupervised autoencoder model, we get some kind of feature importances to the model without having to rely on model agnostic explainations or gradient-based explainations. "
We will be using this pretrained layer now for use in our next fine-tuning experiment. 
\n\n#### 5.2 |Pairplot
\n\nMulticollinearity detected !  \n    \nWe see here that there is correlation between some variables. And this is what we don't want. This problem is called 'multicollinearity'
As it can be seen there is not correlation between 'residual_sugar' and 'pH' variables
As it can be seen there is not correlation between 'alcohol' and 'pH' variables
"### What do we see when we observe the histograms above? 🤔\n\nHere we see the distributions of values of all variables. As it can be seen from the charts, the values of 'pH' and ""density"" variables are relatively normally distributed.\n\n1. Most of the values ​​of  the ""fixed_acidity"" variable are in the range of 7 - 8;\n\n2. Most of the values ​​of  the ""volatile_acidity"" variable are in the range of 0.4 - 0.7;\n\n3. Most values ​​of the ""citric_acid"" variable are in the range of 0.0 - 0.1;\n\n4. Most of the values ​​of  the ""residual_sugar"" variable are in the range of 1 - 2.5;\n\n5. Most of the values ​​of the ""chlorides"" variable are in the range of 0.085 - 0.15;\n\n6. Most values ​​of the ""free_sulfur_dioxide"" variable are in the range 0 - 15;\n\n7. Most values ​​of the ""total_sulfur_dioxide"" variable are in the range 0 - 30;\n\n8. Most of the values ​​of the ""density"" variable are in the range of 0.996 - 0.998;\n\n9. Most of the values ​​of the ""pH"" variable are in the range of 3.2 - 3.4;\n\n10. Most of the values ​​of the ""sulphates"" variable are in the range of 0.50 - 0.75;\n\n11. Most of the values ​​of the ""alcohol"" variable are in the range of 9 - 10;\n\n12. Most values ​​of the ""quality"" variable are 5 and 6."
\n\n#### 5.5 |Regplot
"**Attention** to the negative linear relationships between the ""alcohol"" - ""volatile acidity"" and ""alcohol"" - ""total_sulfur_dioxide"" variables."
"\n\n#### 5.6 |Hexagonal Binned Plot\n\n\nhexbin is a 2D histogram plot, in which the bins are hexagons and the color represents the number of data points within each bin."
\n\n#### 5.7 |Visualization with Plotly Express
------------------------------------------------\n## Parch ( Number of parents/children )
----------------------------------------------------------------------\n## SibSp ( Number of siblings/spouses )
-------------------------------------------------\n## FamilySize ( Derived variable )
 Observation:\n\nThe mortality rate is higher in the case of Mr. I think it will help with learning.
" Observation:\n* In the case of Mr, the number of survivors is small.\n* In the case of Mrs and Miss, there are many survivors.\n\nI think it will be helpful in judging survivors using this.\nHowever, it seems difficult to find the relationship between age and title from the above distributions. Therefore, it seems difficult to use this to fill in the missing values of age."
### 6.Session Start for Model training\n\n\n![](https://camo.githubusercontent.com/7491264fba17ff7eb3ec5cce2e0f8db3e58e1c7b/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f312d312d322e676966)
"### Other Related Notebook for learning\n- **Linear Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression.py)). Implement a Linear Regression with TensorFlow.\n- **Linear Regression (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression_eager_api.py)). Implement a Linear Regression using TensorFlow's Eager API.\n- **Logistic Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py)). Implement a Logistic Regression with TensorFlow.\n- **Logistic Regression (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression_eager_api.py)). Implement a Logistic Regression using TensorFlow's Eager API.\n- **Nearest Neighbor** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/nearest_neighbor.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/nearest_neighbor.py)). Implement Nearest Neighbor algorithm with TensorFlow.\n- **K-Means** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/kmeans.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/kmeans.py)). Build a K-Means classifier with TensorFlow.\n- **Random Forest** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/random_forest.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/random_forest.py)). Build a Random Forest classifier with TensorFlow.\n- **Gradient Boosted Decision Tree (GBDT)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/gradient_boosted_decision_tree.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/gradient_boosted_decision_tree.py)). Build a Gradient Boosted Decision Tree (GBDT) with TensorFlow.\n- **Word2Vec (Word Embedding)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/word2vec.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/word2vec.py)). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow."
### 1. Library
### 2.Data Generation
### 3.Plot data
### 4. Load data into placeholder
### 6. Model Result
## Save and reload\n---\n[**Go to Top**](#Tensorflow-Tutorial)
### 1. Load Library and Generate Data
### 2.Model Save
### 3. Model Reload
### 4.Model Save Loss and Reload Model Loss
### 1.Load Library
### 2.Define Parameter and Generate Data
As the `age` increases the `sprint speed` decreases
`Left Footed Players` vs `Right Footed Players`
`Crossing` vs `Dribbling`
Relation between `potential` and `age` with respected `value` of players
"**Lowest correlation** among the goalkeeping side with other columns and high among themselves \n\n**High correlation** between `Dribbling`, `Volleys`, `Passing` etc..."
"We will have comparisions for `Age`, `Overall`, `Potential`, `Accelaration`, `SprintSpeed`, `Agility` , `Stamina`, `Strength`"
"One can see how, by manipulating the threshold, we can catch more strokes. \n\nHowever, one needs to be careful with this approach. We could just change the threshold such that every patient is predicted to have a stroke so as not to miss any - but this helps no one.\n\nThe art is in finding the balance between 'hits' and 'misses'. \n\nF1 score is a decent starting point for this as it is the weighted average of several metrics.\n\nHere's a chart showing what I mean"
"# For completeness, I will try to optimize SVM also"
"## Loading image regions\n\nSimilar to OpenSlide, we can extract regions from the whole image. Because the image is already in memory, this boils down to a slice on the numpy array. To illustrate we use the same coordinates as in the OpenSlide example:"
"To load the same region from level 1, we have to devide the coordinates with the downsample factor (4 per level). This is different from Openslide that always works with coordinates from level 0."
"## 2. 3. Clustering\n\nI will be using 2 different methods for clustering these series. The first of the methods is Self Organizing Maps(SOM) and the other method is K-Means.\n\n### 2. 3. 1. SOM\n \nSelf-organizing maps are a type of neural network that is trained using unsupervised learning to produce a low-dimensional representation of the input space of the training samples, called a map.\n\n![SOM](https://raw.githubusercontent.com/izzettunc/Kohonen-SOM/master/data/screenshots/landing.png)\nSource : Github Repo: landing.png\n    \nAlso, self-organizing maps  differ from other artificial neural networks as they apply competitive(or cooperative) learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\n\n![Learning process of som](https://upload.wikimedia.org/wikipedia/commons/3/35/TrainSOM.gif)\nSource : Wiki Commons: TrainSOM.gif\n\nBecause of the ability to produce a map, som deemed as a method to do dimensionality reduction. But in our case, when each node of the som is accepted as medoids of the cluster, we can use it for clustering. To do so, we should remove our time indices from our time series, and instead of measured values of each date, we should accept them as different features and dimensions of a single data point.\n\nFor more info about some, you can check [this medium post](https://medium.com/@abhinavr8/self-organizing-maps-ff5853a118d4)."
"For the implementation of the som algorithm I used [miniSom](https://github.com/JustGlowing/minisom) and set my parameters as follows:\n- sigma: 0.3\n- learning_rate: 0.5\n- random weight initialization\n- 50.000 iteration\n- Map size: square root of the number of series\n\nAs a side note, I didn't optimize these parameters due to the simplicity of the dataset."
#### 2. 3. 1. 2. Cluster Distribution\nWe can see the distribution of the time series in clusters in the following chart.
"#### 2. 3. 1. 3. Cluster Mapping\n\n(Thank you for this wonderful question Stephen Tseng)\nWell, we did cluster our series but how de we know which series belonging to which cluster? Ain't that the whole purpose of clustering? \n\nAs we can see in [these illustrations](#2.-3.-1.-SOM) each node (or multiple of nodes in some cases) represents a cluster. Therefore we can find out which series is belonging to which cluster by checking the winner node of each series. "
"#### 2. 3. 2. 1. Results\n\nAfter the training, I plotted the results as I did with the som. For each cluster, I plotted every series, a little bit transparent and in gray, and in order to see the movement or the shape of the cluster, I took the average of the cluster and plotted that averaged series in red."
"As you can see from the plot below, k-means clustered the 23 different series into 5 clusters. 2 of the clusters contains only 1 time series which may be deemed as an outlier."
"As I did before, I used [DBA](https://github.com/fpetitjean/DBA) to see much more time dilated series."
"#### 2. 3. 2. 2. Cluster Distribution\n\nWe can see the distribution of the time series in clusters in the following chart. And it seems like k-means clustered 15 of the time series as cluster 1, which is a bit skewed. The reason why this happens is the most probably ```The Curse of Dimentionality``` You can check it out from the links that I provided at section 5 (See Also)\n"
"#### 2. 3. 2. 3. Cluster Mapping\n\nAs we did before, in this part we will be finding which series belonging to which cluster. Thanks to awesome scikit-learn library we actually already have that information. Order of the labels is the same order with our series.\n"
Transactions and Total Amount per day
Fraud Transactions by Date
# Feature Engineering
"Below is the code showing the progress of scores versus the iteration. For random search we do not expect to see a pattern, but for Bayesian optimization, we expect to see the scores increasing with the search as more promising hyperparameter values are tried."
"Sure enough, we see that the Bayesian hyperparameter optimization scores increase as the search continues. This shows that more promising values (at least on the cross validation reduced dataset) were tried as the search progressed. Random search does record a better score, but the results do not improve over the course of the search. In this case, it looks like if we were to continue searching with Bayesian optimization, we would eventually reach higher scores on the cross vadidation data. \n\nFor fun, we can make the same plot in Altair."
"Same chart, just in a different library for practice! \n\n## Learning Rate Distribution\n\nNext we can start plotting the distributions of hyperparameter values searched. We expect random search to align with the search domain, while the Bayesian hyperparameter optimization should tend to focus on more promising values, wherever those happen to be in the search domain.\n\nThe dashed vertical lines indicate the ""optimal"" value of the hyperparameter."
\nFare Column 
We can see that lot of zero values are there in Fare column so we will replace zero values with mean value of Fare column later. 
\nSibSp Column 
That's where we are wrong. You can't trust feature importances from XGBoost because they are inconsistent across different calculations. Watch how feature importances change with the calculation type:
"In contrast, feature importances obtained from Shapley values are consistent and trustworthy.\n\nWe won't also stop here. In the above plots, we only looked at absolute values of importance. We don't know which feature positively or negatively influences the model. Let's do that with SHAP `summary_plot`:"
\n\n    \n    \n# Q6 & Q7 Languages used for each range of age
"*It seems that Python and SQL are the most famous languages for most age ranges except for ages 18-22 where Python, C and C++ are the most common languages.*"
\n\n    \n    \n# Q8 Recommended First Programming language
*when asked about Recommended Programming language to learn first most people recommended Python as it is easy to learn and has alot of support and libraries*
\n\n    \n    \n# **Q9 Favourite IDEs**
"*While there are many Integrated Development environments, Jupyter is the most favored one.*****"
\n\n    \n    \n# **Q10 Most hosted notebooks**
"*As there are many developers prefer to use IDE's, there are others who prefer to code on Hosted Notebooks especially Google Colab and Kaggle Notebooks*"
"\n\n    \n    \n# Q24 Current yearly compensation\nAnnual compensation, in the simplest terms, is the combination of your base salary and the value of any financial benefits your employer provides ex: Annual bonuses or commissions, insurance and so on."
*It makes sense that most of beginners take lower compensations while other compensation rates varies much.*
\n\n    \n    \n# Q15 Years of using Machine learning methods
*As in the graph most Machine learning method users just started using them in the last 2 years.*
\n\n    \n    \n# Q16 Machine Learning Frameworks used
"*Scikit-learn, TensorFlow libraries and Keras are used the most while JAX and MXNet are the fewest*"
\n    \n    \n# Q17 Machine learning Algorithms used
*Ofcourse Linear and Logistic regressions are on the top followed by Decision Trees and Random Forests*
Below we are plotting heatmap showing nullity correlation between various columns of dataset.\n\nThe nullity correlation ranges from -1 to 1.\n\n* -1 - Exact Negative correlation represents that if the value of one variable is present then the value of other variables is definitely absent.\n* 0 - No correlation represents that variables values present or absent do not have any effect on one another.\n* 1 - Exact Positive correlation represents that if the value of one variable is present then the value of the other is definitely present.
"#  k-Nearest Neighbour Imputation  \n\n\nA fancy way of filling in the missing values would be to use a **k-nearest neighbour** method. You can select a sample with missing values and find the nearest\nneighbours utilising some kind of distance metric, for example, Euclidean distance. Then you can take the mean of all nearest neighbours and fill up the missing value. You can use the KNN imputer implementation for filling missing values like this.\n\n![image1](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png)\n\n\n[K-Nearest Neighbors (KNN) Algorithm for Machine Learning](https://medium.com/capital-one-tech/k-nearest-neighbors-knn-algorithm-for-machine-learning-e883219c8f26)\n\n\n### How it works?\n\nStep-1: Select the K number of the neighbors\n        \nlet say we select K = 5\n        \n![image2](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning3.png) \n\n\nStep-2: Calculate the Euclidean distance of K number of neighbors\n\nIn this step we search for those k = 5 neighbors having minimum Euclidean Distance from unknown data point\n\n\n![Image4](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning4.png)\n\n        \nStep-3: Among these k neighbors, count the number of the data points in each category.\n\n![image3](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning5.png)\n\n\nStep-4: Assign the new data points to that category for which the number of the neighbor is maximum.\n\nYou can also visit below given youtube video link to understand it bit nicely\n\n[Step-by-Step procedure of KNN Imputer for imputing missing values](https://www.youtube.com/watch?v=AHBHMQyD75U)"
"# Evaluation Metrics \n\nI think before selecting an optimal model for given data first we have to analayze target feature. Target Feature can be discrete in case of classification problem or continuous in case of Regression Problem\n\nIf we talk briefly about classification problems, the most common metrics used are:\n\n\n- **[Accuracy](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : It is one of the most straightforward metrics used in machine learning. It defines how accurate your model is. For example, if you build a model that classifies 90 samples accurately, your accuracy is 90% or 0.90. If only 83 samples are classified correctly, the accuracy of your model is 83% or 0.83. Simple.\n             \n     **[Scikit-learn user guide for accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)**\n     \n\n- **[Precision](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** :  Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good\n\n     **[True Positives (TP)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted  class is also yes. E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.\n\n     **[True Negatives (TN)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n\n     False positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n\n     **[False Positives (FP)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** – When actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.\n\n     **[False Negatives (FN)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** – When actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.\n\n     **[Scikit-learn user guide for Precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)**\n     \n     ![](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)     \n     \n\n- **[Recall(Sensitivity)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? We have got recall of 0.631 which is good for this model as it’s above 0.5.\n\n     **[Scikit-learn user guide for Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)**\n\n\n- **[Confusion Matrix](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/)** : A much better way to evaluate the performance of a classifier is to look at the confusion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion matrix.\n\n    **[Scikit-Learn user guide for Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)**\n     \n     \n- **[F1 score (F1)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. In our case, F1 score is 0.701. \n\n     **[Scikit-learn user guide for F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)**\n\n\n- **[Area under the ROC (Receiver Operating Characteristic) curve](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)** : AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease.\n\n     **[Scikit-learn user guide for AUC under the ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)**\n\n\n\n\n\n\n**When it comes to regression, the most commonly used [evaluation metrics](https://towardsdatascience.com/evaluation-metrics-model-selection-in-linear-regression-73c7573208be) are:**\n\n\n- Mean absolute error (MAE)\n- Mean squared error (MSE)\n- Root mean squared error (RMSE)\n- Root mean squared logarithmic error (RMSLE)\n- Mean percentage error (MPE)\n- Mean absolute percentage error (MAPE)\n- R2\n\n **[Scikit-learn user guide for regression evaluation metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)**"
"We see that the target is **skewed** and thus the best metric for this binary classification problem would be Area Under the ROC Curve (AUC). We can use precision and recall too, but AUC combines these two metrics. Thus, we will be using AUC to evaluate the model that we build on this dataset."
It's a high variance problem 
Let's try to increase data in balanced manner using Synthetic Minority Oversampling Technique (SMOTE) 
**1 ) OverallQual:**
**2 ) LotFrontage:**
**3 ) GrLivArea:**
seems that i deleted them before !!
**4 ) GarageArea:**
**5 ) LotArea:**
**6 ) Year Built:**
**7 ) TotalBsmtSF:**
**8 ) 1stFlrSF :**
\n        \n            Fix Features Skewness:\n        \n\n
## Prediction  
"## Validation \n\nA simple indicator of how accurate out forecast is is the root mean square error (RMSE). So lets get a baseline and then calcualte the RMSE for the one-step ahead predictions starting from 2015, through to the end of 2017.\n\n### Baseline \n\nThe naive forecast, where the observation from the previous time step is used as the prediction for the observation at the next time step, can be taken as a simple baseline."
"A better representation of longer-term predictive power can be obtained using dynamic forecasts. In this case, we only use information from the time series up to a certain point, and after that, forecasts are generated using values from previous forecasted time points.\n\nIn the code chunk below, we specify to start computing the dynamic forecasts and confidence intervals from mid 2017 onwards."
This is pretty bad but not surprising given how the prices started going insane around late 2017. Lets see how well the model does in a different time period when things were a little more normal.
"The results indicate that the model is still a little rough and not something we should use as trading advice, but that was not unexpected due to the extremely volatile nature of cryptocurrencies, especially in the last 6 months.\n\nIt is probably also not such a good idea to try and predict 6 months into the future as we can see how insane even the 80% confidence interval becomes out this far. Maybe sticking to 1 month advance predictitons is more sensible. Or maybe even predicting on a daily basis.\n\nThere are a number of things we could do to potentially improve the model. \n    - Use a different technique to standardise the distribution\n    - Use different differentiation techniques\n    - Discard the time period before cryptocurrencies started taking off\n    - Try modelling per day instead of month, with a smaller forecast window\n    - Use other models or machine learning instead of ARIMA\n"
We can graph this to look at the full set of 10 channels
"Let's look at the 0th image, predictions, and prediction array. "
Let's plot several images with their predictions. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percent (out of 100) for the predicted label. Note that it can be wrong even when very confident. 
"Finally, use the trained model to make a prediction about a single image. "
# Awesome Computer Vision CV Resources
"## Credits (Reference)\n\n> * [TensorFlow Docs - GitHub](https://github.com/tensorflow/docs)\n> * [GitHub Deep Learning Topic](https://github.com/topics/deep-learning)\n> * [Jiwon Kim](https://github.com/kjw0612/awesome-deep-vision)\n> * [Jia-Bin Huang](https://github.com/jbhuang0604/awesome-computer-vision)\n\n## License\n\n[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)\n\n### Please ***UPVOTE*** my kernel if you like it or wanna fork it.\n\n##### Feedback: If you have any ideas or you want any other content to be added to this curated list, please feel free to make any comments to make it better.\n#### I am open to have your *feedback* for improving this ***kernel***\n###### Hope you enjoyed this kernel!\n\n### Thanks for visiting my *Kernel* and please *UPVOTE* to stay connected and follow up the *further updates!*"
Trying to plot all the numerical features in a seaborn pairplot will take us too much time and will be hard to interpret. We can try to see if some variables are linked between each other and then explain their relation with common sense.
"A lot of features seems to be correlated between each other but some of them such as `YearBuild`/`GarageYrBlt` may just indicate a price inflation over the years. As for `1stFlrSF`/`TotalBsmtSF`, it is normal that the more the 1st floor is large (considering many houses have only 1 floor), the more the total basement will be large.\n\nNow for the ones which are less obvious we can see that:\n- There is a strong negative correlation between `BsmtUnfSF` (Unfinished square feet of basement area) and `BsmtFinSF2` (Type 2 finished square feet). There is a definition of unfinished square feet [here](http://www.homeadvisor.com/r/calculating-square-footage/) but as for a house of ""Type 2"", I can't tell what it really is.\n- `HalfBath`/`2ndFlrSF` is interesting and may indicate that people gives an importance of not having to rush downstairs in case of urgently having to go to the bathroom (I'll consider that when I'll buy myself a house uh...)\n\nThere is of course a lot more to discover but I can't really explain the rest of the features except the most obvious ones."
Let's look at their distribution.
"We can see that features such as `TotalBsmtSF`, `1stFlrSF`, `GrLivArea` have a big spread but I cannot tell what insights this information gives us"
And finally lets look at their distribution
"We can see that some categories are predominant for some features such as `Utilities`, `Heating`, `GarageCond`, `Functional`... These features may not be relevant for our predictive model"
"The best way of\nconfirming that the data contains enough information so that a ML algorithm \ncan make strong predictions, is to try and directly visualize the \ndifferences between fraudulent and genuine transactions. Motivated by this\nprinciple, I visualize these differences in several ways in the plots below."
\n##### 5. 1. Dispersion over time
**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.
"The target variable is right skewed.  As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n"
\n\n4.4 INVESTIGATE SEX\n\n---\n\nOBSERVATIONS\n* Women do not have a prostate and as such there are no prostate examples where gender is Female.\n* Kidney's seem skewed towards the Male gender\n* All other organ's appear evenly distributed\n\n
\n\n4.5 INVESTIGATE FTUS ACROSS DIFFERENT ORGANS\n\n---\n\nHere is a zoom sequence from the human body to the single-cell level for the kidney. \n* **Notice that FTU is one level higher than cells.**\n\n\n\n\n\nFTUs in the organs presented in this competition are: \n* **glomeruli** in the **kidney** – (a)\n* **crypt** in the **large intestine**  – (b)\n* **alveolus** in the **lung**  – (c)\n* **glandular acinus** in the **prostate**  – (d)\n* **white pulp** in the **spleen**  – (e)\n\n\n\n\n\nOBSERVATIONS\n* TBD
\n\n4.6 AVERAGE FTU SIZE (BOUNDING-RECT) BY ORGAN\n\n---\n\nSince it would take too much processing to comb through all the available images and respective FTUs we will look at the previously obtained sampling of crops (bounding-rects) and the respective dimensions therewithin.\n\nAs we know all the HPA images have a pixel resolution (size) of 0.4µm we can therefore determine the real dimensions of the bounding rectangles around each FTU.\n\nNote dimensions are represented as (HEIGHTxWIDTH) unless otherwise communicated\n\nOBSERVATIONS\n* **Kidney**\n    * **Actual** FTU size is on-average SQUARE (most images aren't square but it averages as such) at **~157x155µm**\n    * **HPA** FTU size in pixels will be **~392x389px**\n    * **HuBMAP** FTU size in pixels will be **~314x310px**\n* **Large Intestine**\n    * **Actual** FTU size is on-average RECTANGULAR (1Hx2W) (images are mostly horizontal but occasionally vertical or square) at **~60x131µm**\n    * **HPA** FTU size in pixels will be **~150x328px**\n    * **HuBMAP** FTU size in pixels will be **~262x572px**\n* **Lung**\n    * FTUs are SMALL!\n    * **Actual** FTU size is on-average SQUARE (images aren't square but average to mostly square) at **~90x80µm**\n    * **HPA** FTU size in pixels will be **~225x200px**\n    * **HuBMAP** FTU size in pixels will be **~119x106px**\n* **Prostate**\n    * **Actual** FTU size is on-average SQUARE (images aren't square but average to mostly square) at **~68x77µm**\n    * **HPA** FTU size in pixels will be **~172x193px**\n    * **HuBMAP** FTU size in pixels will be **~11x13px**\n* **Spleen**\n    * **Actual** FTU size is on-average RECTANGULAR (2Hx1W) at **~407x197µm**\n    * **HPA** FTU size in pixels will be **~1018x493px**\n    * **HuBMAP** FTU size in pixels will be **~823x399px**\n\n
"\n\n4.8 WHY DO WE HAVE JSON FILES AND RLE?\n\n---\n\nOBSERVATIONS\n* The RLE is the entire binary mask as a simple compressed string\n* The JSON contains the description of the masks as a collection (list) of polygons representing the respective masks.\n    * A polygon is represented as a collection (list) of vertices.\n    * A vertex is a point represented by a pair of values –– the x,y position\n    * If you took these polygons and plotted them alongside the RLE decoded mask, they SHOULD be the same. I think it's simply another representation for us to use.\n\nNOTE: The RLE is more accurate than the Polygon Mask and should be used.\n\n\n\n"
**VISUALIZE DAB STAIN AND HEMATOXYLIN COUNTERSTAIN**\n* DAB is brown and present at locations of enzymatic activity\n* HEMATOXYLIN is light blue when found within cells\n* HEMATOXYLIN is dark blue when found within nuclei
\n\n\n\n\n\n\n    5  MODELLING    ⤒\n\n\n---
\n\n5.1 ALL ONES WITH SOME INTELLIGENCE\n\n---\n\nWe simply submit a mask made up of ones in a circular shape in the centre of the respective image.\n\n
\n\n\n\n\n\n\n    9  APPENDIX    ⤒\n\n\n---\n\nThis section is mostly if not entirely copied from various sources listed below:\n* Yashvardhan Jain's Background Info Notebook
"### Correlating numerical and ordinal features\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\n**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training."
"### Correlating categorical features\n\nNow we can correlate categorical features with our solution goal.\n\n**Observations.**\n\n- Female passengers had much better survival rate than males. Confirms classifying (#1).\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions.**\n\n- Add Sex feature to model training.\n- Complete and add Embarked feature to model training."
"### Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\n**Observations.**\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n- Consider banding Fare feature."
![](https://pics.conservativememes.com/exit-titanic-uextremopolis-the-lceberg-heave-ho-the-ship-is-63789220.png)
"### Completing a numerical continuous feature\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).\n\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2."
Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.
Sexwise distribution of skin cancer type
"# Step 6: Loading and resizing of images\nIn this step images will be loaded into the column named image from the image path from the image folder. We also resize the images as the original dimension of images are 450 x 600 x3 which TensorFlow can't handle, so that's why we resize it into 100 x 75. As this step resize all the 10015 images dimensions into 100x 75 so be patient it will take some time."
## Distribution of ord_5 features
"Cool!\nWe can see that many values in ord_5 have ~2100 entries; \nAlso, the data has many category's with ~100 entries; "
## Compensation By Job Title
Operations Research Practitioner has the highest median salary followed by Predictive Modeler and Data Scientist. Computer Scientist and Programmers have the lowest compensation.
## Machine Learning
"It is evident that most of the respondents are working with Supervised Learning, and Logistic Regression being the favorite among them."
"It is evident that the next year is going to see a jump in number of **Deep Learning** practitioners. Deep Learning and neural nets or in short AI is a favorite hot-topic for the next Year. Also in terms of Tools, Python is preferred more over R. Big Data Tools like Spark and Hadoop also have a good share in the coming years."
## Best Platforms to Learn
"My personal Kaggle, is the most sought after source for learning Data Science."
## Hardware Used
"Since majority of the respondents fall in the age category below 25, which is where a majority of students fall under, thus a basic Laptop is the most commonly used machine for work."
## Where Do I get Datasets From??
"With hundreds of Dataset available, Kaggle is the most sought after source for datasets."
## Code Sharing
## Challenges in Data Science
"The main challenge in Data Science is **getting the proper Data**. The graph clearly shows that dirty data is the bigget challenge. Now what is dirty data?? Dirty data is a database record that contains errors. Dirty data can be caused by a number of factors including duplicate records, incomplete or outdated data, and the improper parsing of record fields from disparate systems. Luckily Kaggle datasets are pretty clean and standardised.\n\nSome other major challenges are the **Lack of Data Science and machine learning talent, difficulty in getting data and lack of tools**. Thats why Data Science is the sexiest job in 21st century.With the increasing amount of data, this demand will substantially grow."
## Job Satisfaction
"Data Scientists and Machine Learning engineers are the most satisfied people(who won't be happy with so much money), while Programmers have the lowest job satisfaction.\n\n## Job Satisfication By Country"
### Recommended Language For Begineers
Clearly Python is the recommended language for begineers. The reason for this maybe due to its simple english-like syntax and general purpose functionality.
## Visualising using Lineplots
It appears that Maruti had a more or less steady increase in its stock price over the from 2004 to the mid of 2018 window.There appears to be some drop in 2019 though.let's further analyse the data for the year 2018. 
We see that there was a dip in the stock prices particularly around end of October and November. Let's zoom in on these dates
So there is a dip in stock prices around the last week of october and first week of November. One could investigate it further by finding out if there was some special event that occured on that day.\n
We can also use time sampling to plot charts for specific columns.
"The above bar plot corresponds to Maruti’s VWAP at year-end for each year in our data set. \n\nSimilarly, year start mean VWAP can be found below. "
# Time Shifting
"\n# Rolling windows\n\nTime series data can be noisy due to high fluctuations in the market. As a result, it becomes difficult to gauge a trend or pattern in the data. Here is a visualization of the Amazon’s adjusted close price over the years where we can see such noise:  "
"As we’re looking at daily data, there’s quite a bit of noise present. It would be nice if we could average this out by a week, which is where a rolling mean comes in. A rolling mean, or moving average, is a transformation method which helps average out noise from data. It works by simply splitting and aggregating the data into windows according to function, such as `mean()`, `median()`, `count()`, etc. For this example, we’ll use a rolling mean for 7 days."
### Correlation Matrix
### Observations:\n\n1)All the features look to be uncorrelated. So we cannot eliminate any features just by looking at the correlation matrix.
"Amazing, but let' breakdown on what we can see from this plot. First, we can see that our plot consists of 3 subplots - that is the power of using catplot; with such output, we can easily proceed with comparing distributions among interesting attributes. Y and X axes stay exactly the same for each subplot, Y-axis represents a count of observations and X-axis observations we want to count. However, there are 2 more important elements: column and hue; those 2 differentiate subplots. After we specify the column and determined hue we are able to observe and compare our Y and X axes among specified column as well as color-coded. So, what do we learn from this? The observation that is definitely contrasted the most is that 'Shared room' type Airbnb listing is barely available among 10 most listing-populated neighborhoods. Then, we can see that for these 10 neighborhoods only 2 boroughs are represented: Manhattan and Brooklyn; that was somewhat expected as Manhattan and Brooklyn are one of the most traveled destinations, therefore would have the most listing availability. We can also observe that Bedford-Stuyvesant and Williamsburg are the most popular for Manhattan borough, and Harlem for Brooklyn."
"Good, scatterplot worked just fine to output our latitude and longitude points. However, it would be nice to have a map bellow for fully immersive heatmap in ourcase - let's see what we can do!"
"Fantastic! After scaling our image the best we can, we observe that we end up with a very immersive heatmap. Using latitude and longitude points were able to visualize all NYC listings. Also, we added a color-coded range for each point on the map based on the price of the listing. However, it is important to note that we had to drop some extremely high values as they are treated as outliers for our analysis. "
`Now SalePrice is normally distributed`
"_As we can see, the multicollinearity still exists in various features. However, we will keep them for now and let the models(e.g. Regularization models such as Lasso, Ridge) do the clean up later on. Let's go through some of the correlations that still exists._\n\n* There is 0.83 or 83% correlation between **GarageYrBlt** and **YearBuilt**. \n* 83% correlation between **TotRmsAbvGrd** and **GrLivArea**. \n* 89% correlation between **GarageCars** and **GarageArea**. \n* Similarly many other features such as**BsmtUnfSF**, **FullBath** have good correlation with other independent feature."
### We using PysparkDataFrame.na.fill() to fill a value to specific column
"#### After handling missing values, we do some simple feature engineering\n#### in Feature engineering, we can use Pyspark multiple condition with syntax: ""When otherwise""\n#### To learn more about multiple condition in pyspark, you can visit at https://sparkbyexamples.com/spark/spark-case-when-otherwise-example/"
### Visualizing AUC metrics
## Random Forest
"## Gradient Boosted Tree\n\nIf you want to run the below part, just uncomment it. I dit it in order to reduce the time of committing the kernel."
I hope you find this notebook beneficial and enjoyable
## Feature importance plot
"The top 4 feature are V17, V14, V12, V10 corresponds to 75% of total. \n\nAlso the f2 score that is the median of recall and precision are on a considerably value"
The 25 countries with the highest average data scientist salary in the map chart can be seen as a bar chart below.
"I compared the salary status of countries in the chart below. This countries are the top 6 countries with more data scientist participants in the survey (see above ""Country Distribution of Data Scientist by Gender"") There are only two countries where data scientists earn more than other professions. These are United States and Russia. It seems that the two rival have understood the power of the artificial intelligence."
"In the following chart i showed the correlation between salary and spent on machine learning and/or cloud computing products. If you choose 0 and 100.000 $ (the first and the last) on spent legend, you can see clearly that the more money data scientists earn, the more spend to ML products."
"In the United States, most data scientist salaries range between 100k and 200k meanwhile in India between 5k and 60k."
"In the following chart, I have showed each data scientist salary group with the percentage distribution of education level in the United States."
### Correlation Heatmap for feature means
### Box Plot for Genres Distributions
### Principal Component Analysis - to visualize possible groups of genres\n\n1. Normalization\n2. PCA\n3. The Scatter Plot
### XGBoost is the winner - 90% accuracy\n\n* create the final model\n* compute confusion matrix\n* Compute Feature Importance
### Feature Importance
### Distribution of Amount w.r.t Class
"**Highlights**\n\nMost the transaction amount falls between 0 and about 3000 and we have some outliers for really big amount transactions and it may actually make sense to drop those outliers in our analysis if they are just a few points that are very extreme. Also, we should be conscious about that these **outliers should not be the fraudulent transaction**. Generally, fraudulent transactions of the very big amount and removing them from the data can make the predicting model bais. \n\nSo we can essentially build a model that realistically predicts transaction as fraud without affected by outliers. It may not be really useful to actually have our model train on these extreme outliers."
## Distribution of Time
"**Highlights**\n\nBy seeing the graph, we can see there are two peaks in the graph and even there are some local peaks. We can think of these as the time of the day like the peak is the day time when most people do the transactions and the depth is the night time when most people just sleeps. We already know that data contains a credit card transaction for only two days, so there are two peaks for day time and one depth for one night time."
### Distribution of transaction type w.r.t amount
## Categorical vs Continuous Features\n\nFinding unique values for each column to understand which column is categorical and which one is Continuous
"## Plot Lags\nLet's Plot our lags \nit is a bit hard to see the small lags (as the Time Series containing few years), but for the longer lags \nsuch as 365, we can see the shift ..."
"## Rolling windows \nFor rolling windows, we will use mean and std (standard deviation)"
### 3.3.1 Extract titles from passenger names\nTitles reflect social status and may predict survival probability\n\n*Select the cell below and run it by pressing the play button.*
### 3.3.2 Extract Cabin category information from the Cabin number\n\n*Select the cell below and run it by pressing the play button.*
### 3.3.3 Extract ticket class from ticket number\n\n*Select the cell below and run it by pressing the play button.*
### 3.3.4 Create family size and category for family size\nThe two variables *Parch* and *SibSp* are used to create the famiy size variable\n\n*Select the cell below and run it by pressing the play button.*
#### Visualize frequency distribution of `target` variable
"#### Interpretation\n\n\n- The above plot confirms the findings that -\n\n   - There are 165 patients suffering from heart disease, and \n   \n   - There are 138 patients who do not have any heart disease."
We can visualize the value counts of the `sex` variable wrt `target` as follows -
"#### Interpretation\n\n- We can see that the values of `target` variable are plotted wrt `sex` : (1 = male; 0 = female).\n\n- `target` variable also contains two integer values 1 and 0 : (1 = Presence of heart disease; 0 = Absence of heart disease)\n\n- The above plot confirms our findings that -\n\n    - Out of 96 females - 72 have heart disease and 24 do not have heart disease.\n\n    - Similarly, out of 207 males - 93 have heart disease and 114 do not have heart disease.\n"
"Alternatively, we can visualize the same information as follows :"
"#### Comment\n\n\n- The above plot segregate the values of `target` variable and plot on two different columns labelled as (sex = 0, sex = 1).\n\n- I think it is more convinient way of interpret the plots."
We can use a different color palette as follows :
We can use `plt.bar` keyword arguments for a different look :
#### Comment\n\n\n- I have visualize the `target` values distribution wrt `sex`. \n\n- We can follow the same principles and visualize the `target` values distribution wrt `fbs (fasting blood sugar)` and `exang (exercise induced angina)`.
\n## Correlation Between Sibsp -- Parch -- Age -- Fare -- Survived
Fare feature seems to have correlation with survived feature (0.26).
\n## SibSp -- Survived
"* Having a lot of SibSp have less chance to survive.\n* if sibsp == 0 or 1 or 2, passenger has more chance to survive\n* we can consider a new feature describing these categories."
\n## Parch -- Survived
* Sibsp and parch can be used for new feature extraction with th = 3\n* small familes have more chance to survive.\n* there is a std in survival of passenger with parch = 3
\n## Pclass -- Survived
\n## Age -- Survived
"* age <= 10 has a high survival rate,\n* oldest passengers (80) survived,\n* large number of 20 years old did not survive,\n* most passengers are in 15-35 age range,\n* use age feature in training\n* use age distribution for missing value of age"
\n## Pclass -- Survived -- Age
* pclass is important feature for model training.
\n## Embarked -- Sex -- Pclass -- Survived
* Female passengers have much better survival rate than males.\n* males have better survşval rate in pclass 3 in C.\n* embarked and sex will be used in training.
\n## Embarked -- Sex -- Fare -- Survived
* Passsengers who pay higher fare have better survival. Fare can be used as categorical for training.
"Other variable stars have very different light curve shapes from eclipsing binaries. Indeed, variable stars were the first example of astronomers using light curve shapes for classification. \n\nHere's a hand-drawn illustration of some of the different known types of variable stars from Popular Science Monthly in 1906 (page 179)."
"#### _Figure 10: The Atlas of Variable Stars, Credit: Popular Science Monthly, from 1906_"
Latent factor models compress user-item matrix into a low-dimensional representation in terms of latent factors. One advantage of using this approach is that instead of having a high dimensional matrix containing abundant number of missing values we will be dealing with a much smaller matrix in lower-dimensional space.  \nA reduced presentation could be utilized for either user-based or item-based neighborhood algorithms that are presented in the previous section. There are several advantages with this paradigm. It handles the sparsity of the original matrix better than memory based ones. Also comparing similarity on the resulting matrix is much more scalable especially in dealing with large sparse datasets.  
"An important decision is the number of factors to factor the user-item matrix. The higher the number of factors, the more precise is the factorization in the original matrix reconstructions. Therefore, if the model is allowed to  memorize too much details of the original matrix, it may not generalize well for data it was not trained on. Reducing the number of factors increases the model generalization."
Distribution of Standard Deviation 
Distribution of min values 
Distribution of max values 
 Distribution of Skewness 
 Distribution of Kurtosis 
 Correlation of Features 
## Step 3. Accuracy and precision the Multilayer Perceptron
## Step 4. Score for each one of the samples
"Instead of loading only one dataset into go.Figure(data=data, layout=layout), we simply need to load them into a list."
"To be able to select multiple data, we need to change the __layout__ to include __update_menus__ that will indicate which feature to load via the __visible__ argument, information which is passed through via the list, lst."
"PLOTTING A TIME SERIES SCATTER CLUSTER MAP\n\n\n- Let's look at a time series evolution of __maximum temperature__ for all regions where data is available using a cluster map.\n- We have quite a few datapoints all across Australia, let's focus our attention to the __Victoria__ when creating a model. "
"- In the above plot we can note some very __clear oscilations__ in maximum temperature, which have a tendency to occur periodically every decade, as well as a very small increase in temperature tendency.\n\nPLOTTING YEARLY MEAN OF MAX TEMPERATURE\n\n- Let's plot the __overall mean trend__ using all data sampling stations, from the current data, we can see that there is a very minimal increase in mean maximum temperature.\n- Geospatial maps often work well together with figure plots to more clearly show the animal dynamic on the map (especially for stationally points). Let's use groupby and evaluate the __mean value__ for all locations."
"GEOSPATIAL MAX TEMPERATURE INTERPOLATION\n\n\nNext, let's use Kriging to predict maximum temperature in different regions of __Victoria__, we'll need to load the location data first as well. "
"### Learn Python. No excuses.\n\nI visualized the top 50 technologies by percent of users familiar with the technology in question (Supply) and percent of job requirements mentioning it (Demand). Technologies which appear in both lists are connected with gray ribbons.\nThe gray-colored skills are the ones missing from either the Supply or the Demand column. \n\nI decided not to include some of the terms which I found too general, but nonetheless these concepts are among the top 50 most important things required by employers. In order of importance: `Machine Learning`, `Big Data`, `Cloud`, `Sys Admin`, `Agile`, `Algorithm`, `NoSQL`, `Database`, `Data Science`, `REST`, `Deep Learning`, `Artificial Intelligence`, `Web-Services`, `Testing`, `Computer Vision`, `QA`, `Security`, `Automation`, `Design`, `Microservices`, `DevOps`, `Data Warehouse`, `NLP`, `Statistics`, `ETL`, `Data`, `Neural Network`, `Time Series`, `Data Modeling`, `UI`, `JSON`, `Apache`, `CI` and `API`.\n"
"#### Skill importance by role\nThe importance of each skill varies between the roles, so while it's useful to have an overview, breaking down the supply and demand by job title gives us a more valuable insight:"
"#### Observations:\n    \n* Learn `Python`. No excuses. [Pandas](https://pandas.pydata.org/) and [Numpy](https://www.numpy.org/), its most important data-wrangling libraries also appear on the top 50 list.\n* Most **IDE**s, notebooks and data visualization libraries are never mentioned in job requirements. \n* On the other hand [Hadoop](https://hadoop.apache.org/), [Spark](https://spark.apache.org/) and [Elasticsearch](https://www.elastic.co/) are some of the most sought-after technologies on the job market, but were not mentioned in the Survey. Other **Big Data** technologies which appear in the top 50 are: [Hive](https://hive.apache.org/), [Cassandra](http://cassandra.apache.org/), [Kubernetes](https://kubernetes.io/) and [Kafka](https://kafka.apache.org/).\n* **Java** is important, but its importance in the global ranking may be exaggerated by the large proportion of **Software-Engineering** jobs in the dataset. The same with **Javascript**, **React** and **Angular**. These jobs were picked because they form in a way part of the data-pipeline, but they tend to over-inflate the presence of some techs in the list.\n* **OS** usage was not surveyed, but it's important to note that **Linux** is in demand, mentioned in over 5% of all data-related job requirements.\n* [Tableau](https://www.tableau.com/) was not mentioned in the survey among the data-visualization tools, even though based on **Google Trends**, [it is more than 2 times more popular](https://trends.google.com/trends/explore?geo=US&q=learn%20tableau,learn%20matplotlib) than the most popular **Python** dataviz tool, **Matplotlib**. Its language-agnostic, versatile, easy to learn and it's on the rise. \n* **Git** and [Docker](https://www.docker.com/) are in my opinion indispensable tools. Even though they appear only in the bottom half of the list, they should be among the first skills to aquire for a developer."
## 5b. Distribution Salary- Placed Students
**Inference**\n* Many candidates who got placed received package between **2L-4L PA**\n* Only **one** candidate got around **10L PA**\n* The **average** of the salary is a little more than 2LPA\n
## 5c. Employability score vs Salary- Joint plot
**Inference**\n* Most of the candidates scored around **60 percentage** got a decent package of **around 3 lakhs PA**\n* **Not** many candidates received salary **more than 4 lakhs PA**\n* The bottom dense part shows the candidates who were **not placed**
## 5d.Distribution of all percentages
**Inference**\n* All the distributions follow **normal distribution** except salary feature\n* Most of the candidates **educational performances are between 60-80%**\n* **Salary distribution got outliers** where few have got salary of 7.5L and 10L PA
## 5e.Work experience Vs Placement Status
**Inference**\n* We have nearly **66.2%** of candidates who never had any work experience\n* Candidates who **never had work experience** have **got hired** more than the ones who had experience\n* We can conclude that **work experience doesn't influence** a candidate in the recruitment process
## 5f. MBA marks vs Placement Status- Does your academic score influence?
"**Inference** \nComparitively there's a slight difference between the percentage scores between both the groups, But still placed candidates still has an upper hand when it comes to numbers as you can see in the swarm. So as per the plot,percentage do influence the placement status"
## 5g.Does MBA percentage and Employability score correlate?
**Inference**\n* There is **no relation** between mba percentage and employability test\n* There are many candidates who **haven't got place**d when they don't have work experience\n* Most of the candidates who performed better in both tests **have got placed**
## 5h. Is there any gender bias while offering remuneration?
**Inference**\n* The **top salaries were given to male**\n* The **average salary** offered were also **higher for male**\n* **More male candidates were placed** compared to female candidates\n
## 5i. Coorelation between academic percentages
"**Inference**\n* Candidates who were good in their academics performed well throughout school,undergrad,mba and even employability test\n* These percentages **don't have any influence over their salary**"
## 5j.Distribution of our data
**Inference**\n* Candidates who has **high score in higher secondary and undergrad got placed**\n* Whomever got **high scores in their schools got placed**\n* Comparing the number of students who got placed candidates who got **good mba percentage and employability percentage**  
\n# 5. Exploratory Data Analysis (EDA)\n\n[🏠 Tabel of Contents](#tabel)\n\nBefore univariate analysis we check distribution of each columns:
\n# 5.1. Univariate Analysis\n\n[🏠 Tabel of Contents](#tabel)
\n# 5.2. Bivariate Analysis\n\n[🏠 Tabel of Contents](#tabel)
"👉 According to above plots:\n - The Income of people who have accepted a bank loan is often higher than that of people who have not accepted a bank loan. Approximately, people whose monthly Income is more than 8 thousand dollars have accepted a bank loan (Fig 1)\n - Most people who accepted a bank loan had mortgage euqal to zero (Fig 2).\n - The CCAvg of people who have accepted a bank loan is often higher than that of people who have not accepted a bank loan. Approximately, people whose CCAvg is more than 3 thousand dollars have accepted a bank loan (Fig 3).\n - It seems that age does not have much influence in determining whether or not to accept a bank loan (Fig 4)"
"👉 According to above plots:\n - Among the people who did not accept the personal loan, most of them had a family equal to 1, but among the people who accepted the personal loan, there is not much difference in terms of family (Fig 5).\n - Among the people who did not accept the personal loan, most of them had an Education of 1, but among the people who accepted the personal loan, the Education was mostly 3 or 2 (Fig 6).\n - Most of the people, both those who accepted the personal loan and those who did not, did not have a Securities Account (Fig 7).\n - Most of the people, both those who accepted the personal loan and those who did not, did not have a CD Account (Fig 8).\n - Most of the people, both those who accepted the personal loan and those who did not, used online banking facilities (Fig 9).\n - Most of the people, both those who accepted the personal loan and those who did not, did not use a Creditcard (Fig 10)."
👉 According to above plots:\n - Customers whose Income is less than `$5 thousand ` per month have not accepted a personal loan.\n - Most customers whose income is less than `$10 thousand ` per month and their CCAvg is less than `$3 thousand ` per month have not accepted a personal loan.\n - 62% of customers whose CD Account was 1 and Education was 2 have accepted a personal loan.
"👉 According to above plots:\n - All customers with  of more than`$10 thousand` and with Education level 2 or 3, accepted Personal Loans.\n - All customers with CCAvg of more than `$5 thousand` and with Education level 2 or 3, accepted Personal Loans.\n - All customers with Income of more than `$10 thousand` and Family 3 or 4, accepted Personal Loans.\n - All customers with CCAvg of more than `$5 thousand` and by Family 3 or 4, accepted Personal Loans.\n - Most customers with Income of more than `$10 thousand` and by CD Account 1, accepted Personal Loans.\n - Most customers with CCAvg of more than `$5 thousand` and by CD Account 1, accepted Personal Loans.\n"
"> From the feature definition, category count is a feature which act as the proxy of popularity of a project category. For example, if in Travel category a large number of projects are posted then its category_count will be higher so it is a popular category on Kickstarter. On the other hand, if in the Entertainment category, very rarely someone adds a project, its category_count will be lesser and so is its popularity. From the plot, we can observe that chances that a project will be successful will be higher if it belongs to a popular category. Also holds true for main category. \n\n### How about specific categories ?   \n\nBy ploting the pdp_isolate graph we can also identify the effect of specific project categories. "
"From the partial dependency plot for project category, we observe that the accuracy of model predicting the project success increases if it belongs to ""Music"", ""Comics"",  ""Theater"", or ""Dance"" categories.  It decreases if it belongs to ""Crafts"", ""Fashion Film & Video"". The same insights can be backed from the **actual predictions plot**."
Let's take a look to the punctuations in our tweets : 
In details about each target 
"1. ***Missing Values***\n\nBoth training and test set have same ratio of missing values in keyword and location.\n\n0.8% of keyword is missing in both training and test set\n33% of location is missing in both training and test set\nSince missing value ratios between training and test set are too close, they are most probably taken from the same sample. Missing values in those features are filled with no_keyword and no_location respectively."
"2. Cardinality and Target Distribution\n\nLocations are not automatically generated, they are user inputs. That's why location is very dirty and there are too many unique values in it. It shouldn't be used as a feature.\n\nFortunately, there is signal in keyword because some of those words can only be used in one context. Keywords have very different tweet counts and target means. keyword can be used as a feature by itself or as a word added to the text. Every single keyword in training set exists in test set. If training and test set are from the same sample, it is also possible to use target encoding on keyword."
Lets plot vanila CNN model change in loss and accuracys with epochs ...
Lets see vanila CNN predictions with confusion matrix ...
Lets plot Resnet models change in loss and accuracys with epochs ...
Lets see ResNet predictions with confusion matrix ...
__Univariate analysis - box plots for numerical attributes__
__Bivariate analysis - scatter plots for target versus numerical attributes__
"### Assess correlations amongst attributes\nThe linear correlation between two columns of data is shown below. There are various correlation calculation methods, but the Pearson correlation is often used and is the default method. It may be useful to note that:\n1. A combination of the correlation figure and a scatter plot can support the understanding of whether there is a non-linear correlation (i.e. depending on the data, this may result in a low value of linear correlation, but the variables may still be strongly correlated in a non-linear fashion)\n2. Correlation values may be heavily influenced by single outliers! \n\n\nSeveral authors have suggested that _""to use linear regression for modelling, it is necessary to remove correlated variables to improve your model""_, and _""it's a good practice to remove correlated variables during feature selection""_\n\nBelow is a heatmap of the correlation of the numerical columns:"
"With reference to the target SalePrice, the top correlated attributes are:"
"Show scatter plots for each numerical attribute (again, but different, less-efficient code) and show correlation value:"
"__Notes for Feature Selection & Engineering:__\nBased on the scatter plots and correlation figures above, consider:\n* Excluding GarageArea - highly (0.88) correlated with GarageCars, which has a higher corr with Price\n* Excluding GarageYrBlt - highly (0.83) correlated with YearBuilt\n* Excluding all attributes with low corr with Price and unclear non-linear correlation - e.g. MSSubClass, MoSold, YrSold, MiscVal, BsmtFinSF2, BsmtUnfSF, LowQualFinSF?"
### Date
"Above, it is shown that the dates of Train and Test data have an empty intersection."
## Let's cross our Pclass with the Age_cat \nWe will aggregate than to get the mean of Fare by each category pair\n
- Very interesting. We can see that babies has the highest mean value. 
### Looking the Fare distribuition to survivors and not survivors\n
\nDescription of Fare variable\n- Min: 0\n- Median: 14.45\n- Mean: 32.20\n- Max: 512.32 \n- Std: 49.69\n\nI will create a categorical variable to treat the Fare expend\nI will use the same technique used in Age but now I will use the quantiles to binning\n\n
Interesting. With 1 or 2 siblings/spouses have more chance to survived the disaster
We can see a high standard deviation in the survival with 3 parents/children person's \nAlso that small families (1~2) have more chance to survival than single or big families
# Step 4: Explore Data #\n\nLet's take a moment to look at some of the images in the dataset.
You can display a single batch of images from a dataset with another of our helper functions. The next cell will turn the dataset into an iterator of batches of 20 images.
"## tuning7, show a sample of data augmented"
"## tuning7, show a sample of data augmented v2"
"## tuning7, show a sample of data augmented v3"
"# Step 5: Define Model #\n\nNow we're ready to create a neural network for classifying images! We'll use what's known as **transfer learning**. With transfer learning, you reuse part of a pretrained model to get a head-start on a new dataset.\n\nFor this tutorial, we'll to use a model called **VGG16** pretrained on [ImageNet](http://image-net.org/)). Later, you might want to experiment with [other models](https://www.tensorflow.org/api_docs/python/tf/keras/applications) included with Keras. ([Xception](https://www.tensorflow.org/api_docs/python/tf/keras/applications/Xception) wouldn't be a bad choice.)\n\nThe distribution strategy we created earlier contains a [context manager](https://docs.python.org/3/reference/compound_stmts.html#with), `strategy.scope`. This context manager tells TensorFlow how to divide the work of training among the eight TPU cores. When using TensorFlow with a TPU, it's important to define your model in a `strategy.scope()` context."
"-----------------------------------------------------------------------------\n## Continous Features\n\nContinuous variables are numeric variables that have an infinite number of values between any two values. A continuous variable can be numeric or date/time. For example, the length of a part or the date and time a payment is received."
" Observation:\n    \n* GarageCars, BsmtHalfBath, BsmtFullBath and GarageCars are discrete variables.\n* Some features have a skewed shape to one side."
 Observation:\n    \nIt seems that there are outliers that deviate from the regression line. Let's check some more.
"-------------------------------------------\n## Discrete Features\n\nDiscrete variables are numeric variables that have a countable number of values between any two values. A discrete variable is always numeric. For example, the number of customer complaints or the number of flaws or defects."
"In the regression problem, discrete and non-order features must be converted to one-hot encoding. "
------------------------------------------------------------------------\n## Checking Outliers
" Observation:\n\nIf you look at the picture above, you can see an outlier. Regression models are sensitive to outliers, so it is better to remove them. Outlier is like gravity. It pulls the regression line. Therefore, it is better to remove outliers.\nHowever, it is not easy to judge an outlier. Domain knowledge may also be required to remove outliers."
Nice way to visualise descriptions is to create a word cloud. To do this we have to tokenize the descriptions and later join all tokens into a bag of words or like in this case into a one long text.
"# 2. Customers database\n\nThis database contains data about customers. Collecting this type of data allows company to tune their recommender system. This data contains data which can be treated as 'static' or slowly-changing, usually these are features like sex, age, address, hight, etc. Let's look what H&M gave us."
"Unique indentifier of a customer:\n* ```customer_id``` - an unique identifier of the customer\n\n5 product related columns:\n* ```FN``` - binary feature (1 or NaN)\n* ```Active``` - binary feature (1 or NaN)\n* ```club_member_status``` - status in a club, 3 unique values\n* ```fashion_news_frequency``` - frequency of sending communication to the customer, 4 unique values\n* ```age```  - age of the customer\n* ```postal_code``` - postal code (anonimized), 352 899 unique values"
"As we remember from the missing values analysis there is also a group of people where we do not have any data. Perhaps, these people are the ones without membership at all - to add them to the visialisation I'll fill NaN with 'N/A'."
"Most of customers have an ```active``` membership status, others are with the ```pre-create``` status. Interestingly, there is nobody with ```left club``` status."
To start the univariate analysis we will plot histograms for the 'redshift' feature column for each class.\n\nThis will tell us how the redshift values are distributed over their range.
"This is an interesting result.\n\nWe can cleary tell that the redshift values for the classes quite differ. \n\n* **Star:** The histogram looks like a truncated zero-centered normal distribution.\n\n* **Galaxy:** The redshift values may come from a slightly right-shifted normal distribution which is centered around 0.075.\n\n* **QSO:** The redshift values for QSOs are a lot more uniformly distributed than for Stars or Galaxies. They are roughly evenly distributed from 0 to 3, than the occurences decrease drastically. For 4 oder ~5.5 there are some outliers.\n\n**The redshift can be an estimate(!) for the distance from the earth to a object in space.**\n\nHence the distplot tells us that most of the stars observed are somewhat closer to the earth than galaxies or quasars. Galaxies tend to be a little further away and quasars are distant from very close to very far.  \n\nPossible rookie explanation: Since galaxies and quasars radiate stronger due to their size and physical structure, they can be observed from further away than ""small"" stars.\n\nAs we can distinct the classes from each other just based on this column - 'redshift' is very likely to be helping a lot classifying new objects."
Let's lvplot the values of dec (Recall: position on celestial equator)!
**First of all: what does this plot tell us?**\n\nThe Letter value (LV) Plot show us an estimate of the distribution of the data. It shows boxes which relate to the amount of values within the range of values inside the box.\n\nIn this case we can observe a clear distinction between Stars and the other two classes. The difference between Galaxies and Quasars is smaller.\n\n* **Star:** The largest part of the data points lay within a 0 to 10 range. Another large part consists of values between about 10 to 55. Only small amounts of the data are lower or higher than these ranges.\n\n* **Galaxy:** The largest part of values lays between 0 and 45. There is a smaller amount of values in the range of 45 to 60. The rest of the data has smaller or higher values.\n\n* **QSO:** This plot looks quite similiar to the GALAXY plot. Only the amount of data points in the range of 0 to 60 is even bigger.\n\nSide Note: The fact that the distribution of dec values of galaxies und quasar objects is almost the same might indicate that one can find both galaxies and quasars at smiliar positions in the night sky.
"Recall: u, g, r, i, z represent the different wavelengths which are used to capture the observations.\n\nLet's find out how much they are correlated."
"Right of the top we observe that the correlation matrices look very similiar for every class.\n\nWe can tell that there are high correlations between the different bands. This feels not really suprising - intuitively one would think that if one of the bands captures some object, the other bands should capture something aswell.\n\nTherefore it is interesting to see that band 'u' is less correlated to the other bands. \n\nRemember: u, g, r, i, z capture light at wavelengths of 354, 476, 628, 769 and 925 nm.\n\nThis might indicates that galaxies, stars and quasar objects shine brighter at wavelengths from 476 - 925 nm. Don't quote me on that though.\n\n**But:** as we can see - the correlation is roughly the same for every class...the different bands behave the same for the different classes!"
We will now plot the right ascension versus the declination depending on the class 
"As we can clearly observe the equatorial coordinates do not differ significantly between the 3 classes. There are some outliers for stars and galaxies but for the bigger part the coordinates are within the same range.\n\nWhy is that?\n\nAll SDSS images cover the same area of the sky. The plot above tells us that stars, galaxies and quasars are observed equally at all coordinates within this area. So whereever the SDSS ""looks"" - the chance of observing a star or galaxy or quasar is always the same.  \n\n**This contradicts our interpretation of the letter value plot of dec from the univariate analysis.**"
"As we know from the first Titanic kernel, survival rate decreses with Pclass.  \nThe hue parameter lets us see the difference in survival rate for male and female. "
Highest survival rate (>0.9) for women in Pclass 1 or 2.  \nLowest survival rate (<0.2) for men in Pclass 3.
"Passengers embarked in ""S"" had the lowest survival rate, those embarked in ""C"" the highest.  \nAgain, with hue we see the survival rate as function of Embarked and Pclass."
But survival rate alone is not good beacuse its uncertainty depends on the number of samples.  \nWe also need to consider the total number (count) of passengers that embarked.
"Passengers embarked in ""C"" had largest proportion of Pclass 1 tickets.  \nAlmost all Passengers embarked in ""Q"" had Pclass 3 tickets.  \nFor every class, the largest count of Passengers  embarked in ""S""."
"Here, the high survival rate for kids in Pclass 2 is easily observed.  \nAlso, it becomes more obvious that for passengers older than 40 the best chance to survive is in Pclass 1,  \nand smallest chance in Pclass 3   "
This violinplot shows exactly the same info like the swarmplot before.
"For a slightly more interesting example (of non-stationary behavior), we can examine the passengers dataset:"
"The stationarity of a series can be checked by examining the distribution of the series: we split the series into 2 contiguous parts and compute the summary statistics like the mean, variance and the autocorrelation. If the stats are quite different, then the series is not likely to be stationary."
Compute the summary statistics:
Let's go back to our hammer-and-nail approach and try some transformations to make the series stationary.
"Applying a logarithm does not remove the trend, but it does seem to stabilize the amplitude (periodic variations have comparable magnitude now). How does that translate into ADF results?"
Bayesian GMM performs the **best** out of all of the models we've tried so far (it scores around ARI=0.59 on the public leaderboard). It does take **longer** to run though.\n\n# 7. Visualise predictions\n\n\n    7.1 Label distribution\n
"\n    7.2 Cluster distributions\n\n\nTo get insight into the underlying distributions, we can plot cluster-wise histograms of each of the remaining features.\n\n**Continuous: (f_22 to f_28)**"
**Discrete: (f_07 to f_13)**
"\n    7.3 Principle Component Analysis (PCA)\n\n\n**Principal Component Analysis (PCA)** was the first dimensionality reduction technique discovered (by Karl Pearson - yes, the guy from Pearson's correlation coefficient) and dates back to as early as **1901**. It is very popular because it is **fast**, **easy to implement** and **easy to interpret**. \n\nPCA works by finding a low dimensional subspace that **maximises the variance** of the data in that subspace and performing a **linear projection**. This basically means the data will be as **spread out** as possible, without changing the relationship between the data points. This allows us to find patterns in dimensions we can visualised."
"**Explained variance** shows how much of the variance/spread of the data is captured in each dimension, i.e. how **important** each additional **principal component** is to the original data representation."
"\n    7.4 t-SNE\n\n\n**t-SNE** (pronounced tiz-knee) stands for **t-distributed Stochastic Neighbor Embedding** and was proposed much more recently by Laurens van der Maaten and Geoffrey Hinton in their [2008 paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf). \nThis works in a similar way to PCA but has some key differences:\n* Firstly, this is a **stochastic method**. So if you run multiple t-SNE plots on the same dataset it can look different.\n* Another difference is that this is an **iterative method**. It works by repeatedly moving datapoints closer or further away from each other depending on how 'similar' they are. \n* The new representation is **non-linear**. This makes it harder to interpret but it can be very effective at 'unravelling' highly non-linear data.\n\nThe main downside to t-SNE is that is **very slow** compared to the other dimensionality techniques. This is because it makes calculations on a pair-wise basis, which does not scale well with large datasets."
Even with just 5% of the data it still takes several minutes to run.
"\n    7.5 UMAP\n\n\n**UMAP**, which stands for **Uniform Manifold Approximation and Projection** was proposed by Leland McInnes, John Healy and James Melville in their [2018 paper](http://gobie.csb.pitt.edu/SML/umap.pdf).\n\nIt is similar to t-SNE in that it learns a non-linear mapping that preserves clusters but its main advantage is that it is **significantly faster**. It also tends to do better at preserving **global structure** of the data compared to t-SNE. \n\nReference: https://pair-code.github.io/understanding-umap/ "
UMAP's **connectivity plot** is a weighted graph that gives insight into the representation of the embedding. It basically shows which connections were most important when creating the projection. 
"**India vs. U.S.A: A comparative analysis of the paticipation trends**\n\n* As already noted previously, there is a conistently growing number of Indians among the survey-participants (29% in 2020). \n* The same holds true for the cohort of female/LGBTQA+ survey-participants; Indians account for 32% of the female/LGBTQA+ survey participants.\n* Meanwhile, it is a bit surprising to see that even based on the absolute numbers of survey-participants, Americans are a shrinking community as far as the Kaggle surveys are concerned. \n* Another positive takeaway from India: the share of female/LGBTQA+ respondents in India saw a sharp jump this year (thus taking the number of female/LGBTQA+ among every 100 Indian survey-participants to 23, straight up from 17 just a year ago!)\n* The share of female/LGBTQA+ respondents in the U.S.A. meanwhile remained stable at ~25%\n\n\n---\n\nThe receding number of respondents from U.S.A. adds to our inhibition that there might be a fewer number of repeat survey-respondents i.e. people who submit the survey once, might be less enthusiastic about filling it up again. This is just a hunch and at the moment unverifiable using the available data. \nRelevant suggestions:💡\n- Going forward, it might be worthwhile to ask the respondents to select whether or not they filled the Kaggle survey in any of the previous years.\n- A prefilled survey (with data from the user's public Kaggle profile) could also be provided to the users to encourage more users to respond to the annual survey."
[Go back to the top](#qa)
"\n### 2.3. Age distribution\nComparing all four years of data, it is clear that on an overall basis, age-group-wise:\n* Historically, (25-29) year old paricipants formed the single largest user-group.\n* The <=21 and (22-24) year old groups are catching up fast. \n* So combining these three age buckets, we find that more than half of the participants are less than 30 years old.\n* Less than 10% respondents are above 50.\n* The <=21 year old cohort is the only consistently expanding age-group. \n\n---\n\n2018 onward, the survey-participants were asked to select their agegroup (out of **10 options**). Only in 2017, the respondents were given a free-choice to specify their age. As a result, in 2017, the respondents had an option of not specifying their age (which ~20% of the people exercised). For the purpose of this analysis therefore, we disregarded the participants who did not specify their age (so the data for 2017 consists of only ~13.5k (instead of 16.7k) users.  \n"
"Limiting our analysis to the female/LGBTQA+ respondents, we find that:\n* 63% of all female/LGBTQA+ respondents are below 40 (vs 56% below 40 out of all survey-respondents).\n* The trend however remains largely same among female/LGBTQA+ participants as well."
"Dissecting the data further, we find that among the top 10 countries with largest number of 2020 survey participants: \n* India has an overwhelmingly high proportion of very young (<=21 year old) survey respondents. \n* Apart from India, China, Nigeria, and Turkey also have a lot of young respondents (younger than 30).\n* On the other end of the spectrum, most of the respondents from countries like U.S.A,, Japan, U.K., and Germany are above 30.  \n\n---\n\n\n*Note that the following table shows the percentage of respondents in each age-group in each of the top 10 countries (by total number of respondents in 2020). Thus, for instance, it shows that 79% of all Indian survey-participants in 2020 are aged 21 years or less, and only 2% of all survey-participants in 2020 were aged 60 years or above.*"
"Looking back at the historical data (2017-20) for India and U.S.A., we find that:\n* The young Indians (<=21 year old) are the most consistently growing bunch.\n* In comparison, the U.S.A. cohort seems to be much more stagnant with minor growth in the older age-groups.\n* Currently, more than a third of the Indian respondents are <=21 year old (vs only 5% users in U.S.A aged <=21 years). \n* The users in the U.S.A. are much more uniformly distributed across different age groups.\n* And a significant 8% of the users in the U.S.A. are aged 60 years or above.\n\nThe age-distribution is largely similar among female/LGBTQA+ Kagglers as well, though -\n* In India: \n    - female/LGBTQA+ are still younger (2-out-of-3 below 25 year old).\n    - In India, only 62 female/LGBTQA+ Kagglers are 40 years or above.\n    - there are no female/LGBTQA+ Kagglers older than 60 years.\n* In U.S.A:\n    - as well, very few women (only 40 female/LGBTQA+ respondents) are in 55 years old or above."
"\n### 2.4. Formal education\nWhile people with masters degree continue to dominate the Kaggling community, the number of users with bachelors degree has started to catch up. There is a sharp drop in the percentage of docatoral candidates this year. Very few users (< 7%) possess no formal degree at all and roughly about 3% users have a professional degree."
"**Takeaways: **\n1. Users with higher degrees dominate Kaggle for the time being (more than half of the users have doctoral, master's, or professional degrees).\n2. Users with only a bachelor's degree or no degree at all are fast catching up (reaching 50%).\n3. The trend is similar across genders.\n\nThese developments seem to directly connect with the growing number of young respondents from India."
"* As expected, more than half of the Indian responents in 2020 hold a Bachelors degree.\n* 1-in-4 respondents from U.K. and Germany have a doctoral degree.\n* About 1-in-10 of the respondents from Japan, Russia, and China are college drop-outs (i.e. studied at some college/ university without any degree). "
"* Majority of the Kagglers in India only have a Bachelor's degree or no degree at all.\n* On the contrary, majority of the Kagglers in U.S.A have a higher degree (master's, doctoral or professional).\n* It is interesting to note that, in India, on a percentage basis, female/LGBTQA+ respondents are consitently more qualified (have a masters degree or higher) compared to their male counterparts. 49% females/ LGBTQA+ respondents have a higher degree (masters, doctoral, or professional) vs. 40% male respondents with higher degrees in India. \n\nTakeaway\n\n* A lot of very young Indians are getting involved in Kaggling. Given their age, they are mostly students/ youngsters with Bachelor's degree only. Given that these young Indians constitute a significantly large portion of the overall user base, their demographics dominate the survey data (and trends) on the whole. \n\n* Though the overall trend is similar across genders, it seems that in India in particular, fewer women/ LGBTQA+ with only a Bachelor's degree (or no degree at all) have access to Kaggle.\n"
"\n### 2.5. Profession (including compensation, company/ team size, and job profile)\n\n* 1-in-4 male respondents is a student.\n* A slightly higher proportion (1-in-3) female/ LGBTQA+ Kagglers is student.\n* Among the professionals, as expected, majority are data scientists (~1-in-10).\n* Software engineers form the next most dominant group (overall).\n* A sizeable portion of the respondents is also currently not employed (about 1-in-10). \n\nNote: \n* Compared to the group of male users, the group of female/ LGBTQA+ users have a higher proportion of students and unemployed people (i.e. non-professionals).  \n* Overall 27+9=36% are either students or unemployed. \n* Another ~4% (759 respondents) left their profession as 'NA' and are therefore left out of any analyses involving the profession of the respondents. "
"Looking at the country-wise data, we find that:\n* Both India (40%) and China (50%) have high percantage of student respondents.\n* In each of Nigeria (15%), Russia (10%) and India (10%), at least 1-in-10 respondents is currently not employed."
\nLet's plot training and validation accuracy as well as loss.
### Classification Report\nWe can summarize the performance of our classifier as follows
"It's apparent that our classifier is underperforming for class 6 in terms of both precision and recall. For class 2, classifier is slightly lacking precision whereas it is slightly lacking recall (i.e. missed) for class 4.\n\nPerhaps we would gain more insight after visualizing the correct and incorrect predictions.\n\nLet us examine the test label and check if it the right classification or not."
### I hope you had a good understanding of CNN Model and its usage in practice using Fashion MNIST dataset.\n\n# Please do share your comments/suggestions and if you like this  kernel appreciate to UPVOTE.\n\nI recently created few useful kernels like below which might be of great interest to you in your data science work .Do visit them and share your thoughts/comments/upvote.
"## 4. Token normalization\n\nToken normalisation means converting different tokens to their base forms. This can be done either by:\n\n- **Stemming** :  removing and replacing suffixes to get to the root form of the word, which is called the **stem** for instance cats - cat, wolves - wolv \n- **Lemmatization** : Returns the base or dictionary form of a word, which is known as the **lemma** \n\n[*source*](https://www.coursera.org/learn/language-processing/lecture/SCd4G/text-preprocessing)"
"It is important to note here that stemming and lemmatization sometimes donot necessarily improve results as at times we donot want to trim words but rather preserve their original form. Hence their usage actually differs from problem to problem. For this problem, I will not use these techniques."
"with limited success; the narrower left hand peak seems to be some other distribution. (Just in case, the $\mu$ of the small Gaussian is located at -1.32, and the large Gaussian at 0.4).\n\n## Cumulative return\n\nLet us take a look at the cumulative daily return over time, which is given by `weight` multiplied by the value of `resp`"
"We can see that the shortest time horizons, `resp_1`, `resp_2` and `resp_3`, representing a more conservative strategy, result in the lowest return.\n\nWe shall now plot a histogram of the `weight` multiplied by the value of `resp` (after removing the 0 weights)"
"\n## Time\nLet us plot the number of `ts_id` per day. Note: I have taken to drawing a vertical dashed line in my plots because I started to wonder [did Jane Street modify their trading model around day 85?](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930) Thanks to comments on that forum the general consenus seems to be that a change in the market took place around that time (perhaps a mean reverting market changing to a momentum market, or *vice versa*)."
If we assume a [trading day](https://en.wikipedia.org/wiki/Trading_day) is 6½ hours long (*i.e.* 23400 seconds) then
Here is a histogram of the number of trades per day (it has been [suggested](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930#1125847) that the number of trades per day is an indication of the [volatility](https://www.investopedia.com/terms/v/volatility.asp) that day)
"If that is the case, then 'volitile' days, say with more than 9k trades (*i.e.* `ts_id`) per day, are the following "
"Also, `feature_0` is the *only* feature in the `features.csv` file that has no `True` tags."
"It is also very interesting to plot the cumulative `resp` and return (`resp`\*`weight`) for `feature_0 = +1` and `feature_0 = -1` individually (Credit: [""*An observation about feature_0*""](https://www.kaggle.com/c/jane-street-market-prediction/discussion/204963) by [therocket290](https://www.kaggle.com/therocket290))"
"It can be seen that ""+1"" and the ""-1"" projections describe very different return dynamics.\nIn the notebook [""*Feature 0, beyond feature 0*""](https://www.kaggle.com/nanomathias/feature-0-beyond-feature-0) written by [NanoMathias](https://www.kaggle.com/nanomathias) a [uniform manifold approximation and projection (UMAP)](https://arxiv.org/abs/1802.03426) is performed and shows that `feature_0`  effectively classifies two distributions of features.\nThere have been many suggestions made regarding the nature of this feature on the discussion topic [""*What is ""feature_0"" ?*""](https://www.kaggle.com/c/jane-street-market-prediction/discussion/199462) such as `feature_0` representing the direction of the trade or things like bid/ask, long/short, or call/put.\n\nOne possibility is that `feature_0` represents something similar to the [Lee and Ready 'Tick' model](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1540-6261.1991.tb02683.x) for classifying individual trades as market buy or market sell orders, using intraday trade and quote data.\nA buy initiated trade is labeled as ""1"", and a sell-initiated trade is labeled as ""-1"" (*Source*: § 19.3.1 of [""*Advances in Financial Machine Learning*""](https://www.wiley.com/en-es/Advances+in+Financial+Machine+Learning-p-9781119482109) by Marcos Lopez de Prado)\n\n$$\nb_t = \n\begin{cases} \n  1  & \mbox{if }\Delta p_t > 0\\\n  -1 & \mbox{if }\Delta p_t < 0\\\n  b_{t-1} & \mbox{if }\Delta p_t = 0\n\end{cases}\n$$\n\nwhere $p_t$ is the price of the trade indexed by $t = 1,\ldots , T$, and $b_0$ is arbitrarily set to\n1.\n\nIf we look at the correlation matrix (see below) it can be seen that there is a strong positive correlation between `feature_0` and the **Tag 12** features, a strong negative correlation with the **Tag 13** features. There is also a negative correlation with the **Tag 25** and **Tag 27** features, and a positive correlation with  the **Tag 24** features.\n\nOther than features 37, 38, 39 and 40 all of the above features are `resp` related features (see below) with the strongest correlation being with the `resp_4` features.\n\n### feature_{1...129}\nThere seem to be four general 'types' of features, here is a plot of an example of one of each:"
"### 'Linear' features\n* 1 \n* 7, 9, 11, 13, 15\n* 17, 19, 21, 23, 25\n* 18,  20,  22,  24, 26\n* 27, 29, 21, 33, 35\n* 28, 30, 32, 34, 36\n* 84, 85, 86, 87, 88\n* 90, 91, 92, 93, 94\n* 96, 97, 98, 99, 100\n* 102 (strong change in gradient), 103, 104, 105, 106\n\nas well as\n41, 46, 47, 48, 49, 50, 51, 53, 54, 69, 89, 95 (strong change in gradient), 101, 107 (strong change in gradient), 108, 110, 111, 113, 114, 115, 116, 117, 118, 119 (strong change in gradient), 120, 122, and 124.\n### Features 41, 42 and 43 (Tag 14)\nThe **Tag 14** set are interesting as they appear to be ""stratified""; only adopting discrete values throughout the day (could these be a value of a [security](https://en.wikipedia.org/wiki/Security_(finance%29)?).\nHere are scatter plots of these three features for days 0, 1 and 3 (Note that I have omitted day 2, which I shall discuss in the *missing data* section below):"
"These three features also have very interesting lag plots, where we plot the value of the feature at `ts_id` $(n)$ with respect to the next value of the feature, *i.e.*  at `ts_id` $(n+1)$, (here for day 0). Red markers have been placed at (0,0) as a visual aid."
"### Tag 18 features: 44 (+ tag 15) and 45 (+ tag 17)\nThese are similar to the Tag 14 features seen above, but are now much more centred around 0"
with the following lag plots
"### Features 60 to 68 (Tag 22)\nWe have the **Tag 22** set:\n* 60, 61, 62, 63, 64, 65, 66, 67, 68"
"Indeed `feature_60` and `feature_61` (both having Tags 22 & 12) are virtually coincident. \nThe same goes for `feature_62` and `feature_63` (both having Tags 22 & 13), \n`feature_65` and `feature_66` (both having Tags 22 & 12) and\n`feature_67` and `feature_68` (both having Tags 22 & 13). Let us plot these features as distributions"
and in between them is `feature_64`
Lets try to fetch information of linear correlated attributes from the data.
"From above heatmap of correlation matrix we can interpret that many attributes are showing linear correlation with each other. So, Lets try to reduce redundancy from the data.\n\nJust to make things simple lets predict the attribute 'density' instead of attribute 'quality'."
Barplot to show probabilities for work interfere
\n## **6. Scaling and fitting** ##\n\n
### **Tuning with cross validation score**
### **Tuning with GridSearchCV** ###
### **Tuning with RandomizedSearchCV** ###
"This confirms that raising the threshold decreases recall. The instance actually represents a 1(True), and the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 2.\n\nTo decide which threshold to use, we first need to get the scores of all instances in the training set using the cross_val_predict() function again, but this time specifying that you want it to return decision scores/probability instead of class:"
"Now we can simply select the threshold value that gives us the best precision/recall tradeoff for our task. let’s suppose you decide to aim for 80% recall. You look up the first plot (zooming in a bit) and find that you need to use a threshold of about 0.32. To make predictions (on the training set for now), instead of calling the classifier’s predict() method, you can just run this code:"
##  Regression Metrices \n\n- Dataset:  Boston House Price dataset.\n- Evaluation Algorithm: Logistic Regression.
"###  Mean Absolute Error \n- Average of the difference between the Original Values and the Predicted Values.\n- Do not gives any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data.\n- Smaller the MAE, better is the model.\n- Robust to outliers\n- Range (0, + infinity]\n\n$$ Mean\ Absolute\ Error = \frac{1}{N} \sum_{i=1}^{N} |y_{i} -  \hat{y_{i}}|$$"
"Before starting to predict test points' labels, I wanted to see the places of these points according to some features. Thus, I drew the below charts which will help us to guess new points' labels."
#  k-NN from Scratch
"In this section, I compare my kNN function with the scikit learn's k-NN function and determine the confusion matrixes for the both models. The results look same but by removing the *random_state* in the *train_test_split* function and chaging the *train_size* different results can be found."
#  Logistic Regression from Scratch
Below confusion matrices show that each model gives the same results when the parameters are selected the same.
"Moreover, below line charts determine the costs for different learning rates. These plots generally used for the sanity check. They look like same for each model. They look pretty good and the effect of learning rate can be observed clearly. Since I used one vs rest, for each learning rate I drew 3 cost lines. "
#  Logistic Regression from Scratch vs scikit-learn Logistic Regression
"This time, I compare my logistic function with the scikit learn's logistic function and determine the confusion matrixes for the both models. Also, I changed the regularization parameter ($\lambda$) for both models and determined the confusion matrixes for them too. The results look similar but they are not the same as expected. By removing the *random_state* in the *train_test_split* function and chaging the *train_size* different results can be obtained. "
#  k-Fold Cross Validation from Scratch\n#### [Return Contents](#0)\n
"In the **bias-variance behavior**, higher training set means higher variance. At this point, depending to our choice of number of folds, accuracies might change. This change can be observed from the below figures for my models.\n"
#  k-Fold Cross Validation from Scratch vs scikit-learn k-Fold Cross Validation
### AGE & SURVIVAL
### FARE & AGE w.r.t SURVIVED
### INITIALS & AGE GROUP
### SURVIVAL vs INITIALS 
"## INSIGHTS\n- FEMALE PASSENGERS WERE SAVED MORE THAN MALE PASSENGERS\n- CLASS 1, CLASS 2, CLASS 3 WAS ALSO CONSIDERED WHILE SAVING THE PASSENGERS\n- NO SUCH PREFERENCE CAN BE SAID ABOUT THE PASSENGERS BASED ON PORT OF EMBARKATION\n- 'Parch' & 'SibSp' DON'T GIVE US MUCH ABOUT THE DATA [MAYBE I NEED TO WORK ON THIS FACTOR ]\n- DISTRIBUTION OF 'FARE' SHOWS PREFERENCE GIVEN TO PEOPLE HAVING PAID MORE FOR FARE THAN OTHERS\n- PROBABLY BECAUSE OF CLASS 1, CLASS 2, CLASS 3\n- WE ALSO CHECKED THE SURVIVAL OF PASSENGERS W.R.T THEIR INITIALS WHICH CAN HELP FOR BETTER RESULTS\n- 'AGE' CLEARLY SHOWS A BULGE AT VALUE 28 WHICH WE FILLED IN PLACE OF 'NAN'\n- WE WILL DROP 'PassengerId','TICKET','CABIN'"
Let's plot the total kills for every player first. It doesn't look like there are too many outliers.
Let's take a closer look.
### Rolling Average Price vs. Time for each store
"In the above graph, I have plotted rolling sales across all stores in the dataset. Almost every sales curve has ""linear oscillation"" trend at the macroscopic level. Basically, the sales oscillate like a sine wave about a certain mean value, but this mean value has an upward linear trend. This implies that the sales are oscillating at a higher and higher level every few months.\n\nThis trend is reminiscent of the **business cycle**, where economies have short-term oscillatory fluctuations but grow linearly in the long run. Maybe, such small-scale trends at the level of stores add up to decide trends we see at the macroeconomic level. Below is an illustration of the macroeconomic business cycle:"
 
"The above plot compares the sales distribution for each store in the dataset. The stores in California seem to have the highest variance in sales, which might indicate that some places in California grow significantly faster than others, *i.e.* there is development disparity. On the other hand, the Wisconsin and Texas sales seem to be quite consistent among themselves, without much variance. This indicates that development might be more uniform in these states. Moreover, the California stores also seem to have the highest overall mean sales."
"From the above graph, we can see the same trends: California stores have the highest variance and mean sales among all the stores in the dataset."
### Rolling Average Price vs. Time (CA)
"In the above graph, we can see the large disparity in sales among California stores. The sales curves almost never intersect each other. This may indicate that there are certain ""hubs"" of development in California which do not change over time. And other areas always remain behind these ""hubs"". The average sales in descending order are CA_3, CA_1, CA_2, CA_4. The store CA_3 has maximum sales while the store CA_4 has minimum sales. "
### Rolling Average Price vs. Time (WI)
"In the above graph, we can see a very low disparity in sales among Wisconsin stores. The sales curves intersect each other very often. This may indicate that most parts of Wisconsin have a similar ""development curve"" and that there is a greater equity in development across the state. There are no specific ""hotspots"" or ""hubs"" of development. The average sales in descending order are WI_2, WI_3, WI_1. The store WI_2 has maximum sales while the store WI_1 has minimum sales. "
### Rolling Average Price vs. Time (TX)
"In the above graph, we can once again see that a very low disparity in sales among Texas stores. The sales curves intersect each other often, albeit not as often as in Wisconsin. This might once again indicate that most parts of Texas have a similar ""development curve"" and that there is a greater equity in development across the state. The variance here is higher than in Wisconsin though, so there might be ""hubs"" of development in Texas as well, but not as pronounced as in California. The average sales in descending order are TX_2, TX_3, TX_1. The store TX_2 has maximum sales while the store TX_1 has minimum sales."
Below are the sales from three sample data points. I will use these samples to demonstrate the working of the models.
"## Naive approach \n\n\nThe first approach is the very simple **naive approach**. It simply forecasts the next day's sales as the current day's sales. The model can be summarized as follows:\n\n\n\nIn the above equation, *yt+1* is the predicted value for the next day's sales and *yt* is today's sales. The model predicts tomorrow's sales as today's sales. Now let us see how this simple model performs on our miniature dataset. The training data is in blue, validation data in orange, and predictions in green."
"After training is done, it's good to check accuracy on data that wasn't used in training."
"When, we're happy with the outcome, we read test data from *test.csv* and predict labels for provided images.\n\nTest data contains only images and labels are missing. Otherwise, the structure is similar to training data.\n\nPredicted labels are stored into CSV file for future submission."
# So far...
"# Implementing SMOTE\n\nSMOTE is a technique that helps deal with imbalanced data sets. \n\nA great introductory article can be found here:\n\nhttps://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/\n\nThe common error I see people making is to use SMOTE and THEN split their data in to train & test sets. This is a big mistake as you will get some serious data leakage and end up predicting synthetic results that you have just created - it does not make sense. \n\nInstead, split your data first, and THEN use SMOTE on the training data only.\n\nLet's see if it helps here..."
# Viewing our results in an accesible way\n\nWe now now need to find a way to view our results which can be easily explained to business stakeholders. \n\nA simple annotated heatmap works well for this!
"# Something else... Borderline SMOTE\n\nThere are many oversampling techniques that one could employ. \n\nA variation of the technique used above is **Borderline SMOTE**.\n\nBorderline SMOTE involves selecting those instances of the minority class that are misclassified.\n\nWe can then oversample just those difficult instances, providing more resolution only where it may be required\n\nA great article cab be found here:\n\nhttps://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n\n"
# 5. Word clouds of each class
"*  By observing the above word cloud we can see some words like earthquake,fire,wildfires etc.., which refer to real disasters."
"*  By examining we can observe words like disney,Wrecked etc.., we need to explore more."
## 6.1 Word Frequency 
## 6.2 Bigram plots
## 7.1 Plotting of meta features vs each target class (0 or 1)
# 8. Histogram plots
## 8.1 Histogram Plots of number of words per each class (0 or 1)
## 8.2 Histogram Plots of number of characters per each class (0 or 1)
## 8.3 Histogram Plots of number of punctuations per each class (0 or 1)
## 8.4 Histogram plots of number of words in train and test sets
## 8.5 Histogram plots of number of chars in train and test sets
## 8.6 Histogram plots of number of punctuations in train and test sets
# 9. Readability features 
"# **W & B Artifacts**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)"
# **Categorical Feature Transformation**
"\n    10.2 | Main training function\n\n\n`train_val` is the main function used to train the model on the **training set** `train_dl` & evaluate on the **validation set** `val_dl`\n\n#### **FUNCTION INPUTS**\n\nThe function requires\n- **PyTorch Classifier** , `cnn_model` (we visualised in section 7)\n- Training **parameter dictionary** `params_train` (which contains both hyperparameters & input data)\n\n#### **FUNCTION PARAMETER DICTIONARY**\n\nThe parameter dictionary requires:\n- Number of training **iterations**,`epochs`\n- Training & validation **data loaders**, `train_dl`, `val_dl`\n- **Optimiser** & **loss function**,  `opt` & `loss_func`\n- **Learning rate adjustor** (on the fly) `lr_change`\n\n#### **POST TRAINING OUTPUT**\n\n`train_val` returns:\n- The best performing model on the validation dataset\n- The Loss per iteration\n- The Evaluation Metric per iteration (which is accuracy)"
"\n    10.3 | Training Process \n\n\n#### **SET PARAMETER DICTIONARY**\n\n- Define the parameters for training `params_train` & train classifier\n- We'll be training the classifier for **50 iterations** using the **Adam optimiser** & the **negative log likelihood loss** function\n- The learning rate will be adjusted on the fly using `ReduceLROnPlateau`, with a factor of 1/2"
# Average congestion per month for each roadway
# Average montly congestion
# Average montly congestion for each roadway
# Average congestion per week
# Average congestion per week for each roadway
# Average weekly congestion
# Average weekly congestion for each roadway
# Average congestion per day
# Average congestion per day for each roadway
# Average daily congestion
# Average daily congestion for each roadway
# Average Monday congestion\n\nAs 30 Sept 1991 where locating the test set is a Monday. It is of interest to examine data of this day of week.
# Average Monday congestion for each roadway
# The nearest Monday 23 Sept
# Correlation between the days
--------------------------------------------------------------------\n## Checking batch
" Observation:\n* We converted audio to image. In other words, the above figures can be regarded as fingerprints of each audio file.\n   \nThe format of the question has been changed. Looking at the picture above, it can be seen that the problem of audio classification has been changed to the problem of image classification.\n    \nQuantum mechanics cannot explain the behavior of quantum clearly, but as it can be solved mathematically, rather than understanding the above figures accurately, we will only think about how the model can understand and learn the above picture."
"**Organic Carbon**: Organic contaminants (natural organic substances, insecticides, herbicides, and other agricultural chemicals) enter waterways in rainfall runoff. Domestic and industrial wastewaters also contribute organic contaminants in various amounts. As a result of accidental spills or leaks, industrial organic wastes may enter streams. Some of the contaminants may not be completely removed by treatment processes; therefore, they could become a problem for drinking water sources. It is important to know the organic content in a waterway."
"**Trihalomethanes**: Trihalomethanes (THMs) are the result of a reaction between the chlorine used for disinfecting tap water and natural organic matter in the water. At elevated levels, THMs have been associated with negative health effects such as cancer and adverse reproductive outcomes."
"**Turbidity**: Turbidity is the measure of relative clarity of a liquid. It is an optical characteristic of water and is a measurement of the amount of light that is scattered by material in the water when a light is shined through the water sample. The higher the intensity of scattered light, the higher the turbidity. Material that causes water to be turbid include clay, silt, very tiny inorganic and organic matter, algae, dissolved colored organic compounds, and plankton and other microscopic organisms."
Scatter Plot Matrix helps in finding out the correlation between all the features.
"As we can see, there seems to be very less correlation between all the features."
Let's make a Heatmap to visualize the correlation.
---
## What Garages tells us about each Price Category:\n\n
# Miscellaneous and Utilities:\n
"\n\n## Interesting insights:\n\n1) **Overall Condition**: of the house or building, meaning that further remodelations are likely to happen in the future, either for reselling or to accumulate value in their real-estate.. \n2) **Overall Quality**: The quality of the house is one of the factors that mostly impacts SalePrice. It seems that the overall material that is used for construction and the finish of the house has a great impact on SalePrice. \n3) **Year Remodelation**: Houses in the **high** price range remodelled their houses sooner. The sooner the remodelation the higher the value of the house. \n"
## Which Material Combination increased the Price of Houses?\n\n\n Roof Material: Hip and Gable was the most expensive since people who bought high value houses tended to buy this material bor he rooftop.\n House Material: Houses made up of stone tend to influence positively the price of the house. (Except in 2007 for High Price House Values. )  \n\n
"**Note:** Interestingly, the Masonry Veneer type of stone became popular after 2007 for the houses that belong to the **high** Price Range category. I wonder why? \n**For some reason during the year of 2007, the Saleprice of houses within the high range made of stone dropped drastically! \n\n"
 Quality of Neighborhoods \n\n\n\n## Which Neighborhoods had the best Quality houses?\n
"## Bivariate Analysis (Detecting outliers through visualizations):\n\n**There are some outliers in some of this columns but there might be a reason behind this, it is possible that these outliers in which the area is high but the price of the house is not that high, might be due to the reason that these houses are located in agricultural zones.**"
 Feature Engineering \n\n## Dealing with Missing Values:\n
"## Encoding Cyclical Features \nThe new time features are cyclical. For example,the feature `month` cycles between 1 and 12 for every year.\nWhile the difference between each month increments by 1 during the year, between two years the `month` feature jumps from 12 (December) to 1 (January). This results in a -11 difference, which can confuse a lot of models."
"Ideally, we want the underlying data to represent the same difference between two consecutive months, even between December and January. A common remedy for this issue is to encode cyclical features into two dimensions with sine and cosine transformation."
"# Exploratory Data Analysis\n\nLet's begin by plotting the seasonal components of each feature and comparing the minima and maxima. By doing this, we can already gain some insights:\n* The depth to groundwater reaches its maximum around May/June and its minimum around November/December\n* The temperature reaches its maxmium around August and its minimum around January\n* The volume reaches its maximum around June and its minimum around August/September. It takes longer to reach its maximum than to reach its minimum.\n* The hydrometry reaches its maximum around March and its minimum around September\n\n* The volume and hydrometry reach their minimum roughly around the same time\n* The volume and hydrometry reach their minimum when the temperature reaches its maximum\n* Temperature lags begind depth to groundwater by around 2 to 3 months"
We can see that the correlation to the target variables increases if we use the time shifted features in comparison to the original features.
"## Autocorrelation Analysis\n\n\n    For further details on this topic, see my other notebook: \n    Time Series: Interpreting ACF and PACF\n\n\n\n\nThis EDA step is especially important when using [ARIMA](#ARIMA). The autocorrelation analysis helps to identify the AR and MA parameters for the [ARIMA](#ARIMA) model.\n\nAutocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)\n\n* **Autocorrelation  Function (ACF)**: Correlation between time series with a lagged version of itself. The autocorrelation function starts a lag 0, which is the correlation of the time series with itself and therefore results in a correlation of 1. -> MA parameter is q significant lags\n* **Partial Autocorrelation Function (PACF)**: Additional correlation explained by each successive lagged term -> AR parameter is p significant lags\n\nAutocorrelation helps in detecting seasonality.\n\nAs we can infer from the graph above, the autocorrelation continues to decrease as the lag increases, confirming that there is no linear association between observations separated by larger lags.\n\nFor the AR process, we expect that the ACF plot will gradually decrease and simultaneously the PACF should have a sharp drop after p significant lags. To define a MA process, we expect the opposite from the ACF and PACF plots, meaning that: the ACF should show a sharp drop after a certain q number of lags while PACF should show a geometric or gradual decreasing trend."
We can see some sinusoidal shape in both ACF and PACF functions. This suggests that both AR and MA processes are present.
### Online Courses Vs Brick & Mortar\n\nWe have seen that online courses have gained a lot of interest in the previous sections. Now let us see what people perceive about the quality of online courses compared to traditional brick and mortar ones.
"**Observations:**\n * About **53%** of the respondents feel that **online courses are better** than traditional brick and mortar courses\n * About 33% of the respondents are neutral\n * Overall, people seem to be more satisfied with online courses.\n \n \n\n \n "
"## Coursera Data Science Courses Review Dataset\n\nNow that we got an idea about the perception of people about online courses, let us check the perception from some other place. Thankfully we also have a [coursera course review dataset](https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset#reviews_by_course.csv) in Kaggle datasets. So in this section let us use this dataset to make some plots and see if they also give similar results."
"**Observations:**\n * Looks like the ""machine learning"" course in coursera is one of the widely popular courses and it has the highest number of reviews\n * ""python-data"" is the second one with most number of reviews followed by ""data-scientist-tools"""
"**Observations:**\n * Machine Learning in coursera is not just the most popular course by count, it is the course with most percentage of ""very positive"" reviews as well. I think this is one of the very popular starting courses for people wanting to learn DS / ML\n * Overall, the positive reviews are higher than the negative reviews for all the courses and is inline with Kaggle survey results as well. \n * Now you know which course to start first ;)"
"## In-person Bootcamps\n\nApart from online courses, one another recent addition that has gained popularity to learn data science is in-person bootcamps. More information about the bootcamps can be seen [here](https://www.cio.com/article/3051124/careers-staffing/10-boot-camps-to-kick-start-your-data-science-career.html) and [here](https://www.switchup.org/rankings/best-data-science-bootcamps).\n\n![BootCamps](https://www.kdnuggets.com/images/nycdsa-data-science-bootcamp.jpg)\n\nIn this section, let us see how people perceive bootcamps compared to traditional brick & mortar courses."
"**Observations:**\n * Seems like a good part of the respondents are not aware of the bootcamps. About 33% of the people took a ""No opinion / I do not know"" stance\n * About 39.5% of the people feel that they are better than traditional institutions\n * About 18.8% of the respondents are neutral on their views"
### Favorite Media Sources
### Favorite Media Sources By Country\n\n  The below plot is interactive. Please rotate them to have a better view of the countries you would like to see.
### Other Media Sources : Free Form Text\n\nWe also have a free form text column for other media sources. So let us look at this column to get an idea of the other favorite ML / DS sources.
"**Observations:**\n * ods.ai seem to be one another popular media source \n * We could also see some social media channels like linkedin, facebook, youtube, quora etc\n * Being from India, I could also see Analytics Vidhya mentioned in few places ( which means it needs some better cleaning ;) )"
\n Total number of matches Win by Each Teams
\nDoes toss winning affects the match winner ?
\nToss/Win Ratio
\nDo you know who won the most player of the match?
\nSuccess rate of winning matches\n
\n Most Titles Wins 
\nTop 10 Playes with Most Runs
\nTop 10 Best Performances in a match\n
\nTop 10 bowlers till 2020
\nTop 10 Bowling Performance till 2020\n
\n Top 10 Cities by Number Of matches\n
"\n Results Based on Duckworth-Lewis \n\n*Duckworth-Lewis is based on the idea of compensating rain-affected teams for the loss of ""run-scoring resources"". The D/L method works on the basis that teams have two resources to make runs with: the number of overs to be bowled and the number of wickets in hand.*"
\nTop 10 Umpire to feature in max number of matches 
\n Season wise match summary of matches won by runs 
\n Top Most Dissmisal Reason 
\n Top 10 Best Fielders in the Field 
\n Toss Decision 
"### 4.1 Graph representation\n\nAs in the correlation-based case, we can create a minumum spanning tree to visualize the *features network*."
"The spanning tree calculated from the information-based metric seems more *stretched*. The features seem to be further apart and less clustered. However, the features that we had identified as highly correlated are also extremely close here (`ft_60`-`ft_61`, `ft_65`-`ft_66`, `ft_62`-`ft_63`, `ft_67`-`ft_68` and so on. Let's see what happens when we plot the distance matrix and compare it to the distance matrix obtained from the correlation based distance."
"### 4.2 Information-based clustering\n\nAs in the correlation-based case, we can use the distance matrix to obtain a linkage matrix and the final clustering. Before proceeding further, it is interesting to compare the correlation-based distance matrix and with the information-based distance matrix."
"Correlation-based and information-based distance matrices show very similar patterns. The information-based distance is on average higher than its correlation-based counterpart (see chart below). It would be interesting to investigate this result further: Does it mean that the information distance is ""less sensitive""? Or perhaps that it is better able to detect meaningful relationships?"
Let's proceed with the linkage matrix.
\n4.2 Correlation Heatmap: annotated full matrix
\n4.3 Correlation Heatmap: lower-triangular & annotated matrix
Back to top\n\n\n5. Annotations\n\nWith annotation you can draw the attention of your reader to the a specific point(s) you would like to focus. Area annotation and text annotation are two examples as demonstated using the examples below. We have seen this already at [section 2.4](#2.4) (text annotation). \n
"\n6.2 Facet grids/pairplots\n\nYou can plot pairplot using the standard plolty function `figure_factory.create_scatterplotmatrix()`. The diagonal plot type can be chosen from *histogram*, *scatter* or *box* types. Background and paper colors can also be customized. "
Another option is to use the `go.Splom` from `plotly.graph_objects`. See the example below which uses the Iris dataset.
\n6.3 Daigonal Facet grids (custom made)\n\nWe can also make a new pairplot with lower-triangular with a kde diagonal plot. This plot was inspiried by [subinium's](https://www.kaggle.com/subinium)  [notebook](https://www.kaggle.com/subinium/dark-mode-visualization-apple-version) which he did using matplotlib/seaborn library.
"Back to top\n\n\n7. Radar Charts\n\nUse `radar chart` when appropriate. When plotting some feature which varies with direction, it can best be represented (plotted) using a radar chart. For example, the effect of wind direction on rainfall (Rainfall in Austrailia dataset) can be a good example. Below I have depicted how this can be demonstrated with line graph and radar chart. In my opinion the radar chart looks/feels right."
\n\n8. 3D Plots\n\nIn 3D plots it helps to use different background colors for the three planes to make the visiblity and visualization a bit more clear. With the help of the legendary iris dataset let's do that. 
\n\n9. Choropleth Map (Animation)\n\n* Use the `scope` argument if you want to focus on one continent only. \n* Africa is selected as the scope argument in the second example. 
"\n\n10. Bonus: Getting creative with annotated heatmaps\n\nIf you haven't seen the periodic table plotted using plotly, you can find it [here](https://plotly.com/python/annotated-heatmap/). Go ahead and check it. It gives you an idea on how to get creative with annotated heatmaps. [@janiobachmann](https://www.kaggle.com/janiobachmann) also used the periodic table structure in his [housing market project](https://www.kaggle.com/janiobachmann/melbourne-comprehensive-housing-market-analysis). Below is a simplified example.\n\nNote: You might have already noticed that the header of this notebook is also an adaptation of the periodic table plot."
\n\n11. Reference\n\n1. [plotly.com](https://plotly.com/) \n2. [Plotly Tutorial for Everyone](https://www.kaggle.com/saurav9786/plotly-tutorial-for-everyone/) by [@saurav9786](https://www.kaggle.com/saurav9786) \n3. [Plotly Tutorial for Beginners](https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners) by [@kanncaa1](https://www.kaggle.com/kanncaa1)\n\n**Note**: Ref 2&3 are very good tutorials for beginners on plotly. I recommend them for anyone who's just starting exploring the plotly plotting library.\n\nOther notebooks where I used plotly:\n\n1. [Students Performance: Practice EDA with plotly](https://www.kaggle.com/desalegngeb/students-performance-practice-eda-with-plotly)\n2. [April TPS: Synthanic EDA + Visualizations](https://www.kaggle.com/desalegngeb/april-tps-synthanic-eda-visualizations)\n3. [English PL Players' stat: Data-viz with Plotly](https://www.kaggle.com/desalegngeb/english-pl-players-stat-data-viz-with-plotly)\n
"**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling ""Scatter"" as follows:"
Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.
"**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:"
# Second-Level Predictions from the First-level Output
**Correlation Heatmap of the Second Level Training set**
There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.
> ㅤThe level distribution of events 
> ㅤLevel and Elapsed_time 
 ⬆️Back to Table of Contents ⬆️
> Page 
> Level_group 
# **Feature Importance**
# **SHAP VALUES**\n\n**SHAP (SHapley Additive exPlanations**) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions
* Majority of passengers borded from Southampton\n* Survival counts looks better at C. Why?. Could there be an influence from sex and pclass features we already studied?. Let's find out 
* We guessed correctly. higher % of 1st class passegers boarding from C might be the reason.
#### Filling Embarked NaN
"* Since 72.5% passengers are from Southampton, So lets fill missing 2 values using S (Southampton)"
---\n### Features: SibSip & Parch[^](#3_2_5)\n**Meaning :**  \nSibSip -> Number of siblings / spouses aboard the Titanic\n\nParch -> Number of parents / children aboard the Titanic\n\nSibSip + Parch -> Family Size 
"* The barplot and factorplot shows that if a passenger is alone onboard with no siblings, he have 34.5% survival rate. The graph roughly decreases if the number of siblings increase."
Lets combine above and analyse family size. 
* This looks interesting! looks like family sizes of 1-3 have better survival rates than others.
### Correlation Between The Features[^](#3_4)
"---\n# PART 2 : Feature Engineering and Data Cleaning[^](#4)\nNow what is Feature Engineering? Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\n\nIn this section we will be doing,\n1. Converting String Values into Numeric\n1. Convert Age into a categorical feature by binning\n1. Convert Fare into a categorical feature by binning\n1. Dropping Unwanted Features\n"
"# Finding Similar Books\n\nWe've trained the model and extracted the embeddings - great - but where is the book recommendation system? Now that we have the embeddings, we can use them to recommend books that our model has learned are most similar to a given book.\n\n\n### Function to Find Most Similar Entities\n\nThe function below takes in either a book or a link, a set of embeddings, and returns the `n` most similar items to the query. It does this by computing the dot product between the query and embeddings. Because we normalized the embeddings, the dot product represents the [cosine similarity](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/) between two vectors. This is a measure of similarity that does not depend on the magnitude of the vector in contrast to the Euclidean distance. (The Euclidean distance would be another valid metric of similary to use to compare the embeddings.)\n\nOnce we have the dot products, we can sort the results to find the closest entities in the embedding space. With cosine similarity, higher numbers indicate entities that are closer together, with -1 the furthest apart and +1 closest together."
"(We know that this function works if the most similar book is the book itself. Because we multiply the item vector times all the other embeddings, the most similar should be the item itself with a similarity of 1.0.)"
\nMultiple Box Plot
## Adding Xticks to Box Plot
# Horizontal Box Plot
"# Area Plot\n\nAn area chart is really similar to a line chart, except that the area between the x axis and the line is filled in with color or shading. It represents the evolution of a numerical variable following another numerical variable. "
# Horizontal Lollipop
"## Don't forget to upvote if you like it!. \nIf you have any doubt reagrding any part of the notebook, feel free to comment your doubt in the comment box.\n\nThank you!!\n\n## Work in Progress... ⏳"
That doesn't look right! The maximum value (besides being positive) is about 1000 years! 
"Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients."
"Well that is extremely interesting! It turns out that the anomalies have a lower rate of default. \n\nHandling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (`np.nan`) and then create a new boolean column indicating whether or not the value was anomalous.\n\n"
"The distribution looks to be much more in line with what we would expect, and we also have created a new column to tell the model that these values were originally anomalous (becuase we will have to fill in the nans with some value, probably the median of the column). The other columns with `DAYS` in the dataframe look to be about what we expect with no obvious outliers. \n\nAs an extremely important note, anything we do to the training data we also have to do to the testing data. Let's make sure to create the new column and fill in the existing column with `np.nan` in the testing data."
"As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often. \n\nLet's start looking at this variable. First, we can make a histogram of the age. We will put the x axis in years to make the plot a little more understandable."
"By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a [kernel density estimation plot](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) colored by the value of the target. A [kernel density estimate plot shows the distribution of a single variable](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn `kdeplot` for this graph."
"The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket. \n\nTo make this graph, first we `cut` the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category."
"All three `EXT_SOURCE` featureshave negative correlations with the target, indicating that as the value of the `EXT_SOURCE` increases, the client is more likely to repay the loan. We can also see that `DAYS_BIRTH` is positively correlated with `EXT_SOURCE_1` indicating that maybe one of the factors in this score is the client age.\n\nNext we can look at the distribution of each of these features colored by the value of the target. This will let us visualize the effect of this variable on the target."
"`EXT_SOURCE_3` displays the greatest difference between the values of the target. We can clearly see that this feature has some relationship to the likelihood of an applicant to repay a loan. The relationship is not very strong (in fact they are all [considered very weak](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf), but these variables will still be useful for a machine learning model to predict whether or not an applicant will repay a loan on time."
"## Pairs Plot\n\nAs a final exploratory plot, we can make a pairs plot of the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable. The [Pairs Plot](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166) is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.\n\nIf you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)! "
"In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the `EXT_SOURCE_1` and the `DAYS_BIRTH` (or equivalently `YEARS_BIRTH`), indicating that this feature may take into account the age of the client. "
"Using a patch with text is more effective.\n\nExcept for path patches, they are provided by default, so you can use them well.\n\n**Arrow is especially effective.**"
"## 4. Details & Examples\n\nIn the plot you can make various settings. \n\n> Of course, adding the text or annotate mentioned above is a good idea.\n\n**You can set the following details:**\n\n- Horizontal and Vertical (barplot)\n- Border(edge) color and thickness\n- Main Color & Sub Color\n- Transparency\n- Span\n\n### Font Weight, Color, Family, Size ...\n\nI usually set the details in font weight and size. It is good because it is easy to see just by setting it to bold. Also, if the size of the text is too big or too small, the proportions are strange.\n\nThe fonts do not vary, but the difference between serifs and sans serifs can make a difference.\n\nThe **Tex** syntax is also applicable, so use that as well.\n\n- **keyword** : `fontsize`, `color`, `fontweight`, `fontfamily`\n\n### Horizontal keyboard & Vertical (barplot)\n\nIn general, when the number of x-axes is large, the **readability** of the barplot is significantly lower.\n\nFor example, seaborn's `countplot` often overlaps the x-axis and often makes it less readable.\n\nThis is due to the fact that in a notebook environment, the width is the length of the monitor.\n\nSo in this case, you can place the graph vertically so that the axes are well read and the graph is easier to read."
"### Main Color & Sub Color\n\nIf you draw a plot of seaborn, it is displayed in various colors by default. You can draw various colorful graphs while changing the palette.\n\nBut basically, the visualization should focus on information, so it's not always nice to have a colorful plot.\n\nRather, think about what data you want to focus on.\nIt is better to make a difference between the main color with **strong color** and the sub color of **achromatic color** system.\n\nAlternatively, it is a good idea to choose the palette according to the application mentioned above.\n\nIn the case of a colormap, you can ***select a palette or pass it to a list***, so it is convenient to pass it to a list when highlighting a specific part."
"### Transparency\n\nI told you to use transparency above, but transparency is a great tool.\n\nScatter plots also contain important points with many **overlapping** points. That's why it's important to know the overlapping data by adjusting transparency.\n\nWhen **comparing** lineplots or barplots, placing two or more plots together using transparency allows you to hold comparison information.\n\nIf you add transparency to the graph, you can complete the graph with refined colors.\n\n- `alpha` : Parameter name of normal transparency setting"
"### Span\n\nGood division is enough to give points. Zoning can be added for a variety of reasons, depending on the content or to classify units.\n\n- `axvspan` : vertical span\n- `axhspan` : horizontal span\n\nyou can set color, alpha(transparency), range, etc"
"If you just use span and add text, you can change it as follows.\n(The title bold processing is below.)"
"## MEME : xkcd theme\n\n- [xkcd](https://xkcd.com/) : Webcomic for Geeks\n\n> Depending on the current kaggle and version, the font is broken, but you can draw a graph like this:"
And let's plot the raw pixelarray distribution for this example and display the related image:
"### Insights\n\n* Uhh! That's interesting! The background seems to be above 3000 this time! So it's competely different to previous competitions.\n* As the raw values depend on the machine, it could be worth it to explore background values dependent on the machine id. "
"At first glance, I would say that the RFR model produced the best predictive results, just looking at the scatter graph plotted. Overall both models, the integer and the dummy encoded models seem to perform relatively similar, although the dummy encoded model has a higher overall predicted mean."
"If we look at what influences the ratings, the top 4 being reviews, size, category, and number of installs seem to have the highest influence. This is quite an interesting observation, while also rationalizable."
"Looking at the breakdown even further, it would seem that indeed Reviews, size and number of install remain as a significant contributer to the predictiveness of app ratings.  What's interesting to me is that how the Tools category of apps have such a high level of predictiveness in terms of ratings, as say compared to the Food and Drink category."
"# Model Evaluation\nNow that we have trained the model, we can evaluate its performance. We will some evaluation metrics and techniques to test the model.\n\nLet's start with the Learning Curve of loss and accuracy of the model on each epoch."
"The model will output a prediction score between 0 and 1. We can classify two classes by defining a threshold value for it. In our case, I have set 0.5 as THRESHOLD value, if the score above it. Then it will be classified as **POSITIVE** sentiment."
"Run a LightGBM model and save for a submission to Kaggle.  This will also output feature importance.  This model scored 1.441 on Kaggle.  For this and the models that follow, remember to adjust the number of jobs(treads or processes) based on the CPU capabilities available.  As noted above, the feature importance from the LightGBM model was abandoned as a feature selection mechanism in favor of Pearson's correlation."
"This is the variant of the model with feature elimination performed by Pearson's correlation.  As noted below, these models usually scored higher individual scores on the Kaggle leader board."
### Feature Selection
"Get ready for what you're about to see. I must confess that the first time I saw these scatter plots I was totally blown away! So much information in so short space... It's just amazing. Once more, thank you @seaborn! You make me 'move like Jagger'!"
"Although we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\n\nOne of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you're trying to buy a bunker).\n\nThe plot concerning 'SalePrice' and 'YearBuilt' can also make us think. In the bottom of the 'dots cloud', we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the 'dots cloud' (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\n\nOk, enough of Rorschach test for now. Let's move forward to what's missing: missing data!"
"We already know the following scatter plots by heart. However, when we look to things from a new perspective, there's always something to discover. As Alan Kay said, 'a change in perspective is worth 80 IQ points'."
"What has been revealed:\n\n* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\n* The two observations in the top of the plot are those 7.something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them."
The point here is to test 'SalePrice' in a very lean way. We'll do this paying attention to:\n\n* Histogram - Kurtosis and skewness.\n* Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.
"Ok, 'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\n\nBut everything's not lost. A simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of positive skewness, log transformations usually works well. When I discovered this, I felt like an Hogwarts' student discovering a new cool spell.\n\n*Avada kedavra!*"
Done! Let's check what's going on with 'GrLivArea'.
Tastes like skewness... *Avada kedavra!*
\n### Scatter Plot on large dataset
⬆️Back to Table of Contents ⬆️
\n### Animated Scatter Plot
####  Press the play ▶️ button to see the animation
\n### Basic Line Plot
\n### Line Plot with Column Encoding Color
\n### Line Plot with markers(symbols)
\n### Line Plot with Date Axes 
\n### Style Line Plot
\n### Basic Pie Plot
\n### Pie Plot with Hover Text
\n### Donut Plot
"https://colab.research.google.com/\n\nGoogle colab just barely edges out Kaggle Hosted Notebook services. Just like the Kaggle hosted interface, it is simple to use and has no limits on the GPU usage. Maybe that is one of the reason why people prefer this? Or is it because, users can directly get their datasets from the google drive? The fact that we can connect to our local runtime? Who knows.\nWhatever might be the case, it is worth to notice that a decent number of people still prefer to run their codes on their local machine. Probably, in my opinion if Kaggle Kernels can be made more user friendly and provide more features similar to that of Google colab, it might turn out to be a popular choice by next year!"
"I spy... A nimbus in the sky!\n\nFrom the word cloud, it is evident that most Kagglers like to use their own Laptop, followed by Cloud Platform and Personal Computer.\nRemember, that from our previous analysis, we saw how most of the individuals on Kaggle are Students? They might not have the money for fancy Deep Learning Platforms and cool gadgets like that. Hence, a Laptop is a very popular choice.\nAnother thing noticing here is that, the next best choice are the cloud computing platforms. Usually this is the case for people who's Laptops or Personal Computers are not compatible enough to run Machine Learning libraries."
"A Kaggler's Secret Ingredient:\n\nKaggle provides free access to NVIDIA TESLA P100 GPUs. These GPUs are useful for training deep learning models, though they do not accelerate most other workflows (i.e. libraries like pandas and scikit-learn do not benefit from access to GPUs).\n\nYou can use up to a quota limit per week of GPU. The quota resets weekly and is 30 hours or sometimes higher depending on demand and resources\n\nRead more about GPUs here."
"Dad... I need an Nvdia RTX 3090Ti for School👉👈\n\nOfcourse I need a 3090Ti for studies. Kidding. Nvdia GPUs are preferred by many for it's ability to process ML models and especially DL models faster!\nThat is followed by Google Cloud TPUs due to it's easy availability.\nAlso, if you notice, there are many Kagglers that do not use any specialized Hardware. Now there is a reason for that. Not everyone knows Deep Learning and Scikit Learn does not support GPUs. Therefore, for most Kagglers, they do not require any hardware as such to increase their computing power!\nNow this is not the case for people using TPUs and GPUs. As of now, I am speculating these Kagglers participate in Competitions a lot! And performing large number of cross-validations/training on several models takes a lot of time and effort if there is no GPUs or TPUs."
"Let's look at Kaggler's TPU usage next: \n\nTPUs are now available on Kaggle, for free. TPUs are hardware accelerators specialized in deep learning tasks. They are supported in Tensorflow 2.1 both through the Keras high-level API and, at a lower level, in models using a custom training loop.\n\nYou can use up to 30 hours per week of TPUs and up to 9h at a time in a single session.\n\nRead more about TPUs here."
∞ Power \n\nTPUs are now available on Kaggle and are mainly used during Competitions or for other Deep Learning and Computer Vision purposes.\nThe reason why most people do not use TPUs remains almost the same as why people do not use GPUs\nMostly Kagglers tend to use it 2-5 times a week or maybe once in a while.
"Tools of Trade:\n\n “Through all of your snares, Blade Fury cuts like silk.” – Juggernaut (DOTA 2) \n\nThe above quote is one of my favourite quotes from the online MOBA game Defense of The Ancients a.k.a DOTA 2. The reason why I put this quote here is because, the data that we get most of the time are unexplored with many small traps! If we understand a data incorrectly or interpret it differently, it may lead us astray. Hence, these are similar to snares in this case. Blade Fury are the tools that we have at our disposal to cut through these snares!\n\nCome, let's find out, the popular tools used by every Kaggler."
"Everyone's Favourite \n\nMatplotlib, Seaborn and Plotly continues to be the popular choice among many Kagglers followed by GGPlot for R users. Especially because they are so easy to implement. We can also see that there are other libraries like Geoplotlib, Bokeh and Folium trending these days on kaggle.\nMoving onto the Machine Learning usage of Kagglers, we notice the correlation again that since there are more number of Students on this platform, we have more number of kagglers who's ML experiece ranges from less than 1 year to 3 years. Another interesting thing to notice here is that a lot of people here on Kaggle do not use Machine Learning. They work on Data Analysis or Descriptive Statistical Analysis, but are not actively involved with Predictive Analystics.\nSkLearn continues to be the top ML framework used by many Kagglers followed by Tensor-flow and Keras. It is also worth noticing that Pytorch, XGBoost, LightGBM and CatBoost also have decent popularity among Kagglers. On the other hand, JAX, MXNET and H2O 3 continue to be at the bottom of the list.\nMoving onto the popular ML Algorithms, we can clearly see that Linear Models, Tree Models and CNN are the most widely preferred Algorithms here on Kaggle. This is followed by Dense Neural Net, RNN and Bayesian Approaches. Now this is expected since most of the population of Kagglers are students and they start usually by learning about Linear and Tree models.\nImage classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc), Image segmentation methods (U-Net, Mask R-CNN, etc) and Object detection methods (YOLOv3, RetinaNet, etc) are the most popular Computer Vision Modules used by Kagglers here. Now this is not used by many Kagglers. These are only used by a little proportion of people.\nNLP methods are also used by a certain part of the population. Not everyone uses this. But for those that do - Word embeddings/vectors (GLoVe, fastText, word2vec), Transformer language models (GPT-3, BERT, XLnet, etc) and Encoder-decoder models (seq2seq, vanilla transformers) are the most popular among them."
*  Ticket Prices for the Titanic
# Cufflinks:\n \nCufflinks is another library that connects the Pandas data frame with Plotly enabling users to create visualizations directly from Pandas
**Data Cleaning**\n\n* We want to fill in missing age data instead of just dropping the missing age data rows.\n\n* One way to do this is by filling in the mean age of all the passengers (imputation). \n\n* However we can be smarter about this and check the average age by passenger class. For example:
"\nWe can see the wealthier passengers in the higher classes tend to be older,which makes sense.\n\nWe'll use these average age values to impute based on Pclass for Age."
"#### **1.5.1 Continuous Features**\nBoth of the continuous features (`Age` and `Fare`) have good split points and spikes for a decision tree to learn. One potential problem for both features is, the distribution has more spikes and bumps in training set, but it is smoother in test set. Model may not be able to generalize to test set because of this reason.\n\n* Distribution of `Age` feature clearly shows that children younger than 15 has a higher survival rate than any of the other age groups\n* In distribution of `Fare` feature, the survival rate is higher on distribution tails. The distribution also has positive skew because of the extremely large outliers"
#### **1.5.2 Categorical Features**\nEvery categorical feature has at least one class with high mortality rate. Those classes are very helpful to predict whether the passenger is a survivor or victim. Best categorical features are `Pclass` and `Sex` because they have the most homogenous distributions.\n\n* Passengers boarded from **Southampton** has a lower survival rate unlike other ports. More than half of the passengers boarded from **Cherbourg** had survived. This observation could be related to `Pclass` feature\n* `Parch` and `SibSp` features show that passengers with only one family member has a higher survival rate
"### **1.6 Conclusion**\nMost of the features are correlated with each other. This relationship can be used to create new features with feature transformation and feature interaction. Target encoding could be very useful as well because of the high correlations with `Survived` feature.\n\nSplit points and spikes are visible in continuous features. They can be captured easily with a decision tree model, but linear models may not be able to spot them.\n\nCategorical features have very distinct distributions with different survival rates. Those features can be one-hot encoded. Some of those features may be combined with each other to make new features.\n\nCreated a new feature called `Deck` and dropped `Cabin` feature at the **Exploratory Data Analysis** part."
"### **2.2 Frequency Encoding**\n`Family_Size` is created by adding `SibSp`, `Parch` and **1**. `SibSp` is the count of siblings and spouse, and `Parch` is the count of parents and children. Those columns are added in order to find the total size of families. Adding **1** at the end, is the current passenger. Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates.\n* Family Size with **1** are labeled as **Alone**\n* Family Size with **2**, **3** and **4** are labeled as **Small**\n* Family Size with **5** and **6** are labeled as **Medium**\n* Family Size with **7**, **8** and **11** are labeled as **Large**"
"There are too many unique `Ticket` values to analyze, so grouping them up by their frequencies makes things easier.\n\n**How is this feature different than `Family_Size`?** Many passengers travelled along with groups. Those groups consist of friends, nannies, maids and etc. They weren't counted as family, but they used the same ticket.\n\n**Why not grouping tickets by their prefixes?** If prefixes in `Ticket` feature has any meaning, then they are already captured in `Pclass` or `Embarked` features because that could be the only logical information which can be derived from the `Ticket` feature.\n\nAccording to the graph below, groups with **2**,**3** and **4** members had a higher survival rate. Passengers who travel alone has the lowest survival rate. After **4** group members, survival rate decreases drastically. This pattern is very similar to `Family_Size` feature but there are minor differences. `Ticket_Frequency` values are not grouped like `Family_Size` because that would basically create the same feature with perfect correlation. This kind of feature wouldn't provide any additional information gain."
### **2.4 Target Encoding**\n`extract_surname` function is used for extracting surnames of passengers from the `Name` feature. `Family` feature is created with the extracted surname. This is necessary for grouping passengers in the same family. 
"`Family_Survival_Rate` is calculated from families in training set since there is no `Survived` feature in test set. A list of family names that are occuring in both training and test set (`non_unique_families`), is created. The survival rate is calculated for families with more than 1 members in that list, and stored in `Family_Survival_Rate` feature.\n\nAn extra binary feature `Family_Survival_Rate_NA` is created for families that are unique to the test set. This feature is also necessary because there is no way to calculate those families' survival rate. This feature implies that family survival rate is not applicable to those passengers because there is no way to retrieve their survival rate.\n\n`Ticket_Survival_Rate` and `Ticket_Survival_Rate_NA` features are also created with the same method. `Ticket_Survival_Rate` and `Family_Survival_Rate` are averaged and become `Survival_Rate`, and `Ticket_Survival_Rate_NA` and `Family_Survival_Rate_NA` are also averaged and become `Survival_Rate_NA`."
Let's repeat the same for the messages that got silver and gold medals. The code here is mostly a copy-paste from the previous section without changing the variable names.
It looks like there might be at least 2 clusters here.
"In one of the clusters the most frequent words are ""competition"", ""kernel"", ""submission"", ""time"", ""score"", etc. These messages probably discuss competition scores and related kernels.\n\nIn another cluster the most frequent words are ""model"", ""feature"", ""score"", ""lb"" (short for ""leaderboard""), ""datum"", etc. These messages probably focus on studying model features and improving scores. Note that the word ""competition"" is also quite frequent in this cluster which means that these messages are also related to competitions."
"The number of bronze medals for each forum title is shown below. Notice that bronze medals are predominantly granted in ""Kernels"". This is in line with the previous observation that the messages with bronze medals mostly express appreciation of other people's work. Indeed, this happens a lot in the comments section of kernels."
"The distriubtion of silver/gold medals is significantly different. The majority of medals are granted in the recent Kaggle competitions such as ""Microsoft Malware Prediction"", ""Quora Insincere Questions Classification"", ""Elo Merchant Category Recommendation"", etc. This is in line with the previous observation that the messages with silver/gold medals mostly focus on competitions."
## 5. Conclusion
"\n  Takeaway: We've learned that our high-end respondents are Python people, usually coding on their personal gear. The usual environments used are Jupyter Notebooks, PyCharm, Colab, and Kaggle.\n  They most often use GPUs, but no worries, No Accelerator whatsoever is a popular choice as well - meaning skill only can bring you some pretty awesome results as well. ;)\n\n\n> The Code Waters is now visible!\n\n\n\n> Closer look 🔎\n\n\n3.5 The Machine Learning Preferences\n\nIn this last chapter, we'll find out what machine learning tools and resources these Data Science veterans use, as well as what we need to focus on when we start our Machine Learning journey.\n\nLet the hunt conclude in elegance!\n\nReliable Sources to Keep Informed\n\nThe learnings below are representative for all 3 levels of Pay Categories (meaning that all behave in the same manner, so the split wasn't shown):\n* For *learning*, the most popular choice was **Coursera**; as you may know, Coursera has the most popular [Machine Learning course](https://www.coursera.org/learn/machine-learning) from Stanford University, with the top instructor being the King of ML, **Andrew Ng**. This aspect might have significantly influenced the overall decision.\n* Other very popular Course platforms were **Kaggle Learn, Udemy, and University-specific courses**.\n* As *resources for getting the daily Data Science intake*, **Youtube** had the lead (possibly from the multitude of videos on different abstract topics, which are better understood from animated content rather than by only reading them). **Blogs** (such as Towards Data Science), **Twitter, and Publications** were a popular choice as well.\n* Hence, the ""go-to"" reliable sources for getting that Data Science information were Coursera, Kaggle Learn, Udemy, Youtube, and Blogs."
"What Frameworks to have in your Pocket?\n\nThe below graphs and explanations are representative for all 3 Pay Categories:\n* For the *overall Frameworks*, **Scikit-learn** has the lead. This is also **the oldest** (13 years), the **most volatile and versatile** library, so the fact that it has the most popularity isn't a shock.\n* Out of the deep learning frameworks, **Tensorflow** is the most popular, but PyTorch is starting to gain some visibility itself (it is *1 year younger* than Tensorflow, so it's only natural that it is a little bit behind).\n* As for the *visualization libraries*, our top-end respondents mainly prefer and use **the originals** Matplotlib, Seaborn, and Plotly, with Ggplot for R.\n* Hence, the industry's top data scientists use lots of Scikit-learn, Tensorflow, Matplotlib, and Seaborn. However, we mustn't lose sight of the emerging libraries that have the potential to overcome soon the giants, such as PyTorch (can you already tell I'm a fan of PyTorch?👀)"
"Machine Learning & Deep Learning Basics\n\nThe below graphs and explanations are representative for all 3 Pay Categories:\n* Awkwardly enough, the most frequent choices for *ML Methods* were **Regressions and Tree-Based methods**, which are more straightforward approaches than Neural Nets. Hence, complicated methodologies aren't necessarily better.\n* **Classification** and **General Image Methods** (like cv2, PIL, or skimage) are the most popular in the *Computer Vision* department.\n* Regarding *NLP*, our top respondents were **Word Embeddings** and **Transformers**.\n* Hence, the most popular choices were the simpler ones, such as Regressions, Tree-Based Methods, General & Classification methods, and Word Embeddings."
"Analysis at Work and Deployment\n\nThe visualization and discussion below are representative for all 3 Pay Categories:\n* Regarding *Deployment*, most of our top respondents prefer to either deploy on **GitHub** or **keep their models locally on their personal gear**. **Kaggle and Colab**, however are popular options as well.\n* For *Tools Analysis*, the most popular are the **Local Development Environments**, like *Jupyter Notebooks* or *R Studio*. \n* So, our top respondents analyze data mostly on Local Environments and deploy models either on GitHub, or keep them on their local machine."
"Machine Learning - Miscellaneous Tools\n\nFinally, the visualization and discussion below are representative for all 3 Pay Categories as well:\n* For *ML Products*, there is a general lack of interest for all the presented options, as **None** was the most popular choice. However, most respondents say that they would like to gain knowledge of **Google Cloud Products** in the next 2 years.\n* Regarding *ML Experiments*, there is even a more prominent lack of interest, as **None** is the predominant answer, followed from far by **TensorBoard**. The **carelessness continues** in the projection over the next 2 years. So, ML Experiments, not so popular at the moment.🤷‍♀️\n* There is a lack of frequent usage in *Automated ML* as well; however, there can be seen a general trend for **Automated Model Selection** for the next 2 years.\n* Hence, *ML products, Experiments, or Automated ML* aren't a popular choice between our top respondents. A reason for that could be the inclination towards old school coding, as they might prefer writing everything from scratch, going through the data, the patterns, and the models with ""their own hands"", instead of relying on a service. But I am sure this fact will change over the years."
"\n  Takeaway: Our top respondents use Youtube, Personal Blogs, and Twitter to keep informed on the latest Data Science ""gossips"" while having lots of trust in Coursera as a reliable source of learning.\n  As for tools and methods, even though one would expect some complicated answers, they still use and rely the most on simple structures, like Regressions, Tree-Based models, Image Classification, or Word Embeddings for NLP. Most loved frameworks are Scikit-learn, Tensorflow, and Matplotlib, with no complicated or additional current preference for miscellaneous tools.\n  There IS something to learn here. There was a tweet from Grandmaster Bojan Tunguz that was sarcastically stating that all models besides Neural Nets are obsolete and you should focus only on these. The takeaway here is that no simple method, tool, or model is ""redundant"", not even for the great of the great. And this chapter shows precisely that.\n\n\n\n\n> And finally, our map is completely revealed!\n\n\n\n> Closer look 🔎\n\n\n\n# 4. Conclusion\n\nI for one would have loved to see a survey like this when I started, some 1 year and a half ago. With no experience and no slight clue what data science is, what programming language do I need to know, how do you do machine learning, what is a notebook, how deep is deep learning, and ... for the love of God, GPUs? And, with the internet full of healthy and diverse opinions, it is easy to get lost and start not to trust the sources.\n\nHence, I truly hope that this analysis will bring some light for anybody in any query they might have. However small or big, I think that it's best to ask a professional, a great veteran in the discipline, a guru, a Grandmaster :) what is the best way to learn that specific subject? And if this notebook helped you just a bit getting closer to your answers, or a mentor, I can declare myself happy and fulfilled. I know it helped me, at least.\n\nGood luck, and may we all succeed with grace. Happy Data Sciencin'!\n\n# 5. References 📜\n\n* [Numbero Cost of Living Index](https://www.numbeo.com/cost-of-living/rankings_by_country.jsp)\n* [Pareto Principle](https://en.wikipedia.org/wiki/Pareto_principle)\n* [TPU vs GPU](https://analyticsindiamag.com/tpu-vs-gpu-vs-cpu-which-hardware-should-you-choose-for-deep-learning/#:~:text=TPU%20vs%20GPU%20vs%20CPU%3A%20A%20Cross%2DPlatform%20Comparison&text=TPU%3A%20Tensor%20Processing%20Unit%20is,small%20batches%20and%20nonMatMul%20computations.)\n* [Machine Learning Course - Stanford](https://www.coursera.org/learn/machine-learning)\n\n\n\n# Specs ⌨️🎨\n### (*tools that helped visualisation & creating the pirate map*)\n* Z8 G4 Workstation 🖥\n* 2 CPUs & 96GB Memory 💾\n* NVIDIA Quadro RTX 8000 🎮\n* RAPIDS version 0.17 🏃🏾‍♀️"
"Frequency components of the signal could be very interesting to look at.  This is a plot of the Fourier transform magnitude for some of the test signals.  Note that there appears to be little information in the signal above the 20,000th frequency line.  Noise appears to mostly disappear above the 25,000th frequency line.  It is difficult to translate this to a frequency because of the signal gaps noted earlier.  Still, it may be best to concentrate signal analysis on frequencies below those represented by the 20,000th frequency line.  Also, there are peaks in the frequency analysis that may be valuable to collect in some manner.  The DC component was eliminated for plotting purposes because it would otherwise dominate the plot and make the other frequencies hard to see.  Also note that while referred to as an ""FFT"" in the code below, this is actually a Discreet Fourier Transform (DFT) because the signal length of 150k samples is not a number that is a power of two."
"This is a set of plots of the Fourier transform, windowed by short cosine tapers near the ends.  The idea of using the window is to avoid any start up transients that might cause ringing in filters applied to the signal.  It is very interesting that windowing seems to emphasize noise.  This noise, however, would almost entirely be removed by a low pass or band pass filter.  Windows are used to force the signal to be periodic in the time domain which reduces leakage effects at the signal endpoints in the Fourier transform."
"Phase plots for the Fourier transform.  The signal was windowed by small sections taken from a Hanning window along the edges in order to not cause an impulse-like transient in a potential filter at start-up.  Phase plots show what appears to be just noise. Probably limited phase features will be all that is desired for the model, perhaps just the standard deviation."
"From the code below, the test set ""big peaks"" seem to match the proportion with which they are found in the training data at 0.38% of signals (test set signals are all 150k samples long).  The number of earthquakes in the training set seems to be approximately proportional to the overall size relationship between the two sets. With 16 quakes in the training set we might expect 10 quakes in the test set based on their relative sizes.  As seen by the code output, there are 10 such files within the test set that exhibit the peak behavior.   However, there is also concern about the possibility of bogus peaks in the test data that do not correlate well with an actual quake.  The expectation of 10 possible quakes in the test set should be considered cautiously and as only an approximation."
> True Positives (TP) – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n> True Negatives (TN) – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n> False Positives (FP) – False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called Type I error.\n\n> False Negatives (FN) – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called Type II error.\n\nThese four outcomes are summarized in a confusion matrix given below.
\n    \n
* It is important to know the distribution of data according to the labels they have.\n* This data set is __homogeneously__ distributed as you see below.
"* __If the data wasn't homogeneously__ distributed what would we do?\n    1. Then we could use data augmentation techniques to generate new data for low quantity labels,\n    2. Or if we have enough data we can discard some high quantity labels"
#### **Image of Handwritten Character** \n[Return Contents](#0)\n\n    \n * An overview of a picture\n * You can change the $num$ variable to see other numbers.
"## **Data Preprocessing** \n### **Normalizing Data** \n[Return Contents](#0)\n\n* What is normalizing? Normalization means that adjusting values measured on different scales to a notionally common scale.\n* Why should you normalize the data?  With a normalized data weight values reach optimum value faster.\n* On image processing applications generally we normalize data to 0-1 scale with dividing data to 255.\n* Because each pixel in every sample of training set has integer values from 0 to 255.\n* In order to normalize training set data, we need to convert x to float type."
> Gender and Response
It is noticable that Gender does not play much role in Response of the Customer and the ratio for both positive and negative response is almost equal.\n
> Age and Response
Young people (under 35) have little interest in insurance.\nPeople over the age of 35 tend to be more interested in insurance.
> Driving License and Response
Having a driver's license doesn't play much of a role in Customer Feedback.\n
> Region code and Response
Less correlation is there between region and response.
> Vintage and Response
It seems that Vintage has no effect on the response. The rate of positive and negative feedback seems to be about the same
> Vehicle_Damage and Response
Damaged vehicles tend to respond positively
> Correlation between attributes
> Highly correlated columns wrt to target columns which can give us `better accuracy`.
\n# Overall Anime Ratings\n
**Insights:**\n\n* Most of the Anime ratings are spread between 5.5 - 8.0\n* Most of the users ratings are spread between 6.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
\n# Top Animes Based On Ratings\n
**Insights:**\n\n* **Mogura no Motoro** wears the crown for highest rating followed by **Kimi no Na wa.** and **Fullmetal Alchemist: Brotherhood**
\n# Category-wise Anime Ratings Distribution\n
**Insights:**\n\n* Most of the Anime ratings are spread between 6.0 - 8.0\n* Most of the users ratings are spread between 6.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 9.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 5.5 - 7.5\n* Most of the users ratings are spread between 5.5 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 4.5 - 8.5\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 9.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 5.5 - 8.0\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 4.0 - 7.0\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 4.0 - 7.5\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 6.5 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
Let's look at the relationship between models and frameworks.
"I can observe the following parts:\n\n- **Scikit-Learn** : Linear/Logistic Regression, Decision Trees, Random Forest\n- **Keras, Tensorflow** : CNN \n- **Pytorch** : More wide deep Learning task\n- GBMs\n\nIf you're a newbie, this library is a good reference.\n\n- [scikit-learn](https://scikit-learn.org/stable/)\n- [Keras](https://keras.io/)\n- [Tensorflow](https://www.tensorflow.org/)\n- [PyTorch](https://pytorch.org/)\n- [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n- [LightGBM](https://lightgbm.readthedocs.io/en/latest/Python-API.html)\n\nAfter all, it's important to know everything.\n\nIf you are a beginner, we recommend studying in the following order.\n\n- Classic Machine Learning \n    - KNN, Linear/Logistic Regression, Decision Tree \n    - with Scikit-Learn\n    - The application of ML is important, but the content of it is important. That's why it's important to learn intuition and mathematical concepts in the classic ML.\n- Ensemble \n    - stacking, bagging, GBMs\n    - XGBoost -> LightGBM -> CatBoost\n    - LGBM seems to be the trend now.\n- Deep learning \n    - Personally I recommend starting with Keras or Pytorch.\n"
"## 6. What is the relationship between Programming Career and Language recommendations?\n\n> Q15. How long have you been writing code to analyze data (at work or at school)?\n\nIn fact, the person doing the analysis here expects to have almost the same distribution because there are many ML workers, but let's do it once."
"## 7. Personal thoughts about the visualization library\n\n> Q20. What data visualization libraries or tools do you use on a regular basis? (Select all that apply) - Selected Choice\n\nI like visualization the most in data analysis.\n\nSo I have made the following kernel : \n\n- [Road to Viz Expert (1) - Unusual tools](https://www.kaggle.com/subinium/road-to-viz-expert-1-unusual-tools)\n- [Road to Viz Expert (2) - Plotly & Seaborn](https://www.kaggle.com/subinium/road-to-viz-expert-2-plotly-seaborn)\n- [Road to Viz Expert (3) - Geo with Plotly Express](https://www.kaggle.com/subinium/road-to-viz-expert-3-geo-with-plotly-express)\n- [Road to Viz Expert (4) - Unusual Tools II](https://www.kaggle.com/subinium/road-to-viz-expert-4-unusual-tools-ii)\n\n- [3D Interactive Car🚗 with Plotly](https://www.kaggle.com/subinium/3d-interactive-car-with-plotly)\n- [Weather Dashboard : EDA & Visualization ⛅🌡️](https://www.kaggle.com/subinium/weather-dashboard-eda-visualization)\n- [Mask visualization, managing with buttons!](https://www.kaggle.com/subinium/mask-visualization-managing-with-buttons)\n\nThe various visualizations of this kernel can be found in my various kernels.\n\nLet's see survey again."
"The US have the largest community of lenders and it is followed by Canada and Australia. On the other hand, the African continent seems to have the lowest number of funders which is to be expected, since it's also the region with highest poverty rates and funding needs.\n\nSo now that we know more about lenders location, let's analyze the textual freeform column *loan_because* and construct a wordcloud to get an insight about their motives for funding proejcts on Kiva."
"Lenders' answers are heartwarming :) Most reasons contain *help people / others* or *want to help*. We also find that it's the *right thing* (to do), it helps *less fortunate* and makes the world a *better place*.  \nKiva provides a platform for people who need help to fund their projects but it also provides a platform for people who want to make a difference by helping others and maybe changing their lives !"
# Decision Tree (only to show actual trees)
# XGBOOST
###Interactive visualisations of LDA representation
*PLEASE CLICK AND MOVE THE SCATTER PLOTS ABOVE. THEY ARE INTERACTIVE. DOUBLE CLICK TO GET BACK TO THE ORIGINAL VIEW*
Having invoked the t-SNE algorithm by simply calling *TSNE()* we fit the digit data to the model and reduce its dimensions with *fit_transform*. Finally let's plot the first two components in the new feature space in a scatter plot
"Now, we are going to analyze both the sex and the heart health situation."
"In this section, the rate of disease is seen less when the gender value is male. This is the result of an analysis for us."
"## Risk Assesment:\nThe main aim in this section is to compare the average interest rate for the loan status belonging to each type of loans (Good loan or bad loan) and see if there is any significant difference in the average of interest rate for each of the groups.\n\n## Summary: \n\n  Bad Loans:   Most of the loan statuses belonging to this group pay a interest ranging from 15% - 16%. \nGood Loans: Most of the loan statuses belonging to this group pay interest ranging from 12% - 13%.  \nThere has to be a better assesment of risk since there is not that much of a difference in interest payments from Good Loans and Bad Loans. \n Remember, most loan statuses are Current so there is a risk that at the end of maturity some of these loans might become bad loans. \n\n\n\n\n*Credits to Zhiwen for providing an important aspect of the analysis (Relationship of interest rates and loan condition).*"
"## Condition of Loans and Purpose:\n\nIn this section we will go into depth regarding the reasons for clients to apply for a loan.  Our main aim is to see if there are purposes that contribute to a  ""higher""  risk whether the loan will be repaid or not.\n\n### Summary: \n\n Bad Loans Count:  People that apply for educational and small business purposed tend to have a higher risk of being a bad loan. (% wise) \nMost frequent Purpose:  The reason that clients applied the most for a loan was to consolidate debt. \nLess frequent purpose: Clients applied less for educational purposes for all three income categories.  \nInterest Rates:  In all reasons for application except (medical, small business and credi card), the low income category has a higher interest rate. Something that could possibly explain this is the amount of capital that is needed from other income categories that might explain why the low income categories interest rate for these puposes are lower.  \nBad/Good Ratio: Except for educational purposes (we see a spike in high income this is due to the reasons that only two loans were issued and one was a bad loan which caused this ratio to spike to 50%.), but we can see that in all other purposed the bad good ratio is lower the higher your income category.  \n\n"
"## Evaluation, prediction, and analysis\n* CART (non-linear algo)"
"## Evaluation, prediction, and analysis\n* SVM (Non-linear algo)"
"## Evaluation, prediction, and analysis\n* Bagged Decision Trees (Bagging)"
"## Evaluation, prediction, and analysis\n* Random Forest (Bagging)"
"## Evaluation, prediction, and analysis\n* Extra Trees (Bagging)"
"## Evaluation, prediction, and analysis\n* AdaBoost (Boosting)"
"## Evaluation, prediction, and analysis\n* Stochastic Gradient Boosting (Boosting)"
"## Evaluation, prediction, and analysis\n* XGBoost"
"## Evaluation, prediction, and analysis\n* MLP (Deep Learning)"
## Make Predictions
That looks much better. Note: removal of data is totally discretionary and may or may not help in modeling. Use at your own preference.
Again with the bottom two data-points. Let's remove those outliers.
"Only 0.01 point Pearson-R Score increase, but looks much better!"
Everything looks fine here.
Looks good.
It seems like houses with more than 11 rooms come with a $100k off coupon. It looks like an outlier but I'll let it slide.
"Although it seems like house prices decrease with age, we can't be entirely sure. Is it because of inflation or stock market crashes? Let's leave the years alone."
"The classifier shows a decent accuracy score of about 80%, let's now look at the permutation importance of every variable. "
"The chart shows a few interesting traits of people associated with university degrees: More use of R than Python, More focus on research and deep learning, mainly young individuals (aged 22-30), involved in coding for at least a year, not doing kaggle or MOOCs. \n\n\nConclusions  \n\nThe main focus of this analysis was to understand and evaluate if there are any significant differences in the self-taught scientists and university degree holders. The insights presented in this notebook, convey a broad message about the importance of ""masters in data science degrees"". We can observe that it is not impossible to achieve certain job roles or position and even compensation without any degree in data science/analytics. In Section 3 we saw that there are as many as self-made data scientists (without any degree) earning similar compensation as with the degree holders. The case is not just restricted to the USA but also true in many other countries. This means that organizations value skills and talent over any degree, that's why they pay huge chunks of money for that skill. We also observed that there are no major distinctions in the type of work individuals in this field do on a daily basis, everyone has to spend a good amount of time in data analysis and cleaning, There are almost equal percentage of respondents who build machine learning services, prototypes and also do experimentation. \n\nIt might be wrong to strongly say that it’s not worth it all. There are many positives associated with it as well: One can get edge during job hunt over those who do not have masters degree, one can establish their network with professors, fellow students, and industry partners, one can get exposure to many new areas, and can polish a lot of skills. Academic institutions are responding to a market demand and generating programs, due to it. It is possible that what students learn might be very different from what a data scientist does.\n\nJeremy Howard said the following in a [reddit AMA](https://www.reddit.com/r/Futurology/comments/2p6k20/im_jeremy_howard_enlitic_ceo_kaggle_past/cmty7kf/): \n> There is nothing that you could learn in a master of computer science that you could not learn in an online course, or by reading books. However, if you need the structure or motivation of a formal course, all the social interaction with other students, you may find that useful. I think that online courses are the best way to go if you can, because you can access the world's best teachers, and have a much more interactive learning environment — for example the use of short quizzes scattered throughout each lecture.\n> Specifically, the courses that I would recommend our the machine learning course on Coursera, the introduction to data science course on Coursera, and the neural networks for machine learning course — surprise surprise, also on Coursera! Also, all of the introductory books here are a good choice.\n> The most important thing, of course, is to practice download some open datasets from the web, and try to build models and develop insights, and post them on your blog. And be sure to compete in machine learning competitions — that's the only way to know whether your approaches are working or not.\n\nThe key takeaway is that one can become a data scientist (or any other similar position in this field) with right skills and can also manage to get decent compensation even without going through ""Masters in Data Science (or similar degrees)"". There are many alternatives - Coursera, Kaggle, Udemy, Udacity, eDX, Linkedin Learning, Fast.ai to name a few. These resources are either completely free or are much cheaper than dedicated university degrees. Additionally, one can complete these courses in a much shorter time as compared to the university courses. If there are any other reasons for the desire to pursuing masters, example - personal goals, passion, interest, utilizing the breaks etc, one should consider pursuing university courses.\n\nIf someone is thinking of getting a degree, in data science, analytics, just because they have the time and money to spare, is not wise. If the interest in analytics is driven by the desire for a job that offers better rewards than what they are getting now, then one should look for alternative paths to make that career move. [Meta S Brown from Forbes](https://www.forbes.com/sites/metabrown/2016/07/29/4-reasons-not-to-get-that-masters-in-data-science/#4813020640c0) says, ""At the end, we need to remember that Data Scientist is a Title. Many give themselves or have this title because that's the work they do, not because they have a particular degree"". \n\nIt's just about taking the first step, start studying from online courses, do relevant projects, start making kaggle contributions, and keep improving your resume and profile. In the end, whatever path someone takes, they will most likely produce similar outcomes. The following Sankey depicts this message very well. "
"""Is Spending $$$ for MS in Data Science worth it?"" \n\n1. If someone wants to learn data science skills only, then no its not worth it because there are many excellent free / cheaper alternatives on the internet. Kaggle is an excellent place to start.    \n2. If someone wants to work as a data scientist (fresher or career change), then it may or may not be worth it because there is no gurantee that one will get a data science job after the university degree. A non degree holder with right skillset, right experience, and right attitude may get a better job than the one with a degree title.  \n3. If someone wants to earn more, then it is not necessary to have university degree, as one with good experience or good skills can grab very high compensation packages.  \n4. If someone wants to do more research, then it may be worth it given that one chooses a right specialization.  \n4. If someone is passionate to learn more and gain exposure then it is definately worth it. University degree can provide a platform to network, participate in different events, and improve the foundation.  \n\n\nReferences   \n\n1. https://www.forbes.com/sites/metabrown/2016/07/29/4-reasons-not-to-get-that-masters-in-data-science/#6b43eb2140c0\n2. https://www.mybaggage.com/blog/why-do-a-masters-degree-the-pros-and-cons/\n3. https://e2i.com.sg/can-you-thrive-without-a-degree-in-singapore/\n4. https://www.kdnuggets.com/2014/06/masters-degree-become-data-scientist.html\n5. https://medium.com/@LiuhuayingYANG/is-your-income-lower-than-others-3d20f9e579bc\n6. https://www.quora.com/Is-a-masters-in-Data-Science-worth-it\n7. https://towardsdatascience.com/was-it-worth-studying-a-data-science-masters-c469e5b3e020\n8. https://www.cleverism.com/4-reasons-not-to-get-that-masters-in-data-science/\n9. https://www.kaggle.com/general/61082\n"
# Values distribution based on sepal length
# Values distribution based on sepal width
- From the above four graph you can see that the distribution of setosa < vericolor < virginica\n- There are few outliers which can be explained by the scatter plot graph.
# Andrew curves
Andrews curves are a method for visualizing multidimensional data by mapping each observation onto a function.\n\nSource - https://dzone.com/articles/andrews-curves
# Linear regression based on sepal
# Linear regression based on petal
We have seen the visualization part\n\nNow lets see the how to apply machine learning to the dataset
A big chunk of data is of patients who are Ex-smokers whereas very few patients who currently smoke.
Records with patient who have never smoked have equal distribution of male and female patients whereas a large majority of ex-smokers are males.
Male and female records are almost distributed throughout the age range.
> **NOTE: Double click on the side legend to isolate a category**
Patients who currently smoke show only a few occurrence along the age range.
"* The value of FVC for current smokers is mostly concentrated around 3000.\n* For patients who never smoked, the value remains below 4400.\n* For Ex-smokers we see a few higher values around 6000 and a large number of records are between 2000 and 3000"
**HAVING A FIRST LOOK AT THE IMAGE**
**Let's view all of the images for the first patient**
"> Looking at the previous two visuals, we notice that the **scans** for both the patients are **different**. Let's have a closer look at the scans of both patients."
> Notice that the first scan has a circular border and the second one is regular. 
**Visualizing and querying the dataset** with W&B 🏋️‍♀️\n\n[Documentation](https://docs.wandb.ai/datasets-and-predictions)
This is a snapshot of the table I just created and added to an artifact.\n\n![](https://i.imgur.com/oF7CloS.png)
**Logging an image** of the wordcloud of image titles🏋️‍♀️
"Unigrams, bigrams and trigrams 🔢 "
#### Visualising Numeric Variables\n\nPairplot of all the numeric variables
"#### Insights:\n- `carwidth` , `carlength`, `curbweight` ,`enginesize` ,`horsepower`seems to have a poitive correlation with price.\n- `carheight` doesn't show any significant trend with price.\n- `citympg` , `highwaympg` - seem to have a significant negative correlation with price."
#### Visualising few more Categorical Variables\n\nBoxplot of all the categorical variables
#### Insights\n- The cars with `fueltype` as `diesel` are comparatively expensive than the cars with `fueltype` as `gas`.\n- All the types of carbody is relatively cheaper as compared to `convertible` carbody.\n- The cars with `rear enginelocation` are way expensive than cars with `front enginelocation`.\n- The price of car is directly proportional to `no. of cylinders` in most cases.\n- Enginetype `ohcv` comes into higher price range cars.\n- `DoorNumber` isn't affecting the price much.\n- HigerEnd cars seems to have `rwd` drivewheel
#### Insights:\n- The number of cylinders used in most cars is `four`.\n- Number of `Gas` fueled cars are way more than `diesel` fueled cars.\n- `Sedan` is the most prefered car type.
#### Relationship between `fuelsystem` vs `price` with hue `fueltype`
#### Relationship between `carbody` vs `price` with hue `enginelocation`
#### Relationship between `cylindernumber` vs `price` with hue `fueltype`
#### Derived Metrices\n- Average Price
"#### Insights:\n- `Jaguar`,`Buick` and `porsche` seems to have the highest average price."
#### Insights:\n- `hardtop` and `convertible` seems to have the highest average price.
We will also create a function that plots the original distribution before and after an imputation(s) is performed:
We will start trying out techniques with `SimpleImputer` from Sklearn:
### Plot the Training Curves
### Create a Confusion Matrix
## Confusion Matrix. Look at the style of the artists which the model thinks are almost similar. 
# Evaluate performance by predicting on random images from dataset
# This portion is just for fun :) Replace the variable `url` with an image of one of the 11 artists above and run this cell.
"### Thank you for reading the  notebook! Please share your thoughts and feedback in comments section below.\n### Please ""upvote"" the kernel if you like it :)\n### Please take a look at http://supratimh.github.io for other projects I am working on. I will eagerly look forward to your feedback and suggestions."
## 2.3. Comprehensive Plots \n\n[Back to Table of Contents](#0.1)
![image.png](attachment:image.png)
## 2.4. Tips for making the Good Visualization from Kaggle Masters and Experts\n\n[Back to Table of Contents](#0.1)
"**The kernel [Tips for making the Informative Visualization](https://www.kaggle.com/subinium/tips-for-making-the-informative-visualization)**\n\n**Thanks to @subinium**\n\n#### Tips for making the Informative Visualization\n* Basic concepts and elements of graphics (their names in different libraries).\n* Typical approaches to displaying data of different types\n* Tips on how to choose colors, color palette, color scheme\n* Tips on how to choose arrangement (layout), margin, ratio, grids, axis, and borders in the charts\n* Examples of how not to do\n* Links to other interesting and useful resources\n\nConcisely about the main thing.\n\n\n*Quote from the notebook:*\n#### ""Information visualization is a process to find 4 main things.\n* **Composition**: What does the data consist of?\n* **Distribution**: What distribution does the data have?\n* **Comparison**: What distribution are features in?\n* **Relationship**: What about the relationship between two or more features?"""
## 3. Matplotlib \n\n[Back to Table of Contents](#0.1)
**The kernel 2 [Rare Visualization Tools](https://www.kaggle.com/kanncaa1/rare-visualization-tools)**\n\n**Thanks to @kanncaa1**\n\n#### - Spider(radar) plot\n\nA spider(radar) plot is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables.
## 4. Seaborn \n\n[Back to Table of Contents](#0.1)
"**The kernel 2 [Seaborn Tutorial for Beginners](https://www.kaggle.com/kanncaa1/seaborn-tutorial-for-beginners)**\n\n**Thanks to @kanncaa1**\n\nIn addition to kernel 1, this kernel contains outhers examples (on ""Fatal Police Shootings in the US"") and:\n* Heatmap\n* Swarm Plot and etc."
**The kernel 1 [Seaborn tutorial for beginners](https://www.kaggle.com/prashant111/seaborn-tutorial-for-beginners)**\n\n**Thanks to @prashant111**\n\n**References of this kernel:**\n\nSeaborn Official Tutorial: http://seaborn.pydata.org/tutorial.html\n\nSeaborn documentation and API reference:\n\n* http://seaborn.pydata.org/\n* http://seaborn.pydata.org/api.html\n\nUseful Seaborn tutorials\n\n* https://www.datacamp.com/community/tutorials/seaborn-python-tutorial\n* https://elitedatascience.com/python-seaborn-tutorial\n* https://www.tutorialspoint.com/seaborn/index.htm#\n\n\n#### Seaborn tutorial\n\n* Seaborn Kernel Density Estimation (KDE) plot\n* Histograms\n* Visualize distribution of values in Preferred Foot variable with Seaborn countplot() function\n* Seaborn catplot() function\n* Seaborn stripplot() function\n* Seaborn boxplot() function\n* Seaborn violinplot() function\n* Seaborn pointplot() function\n* Seaborn barplot() function\n* Visualizing statistical relationship with Seaborn relplot() function\n* Seaborn scatterplot() function\n* Seaborn lineplot() function\n* Seaborn regplot() function\n* Seaborn lmplot() function\n* Multi-plot grids\n* Seaborn Facetgrid() function\n* Seaborn Pairgrid() function\n* Seaborn Jointgrid() function\n* Controlling the size and shape of the plot\n* Seaborn figure styles
## 5. Plotly \n\n[Back to Table of Contents](#0.1)
"**The kernel 2 [Simple EDA-Model](https://www.kaggle.com/siavrez/simple-eda-model)**\n\n**Thanks to @siavrez**\n\nEDA for the prize competition [University of Liverpool - Ion Switching](https://www.kaggle.com/c/liverpool-ion-switching)\n\nVery good visual interactive plots based on plotly: scatter, box, density_heatmap and others."
"**The kernel 1 [Plotly Tutorial for Beginners](https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners)**\n\n**Thanks to @kanncaa1**\n\n#### Plotly tutorial\n\n* Line Charts, Scatter Charts, Bar Charts, Pie Charts, Bubble Charts\n* Histogram\n* Word Cloud\n* Box Plot\n* Scatter Plot Matrix\n* Data Visualization\n* Inset Plots\n* 3D Scatter Plot with Colorscaling\n* Multiple Subplots and etc."
"**The kernel [COVID-19 EDA and Forecasting](https://www.kaggle.com/n1sarg/covid-19-eda-and-forecasting)**\n\n#### Research for Competition ""COVID19 Global Forecasting (Week 5)""\nMany interesting plots with PlotLy"
## 6. Interactive plots with Bokeh \n\n[Back to Table of Contents](#0.1)
"**The kernel 1 [EDA using Bokeh Visualisation](https://www.kaggle.com/pavansanagapati/eda-using-bokeh-visualisation)**\n\n**Thanks to @pavansanagapati**\n\nVery good examples!\n\n**Bokeh is an interactive visualization library** that targets modern web browsers for presentation. Its goal is to provide elegant, concise construction of versatile graphics, and to extend this capability with high-performance interactivity over very large or streaming datasets. Bokeh can help anyone who would like to quickly and easily create interactive plots, dashboards, and data applications."
[Gallery of Bokeh - examples](https://docs.bokeh.org/en/latest/docs/gallery.html)
**The kernel 2 https://www.kaggle.com/kanncaa1/visualization-bokeh-tutorial-part-1**\n\n**The kernel 3 https://www.kaggle.com/kanncaa1/interactive-bokeh-tutorial-part-2**\n\n**Thanks to @kanncaa1**
## 7. Network Analysis \n\n[Back to Table of Contents](#0.1)
**Embarked and Sex**
"**Embarked, Pclass and Sex :**\n\n** Practically all women of Pclass 2 that embarked in C and Q survived, also nearly all women of Pclass 1 survived. **\n\n** All men of Pclass 1 and 2 embarked in Q died, survival rate for men in Pclass 2 and 3 is always below 0.2 **\n\n** For the remaining men in Pclass 1 that embarked in S and Q, survival rate is approx. 0.4 **"
"because Since our target feature is numeric and continuous, and the dependent features are also continuous, then it will be enough to build a correlation matrix."
"The hypothesis about the multicollinearity of the features was confirmed, let's remove the Whisker features."
Let's Detect The Outliers:-
Categorical Variables:-
Find out the relationship between categorical variable and dependent feature SalesPrice:-
Let's Find The Missing values:-
"We can visualize the log return for our two assets. See how the signal now looks more like white noise, with less drift than the time series for prices."
## Correlation between assets\n\nWe hypothesized before that crypto asset returns may exhibit some correlation. Let's check this in more detail now.\n\nWe can check how the correlation between Bitcoin and Ethereum change over time for the 2021 period we selected. 
"Note the high but variable correlation between the assets. Here we can see that there is some changing dynamics over time, and this would be critical for this time series challenge, that is, how to perform forecasts in a highly non-stationary environment.\n\nA stationary behaviour of a system or a process is characterized by non-changing statistical properties over time such as the mean, variance and autocorrelation. On the other hand, a non-stationary behaviour is characterized by a continuous change of statistical properties over time. Stationarity is important because many useful analytical tools and statistical tests and models rely on it.\n\nWe can also check the correlation between all assets visualizing the correlation matrix. Note how some assets have much higher pairwise correlation than others."
"In this section you can configure the following:\n* Features used for training\n* Basic training setup: BATCH_SIZE and EPOCHS,\n* Configuration for the loss function\n* Optimizers, Learning-Rate-Schedulers incl. Learning Rate start- & endpoint\n* Custom Logging Callback\n* Checkpoint-Saving Callback\n\nThe Learning-Rate scheduler below is inspired by Chris great [Melanoma-detection notebook](https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords).  \nFeel free to experiment with the scheduler and it's max/min and decay values.\n\n**Ever wondered why lr_max is scaled by BATCH_SIZE and therefore bigger for larger batches?** The reason for this is the following: the larger the BATCH_SIZE, the more averaged & smoothened a step of gradient decent is and the bigger our confidence in the *direction* of the step is. As there is less ""randomness"" in a huge averaged batch (compared with for example Stochastic Gradient Decent (=SGD) with batch size = 1) and our confidence in the direction is higher, the learning rate can be bigger to advance fast to the optimum."
### Loss Function
### Let's check the accuracy for various values of n for K-Nearest nerighbours
Above is the graph showing the accuracy for the KNN models using different values of n. 
"# 2.2.3.2 Gender vs. Country\n\n- While it's important to have diversity among the countries it's also important to have similar diversity between genders too, let's see if we can see different results in that regard..."
"- Hmm, looks like general trend with global gender distribution is about the same with countries too.\n- You can also notice that people who prefer not to share their gender also likely to don't share their location than rest too, I respect their privacy :)"
"# 2.2.4.1 Education vs Age\n\n- Let's take a closer look to these two degrees with age involved, if we can see something new."
"- Well it doesn't look like anything to me :) It looks like what it should be looking like, maybe at the age group of 18-21 having some participants master's degree plans, we would assume they are older than 20? Actually it depends on education starting age and education system for specific country so we can't be sure...\n- Anyways let's take look at bigger picture with heatmap:"
"- We can observe that most popular education level starts with bachelor's degree at the age of 18 till 24,\n- Then master's degree taking the lead for big chunk of age groups untill age bin of 70+ where doctoral degree taking first place at that point.\n- **Special spot for the people who are attending/planning(next 2 years) Doctoral/Professional doctorate degree at the age of 18-21, taking 0.0053% of the age group, you are the real champions :))**"
# 2.2.4.2 Education vs Country
"- Highest master's degree ratio goes to France, which makes sense since 25-29 age group has the biggest percentage in France if you recall from Age chapter and Master's Degree was the most popular one among age 25-29 group globally...\n- Highest doctoral rate goes for Denmark, again their age group for country and education looks correlated. Also number of participants from that country should be taken into account.\n- In general these Age-Country-Education relation seems solid but still hypothetical, because there are some countries where this trend doesn't fit, like Uganda; this might due to low number of participants.\n- Maybe looking at countries where number of participants greater/less than a threshold would give us a cleaner view:"
"# 2.2.4.3 What About Courses?\n\n- As we all know the demand for a data scientist is on the rise, at least that's what media says :/\n- Students with analytical talents are getting more and more attracted to career in a data science field.\n- But usually formal education is not enough or person changing his field without getting another formal education about data science.\n- So you will need some specialized/techical education in order to get more data centric roles.\n- This brings us to the courses, selecting a right course might boost your career a lot!\n- Let's see what course platforms are popular among  kaggle  users."
- Looks interesting... We do have some leading courses like Coursera or  kaggle  but still we don't have a clear winner.\n- Let's see the effects of courses on other features like formal education.
# 2.2.4.3.1 Formal Education vs Courses
"- Well, we can see the importance of online learning/courses from the table above.\n- The higher the person gets formal education there is also higher chance for he/she took one or more courses for data science in past.\n- Of course there is time effect on the hypothesis above but still it shows you something for future: Learning never stops in data field!"
# 2.2.5.1 Role vs. Age
"- We see that most of the younger people are still studens, who would have expected that right!? Haha...\n- Anyways, we can also see that the more participant gets older the more he/she gets a specialized role.\n- You can see what I mean above by looking at ""Other"" role on X axis, percentages almost increasing consistently by age.\n- On the other hand we can also observe that data science is still growing field where fraction of data scientists are going down with the age..."
# 2.2.5.2 Role vs. Country
"- Here we can see roles are mainly occupied by ""Student"" for most countries. It was expected since most of the participants are students globally.\n- Some countries almost having 50% of their participants as students like China and Bangladesh, followed by India and Sri Lanka\n- These numbers are cool, but again we don't have balanced distribution for some countries so what I wonder is how are the roles are spread when we count total number of users from these countries.\n- This might give us insights about how are the data related roles developing across the world."
### Seaborn PairGrid
"\nWow this figure has a lot going on! But thats okay, we only need to look at it peice by piece do derive some meaningful understanding.\n\nI like the PairGrid better because it provides more customizable options compared to the Pairplot. As you can tell I can have two different kinds of graphs on the upper and lower halves of the grid. In the PairGrid above we see 3 types of graphs:\n\n> 1. Regression scatter plot in the upper right\n> 2. Density Plot down the diagonal\n> 3. Bivariate Denisty plot with contour levels \n\nIn the bottow row, we see 6 empty graphs. The bivariate desity plots cannot plot due to the different scale of values. Win Percentage values range [0,1] while the independent vairables (HP, Attack,...) range from [0,200+].\n\nThe diagonal shows the density plots. For most of the independent variables look realtively normal distribution with a right skew. This means a majority of the pokemon stats are on average higher than the median value for the set. However, the density for the win percentage is different. Comparing the density plot with the frequency plot in the pairplot for 'Win Percentage' we see a more uniformly distribution of the rate at which pokemon win with a slight decrease of the frequency at higher levels. \n\nThe upper right section is the most easy to understand and probably the most useful for our analysis. These are regression plots with a line of best fit. Esentially the slope of the line y=mx+b, where m is the slope is the correlation value between the two variables. Thus we would expect to see a simialr pattern if we were to build a heat map or a correlation table. What I am most interested in is the relationship between each independent variable and the dependent variable (Win Percentage). The greater the slope, m, the more correlated the values are in determining the liklihood of winning. Just by 'eye balling' it (not the most mathmematically correct method), but it appears that **speed** and **attack** have the largest relationship to winning. Lets take a closer look into these two plots given below. \n\nWe begin by replicating the regplot to that we can see it better. To dive deeper, lets take a look at if there any trends when we further break down the data into 'Type 1'.\n\n## Correlation Table"
## XGBoost
We shall define a simple function which will plot the augmented images for us. 
## **Original Images**\nFirst let us plot the original images so that the augmentations become more clear to us. After this one by one we will implement different augmentations.
"We may be able to solve these two missing values by looking at other independent variables of the two raws. Both passengers paid a fare of $80, are of Pclass 1 and female Sex. Let's see how the **Fare** is distributed among all **Pclass** and **Embarked** feature values"
"Here, in both training set and test set, the average fare closest to $80 are in the C Embarked values where pclass is 1. So, let's fill in the missing values as ""C"" "
"Image below shows trends for each of the drug categories, represented by the 365-d rolling means for each of those categories."
"Trends and seasonality can be explored in time series decomposition view, based on 30d rolling means."
"Function seasonal_decompose can be used for analysis of the portions of each component of time series. This is especially useful when determining uptake of residuals in data, based on the decomposed data. The volume of this uptake implies the predictability of the time series - higher the residuals, lower the predictability. To some extent, the proportion of the residuals when comparing with trend and seasonality can be also illustrated by the rolling means and standard deviation plots above."
"Autocorrelation analysis illustrates the potential for time series data prediction. Autocorrelation plots graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps. Pearson coefficient is used to measure autocorrelation. Thus, the following analysis is relevant only for data with normal Gaussian distribution.\n\nA plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function (ACF). This plot is sometimes called a correlogram or an autocorrelation plot. Plot shows the lag value along the x-axis and the correlation on the y-axis between -1 and 1. Confidence intervals are drawn as a cone. By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this code are very likely a correlation."
"In general, the ""partial"" correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables. For example, if we are regressing a variable Y on other variables X1, X2, and X3, the partial correlation between Y and X3 is the amount of correlation between Y and X3 that is not explained by their common correlations with X1 and X2."
"Minor autocorrelation is observed at ACF (Auto-Correlation Function) and PACF (Partial Auto-Correlation Function) plots for all series, with exception of N05C sales. N02BE, R03 and R06 series were found to exhibit annual seasonality. "
"Chart with daily sales for different categories of interest is shown below. N02BE and N05B charts, though showing the similar trends, are suppresed because of the larger scale which makes the other illustrations less readable."
"Chart with weekly sales for different categories of interest was shown below. N02BE and N05B charts, though showing the similar trends, are suppresed because of the larger scale which makes the other illustrations less readable."
## 4.3. Time series forecasting 
#### 4.3.1.1. Naïve forecasting
#### 4.3.1.2. Average method forecasting
### 2.13 Distribution of Amount AMT_GOODS_PRICE 
### 2.14 Distribution of Amount REGION_POPULATION_RELATIVE 
### 2.15 Distribution of Amount DAYS_BIRTH 
### 2.16 Distribution of Amount DAYS_EMPLOYED 
### 2.17 Distribution of Number of Days for Registration
### 2.18 How many Family Members does the applicants has 
> Most of the applicants who applied for loan had 2 family members in total\n\n###  2.19 How many Children does the applicants have 
"> A large majority of applicants did not had children when they applied for loan\n\n## 3. Exploration of Bureau Data\n\nAll client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample). For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n\n### 3.1 Snapshot of Bureau Data"
### 7.2 Contract Status Distribution in Previously Filed Applications
"> - A large number of people (about 62%) had their previous applications approved, while about 19% of them had cancelled and other 17% were resued. \n\n### 7.3 Suite Type Distribution of Previous Applications"
> - A majority of applicants had previous applications having Unaccompanied Suite Type (about 60%) followed by Family related suite type (about 25%)\n\n### 7.4 Client Type of Previous Applications
"> - About 74% of the previous applications were Repeater Clients, while only 18% are new. About 8% are refreshed. \n\n### 7.5 Channel Type - Previous Applications "
## 8. Exploration of Installation Payments \n### 8.1 Snapshot of Installation Payments 
"### Trend & Seasonality\n\nConsider the example of **shampoo sales dataset**. \n\nThis Dataset describes the monthly number of sales of shampoo over a 3 year period. The units are a sales count and there are 36 observations. The original dataset is credited to Makridakis, Wheelwright, and Hyndman (1998).\n\n**Data source**:https://github.com/jbrownlee/Datasets\n\nBelow is a sample of the first 5 rows of data, including the header row.\n\n| Month | Sales |\n| ---- | -------- |\n| 1-01 | 266.0 | \n| 1-02 | 145.9 | \n| 1-03 | 183.1 | \n| 1-04 | 119.3 | \n| 1-05 | 180.3 | \n\n\n"
### The above plot shows an increasing trend.\n
"**Minimum Daily Temperatures Dataset**\n\nhttps://machinelearningmastery.com/time-series-seasonality-with-python/\n\nThis dataset describes the minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia.\n\nData source: Data Market https://datamarket.com/data/set/22r0/sales-of-shampoo-over-a-three-year-period\n\nThe units are in degrees Celsius and there are 3,650 observations. The source of the data is credited as the Australian Bureau of Meteorology.\n\nA sample of the first 5 rows of data, including the header row is shown below:\n\n| Date | Temperature |\n| ------- | -------- | \n| 1981-01-01 | 20.7 | \n| 1981-01-02 | 17.9 | \n| 1981-01-03 | 18.8 | \n| 1981-01-04 | 14.6 | \n| 1981-01-05 | 15.8 | \n"
### The above plot shows a strong seasonality component.
We can draw a boxplot to check the variation across months in a year (1990).\nIt appears that we have a seasonal component each year showing swing from summer to winter.
"This plot shows the signiﬁcant change in distribution of minimum temperatures across the months of the year from the Southern Hemisphere summer in January to the Southern Hemisphere winter in the middle of the year, and back to summer again."
We group the Minimum Daily Temperatures dataset by years. A box and whisker plot is then created for each year and lined up side-by-side for direct comparison.
We don't observe much year-by-year variation 
The above plot shows a strong seasonality and trend component.\n\nWe can draw a boxplot to check the variation across months in a year (2011). 
It appears that we have a seasonal component each year showing swing from May to Aug.
"### 1. Anti-Jupyters are more diverse in their use of programming languages\nWhile Python is the clear favorite amonst all kagglers, and R is a close second. We find the the non-jupyter cluster has consistantly more percentage of their group that chooses other programming languages as their language of choice. It stands out that those that don't use jupyter may fall into these two categories:\n- Stictly R Users: who prefer to use RStudio and rather not deal with jupyter's R integration.\n- Database engineers: Who code much in raw SQL\n- Software Developers: who code in languages like C++, C# and Javascript\n- Excel users who code in VBA\n- Specialized programmers who prefer non-open source langagues like SAS and STATA"
### 2. Anti-Jupyters are more likely to code less\nAre you in the non jupyter group? It's more likely you spend less time coding. Use jupyter? You probably spend more time coding. There however is one exception. Anti-Jupyters actually have a higher percentage of their group that codes 100% of the time.
"### 3. Anti-Jupyters have less expereince with Machine Learning, but they plan to learn it!\nThe percentage of non jupyter users are much higher for those with <1 year of experience with machine learning. They also have a much higher percentage of that plan to learn machine learning methods. These would be the perfect type of user that the kaggle team should make an effort to encourage using kaggle kernels!"
"### 4. Those that use Jupyter are much more likely to consider themselves ""Data Scientists""\nWhile around 55% of respondents who use jupyter consider themselves a data scientist. On the other hand 41.3% of the non jupyter user group consider themselves data scientists.\n\nThose who consider themselves ""definately not"" data scientists consist of 13.7% of anti-jupyters while that percentage is only 6.8% of the rest of respondents."
"### 5. Anti-Jupyters spend less time exploring for model insights\nJupyter is well known for it's use in explorative and iterative scientific programming. As such, those who spend less time exloring for model insights also tend to be in the anti-Jupyter group."
"# What can we take away from this analysis?\n\nIn this analysis, we've looked at the responses from the 2018 Kaggle survey. Specifically, we looked at the types of development environments (or IDES) that Kagglers have said they've used over the past 5 years. We found some interesting relationships between the software and tools that kagglers use and how it relates to their job title, salary, and country of origin. We not only looked at what IDEs were popular, but how respondents overlapped in their use of different IDEs. By looking into the freeform responses, we've identified some popular IDEs that the kaggle team might want to include in next year's survey.\n\nFinally, we used unsupervised machine learning techniques to cluster the kaggle respondents into four distinct groups. These groups were each unique, but the cluster that consisted of respondents who had not used Jupyter/IPython stood out in their responses to the other survey questions as being the least like the other groups. The ""Ani-Jupyter"" group tends to be more diverse in their use of programming languages, less likely to consider themselves a Data Scientist, and they spend less time in data projects exploring for model insights.  This is not to say that the anti-Jupyter group does not want to learn new things.  Suprisingly, 23.6% of them say they ""have never used machine learning methods **but plan to**"".\n\nThe anti-Jupyter (or non-jupyter) are an important part of the kaggle community. Whether they are experienced software developers who aren't interested in using jupyter - or if they are new to programming and data science- they are part of the community that Kaggle should focus on reaching. Kaggle Kernels are a great resource and remove many of the barriers that some of the community may have with installing Jupyter on their local computers. Hopefully, more of them will join the community, become involved in using Kaggle kernels, and help us grow and become more diverse.\n\nThanks for reading!"
## Compensación por título de trabajo
El Practicante de Investigación de Operaciones tiene el salario medio más alto seguido por el Predictive Modeler y el Data Scientist. El informático y los programadores tienen la compensación más baja.
"Es evidente que la mayoría de los encuestados están trabajando con Aprendizaje supervisado, y la Regresión logística es la favorita entre ellos. No existe un algoritmo que sea el mejor para todos los dominios de clasificación. Una forma de seleccionar un algoritmo para un dominio en particular es mediante la validación cruzada en los datos de entrenamiento."
"Es evidente que el próximo año va a ver un aumento en el número de practicantes de **Deep Learning**. El aprendizaje profundo y las redes neuronales o, en definitiva, la IA es un tema candente para el próximo año. También en términos de herramientas, Python se prefiere más a R. Las herramientas de Big Data como Spark y Hadoop también tienen una buena participación en los próximos años."
## Las mejores plataformas para aprender
"Mi favorito es Kaggle, es la fuente más buscada para aprender Data Science, ya que tiene notebooks de Data Scientists realmente experimentados. La siguiente opción son los cursos en línea, es decir, los MOOC. Plataformas como coursera, udacity proporcionan videos interactivos y ejercicios para el aprendizaje. De manera similar, los canales de Youtube como Siraj Raval y otros ofrecen un medio gratuito para estudiar. Todos estos medios están por encima de los libros de texto. La razón quizás sea que los libros de texto a menudo tienen un contenido limitado, o que las personas les gusta más mirar videos y aprender."
## Hardware utilizado
"Dado que la mayoría de los encuestados se encuentran en la categoría de edad inferior a 25, que es donde la mayoría de los estudiantes se clasifican, una computadora portátil básica es la máquina más utilizada para el trabajo."
## De dónde obtengo conjuntos de datos??\n
"Con cientos de conjuntos de datos disponibles, Kaggle es la fuente más buscada para datasets."
## Código compartido
"Github es la plataforma más utilizada para compartir código y proyectos. Las ventajas de usar github son:\n\n1) Control de versiones de tus proyectos.\n\n2) Explore los proyectos de otros en GitHub, obtenga un código inspirado más o contribuya a su proyecto.\n\n3) Colabore con otros, permitiendo que otras personas contribuyan a sus proyectos o usted contribuya a otros proyectos, y muchos más.\n\n## Desafíos en la ciencia de datos"
"El principal desafío en Data Science es **obtener la información adecuada**. El gráfico muestra claramente que los datos sucios son el desafío mayor. Ahora, ¿qué son los datos sucios? Los datos sucios son un registro de base de datos que contiene errores. Los datos sucios pueden ser causados por una serie de factores que incluyen registros duplicados, datos incompletos o desactualizados y el análisis incorrecto de campos de registros de sistemas dispares. Afortunadamente, los conjuntos de datos de Kaggle están bastante limpios y estandarizados.\n\nAlgunos otros desafíos importantes son la **falta de ciencia de datos y talento para el aprendizaje automático, dificultad para obtener datos y falta de herramientas**. Es por eso que la ciencia de datos es el trabajo más sexy en el siglo XXI. Con la creciente cantidad de datos, esta demanda crecerá sustancialmente."
## Satisfacción laboral
"Los científicos de datos y los ingenieros de Aprendizaje automático son las personas más satisfechas (quienes no estarán contentos con tanto dinero), mientras que los programadores tienen la satisfacción laboral más baja. Pero lo que hay que notar aquí es que incluso si los Computer Scientist  tienen un salario más bajo que los programadores, también tienen un buen nivel de satisfacción laboral que los programadores. Así, el salario no es el único criterio o la satisfacción laboral.\n\n## Satisfacción laboral por país"
### Lenguaje recomendado para principiantes
Claramente Python es el lenguaje recomendado para principiantes. La razón de esto tal vez se deba a su sintaxis similar a la del inglés y su funcionalidad de propósito general.
#### Visualizing Part 1 Features w.r.t Outcome :
"- For **Pregnancies**, cases of diabetes is present throughout the data. There is no specific range of values for which higher diabetes cases are found. **Pregnancies** range of values from **7 - 9** does highlight more cases of diabetes than non-diabetes for the 1st indicating a pattern but this claim gets rejected if we observe the values ahead.\n- **BloodPressure** range of values **60 - 90 mm/hg** highlights a high number of diabetes patients.\n- **SkinThickness** displays very low number of diabetes cases for all the values. Out of those values, **24 - 42** has some prominent peaks of diabetes cases.\n- When it comes to **Age**, young females are more prone to diabetes than older women. As the age increases, the number of diabetes cases have decreased. **Age** group **21 - 50** displays a higher probability of being diagnosed with diabetes."
### Numerical features vs Numerical features w.r.t Target variable (Outcome) :
"- **Pregnancies** with values between **7 - 10** have high chances of diabetes. This range does not display a complete dominance but it has some presence.\n- **Glucose** values higher than **125** indicate very high chances of diabetes.\n- **BloodPressure** values between **60 - 100** highlight many cases of diabetes coupled with any feature.\n- When both, **BMI** and **SkinThickness**, feature values are between **20 - 50**, probability of diabetes is very high.\n- **Insulin** values between **0 - 300** increases the risk of diabetes. When above **400**, the female has a sure shot chance of being diabetic. \n- For **Age** group of **20 - 50** as well as **DiabetesPedigreeFunction** values ranging from **0 - 1.5** results in a diabetic condition."
- It is a huge matrix with too many features. We will check the correlation only with respect to **Outcome**. 
"- **SkinThickness** and **BloodPressure** does not display any kind of correlation.\n- **Glucose** displays the highest positive correlation with **Outcome**. It is followed by **BMI**, **Age**, **Pregnancies**, **DiabetesPedigreeFunction** & **Insulin**!"
#### ANOVA Test :
"- According to the **ANOVA test**, **higher the value of the ANOVA score, more is the importance of the feature**.\n- From the above results, we need to drop **SkinThickness** & **BloodPressure**. We will take the remaining features into consideration for modeling."
"###  Daily return of stock prices analysis \n\n\n\n### Daily return hypothesis test\n\n- **In stock market, you will often hear that daily return of any stock price is 0% which means you will get zero return on your investment in one day.**\n- **So let's prove the hypothesis by analysing top 10 most traded stocks and assesing their daily return distribution in this section**\n\n- **H0: Daily return is zero**\n- **Ha: Daily return is not zero**\n\n- **We will prove this hypothesis as a one sample t-test as we know population mean but are not aware of std deviation. if p-value is greater than 0.05 than we can not reject the null hypothesis and if it is less than 0.05 than we have to reject the null hypothesis**"
"- **From above results, we can learn that pvalues of stocks 'MSFT'.'INTC' and 'CSCO' are less than 0.05 so we can reject the null hypothesis and accept alternative hypothesis that is 'Daily return is not zero' while for other stocks we cannot reject null hypothesis.**\n\n- **Stistically it proves that 7 out of 10 have daily return of zero percentage which is the most general case.**"
#  Technical analysis of stocks using candle stick charts and moving average \n\n
"### Technical analysis of stocks\n\n- **An open-high-low-close chart (also OHLC) is a type of chart typically used to illustrate movements in the price of a financial instrument over time. Each vertical line on the chart shows the price range (the highest and lowest prices) over one unit of time, e.g., one day or one hour. Tick marks project from each side of the line indicating the opening price (e.g., for a daily bar chart this would be the starting price for that day) on the left, and the closing price for that time period on the right. The bars may be shown in different hues depending on whether prices rose or fell in that period.**\n\n*source: Wikipedia*\n\n![image.png](attachment:422da312-c37d-4824-ab3c-4ef82ff1c966.png)"
###  Candlestick charts of stocks to visualize OHLC prices \n
###  Moving Averages charts of Facebook and Apple \n
"5.1  CLASS TO HELP PARSE THE DATA \n\n---\n\nClasses are very useful when we need to bind data to functionality. In this case, I have created a class (unwieldy as it may be currently in it's initial version) to help with that called **`TrainData`**.\n\nI will get into the details of how the methods work at a later time... and for today I will simply generate the outputs using each method to show their functionality."
PLOT IMAGES FROM THE CORRESPONDING IMAGE IDS\n\nSummary to be done later\n\n---\n\nMore detail to come later
#### KERNEL SVM + PCA
" \n# K-NEAREST NEIGHBOR\n\nK-NN is a supervised learning method that considers the K closest training examples to the point of interest for predicting its class. The point is assigned to the class that is closest. \nCould be applied different distance metrics such as: Euclidian, Weighted, Gaussian, etc.\nSteps are pretty easy:\n\n*  Receive an unclassified data\n\n*  Measure the distance with choosen metrics from the new data to all others data that are already classified.\n\n*  Gets the K smaller distances\n\n*  Check the list of classes that had the shortest distance and count the amount of each class that appears\n\n*  Takes as correct class the class that appeared the most times\n\n*  Classifies the new data with the class that you took in previous step \n"
"#### COMMENT\n\nFrom this graph is possible to understand how the best value of K is equal to **2**, because the Test Accuracy reaches the best accuracy score and then start decreasing. \n\nTraining accuracy still maintain 100% accuracy starting decreasing for last numbers of K "
"# DECISION TREE\nIn a decision tree each intermediate node of the tree contains splitting attributes used to build different paths, while leaves contains class labels.\n\nThere are differt algorithms to build a decision tree, all are made with a greedy approach, optimal locally.The most famous is **Hunt's algoritm**. \n\n![](https://cdn-images-1.medium.com/max/880/0*QctkHiOX2G2pvfD_.jpg)\n\nStrarting from an empty tree, we need to find iteratively best attribute on which split the data locally at each step. If a subset contains records that belongs to the same class then the leaf containing such class label is created, otherwise if a subset is empty is assigned to default to mayor class.\n\nCritical points of decision trees are test condition, the selection of the best attribute and the splitting condition. \nFor the selection of the best attribute is generally choosen the attribute that generate homogeneus nodes. \nThere are different metrics in order to find the best splitting homogenity, the most common are:\n* **GINI IMPURITY INDEX**: Given **$n$** classes and $p_i$ the fraction of items of class $i$ in a subset p, for $i$∈{1,2,...,n}. Then the GINI index is defined as: $$ GINI = 1 − \sum_{i=1}^n p_i^2 $$\n\n* **INFORMATION GAIN RATIO**: The information gain is based on the decrease of entropy after a data-set is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches).Entropy is defined as $H(i) = -\sum_{i=1}^n p_i\log_2 p_i $. So then Information gain is defined as: \n\n$$ IG = H(p) - H(p,i)  = H(p) - \sum_{i=1}^n \frac{n_i}{n} H(i) $$\n\nwhere p is the parent node.\nAdvantages of Decision Trees are velocity, easy to interpretate and good accuracy, but they could be affected by missing values."
"Accuracy on Training phase increase while the Accuracy on Test phase is decreasing, this means that the model **overfit** increasing the max depth of the tree."
"# MODEL EVALUATION\n\nIn order to find the most suitable algorithm to this dataset, different evaluation methods will be presented: **Accuracy, Confusion Matrix, ROC Curve.**\n\nGiven: \n* TP = #samples for which the prediction is Fruit1 and the true label is Fruit1\n* FP = #samples for which the prediction is Fruit2 but the true label is Fruit1\n* TN = #samples for which the prediction is Fruit2 and the true label is Fruit2\n* FN = #samples for which the prediction is Fruit1 but the true label is Fruit2\n\nWe can define: \n\n* **ACCURACY: $\frac{TP+TN}{TP+FP+TN+FN}$ that is the percentage of samples classified correctly. **\n\n* **CONFUSION MATRIX:  A simple table with previous values used to show performance of a classifier**\n\n* **ROC CURVE: Area Under the Receiver Operating Characteristic curve (AUC)**\nTo introduce this concept, we define the following two metrics:\n\n    * **True positive rate (TPR):**   TPR = recall = $\frac{TP}{FN+TP}$\n    \n    * **False positive rate (FPR):**   FPR = $\frac{FP}{TN+FP}$\n    \nIn order to plot the Receiver Operating Characteristic (ROC) curve we need to compute TPR and FPR and choose a number of thresholds for the classification (AUG). Area under the ROC curve, performed plotting TPR and FPR is used as evaluation matrics for the different classifiers."
"### COMMENT:\nSVM and K-NN performs better with this classification with an AUC = 0.99x, instead Decision Tree is worst with an AUC of 0.65 "
**Chances for Survival by Port Of Embarkation --**
"`Embarked` is a categorical feature and there are only **2** missing values in whole data set. Both of those passengers are female, upper class and they have the same ticket number. This means that they know each other and embarked from the same port together. The mode `Embarked` value for an upper class female passenger is **C (Cherbourg)**, but this doesn't necessarily mean that they embarked from that port."
### **Pclass**
"Here we see clearly, that `Pclass` is contributing to a persons chance of survival, especially if this person is in class 1. \n\nLooking at the BarPlot , we can easily infer that survival for Women from Pclass1 is about 95-96%, as only 3 out of 94 Women from Pclass1 died.\n\nIt is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\n\nLooks like Pclass is also an important feature. "
"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive."
#### Histogram
"\n\n Distributions\n    \n    \n    \n    \nAs it can be seen from the graphs above, the distributions of the variables (except 'mileage' variable) are not normal."
### Numerical Features vs Target Variable (HeartDisease) :
"- Because of too many unique data points in the above features, it is difficult to gain any type of insight. Thus, we will convert these numerical features,except age, into categorical features for understandable visualization and gaining insights purposes. \n- Thus, we scale the individual values of these features. This brings the varied data points to a constant value that represents a range of values.\n- Here, we divide the data points of the numerical features by 5 or 10 and assign its quotient value as the representative constant for that data point. The scaling constants of 5 & 10 are decided by looking into the data & intuition. "
#### Sex vs Numerical Features :
"- **Male** population displays heart diseases at near about all the values of the numerical features. Above the age of 50, positive old peak values and maximum heart rate below 140, heart diseases in male population become dense.\n- **Female** population data points are very less as compared to **male** population data points. Hence, we cannot point to specific ranges or values that display cases of heart diseases. "
#### ChestPainType vs Numerical Features :
- **ASY** type of chest pain dominates other types of chest pain in all the numerical features by a lot.
#### FastingBS vs Numerical features :
"- Above the **age** 50, heart diseases are found throughout the data irrespective of the patient being diagnosed with Fasting Blood Sugar or not.\n- **Fasting Blood Sugar** with **Resting BP** over 100 has displayed more cases of heart diseases than patients with no fasting blood sugar.\n- **Cholesterol** with **Fasting Blood Sugar** does not seem to have an effect in understanding reason behind heart diseases.\n- Patients that have not been found positive with **Fasting Blood Sugar** but have maximum heart rate below 130 are more prone to heart diseases."
#### RestingECG vs Numerical Features :
"- Heart diseases with **RestingECG** values of **Normal**, **ST** and **LVH** are detected starting from 30,40 & 40 respectively. Patients above the age of 50 are more prone than anyother ages irrespective of **RestingECG** values.\n- Heart diseases are found consistently throughout any values of **RestingBP** and **RestingECG**.\n- **Cholesterol** values between 200 - 300 coupled with **ST** value of **RestingECG** display a patch of patients suffering from heart diseases. \n- For **maximum Heart Rate** values, heart diseases are detected in dense below 140 points and **Normal** RestingECG. **ST** & **LVH** throughout the maximum heart rate values display heart disease cases."
#### ExerciseAngina vs Numerical Features :
- A crsytal clear observation can be made about the relationship between **heart disease** case and **Exercise induced Angina**. A positive correlation between the 2 features can be concluded throughout all the numerical features. 
#### ST_Slope vs Numerical Features :
"- Another crystal clear positive observation can be made about the positive correlation between **ST_Slope** value and **Heart Disease** cases. \n- **Flat**, **Down** and **Up** in that order display high, middle and low probability of being diagnosed with heart diseases respectively."
### Numerical features vs Numerical features w.r.t Target variable(HeartDisease) :
"- For **age** 50+, **RestingBP** between 100 - 175, **Cholesterol** level of 200 - 300,**Max Heart Rate** below 160 and positive **oldpeak** values displays high cases of heart disease.\n- For **RestingBP** values 100 - 175, highlights too many heart disease patients for all the features.\n- **Cholesterol** values 200 - 300 dominates the heart disease cases.\n- Similarly, **Max Heart Rate** values below 140 has high probability of being diagnosed with heart diseases."
" \n### ROC Curve with Logistic Regression \n* logistic regression output is probabilities\n* If probability is higher than 0.5 data is labeled 1(abnormal) else 0(normal)\n* By default logistic regression threshold is 0.5\n* ROC is receiver operationg characteristic. In this curve x axis is false positive rate and y axis is true positive rate\n* If the curve in plot is closer to left-top corner, test is more accurate.\n* Roc curve score is auc that is computation area under the curve from prediction scores\n* We want auc to closer 1\n* fpr = False Positive Rate\n* tpr = True Positive Rate\n* If you want, I made ROC, Random forest and K fold CV in this tutorial. https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv/"
 \n### HYPERPARAMETER TUNING\nAs I mention at KNN there are hyperparameters that are need to be tuned\n* For example: \n    * k at KNN\n    * alpha at Ridge and Lasso\n    * Random forest parameters like max_depth\n    * linear regression parameters(coefficients)\n* Hyperparameter tuning: \n    * try all of combinations of different parameters\n    * fit all of them\n    * measure prediction performance\n    * see how well each performs\n    * finally choose best hyperparameters\n* This process is most difficult part of this tutorial. Because we will write a lot of for loops to iterate all combinations. Just I am kidding sorry for this :) (We actually did it at KNN part)\n* We only need is one line code that is GridSearchCV\n    * grid: K is from 1 to 50(exclude)\n    * GridSearchCV takes knn and grid and makes grid search. It means combination of all hyperparameters. Here it is k.\n
"The new question is that we know how many class data includes, but what if number of class is unknow in data. This is kind of like hyperparameter in KNN or regressions. \n* inertia: how spread out the clusters are distance from each sample\n* lower inertia means more clusters\n* What is the best number of clusters ?\n    *There are low inertia and not too many cluster trade off so we can choose elbow"
 \n### STANDARDIZATION\n* Standardizaton is important for both supervised and unsupervised learning\n* Do not forget standardization as pre-processing\n* As we already have visualized data so you got the idea. Now we can use all features for clustering.\n* We can use pipeline like supervised learning.
 \n### HIERARCHY\n* vertical lines are clusters\n* height on dendogram: distance between merging cluster\n* method= 'single' : closest points of clusters
 \n### T - Distributed Stochastic Neighbor Embedding (T - SNE)\n * learning rate: 50-200 in normal\n * fit_transform: it is both fit and transform. t-sne has only have fit_transform\n * Varieties have same position relative to one another
 \n### PRINCIPLE COMPONENT ANALYSIS (PCA)\n* Fundemental dimension reduction technique\n* first step is decorrelation:\n    * rotates data samples to be aligned with axes\n    * shifts data asmples so they have mean zero\n    * no information lost\n    * fit() : learn how to shift samples\n    * transform(): apply the learned transformation. It can also be applies test data\n* Resulting PCA features are not linearly correlated\n* Principle components: directions of variance
* Second step: intrinsic dimension: number of feature needed to approximate the data essential idea behind dimension reduction\n* PCA identifies intrinsic dimension when samples have any number of features\n* intrinsic dimension = number of PCA feature with significant variance\n* In order to choose intrinsic dimension try all of them and find best accuracy\n* Also check intuitive way of PCA with this example: https://www.kaggle.com/kanncaa1/tutorial-pca-intuition-and-image-completion
"# CONCLUSION\nThis is the end of DATA SCIENCE tutorial. The first part is here:\n  https://www.kaggle.com/kanncaa1/data-sciencetutorial-for-beginners/\n**If you have any question or suggest, I will be happy to hear it.**"
